{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 1,
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import datetime\n",
    "import os\n",
    "import openai\n",
<<<<<<< HEAD
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import json\n",
=======
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
=======
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-4\"\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loader = PyPDFLoader(\"docs/Biometric Vehicle Access System.pdf\")\n",
    "pdf_abstract = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Biometric Vehic le Access S ystem  \\n \\nAbstract:  \\nThe Biometric Vehicle Access System (BVAS) is an innovative technology  designed to revolutionize \\ntraditional vehicle security and access methods.  \\nThis system employs advanced biometric authentication, including fingerprint and facial recognition, to \\nensure secure and convenient entry and ignition processes.  \\nBVAS enhances vehicle security by replacing traditional key -based and electronic fob systems with a \\nseamless and personalized biometric verification process. The technology  integrates biometric sensors \\ninto door handles, steering wheels, and ignition systems, allowing for quick and reliable user  \\nauthentication. The BVAS not only provides an additional layer of security  against unauthorized access but \\nalso enhances user convenience by eliminating the need for physical keys  or key fobs. Users can \\neffortlessly unlock, start, and operate their vehicles through a simple and rapid  biometric scan. The \\nsystem is designed with robust anti -spoofing measures to prevent unauthorized access attempts.  \\nFurthermore, BVAS contributes to the growing trend of biometric integration in smart vehicles, aligning \\nwith the industry's  commitment to innovation, user experience, and safety. As vehicles continue to evolve \\ninto interconnected and autonomous  entities, BVAS sets a new standard for personalized and secure \\naccess, catering to the increasing demand for sophisticated  yet user -friendly solutions in the automotive \\nsector.\", metadata={'source': 'docs/Biometric Vehicle Access System.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "print(pdf_abstract)"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract1 = \"\"\"\n",
<<<<<<< HEAD
    "The Biometric Vehicle Access System (BVAS) is an innovative technology\\\n",
    "designed to revolutionize traditional vehicle security and access methods.\\\n",
    "This system employs advanced biometric authentication, including fingerprint \\\n",
    "and facial recognition, to ensure secure and convenient entry and ignition processes.\\\n",
    "BVAS enhances vehicle security by replacing traditional key-based and electronic \\\n",
    "fob systems with a seamless and personalized biometric verification process. The technology\\\n",
    " integrates biometric sensors into door handles, steering wheels, and ignition systems, allowing \\\n",
    "for quick and reliable user authentication. The BVAS not only provides an additional layer of security\\\n",
    "against unauthorized access but also enhances user convenience by eliminating the need for physical keys\\\n",
    " or key fobs. Users can effortlessly unlock, start, and operate their vehicles through a simple and rapid\\\n",
    " biometric scan. The system is designed with robust anti-spoofing measures to prevent unauthorized access attempts. \\\n",
    "Furthermore, BVAS contributes to the growing trend of biometric integration in smart vehicles, aligning with the industry's\\\n",
    " commitment to innovation, user experience, and safety. As vehicles continue to evolve into interconnected and autonomous\\\n",
    " entities, BVAS sets a new standard for personalized and secure access, catering to the increasing demand for sophisticated\\\n",
    " yet user-friendly solutions in the automotive sector.\n",
=======
    "The Biometric Vehicle Access System (BVAS) is an innovative technology designed to revolutionize traditional vehicle security and access methods. This system employs advanced biometric authentication, including fingerprint and facial recognition, to ensure secure and convenient entry and ignition processes. BVAS enhances vehicle security by replacing traditional key-based and electronic fob systems with a seamless and personalized biometric verification process. The technology integrates biometric sensors into door handles, steering wheels, and ignition systems, allowing for quick and reliable user authentication. The BVAS not only provides an additional layer of security against unauthorized access but also enhances user convenience by eliminating the need for physical keys or key fobs. Users can effortlessly unlock, start, and operate their vehicles through a simple and rapid biometric scan. The system is designed with robust anti-spoofing measures to prevent unauthorized access attempts. Furthermore, BVAS contributes to the growing trend of biometric integration in smart vehicles, aligning with the industry's commitment to innovation, user experience, and safety. As vehicles continue to evolve into interconnected and autonomous entities, BVAS sets a new standard for personalized and secure access, catering to the increasing demand for sophisticated yet user-friendly solutions in the automotive sector.\n",
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract2 = \"\"\"\n",
<<<<<<< HEAD
    "The Biometric Vehicle Access System (BVAS) employs advanced fingerprint\\\n",
    " and facial recognition for secure and convenient vehicle entry. It replaces \\\n",
    "traditional key and fob systems with seamless biometric verification integrated \\\n",
    "into handles, steering wheels, and ignition. BVAS enhances security, eliminates the \\\n",
    "need for physical keys, and allows users to unlock and start their vehicles with a rapid \\\n",
    "biometric scan. The system includes anti-spoofing measures and aligns with the growing trend \\\n",
    "of biometric integration in smart vehicles, setting a new standard for personalized and secure access in the automotive sector.\n",
=======
    "The Biometric Vehicle Access System (BVAS) employs advanced fingerprint and facial recognition for secure and convenient vehicle entry. It replaces traditional key and fob systems with seamless biometric verification integrated into handles, steering wheels, and ignition. BVAS enhances security, eliminates the need for physical keys, and allows users to unlock and start their vehicles with a rapid biometric scan. The system includes anti-spoofing measures and aligns with the growing trend of biometric integration in smart vehicles, setting a new standard for personalized and secure access in the automotive sector.\n",
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract3 = \"\"\" \n",
    "BVAS revolutionizes vehicle security with advanced biometric authentication, eliminating keys for convenient entry. Integrated into handles and steering wheels, it sets a new standard for secure, user-friendly access, aligning with the trend of biometric integration in smart vehicles. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract4 = \"\"\"\n",
    "The innovation involves the development of an integrated hydrogen fuel cell \\\n",
    "and battery hybrid system for vehicles. This system combines the advantages of hydrogen \\\n",
    "fuel cell technology with the storage capabilities of advanced batteries. The idea is to create \\\n",
    "a flexible propulsion system that utilizes both the high energy density of hydrogen fuel cells and \\\n",
    "the fast recharging and energy recovery through regenerative braking offered by battery technology.\n",
    "The vehicle would primarily use hydrogen for longer distances and quick refueling, while the battery \\\n",
    "would be employed for short distances and situation-dependent requirements. This dual-technology approach \\\n",
    "could overcome the range limitations of purely battery-electric vehicles while harnessing the benefits of \\\n",
    "emission-free hydrogen technology. Additionally, the system could be intelligently controlled to ensure optimal \\\n",
    "utilization of both energy sources based on vehicle usage, distance traveled, and the availability of hydrogen refueling stations.\n",
    "This innovation would not only enhance the practicality of hydrogen vehicles but also enable seamless integration into existing \\\n",
    "infrastructures, further strengthening the acceptance of hydrogen as a sustainable propulsion solution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f\"\"\"```{pdf_abstract}```\\\n",
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f\"\"\"```{abstract1}```\\\n",
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
    "The abstract above describes a concept for a novel invention.\\\n",
    "I would like to search a patent database to find out whether \\\n",
    "there are already patents for such a concept. Name 5 phrases that I can \\\n",
    "use for the search. Each phrase should contain between 5 to 10 words. \\\n",
    "Optimize the phrases to get back more results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "prompt2 = f\"\"\"```{pdf_abstract}```\\\n",
    "The abstract above describes a concept for a novel invention.\\\n",
    "I would like to search a patent database to find out whether \\\n",
    "there are already patents for such a concept. Please list me the codes of the 5 most relevant \\\n",
    "IPC classifications to a possible patent for this concept without explanations for the codes.\"\"\""
=======
    "prompt2 = f\"\"\"```{abstract1}```\\\n",
    "The abstract above describes a concept for a novel invention.\\\n",
    "I would like to search a patent database to find out whether \\\n",
    "there are already patents for such a concept. Please list me the codes of the 5 most relevant \\\n",
    "USPTO classifications to a possible patent for this concept without explanations for the codes.\n",
    "\"\"\""
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "response_classes = get_completion(prompt2)"
=======
    "response = get_completion(prompt2)"
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "response_keywords = get_completion(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
=======
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "G06K9/00, B60R25/10, G07C9/00, G06F21/32, B60R25/00\n",
      "1. \"Biometric Vehicle Access System technology\"\n",
      "2. \"Advanced biometric authentication in vehicles\"\n",
      "3. \"Biometric sensors in door handles and ignition systems\"\n",
      "4. \"Anti-spoofing measures in vehicle access\"\n",
      "5. \"Biometric integration in smart vehicles\"\n"
=======
      "1. G06K9/00\n",
      "2. B60R25/10\n",
      "3. G07C9/00\n",
      "4. E05B49/00\n",
      "5. G06F21/32\n"
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(response_classes)\n",
    "print(response_keywords)"
=======
    "print(response)"
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "The abstract above describes a concept for a novel invention.\\\n",
    "I would like to search a patent database to find out whether \\\n",
    "there are already patents for such a concept. Please list me the codes of the 5 most relevant \\\n",
    "USPTO classifications to a possible patent for this concept without explanations for the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = requests.get(\"https://api.open-notify.org/this-api-doesnt-exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf']\n"
     ]
    }
   ],
   "source": [
    "result_api = []\n",
    "f = open(\"data_dump/test.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "#print(data)\n",
    "\n",
    "for i in data[\"organic_results\"]:\n",
    "    result_api.append(i['pdf'])\n",
    "\n",
    "\n",
    "print(result_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(page_content='(12) United States Patent \\n Belhumeur et al. US009275273B2 \\n US 9.275,273 B2 \\n Mar. 1, 2016 (10) Patent No.: \\n (45) Date of Patent: \\n (54) \\n (71) \\n (72) \\n (*) \\n (21) \\n (22) \\n (65) \\n (63) \\n (60) \\n (51) \\n (52) \\n (58) METHOD AND SYSTEM FOR LOCALIZING \\n PARTS OF AN OBJECT IN AN MAGE FOR \\n COMPUTER VISION APPLICATIONS (56) References Cited \\n U.S. PATENT DOCUMENTS \\n Applicant: Kri Beh Visi 5,828,769 A * 10/1998 Burns ........................... 382,118 pplicant: regman-Iseinurneur vision 6,252,974 B1* 6/2001 Martens et al. ............... 382/107 Technologies, LLC, San Diego, CA 6,502,082 B1* 12/2002 Toyama et al. ................. TO6, 16 \\n (US) 8,873,840 B2 * 10/2014 Krupka et al. ... ... 382,159 \\n 2004/000293.0 A1 1/2004 Oliver et al. .................... TO6/46 \\n Inventors: Peter N. Belhumeur, New York, NY 2004/001793.0 A1 1/2004 Kim et al. ..................... 382,103 \\n (US); David W. Jacobs, Bethesda, MD (Continued) (US); David J. Kriegman, San Diego, \\n CA (US); Neeraj Kumar, Seattle, WA OTHER PUBLICATIONS \\n (US) \\n Cristinace D. et al., Feature detection and tracking with constrained \\n Notice: Subject to any disclaimer, the term of this local models, British Machine Vision Conference pp. 231-240, patent is extended or adjusted under 35 2004. \\n Appl. No.: 14/324,991 \\n Filed: Jul. 7, 2014 Primary Examiner — Barry Drennan \\n Assistant Examiner — Aklilu Woldemariam \\n Prior Publication Data (74) Attorney, Agent, or Firm — Keller Jolley Preece \\n US 2015/OO78631 A1 Mar. 19, 2015 \\n Related U.S. Application Data \\n Continuation of application No. 13/488,415, filed on \\n Jun. 4, 2012, now Pat. No. 8,811,726. \\n Provisional application No. 61/492.774, filed on Jun. \\n 2, 2011. \\n Int. C. \\n G06K 9/00 (2006.01) \\n GO6K 9/62 (2006.01) \\n U.S. C. \\n CPC ........ G06K9/00281 (2013.01); G06K 9/00248 \\n (2013.01); G06K9/6278 (2013.01); G06K 9/00 \\n (2013.01); G06K 9/62 (2013.01) \\n Field of Classification Search \\n USPC ......... 382/118, 155, 159, 180, 224, 225, 107, \\n 382/222 \\n See application file for complete search history. \\n CCLECEMAGE l \\n EXEMPARS \\n 101 (57) ABSTRACT \\n A system is provided for localizing parts of an object in an \\n image by training local detectors using labeled image exem \\n plars with fiducial points corresponding to parts within the \\n image. Each local detector generates a detector score corre \\n sponding to the likelihood that a desired part is located at a given location within the image exemplar. A non-parametric \\n global model of the locations of the fiducial points is gener \\n ated for each of at least a portion of the image exemplars. An input image is analyzed using the trained local detectors, and \\n a Bayesian objective function is derived for the input image \\n from the non-parametric model and detector scores. The Bayesian objective function is optimized using a consensus of \\n global models, and an output is generated with locations of \\n the fiducial points labeled within the object in the image. \\n 20 Claims, 9 Drawing Sheets \\n INPUMAGE \\n FOR \\n EWAAON \\n 106 \\n GNERATE CJPJ \\n WHAR \\n LOCAONSLABEED \\n NIMAGE \\n 112 FA USE:0CA. \\n DETECORSTO \\n MARKFICAL MSED reAUre RAIN LOCAL GENERAE \\n poNS in RAINNG SLection otectors - DEECORSCORES \\n TRAININGIMAGE iMAGESINC 04 105 107 \\n EXEMPARS DAABASE \\n O2 03 \\n SEEC USEBAYESRUEO GENERAENON \\n JSE SLECED GO3A. OPTIEROBABY PARAWRC \\n MODE. O.ABEL MOWIE HAA GCBAMODEL GCEACDELS \\n AR LOCATONS BEST FIT . FIES HE DEECTOR -. FROMAGE \\n MIMAGE H 11 SCORES ExoMPARs \\n 111 109 O3 \\n CAL \\n RECOGNOM \\n SYSTEM', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 0}), Document(page_content='US 9,275,273 B2 \\n Page 2 \\n (56) References Cited 2010, 0077382 A1 3/2010 Sasaki ........................... 717/124 \\n 2011 0116690 A1* 5, 2011 ROSS et al. ... 382.118 \\n U.S. PATENT DOCUMENTS 2012/0022952 A1 1/2012 Cetin et al. ................. 705/14.73 \\n 2004/0181749 A1* 9/2004 Chellapilla et al. ........... 715,505 OTHER PUBLICATIONS \\n ck 28393. A. ck $39. SE al. .70743. Eckhardt Metal., Towards Practical facial feature detection, In, J. of \\n 2007/0258648 A1* 11/2007 Perronnin ..................... 382,224 Pattern Recognition and Artificial Intelligence 23 (3) 379-400, \\n 2008.0109041 A1* 5/2008 de Voir .............................. 6O7/7 2009.* \\n 2008/0267459 A1 * 10, 2008 Nakada et al. ................ 382,118 \\n 2010.0054592 A1 3/2010 Nanu et al. .................... 382, 167 * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 1}), Document(page_content='US 9,275.273 B2 Sheet 1 of 9 Mar. 1, 2016 U.S. Patent \\n - - -; \\n OLL LOETES SER HOOS RIO LOE LEGI \\n El LV-JENESOT\\\\/OOT NIV/>| 1 O L SÈJO LOE LEGI TV/OOT EST', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 2}), Document(page_content='US 9,275.273 B2 Sheet 2 of 9 Mar. 1, 2016 U.S. Patent \\n FIG. 2', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 3}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 3 of 9 US 9,275.273 B2 \\n s', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 4}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 4 of 9 US 9,275.273 B2 \\n o \\n N \\n s g P C C C C C C C C. O o \\n eoubSIO JenoO-Jeu / JOu ueeW', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 5}), Document(page_content='US 9,275.273 B2 Sheet 5 Of 9 Mar. 1, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 6}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 6 of 9 US 9.275,273 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 7}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 7 Of 9 US 9,275.273 B2 \\n s', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 8}), Document(page_content='US 9,275.273 B2 Sheet 8 of 9 Mar. 1, 2016 U.S. Patent \\n O \\n 32 O LO r \\n O O O \\n O', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 9}), Document(page_content='US 9,275.273 B2 Sheet 9 Of 9 Mar. 1, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 10}), Document(page_content=\"US 9,275,273 B2 \\n 1. \\n METHOD AND SYSTEM FOR LOCALIZING \\n PARTS OF AN OBJECT IN AN IMAGE FOR \\n COMPUTER VISION APPLICATIONS \\n RELATED APPLICATIONS \\n This application is continuation of U.S. application Ser. \\n No. 13/488,415, filed Jun. 4, 2012, which claims the benefit of the priority of U.S. provisional application No. 61/492,774, \\n filed Jun. 2, 2011, which is incorporated herein by reference in its entirety. \\n GOVERNMENT RIGHTS \\n This invention was made with government funding from \\n the Office of the Chief Scientist of the Central Intelligence Agency. The government has certain rights in the invention. \\n FIELD OF THE INVENTION \\n The present invention relates to a method for computer aided analysis of images, and more specifically to a method \\n for localizing features within an image. \\n BACKGROUND OF THE INVENTION \\n Over the last decade, new applications in computer vision and computational photography have arisen due to earlier \\n advances in methods for detecting human faces in images. \\n These applications include face detection-based autofocus \\n and white balancing in cameras, Smile and blink detection, new methods for sorting and retrieving images in digital \\n photo management Software, obscuration of facial identity in digital photos, facial expression recognition, virtual try-on, \\n product recommendations, facial performance capture, ava \\n tars, controls, image editing software tailored for faces, and systems for automatic face recognition and verification. The first step of any face processing system is the detection \\n of locations in the images where faces are present. However, face detection from a single image is challenging because of \\n variability in Scale, location, orientation, and pose. Facial expression, occlusion, and lighting conditions also change \\n the overall appearance of faces. \\n Given an arbitrary image, the goal of face detection is to \\n determine whether or not there are any faces in the image and, \\n if present, return the image location and extent of each face. \\n The challenges associated with face detection can be attrib uted to the following factors: \\n Pose: The images of a face vary due to the relative camera face pose (frontal, 45 degree, profile, upside-down), and some \\n facial features such as an eye or the nose may become par tially or wholly occluded. \\n Presence or absence of structural components: Facial fea tures such as beards, moustaches, and glasses may or may not be present, and there is a great deal of variability among these components including shape, color, and size. \\n Facial expression: The appearance of faces is directly \\n affected by a person’s facial expression. \\n Occlusion: Faces may be partially occluded by other objects. In an image with a group of people, some faces may \\n partially occlude other faces. Image orientation: Face images vary directly for different \\n rotations about the camera's optical axis. Imaging conditions: When the image is formed, factors Such as lighting (spectra, Source distribution and intensity) \\n and camera characteristics (sensor response, lenses, filters) affect the appearance of a face. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n Camera Settings: The settings on the camera and the way \\n that is used can affect the image focus blur, motion blur, depth \\n of field, compression (e.g., jpeg) artifacts, and image noise. \\n Face detectors usually return the image location of a rect angular bounding box containing a face—this serves as the \\n starting point for processing the image. A part of the process \\n that is currently in need of improvement is the accurate detec \\n tion and localization of parts of the face, e.g., eyebrow cor \\n ners, eye corners, tip of the nose, earlobes, hair part, jawline, \\n mouth corners, chin, etc. These parts are often referred to as facial feature points or “fiducial points'. Unlike general inter\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 11}), Document(page_content=\"ners, eye corners, tip of the nose, earlobes, hair part, jawline, \\n mouth corners, chin, etc. These parts are often referred to as facial feature points or “fiducial points'. Unlike general inter \\n est or corner points, the fiducial point locations may not correspond to image locations with high gradients (e.g., tip of \\n the nose). As a result, their detection may require larger image \\n Support. \\n A number of approaches have been reported which have demonstrated great accuracy in localizing parts in mostly \\n frontal images, and often in controlled settings. \\n Early work on facial feature detection was often described as a component of a larger face processing task. For example, \\n Burl, et al. take a bottom-up approach to face detection, first \\n detecting candidate facial features over the whole image, then \\n selecting the most face-like constellation using a statistical \\n model of the distances between pairs of features. Other works detect large-scale facial parts such as each eye, the nose, and \\n the mouth and return a contour or bounding box around these components. \\n There is a long history of part-based object descriptions in \\n computer vision and perceptual psychology. Recent \\n approaches have shown a renewed emphasis on parts-based \\n descriptions and attributes because one can learn descriptions of individual parts and then compose them, generalizing to an \\n exponential number of combinations. The Poselets work by Bourdev and Malik, incorporated herein by reference, \\n describes a data-driven search for object parts that may be a \\n useful approach for addressing some of the described inad equacies of the prior art in order to achieve precise face \\n detection in uncontrolled image conditions. Many fiducial point detectors include classifiers that are trained to respond to a specific fiducial (e.g., left corner of the \\n left eye). These classifiers take as input raw pixel intensities \\n over a window or the output of a bank of filters (Gaussian \\n Derivative filters, Gabor filters, or Haar-like features). These local detectors are scanned over a portion of the image and \\n may return one or more candidate locations for the part or a \\n “score” at each location. This local detector is often a binary \\n classifier (feature or not-feature). For example, the Viola Jones style detector, which uses an image representation \\n called an “integral image' rather than working directly with \\n image intensities, has been applied to facial features. False \\n detections occur often, even for well-trained classifiers, because portions of the image have the appearance of a fidu \\n cial under some imaging condition. For example, a common \\n erroris for a “left corner of left eye' detector to respond to the \\n left corner of the right eye. Eckhart, et al. achieve robustness and handle greater pose variation by using a large area of \\n Support for the detector covering, e.g., an entire eye or the \\n nose with room to spare. Searching over a smaller region that \\n includes the actual part location reduces the chance of false detections with minimal impact of missing fiducials. While \\n this may be somewhat effective for frontal fiducial point \\n detection, the location of a part within the face detector box can vary significantly when the head rotates in three-dimen sions. For example, while the left eye is in the upper-left side \\n of the box when frontal, it can move to the right side when the face is seen in profile.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 11}), Document(page_content='US 9,275,273 B2 \\n 3 \\n To better handle larger variations in pose, constraints can \\n be established about the relative location of parts with respect \\n to each other rather than the actual location of each part to the detector box. This can be expressed as predicted locations, bounding regions, or as a conditional probability distribution \\n of one part location given another location. Alternatively, the \\n joint probability distribution of all the parts can be used, and \\n one model is that they form a multivariate normal distribution whose mean is the average location of each part. This is the model underlying Active Appearance Models and Active \\n Shape Models, which have been used for facial feature point detection in near frontal images. Saragih, et al. extend this to \\n use a Gaussian Mixture Model, whereas Everingham, et al. handle a wider range of pose, lighting and expression varia \\n tions by modeling the joint probability of the location of nine \\n fiducials relative to the bounding box with a mixture of Gaus sian trees. As pointed out in this work, a joint distribution of \\n part locations over a wide range of poses cannot be adequately modeled by a single Gaussian. \\n While a number of approaches balance local feature detec tor responses on the image with prior global information about the feature configurations, optimizing the resulting \\n objective function remains a challenge. The locations of some parts vary significantly with expression (e.g., the mouth, eye \\n brows) whereas others. Such as the eye corners and nose, are more stable. Consequently, Some detection methods organize \\n their search to first identify the stable points. The location of the mouth points are then constrained, possibly through a \\n conditional probability, by the locations of stable points. \\n However, this approach fails when these stable points cannot be reliably detected, for example, when the eyes are hidden by Sunglasses. \\n The need for the ability to reliably detect and identify \\n features within an image is not limited to human facial rec ognition. Many other disciplines rely on specific features \\n within an image to facilitate identification of an object within an image. For example, conservation organizations utilize \\n markings such as ear notches, Scars, tail patterns, etc., on wild \\n animals for identification of individual animals for study of migration patterns, behavior and survival. The ability to reli \\n ably locate and identify the unique features within an image \\n of an animal could provide expanded data for use in Such studies. Other applications of image analysis that could ben \\n efit from improved feature location capability include identi \\n fication of vehicles within images for military or law enforce \\n ment applications, and identification of structures in satellite images, to name a few. \\n SUMMARY OF THE INVENTION \\n According to the present invention, a method is provided \\n for facial feature detection by combining the outputs of a \\n plurality of local detectors with a consensus of non-paramet \\n ric global models for part locations. \\n The inventive method for face detection begins with a large collection of pre-specified (labeled) parts in images of human \\n faces taken under a variety of acquisition conditions, includ ing variability in pose, lighting, expression, hairstyle, Subject \\n age, Subject ethnicity, partial-occlusion of the face, camera \\n type, image compression, resolution, and focus. This collec tion captures both the variability of appearance of each part \\n and the variability in the relative positions of each part. \\n The collection of labeled exemplar images is used as a \\n training set for a local detector, which evaluates Small win \\n dows in the image to determine whether the area within the window contains the desired part. Using scale-invariant fea \\n tures within the image, the local detector, which is a sliding 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='tures within the image, the local detector, which is a sliding 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n window classifier, generates a detector score for each point in \\n the image, with the score corresponding to the likelihood that \\n the desired part is located at a given point in the image. The \\n sliding window classifier may be a neural network, Support \\n vector machine (SVM), Viola-Jones-style detector, or other learning machine that is appropriate for image analysis appli \\n cations. The sliding window classifier may be applied over the expected range of image locations for the particular part. \\n Next, a global detector is developed for a collection offiducial \\n points by combining the outputs of the local detectors with a \\n non-parametric prior model of face shape. By assuming that \\n global model images generate the part locations as hidden \\n variables, a Bayesian objective function can be derived. This \\n function is optimized using a consensus of models for the \\n hidden variables to determine the most likely values of the \\n hidden variables from which the part location can be deter \\n mined. \\n In a Bayesian sense, a generative probabilistic model \\n P(W) for the i-th fiducial can be constructed this is the probability distribution of the image feature W from the marked fiducials in the collection. Letting X={x\\', x, ... x\"} denote the locations of the n fiducials, the prior probability \\n distribution of the fiducial location PCX) can be estimated from the fiducial locations in the image collection. Detection of the parts in an input image can use the probabilistic models \\n on the appearance P(W) and fiducial locations POX) to find the fiducial locations in an input image by maximizing a Bayesian objective function. Alternatively, image features \\n that are not at the locations of the fiducials (negative \\n examples) and the features at the locations of fiducials (posi \\n tive examples used to construct P(W)) can be used to con struct classifiers or regressors for each fiducial. These classi fier outputs can be combined using the prior model on fiducial location POX) to detect fiducials in an input image. Existing \\n methods for detecting parts in faces that create a prior model \\n on the configuration of fiducials have used parametric forms \\n for PCX) such as a multivariate normal distribution or a mix \\n ture of Gaussian, whereas the inventive method uses a non parametric form that is dictated by the training collection. \\n In one aspect of the invention, the facial feature localiza tion method is formulated as a Bayesian inference that com \\n bines the output of local detectors with a prior model of face shape. The prior on the configuration of face parts is non \\n parametric, making use of a large collection of diverse, \\n labeled exemplars. Hidden (latent) variables are introduced for the identity and parts locations within the exemplar to \\n generate fiducial locations in a new image. The hidden vari ables are marginalized out, but they nonetheless provide valu \\n able conditional independencies between different parts. To marginalize efficiently, a RANdom SAmple Consensus \\n (RANSAC)-like process to sample likely values of the hidden \\n variables. This ultimately leads to part localization as a com \\n bination of local detector output and the consensus of a vari ety of exemplars and poses that fit this data well. \\n In another aspect of the invention, a method for localizing parts of an object in an input image comprises training a \\n plurality of local detectors using at least a portion of a plural ity of image exemplars as training images, wherein each \\n image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images \\n corresponding to a likelihood that a desired part is located at the location within the training image; generating a plurality of non-parametric global models using at least a portion of the plurality of image exemplars; inputting data corresponding to', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='US 9,275,273 B2 \\n 5 \\n the input image; applying the trained local detectors to the \\n input image to generate detector scores for the input image; \\n deriving a Bayesian objective function from the plurality of non-parametric global models using an assumption that loca \\n tions of fiducial points within the image exemplar are repre sented within its corresponding global model as hidden vari ables; optimizing the Bayesian objective function to obtain a \\n consensus set of global models for the hidden variables that best fits the data corresponding to the input image; and gen erating an output comprising locations of the fiducial points \\n within the object in the image. In one embodiment, the object \\n is a face. \\n In another aspect of the invention, a method for localizing \\n parts of an object in an input image comprises training a \\n plurality of local detectors using at least a portion of a plural \\n ity of image exemplars as training images, wherein each \\n image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a \\n plurality of locations of fiducial points in the training images \\n corresponding to a likelihood that a desired part is located at \\n the location within the training image; generating a non \\n parametric model of the plurality of locations of the fiducial \\n points in each of at least a portion of the plurality of image \\n exemplars; inputting data corresponding to the input image: \\n applying the trained local detectors to the input image to \\n generate detector scores for the input image; deriving a Baye \\n sian objective function for the input image from the non \\n parametric model and detector scores; and generating an out \\n put comprising locations of the fiducial points within the \\n object in the image. In one embodiment, the step of deriving \\n a Bayesian objective function comprises using an assumption \\n that locations offiducial points within the image exemplar are \\n represented within its corresponding global model as hidden \\n variables; and optimizing the Bayesian objective function to \\n obtain a consensus set of global models for the hidden vari \\n ables that best fits the data corresponding to the image. \\n In still another aspect of the invention, a computer-program product embodied on a non-transitory computer-readable \\n medium comprising instructions for receiving a plurality of image exemplars, and further comprises instructions for training a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, wherein \\n each image exemplar is labeled with fiducial points corre \\n sponding to parts within the image, and wherein each local \\n detector generates a detector score when applied at one loca tion of a plurality of locations offiducial points in the training images corresponding to a likelihood that a desired part is \\n located at the location within the training image; generating a non-parametric model of the plurality of locations of the fiducial points in each of at least a portion of the plurality of image exemplars; inputting data corresponding to the input \\n image; applying the trained local detectors to the input image to generate detector scores for the input image; deriving a Bayesian objective function for the input image from the \\n non-parametric model and detector scores; and generating an \\n output comprising locations of the fiducial points within the object in the image. In an exemplary embodiment, deriving a Bayesian objective function comprises using an assumption \\n that locations offiducial points within the image exemplar are represented within its corresponding global model as hidden variables; and optimizing the Bayesian objective function to \\n obtain a consensus set of global models for the hidden vari ables that best fits the data corresponding to the image. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 13}), Document(page_content='obtain a consensus set of global models for the hidden vari ables that best fits the data corresponding to the image. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 is a block diagram showing the steps of the inventive process for localizing parts of a face. \\n FIG. 2 is an image of a human face in the LFPW dataset, \\n where the left image shows hand-labeled fiducial points and the right image shows the points numbered to match FIG. 4 \\n FIG.3 is a set of photographs of human faces with fiducial \\n points indicated according to the inventive method. \\n FIG. 4 is a plot showing the mean error of the fiducial \\n detector on the LFPW dataset (gray bar of each pair) com pared to the mean variation in human labeling (black bar of \\n each pair) using the fiducial labels shown in FIG. 2. The error \\n is the fraction of inter-ocular distance. \\n FIG. 5 is a comparison of the performance of the inventive \\n method and the detector of Everingham, et al., showing that \\n the inventive method is roughly twice as accurate as both. FIG. 6 is a collection of photographic images from the \\n Labeled Face Parts in the Wild (LFPW) along with parts \\n located by the inventive detector. FIG. 7 is a set of images from BioID, with the parts local \\n ized by the inventive detector. FIG. 8 is a plot of cumulative error distribution comparing \\n the inventive method with methods described by others. \\n FIG. 9 is a plot of cumulative error distribution of the inventive method on the LFPW dataset compared to locations predicted using the face detector box or found with only the \\n local detectors. \\n DETAILED DESCRIPTION \\n A method is provided for localizing parts of an object \\n within an image by detecting fine-scale fiducial points or \\n microfeatures. Although the examples described herein relate to detecting parts of a human face, the inventive method may be used for detecting parts within images of many other \\n classes of objects, e.g., animals (dogs, cats, wild animals, \\n marine mammals, etc.) or animal faces, parts of bicycles, \\n vehicles, or structures. Thus, the described examples of face part detection are not intended to be limiting. \\n FIG.1 provides the basic steps of the inventive method 100. In step 101, a large number of image exemplars are collected, \\n preferably without constraint as to pose, expression, occlu \\n sion or other characteristics that would make the detection \\n process easier. In step 102, the image exemplars are marked \\n by hand with fiducial points. Examples of these fiducial \\n points are shown in FIG. 2, overlaid on an image obtained \\n from the Internet. In addition to the marked fiducials, a virtual fiducial location in each training image can be defined as a \\n mathematical function of a subset of the marked fiducial. For \\n example, a virtual fiducial in the center of the chin could be created by taking the average location of a fiducial located on \\n the lower lip and a fiducial located on the chin. In step 103, the marked image exemplars are downloaded \\n into a database that can be accessed by a processor that is programmed to execute a learning machine. The following steps are part of a computer program that \\n may be embodied within a non-transitory computer-readable \\n medium for causing a computer processor to execute the \\n specified steps. (For purposes of the present invention, a “computer processor includes a Supercomputing grid or \\n cluster, a network server, a private or dedicated server, a standard personal computer or laptop, a tablet computer, a \\n Smartphone, video game console or controller, digital cam \\n era, or any other known computing device (stand-alone or \\n embedded within another electronic device) that is capable of processing images.) In step 104, an optional pre-processing', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 13}), Document(page_content='US 9,275,273 B2 \\n 7 \\n or feature selection step, an algorithm is applied to select the \\n features that are most determinative of the patterns to be \\n recognized. In one example, known feature selection algo \\n rithms such as the well-known AdaBoost may be used to improve the performance of the Subsequent classifiers. In \\n another example, techniques for image feature detection and \\n characterization, e.g., Hough transforms, edge detection, \\n blob detection, histograms of Gradients (HOG), Speeded Up \\n Robust Feature (SURF), and others, may be applied. As \\n described below, in the preferred embodiment, a scale-invari \\n ant feature transform (SIFT) is used. \\n In step 105 local detectors are trained using the training \\n images selected from the set of image exemplars and features \\n extracted from the image. In the preferred embodiment, the \\n classifier is a non-linear support vector machine (SVM) with \\n a radial basis function (RBF) kernel. The local detectors return a score indicating the likelihood of an image location \\n being a particular fiducial point. The local detector is scanned \\n overan image and a score at each image location is computed; \\n a high score is a more likely location in the image for the \\n feature. The image locations that are local maxima of the detector output are referred to as “detector peaks.” \\n In step 106, data for an image to be evaluated is input into \\n the processor, and, in step 107, the trained local detectors are used to generate detector scores for each fiducial point type for each point in the image. Although not shown, the input image may require some pre-processing if the input image is \\n more that just a face. In this case, a face detector may be applied to the image to identify a region of the image that \\n contains a face, and this region would be the input to Subse quent steps. For example, in a group photograph, or in a \\n photograph where the Subject is Surrounded by structures or other background, a rectangle of an input image can be \\n cropped out and used as the input for face part detection. The \\n inventive method takes as input an image that contains a face. This optional face detector step can be bypassed when it is known that input contains only a single face at an approxi \\n mately known or specified location, orientation, and size (e.g., head shots in a yearbook). \\n In step 108, a plurality of non-parametric global models is generated using the image exemplars. (Note that step 108 can \\n follow either step 102 or step 103.) In step 109, the global \\n models from step 108 and the detector scores from step 107 are combined. A Bayesian objective function is derived by \\n assuming that the global models generate locations of the \\n parts as hidden variables. This function is optimized using a \\n consensus of global models for the hidden variables. The models that provide the best fit to the data are selected in step \\n 110. In step 111, the selected set of best fitting models is used to label the part locations in the image that was input in step 106. An output is generated in step 112, where the image 113 \\n may be stored in memory for further processing and/or dis played via agraphic display or printoutata user interface with \\n the part locations (fiducial points) labeled, as shown in FIG.2. \\n Note that the dots shown in the image are used to indicate that the parts were correctly found and accurately located. \\n In optional step 114, the labeled output image 113 can be further processed using the same or a different processor \\n programmed with facial recognition Software to identify the \\n individual whose face parts have been labeled by the inven \\n tive method. For Such processing, the marked locations will \\n be characterized as coordinates within the image, which will then be stored as an output. A face recognition system will use \\n the coordinates corresponding to the fiducial locations along with the input image for performing the recognition proce \\n dure. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content=\"the coordinates corresponding to the fiducial locations along with the input image for performing the recognition proce \\n dure. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n For purposes of this description, “facial features”, “face \\n parts”, “facial landmarks' and “parts' may be used inter \\n changeably to refer to a feature of a face. “Fiducial points' \\n refers to the location within an image, commonly seen as a \\n grid or array of pixels, at which the facial features, landmarks or parts may be found. \\n The dataset used for training and testing the inventive method was a collection of face images acquired from inter net search sites using simple text queries. More than a thou sand images were hand-labeled to create the dataset, which is referred to as “Labeled Face Parts in the Wild', or “LFPWA commercial, off-the-shelf (COTS) face detector was used to detect faces within the collected images. (No intentional fil tering was applied to exclude poor quality images.) Unlike datasets that are acquired systematically in laboratories, there were few preconditions in the LFPW dataset that would tend to make detection easier. Rather, images were included where the eyes might be occluded by glasses, Sunglasses, or hair; there may be heavy shadowing across features; the facial expression may be arbitrary; the face may have no makeup or be made up theatrically; the image may actually be an artistic rendering; the pose may be varied; there may be facial hair that occludes the fiducial points; and part of the face may be occluded by a hat, wall, cigarette, hand, or microphone. For example, FIGS. 3 and 6 illustrate a number of these condi tions. As a result, this dataset stands in contrast to datasets such as FERET or BioID which have been used for evaluating fiducial point detection in that the images are not restricted to frontal faces or collected in a controlled manner, e.g., using the same camera for all images. The inventive method consists of two basic processes. Given an input image containing a face, local detectors gen \\n erate detector scores for points in the image, where the score \\n provides an indication of the likelihood that a desired part is \\n located at a given point within the image. The detector scores \\n generated by the local detectors are then combined with a non-parametric prior model of a face shape. \\n For each local detector, a sliding window detector is scanned over a region of the image. The sliding window detectors may be a learning machine Such as a neural net work, linear or nonlinear support vector machine (SVM) or other learning machine. In a preferred embodiment, the detectors are SVM regressors with grayscale SIFT (Scale Invariant Feature Transform) descriptors as features. The SIFT descriptor window may be computed at two scales: \\n roughly /4 and /2 the inter-ocular distance. These two SIFT descriptors are then concatenated to form a single 256 dimen sional feature vector for the SVM regressor. In another embodiment, color SIFT is used to capture color variation of face parts. It will be recognized by practitioners with ordinary \\n skill in the art that there are other feature descriptors besides SIFT that could be used including histograms of gradients (HOG), color histograms, SURF ORB, Binary Robust Inde pendent Elementary Features (BRIEF), etc. It will also be recognized by those of ordinary skill in the art that other regressors that are trained with positive and negative training examples and take a feature descriptor as input and return a \\n score can be used. \\n For all of the training samples, the images were rescaled so \\n that the faces have an inter-ocular distance of roughly 55 pixels. Positive samples are taken at the manually annotated \\n part locations. Negative samples are taken at least 4 of the \\n inter-ocular distance away from the annotated locations. In addition, random image plane rotations within t20 are used to synthesize additional training samples.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content='inter-ocular distance away from the annotated locations. In addition, random image plane rotations within t20 are used to synthesize additional training samples. \\n These local detectors return a score at each point X in the image (or in some Smaller region of the face as inferred from \\n an earlier face detection step). The detector score d(x) indi \\n cates the likelihood that the desired part is located at point X in the image. This score is normalized to behave like a prob', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content='US 9,275,273 B2 \\n 9 \\n ability by dividing by the sum of the scores in the detector \\n window. Once normalized, this score is written as P(xd), i.e., \\n the probability that the fiducial is at location X given all the \\n scores in the detection window. \\n As the local detectors are imperfect, the correct location \\n will not always be at the location with the highest detector \\n score. This can happen for many of the aforementioned rea \\n Sons, including occlusions due to head pose and visual \\n obstructions such as hair, glasses, hands, microphones, etc. \\n These mistakes in the local detector almost always happen at \\n places that are inconsistent with positions of the other, cor \\n rectly detected fiducial points. Nonetheless, the global detec \\n tors can be built to better handle the cases where the local \\n detectors are likely to go astray. \\n Although faces come in different shapes, present them \\n selves to the camera in many ways, and may possess extreme facial expressions, there are strong anatomical and geometric \\n constraints that govern the layout of face parts and their \\n location in images. These constraints are not modeled explic itly, but rather the training data dictates this implicitly. All the part locations are taken together to develop a global detector \\n for a collection offiducial points. A global model encodes the configuration of part locations. More formally, let X={x\\', x, ... x\"} denote the locations of n parts, where x\\' is the location of the i\\' part. Let D={d\\', d\\'. ... d\" the measured detector responses, where d\\' is the window of scores returned by the i\\' local detector. The goal is to find the value of X that maximizes the probability of X \\n given the measurements from the local detectors, i.e., \\n X* = argmax P(X D) (1) \\n X \\n Let X (where k=1,..., m) denote the locations of the n parts in the k\" of m exemplars, and let X be the locations of the parts in exemplark transformed by Some similarity trans \\n formation t. (Examples of similarity transformations include, \\n but are not limited to, reflection, rotation, translation, etc.) X is referred to as a “global model,” while k and t are the “hidden variables. \\n Assuming that each Xis generated by one of the global \\n models X, PCXD) can be expanded as follows: \\n i (2) P(X| D) =X IPX | X, D)P(X | D)dt sie \\n where the collection of m exemplars X along with similarity \\n transformations thave been introduced into the calculation of \\n PCXID) and then marginalized out. \\n By conditioning on the global model X, the locations of the parts x\\' can now be treated as conditionally independent \\n of one another and the first term of Eq. 2 can be rewritten as \\n (3) \\n 4 P(x; di) (4) \\n Since knowing the true location of the parts trumps any information provided by the detector, P(x,x)=P(x,x). 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n Also, since the relation between the transformed model fidu \\n cial and the true fiducial is translationally invariant, it should only depend on AX, -X-X\\'. With these observations, Eq.4 \\n can be rewritten as \\n P(X | X, D) = \\n Moving to the second term in Eq. 2, Bayes\\' rule can be used \\n to obtain \\n P(x,D) = f kit) (6) \\n P.X.) T = nil Pals. (7) \\n where again conditioning on the global model X, allows the detector responses d\\' to be treated as conditionally indepen \\n dent of one another. \\n A final application of Bayes\\' rule rewrites Eq. 7 as \\n \"Pips, \\n = CIP(x, Id) (9) \\n i=1 \\n Note that the terms within the square bracket in Eq. 8 that \\n depend only on Dare constant given the image. Also note that \\n the terms within the square bracket that depend only on X, \\n are also constant because a uniform distribution is assumed \\n on the global models. This allows all the terms within the \\n square bracket to be reduced to a single constant C. Combining Eqs. 1, 2, 5 and 9 yields', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 15}), Document(page_content='on the global models. This allows all the terms within the \\n square bracket to be reduced to a single constant C. Combining Eqs. 1, 2, 5 and 9 yields \\n where X* is the estimate for the part locations. The first term P(Ax) is taken to be a 2D Gaussian distri bution centered at the model location AX: though other probability distributions. Each part i has its own Gaussian \\n distribution. These distributions model how well the part \\n locations in the global model fit the true locations. If we had a large number of exemplars in our labeled dataset from \\n which to construct these global models—i.e., if m were very large—one would expect a close fit and low variances for \\n these distributions. The following steps are used to estimate \\n the covariance matrices for the part locations: For each exemplar X, from the labeled dataset of image exemplars, a sample X is obtained from the remaining exem \\n plars and a transformation t that gives the best La fit to X. Compute the difference X-X, and normalize the result by the inter-ocular distance. These normalized differences are \\n used to compute the covariance matrices for each part loca \\n tion.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 15}), Document(page_content='US 9,275,273 B2 \\n 11 \\n The second term P(x\\'ld)) is computed as follows. Take the estimated locationx for parti and lookup the response for the i\\' detector at that point in the image, i.e., d\\'(x). This value is then normalized to behave like a probability by dividing by \\n the sum of d\\'(x) for all x in the detector window. Computing the sum and integral in Eq. 10 is challenging, as \\n they are taken over all global models k and all similarity \\n transformations t. However, as noted from Eq. 2, if P(XID) is very small for a given k and t, it will be unlikely to contrib \\n ute much to the overall sum and integration. Thus, the strategy \\n is to consider only those global models k with transforma tions t for which P(XID) is large. In a sense, one performs a Monte Carlo integration of Eq. \\n 10 where the global models X chosen are those that are likely to contribute to the sum and integral. In the following, \\n the process is described for selecting a list of k and t that are used to compute this integration. \\n The goal is to optimize PCXID) over the unknownsk and t. This optimization is non-linear, and not amenable to gradi \\n ent descent-type algorithms. First, k is a discrete variable with \\n a large number of possible values. Second, even for a fixed k, different values oft can be expected to produce large numbers of local optima because the fiducial detectors usually produce \\n a multi-modal output. Transformations that align a model \\n with any subsets of these modes are likely to produce local optima in the optimization function. \\n To address this issue, a generate-and-test approach similar \\n to RANSAC (RANdom SAmple Consensus) can be used by generating a large number of plausible values for the hidden \\n variables k and t. Each of these values is evaluated using Eq. \\n 9, keeping track of them best global models, i.e., them best pairs k and t. This is done in the following steps: \\n 1. Select a random k. \\n 2. Select two random parts. Randomly match each model part to one of the ghighest peaks of the detector output \\n (i.e., highest detector scores) for that part. \\n 3. Set t to be the similarity transformation that aligns the two model fiducial points with two matching peaks from \\n Step 2. \\n 4. Evaluate Eq.9 for this k, t. 5. Repeat Steps 1 to 4 r times. \\n 6. Record in a set -- the m pairs k and t for which Eq.9 in Step 4 is largest. \\n In the experimental system, the values r-10,000, g=2, and \\n m=100 were used. \\n Estimating X \\n In the previous subsection, a RANSAC-like procedure was \\n used to find a list v? of m global models X for which P(XID) is largest. With these in hand, an approximate opti mization for X in Eq. 10 is \\n where the sum is now only taken over those k, te v. To find the best X*, first find an initial estimate X\" for each partias \\n k,te Wi \\n This is equivalent to solving for Xo by setting all P(AX.) and P(x\\'id) to a constant in Eq. 10 for allizi. To compute each 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n Xo, multiply the detector output by a Gaussian function cen \\n tered at x\\', with the covariances calculated as described above. Next, find the image location X, where the sum of the resulting products is maximized. The initial estimates, X\\', ie1 ... n can then be used to initialize an optimization of Eq. 11 to find the final estimates x* that make up X*. For some uses of the fiducial output, X\\', ie 1 . . . n may be \\n Sufficiently accurate estimates of the fiducial location and further optimization may be unnecessary. \\n It will be recognized by one of ordinary skill in the art that \\n the method and system for localizing parts of faces can be applied for localizing parts of many other object classes. The \\n method described hereintakes as training input a collection of images with marked part locations and then given a new \\n image, the method will find the location of the parts in the new image. Thus, the method could be applied to find face parts of', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 16}), Document(page_content='image, the method will find the location of the parts in the new image. Thus, the method could be applied to find face parts of \\n non-humans such as cats, dogs and other animals, as well as parts of inanimate objects, as well as for identification of distinct features within Some other image. For example, indi vidual humpback whales are identified by color patterns, \\n notches and Scars on their tails, so a parts locator could assist in computer-aided identification of a whale within an image \\n where angle, rotation and other variables may make it difficult to locate the body parts, especially given the short periods of \\n time during which the whales are visible Rhinoceros in the \\n wild are identified by features such as ear notches, horn characteristics, and tail length and curvature, so the ability to automatically locate these body parts within a photograph of \\n these shy animals would be helpful for conservation studies. \\n In another example, the inventive method may be used to find parts of a bicycle Such as the front hub, rear hub, peddles, seat, \\n left handlebar, right handlebar, crank arms, and brake levers. The method would be trained with images of bicycles where \\n the locations of these parts are marked. Local detectors are trained in the same way and the global model can be directly \\n used. The inventive method can be applied to find parts of many other objects of interest for use in various computer vision applications. \\n EXAMPLES \\n The present invention focuses on localizing parts in natural face images, taken under a wide range of poses, lighting \\n conditions, and facial expressions, in the presence of occlud ing objects such as Sunglasses or microphones. Existing \\n datasets for evaluating part localization do not contain the range of conditions. \\n Since researchers have recently reported results on BioID, the results produced by the present invention are compared to \\n prior results on BioID. Like most datasets used to evaluate part localization on face images, BioID contains near-frontal \\n views and less variation in viewing conditions than LFPW. \\n LFPW consists of 3,000 faces from images downloaded from the web using simple text queries on sites such as \\n google.com, flickr.com, and yahoo.com. The 3,000 faces \\n were detected using a commercial, off-the-shelf (COTS) face detection system. Faces were excluded only if they were \\n incorrectly detected by the COTS detector or if they con \\n tained text on the face. Note also that the COTS face detector \\n does not detect faces in or near profile, and so these images are implicitly excluded from the dataset. To obtain ground truth data, thirty-five fiducial points on \\n each face were labeled by workers on Amazon Mechanical Turk (MTurk), a crowdsourcing Internet marketplace that \\n enables computer programmers to coordinate the use of human intelligence to perform tasks that computers are cur \\n rently unable to do. The fiducial points are as follows:', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 16}), Document(page_content='US 9,275,273 B2 \\n 13 \\n left eyebrow out right eyebrow out \\n left eyebrow in right eyebrow in \\n left eyebrow center top \\n left eyebrow center bottom right eyebrow center top \\n right eyebrow center bottom \\n left eye out right eye out \\n left eye in right eye in \\n left eye center top \\n left eye center bottom right eye center top \\n right eye center bottom left eye pupil right eye pupil \\n left nose out \\n right nose out \\n nose center top \\n nose center bottom \\n left mouth out \\n right mouth out \\n mouth center top lip top \\n mouth center top lip bottom \\n mouth center bottom lip top \\n mouth center bottom lip bottom left ear top right ear top \\n left ear bottom \\n right ear bottom \\n left ear canal \\n right ear canal \\n chin \\n Of these third-five points, only twenty-nine were used in \\n the example shown here—the six points associated with the \\n ears were excluded. FIG. 2 illustrates the location of the 29 \\n points. Each point was labeled by three different MTurk workers. The average location was used as ground truth for \\n the fiducial point. FIG. 6 shows example images from LFPW, along with the \\n results. There is a degree of Subjectivity in the way humans \\n label the location of fiducial points in the images, and this is \\n seen in FIG.4, which shows the variation amongst the MTurk \\n workers. Some parts like the eye corners are more consis \\n tently labeled whereas the brows and chin are labeled less accurately. \\n The publicly available BioID dataset contains 1,521 images, each showing a frontal view of a face of one of 23 \\n different subjects. Seventeen fiducial points that had been \\n marked for the FGNet project were used, and the me, error \\n measure as defined by Cristinacce and Cootes was used to compare detected locations from ground truth locations. This \\n dataset has been widely used, allowing the results to be benchmarked with prior work. Note that training was per \\n formed using the LFPW dataset, and while testing was done \\n using the BioID data. There are considerable differences in the viewing conditions of these two datasets. Furthermore, \\n the locations of parts in LFPW do not always match those of BioID, and so a fixed offset was computed between parts that were defined differently. For example, where the left and right \\n nose points are outside of the nose in LFPW, they are below the nose in BioID). FIG. 7 shows some sample images, along \\n with the results obtain using the inventive method. The LFPW dataset was randomly split into 1,100 training \\n images and 300 test images. (An additional 1,600 images 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n were held out for subsequent evaluations at future dates.) Training images were used to train the SVM-based fiducial detectors and served as the exemplars for computing the \\n global models X. \\n The results of each localization were evaluated by measur ing the distance from each localized part to the average of \\n three locations supplied by MTurk workers. Error is mea \\n Sured as a fraction of the inter-ocular distance, to normalize for image size. FIG. 4 shows the resulting error broken down by part. This figure also compares the error in the inventive \\n method to the average distance between points marked by one MTurk worker and the average of the points marked by the \\n other two. As shown, this distance almost always exceeds the distance from points localized by the inventive approach to \\n the average of the points marked by humans. It is worth noting \\n that the eye points (9-18) are the most accurate, the nose and mouth points (19-29) are slightly worse, and the chin and eye \\n brows (1-8, 29) are least accurate. This trend is consistent between human and automatic labeling. FIGS. 3 and 6 show results on some representative images. \\n To highlight a few characteristics of these results, these images include non-frontal images including viewpoints \\n from below (FIG.3: Row 1, Col. 2 and FIG. 6: Row 2, Col. 2), difficult lighting (FIG. 6: Row 4, Col. 1), glasses (FIG. 6: Row', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 17}), Document(page_content='from below (FIG.3: Row 1, Col. 2 and FIG. 6: Row 2, Col. 2), difficult lighting (FIG. 6: Row 4, Col. 1), glasses (FIG. 6: Row \\n 1, Col. 5), sunglasses (FIG. 6: Row 2, Col. 4 and FIG. 6: Row 4, Col. 3), partial occlusion (FIG. 6: Row 2, Col. 5 by a pipe \\n and FIG. 6: Row 3, Col. 4 by hair), an artist drawing (FIG. 6: \\n Row 1, Col. 3), theatrical makeup (FIG. 6: Row 2, Col. 1), etc. \\n The localizer requires less than one second per fiducial on an \\n INTEL(R) Core i7 3.06 GHz machine; most of the time is spent evaluating the local detectors. \\n FIG.5 provides a comparison of the LFPW results from the \\n inventive method against those of the detector of Everingham \\n et al. At roughly 3% mean error rate, the results for the \\n inventive method are roughly twice as accurate as those of the prior art detector. \\n FIG. 6 shows a few examples of errors in the inventive \\n system. In Row 1, Cols. 2 and 5, local cues for the chin are \\n indistinct, and the chin is not localized exactly. Row 2, Col. 4 shows an example in which the lower lip is incorrectly local \\n ized. This can happen when the mouth is open and a row of \\n teeth are visible. It is believed that these errors can be prima \\n rily attributed to the local detectors and should be overcome by employing color-based representations that can more eas \\n ily distinguish between lips and teeth. In Row 4, Col. 1, the \\n left corner of the left eyebrow is too low, presumably due to \\n occlusion from the hair. \\n FIG. 7 illustrates results from the application of the inven \\n tive part localizer to the BioID faces. Results have been reported on this dataset by a number of authors. FIG. 8 shows \\n the cumulative error distribution of the me, error measure (mean error of 17 fiducials) defined by Cristinacce and Cootes, and compares the results of the inventive method to those reported by Cristinacce and Cootes, Milborrow and \\n Nicolls, Valstaret al., and Vukadinovic and Pantic. The results \\n of the inventive part localizer are similar to, but slightly better \\n than, those of Valstar et al., who have reported the best current \\n results on this dataset. It should be noted that training of the inventive system occurred using a very different dataset \\n (LFPW), and that locations of some fiducials were defined a bit differently. \\n FIG.9 shows the cumulative error distribution of the me, error measure for the inventive method applied to LFPW. Even though LFPW is a more challenging dataset, the cumu \\n lative error distribution curve on LFPW is almost identical to \\n the cumulative error distribution curve on BioID. (Note that the two figures have different scales along the x-axis.) FIG.9', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 17}), Document(page_content='US 9,275,273 B2 \\n 15 \\n also shows the cumulative error distribution when only the \\n local detectors are used and when locations are predicted \\n solely from the face box. While the local detectors are effec tive for most fiducial points, there is a clear benefit from using the consensus of global models. Many of the occluded fidu cial points are incorrectly located by the local detectors, as \\n evidenced by the slow climb toward 1.0 of the local detectors \\n CUV. \\n The inventive method provides a new approach to localiz ing parts in face images. The method utilizes a Bayesian \\n model that combines local detector outputs with a consensus of non-parametric global models for part locations, computed \\n from exemplars. The inventive parts localizer is accurate over a large range of real-world variations in pose, expression, lighting, makeup, and image quality, providing a significant \\n improvement over the limitations of existing approaches. \\n REFERENCES \\n 1. 1st Intl. Workshop on Parts and Attributes. 2010. 546 \\n 2. L. Bourdev and J. Malik. Poselets: body part detectors \\n trained using 3d human pose annotations. In IEEE Confer ence on Computer Vision and Pattern Recognition, page \\n 1365 1372, 2009. 546 \\n 3. M. Burl, T. Leung, and P. Perona. Face localization via shape statistics. In Workshop on Automatic Face and Ges \\n ture Recognition, 1995. 546 \\n 4. P. Campadelli, R. Lanzarotti, and G. Lipori. Automatic \\n facial feature extraction for face recognition. In Face Rec ognition. I-Tech Education and Publishing, 2007. 546 \\n 5. D. Cristinacce and T. Cootes. Feature detection and track \\n ing with constrained local models. In BMWC, pages 929 \\n 938, 2006. 546, 549, 550 \\n 6. D. Cristinacce, T. Cootes, and I. Scott. A multi-stage approach to facial feature detection. In BMWC, pages 231 \\n 240, 2004. 546 \\n 7. L. Ding and A. M. Martinez. Precise detailed detection of faces and facial features. In IEEE Computer Vision and Pattern Recognition (CVPR), 2008. 546 \\n 8. M. Eckhardt, I. Fasel, and J. Movellan. Towards practical \\n facial feature detection. Int. J. of Pattern Recognition and Artificial Intelligence, 23(3):379-400, 2009. 546 \\n 9. M. Everingham, J. Sivic, and A. Zisserman. “Hello! My \\n name is ... Buffy’ automatic naming of characters in TV \\n video. In BMVC, 2006. 546, 547, 550 \\n 10. N. Gourier, D. Hall, and J. L. Crowley. Facial features detection robust to pose, illumination and identity. In Int. Confon Systems, Man and Cybernetics, 2004. 546 11. L. Gu and T. Kanade. A generative shape regularization \\n model for robust face alignment. In European Conference \\n on Computer Vision (ECCV), pages 413-426, 2008. 546 \\n 12. E. Holden and R. Owens. Automatic facial point detec tion. In Asian ConfComputer Vision, pages 731-736, 2002. \\n 546 \\n 13. Jesorsky, K. J. Kirchberg, and R. W. Frischholz. Robust \\n face detection using the Hausdorff distance. In Confon \\n Audio-and Video-Based Biometric Person Authentication, pages 90-95. Springer, 2001. 549 \\n 14. N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. \\n Attribute and simile classifiers for face verification. In \\n IEEE International Conference on Computer Vision, 2009. \\n 546 \\n 15. B. Leibe, A. Ettlin, and B. Schiele. Learning semantic object parts for object categorization. Image and Vision \\n Computing, 26:15-26, 1998. 546 \\n 16. D. Lowe. Distinctive image features from scale-invariant keypoints. Intl. Journal of Computer Vision, 2003. 547 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n 17. S. Milborrow and F. Nicolls. Locating facial features with \\n an extended active shape model. In European Conf On \\n Computer Vision, pages 504-513, 2008. 546, 550 \\n 18. M. Reinders, R. W. C. Koch, and J. Gerbrands. Locating facial features in image sequences using neural networks. \\n In Confon Automatic Face and Gesture Recognition, pages \\n 230-235, 1997.546 19. J. M. Saragih, S. Lucey, and J. Cohn. Face alignment \\n through Subspace constrained mean-shifts. In Interna', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 18}), Document(page_content='In Confon Automatic Face and Gesture Recognition, pages \\n 230-235, 1997.546 19. J. M. Saragih, S. Lucey, and J. Cohn. Face alignment \\n through Subspace constrained mean-shifts. In Interna \\n tional Conference of Computer Vision (ICCV), September \\n 2009. 546 \\n 20. M. Valstar, B. Martinez, X. Binefa, and M. Pantic. Facial \\n point detection using boosted regression and graph models. \\n In IEEE Computer Vision and Pattern Recognition \\n (CVPR), pages 2729-2736, 2010. 546, 547, 550, 552 \\n 21. P. Viola and M. Jones. Robust real-time face detection. \\n Intl. Journal of Computer Vision, 57:137-154, 2004. 545, \\n 546 \\n 22. D. Vukadinovic and M. Pantic. Fully automatic facial \\n feature point detection using Gabor feature based boosted \\n classifiers. In Int. Confon Systems, Man and Cybernetics, \\n pages 1692-1698, 2005. 546, 547, 550 23. M.-H. Yang, D. J. Kriegman, and N. Ahuja. Detecting \\n faces in images: A Survey. IEEE Trans. On Pattern Analysis \\n and Machine Intelligence, 24(1):34-58, 2002. 545 \\n 24. C. Zhan, W. Li, P. Ogunbona, and F. Safaei. Real-time \\n facial feature point extraction. In Advances in multimedia information processing, pages 88-97. Springer-Verlag, \\n 2007. 546 \\n The invention claimed is: \\n 1. A system for localizing parts of an object in an input \\n image, the System comprising: \\n at least one computing device configured to: \\n train a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, \\n wherein each image exemplar is labeled with fiducial points corresponding to parts within the image, and \\n wherein each local detector generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images corresponding to a \\n likelihood that a desired part is located at the location within the training image: \\n generate a non-parametric model of the plurality of loca \\n tions of the fiducial points in each of at least a portion of the plurality of image exemplars, the non-parametric \\n model denoting locations of parts within each of the at least a portion of the plurality of image exemplars; \\n receive data corresponding to an input image; \\n apply the trained local detectors to the input image to generate detector scores for the input image; derive a Bayesian objective function for the input image \\n from the non-parametric model and detector scores by using an assumption that locations of the fiducial points \\n in each of the at least a portion of the plurality of image exemplars are represented within the non-parametric \\n model as hidden variables; and generate an output comprising locations of the fiducial \\n points within the object in the image based on results of the Bayesian objective function for the input image. \\n 2. The system of claim 1 wherein the object is a face. 3. The system of claim 1, wherein generating the output \\n comprising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image further comprises:', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 18}), Document(page_content='US 9,275,273 B2 \\n 17 \\n optimizing the Bayesian objective function to obtain a \\n consensus set of one or more global models for the \\n hidden variables that best fits the data corresponding to the image. \\n 4. The system of claim 3, wherein deriving the Bayesian objective function for the input image from the non-paramet \\n ric model and detector scores further comprises: Selecting a random image exemplar from the plurality of image exemplars; \\n Selecting from within the random image exemplar at least \\n two random fiducial points; applying a similarity transform to align the at least two \\n random fiducial points with peaks of the detector output; evaluating a fit of the random image exemplar and similar \\n ity transform to the data; and repeating the steps of selecting a random image exemplar, Selecting at least two random fiducial points, applying a similarity transform and evaluating a fit for a plurality of \\n iterations until a desired fit to the data is obtained. \\n 5. The system of claim 1, wherein the plurality of local detectors comprise sliding window detectors. 6. The system of claim 5, wherein the sliding window detectors comprise support vector machines. 7. The system of claim 5, wherein the sliding window detectors use features comprising scale invariant feature transform descriptors. 8. The system of claim 1, wherein generating the output comprises displaying on a monitor or printout the image of the object with markings indicating the fiducial points within the input image. 9. The system of claim 1, wherein generating the output comprises storing the output in a memory for further process ing by an image recognition system. \\n 10. The system of claim 3, wherein the consensus set contains only one global model for each of the hidden vari \\n ables. \\n 11. The system of claim 1, wherein the input image com prises features in addition to the object and further compris ing pre-processing the input image to select and extract an area within the input image which contains only the object. 12. A method for localizing parts of an object in an input image, the method comprising: training, using at least one processor, a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, wherein each image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detec tor generates a detector score when applied at one loca tion of a plurality of locations of fiducial points in the training images corresponding to a likelihood that a \\n desired part is located at the location within the training image; \\n generating, using the at least one processor, a non-paramet \\n ric model of the plurality of locations of the fiducial points in each of at least a portion of the plurality of image exemplars, the non-parametric model denoting 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 18 \\n locations of parts within each of the at least a portion of the plurality of image exemplars; receiving data corresponding to an input image: applying, using the at least one processor, the trained local detectors to the input image to generate detector scores for the input image: deriving, using the at least one processor, a Bayesian objec tive function for the input image from the non-paramet ric model and detector scores by using an assumption that locations of the fiducial points in each of the at least a portion of the plurality of image exemplars are repre sented within the non-parametric model as hidden vari \\n ables; and generating, using the at least one processor, an output com prising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image. \\n 13. The method as recited in claim 12, wherein the object is \\n a face.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 19}), Document(page_content='13. The method as recited in claim 12, wherein the object is \\n a face. \\n 14. The method as recited in claim 12, wherein generating the output comprising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image further comprises opti mizing the Bayesian objective function to obtain a consensus \\n set of one or more global models for the hidden variables that best fits the data corresponding to the image. 15. The method as recited in claim 14, whereinderiving the Bayesian objective function for the input image from the non-parametric model and detector scores further comprises: Selecting a random image exemplar from the plurality of image exemplars; \\n Selecting from within the random image exemplar at least \\n two random fiducial points: applying a similarity transform to align the at least two random fiducial points with peaks of the detector output; evaluating a fit of the random image exemplar and similar \\n ity transform to the data; and repeating the steps of selecting a random image exemplar, \\n selecting at least two random fiducial points, applying a similarity transform and evaluating a fit for a plurality of \\n iterations until a desired fit to the data is obtained. \\n 16. The method as recited inclaim 12, wherein the plurality of local detectors comprise sliding window detectors. 17. The method as recited in claim 16, wherein the sliding window detectors comprise Support vector machines. \\n 18. The method as recited in claim 16, wherein the sliding window detectors use features comprising scale invariant fea ture transform descriptors. \\n 19. The method as recited in claim 14, wherein the consen Sus set contains only one global model for each of the hidden \\n variables. \\n 20. The method as recited in claim 12, wherein the input image comprises features in addition to the object and further comprising pre-processing the input image to select and \\n extract an area within the input image which contains only the object.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 19})], [Document(page_content='(12) United States Patent \\n Cheng et al. USOO9432632B2 \\n US 9.432,632 B2 \\n Aug. 30, 2016 (10) Patent No.: \\n (45) Date of Patent: \\n (54) ADAPTIVE MULTI-MODAL INTEGRATED \\n BOMETRIC IDENTIFICATION AND \\n SURVELLANCE SYSTEMS \\n (71) Applicant: Proximex Corporation, Cupertino, CA \\n (US) \\n (72) Inventors: Ken P. Cheng, Saratoga, CA (US); \\n Edward Y. Chang, Santa Barbara, CA (US); Yuan-Fang Wang, Goleta, CA \\n (US) \\n (73) Assignee: Proximex Corporation, Cupertino, CA \\n (US) \\n Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 0 days. \\n (21) Appl. No.: 14/607,201 \\n (22) Filed: Jan. 28, 2015 \\n (65) Prior Publication Data \\n US 2015/O138332 A1 May 21, 2015 \\n Related U.S. Application Data \\n (60) Division of application No. 13/738,655, filed on Jan. \\n 10, 2013, now Pat. No. 8,976,237, and a continuation of application No. 13/101,149, filed on May 5, 2011, \\n now Pat. No. 8.373,753, and a division of application (*) Notice: \\n (Continued) \\n (51) Int. Cl. \\n H04N 7/8 (2006.01) \\n A6 IB I/00 (2006.01) \\n (Continued) \\n (52) U.S. Cl. \\n CPC H04N 7/18 (2013.01); G06K 9/00 (2013.01); G06K 9/00288 (2013.01); \\n (Continued) \\n (58) Field of Classification Search \\n USPC ........... 348/77; 340/506; 358/143, 147, 161, \\n 358/169; 707/4, 103: 382/103, 209, 276, 382/277, 289, 291, 293, 294, 295, 282,305, \\n 382/115, 107, 190 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 5,258,837 A \\n 5,473,369 A 11/1993 Gormley \\n 12, 1995 Abe \\n (Continued) \\n FOREIGN PATENT DOCUMENTS \\n WO 2007/044037 A1 4/2007 \\n OTHER PUBLICATIONS \\n PCT/US05/44656 International Search Report and Written Opinion, \\n Jun. 26, 2006. \\n (Continued) \\n Primary Examiner — Jerome Grant, II \\n (74) Attorney, Agent, or Firm — Dean D. Small: The Small \\n Patent Law Group, LLC. \\n (57) ABSTRACT \\n A Surveillance system is provided that includes at least one sensor disposed in a security area of a Surveillance region to sense an occurrence of a potential security breach event; a plurality of cameras is disposed in the Surveillance region; at \\n least one camera thereof has a view of the security area and can be configured to automatically gather biometric infor mation concerning at least one subject person in the vicinity of the security area in response to the sensing of a potential \\n security breach event; one or more other of the plurality of cameras can be configured to search for the at least one Subject person; a processing system is programmed to \\n produce a dossier corresponding to the at least one subject \\n person to match biometric information of one or more persons captured by one or more of the other cameras with corresponding biometric information in the dossier. \\n 35 Claims, 8 Drawing Sheets \\n Apply Monitoring Rule on Security Area \\n configure device and Security Area specific parameters (Door, \\n restricted area, etc.) \\n Environment \\n (Map) Admin \\n Security Area \\n Admin display a sist of available Monitoring rules to apply to security area (Based on device characteristics Display a list of available devices to selecticeselect for applying Monitoring rule Select Security Area to configure Monitoring rule \\n Monitoring rule (Security Function) \\n Admin Configure Schedule & \\n Eable Rule to start monitoring configured \\n area Setect Cotstation Monitoring rule', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 0}), Document(page_content='US 9,432.632 B2 \\n Page 2 \\n (60) \\n (51) \\n (52) \\n (56) Related U.S. Application Data \\n No. 1 1/231,353, filed on Sep. 19, 2005, now Pat. No. \\n 7.956,890. \\n Provisional application No. 60/610,998, filed on Sep. \\n 17, 2004. \\n Int. C. \\n G06K 9/00 (2006.01) \\n G06K 9/62 (2006.01) \\n G08B I3/196 (2006.01) \\n G06T 7/40 (2006.01) \\n U.S. C. \\n CPC ..... G06K 9/00771 (2013.01); G06K 9/00885 \\n (2013.01); G06K 9/00892 (2013.01); G06K 9/6293 (2013.01); G06T 7/408 (2013.01); \\n G08B 13/196 (2013.01); H04N 7/181 (2013.01) \\n References Cited \\n U.S. PATENT DOCUMENTS \\n 5,479,574 A 12/1995 Glier et al. \\n 5,701,398 A 12/1997 Glier et al. \\n 5,769,074 A 6, 1998 Barnhill et al. \\n 5,835,901 A 11/1998 Duvoisin et al. \\n 5,838,465. A 11/1998 Satou et al. \\n 5,912,980 A 6, 1999 Hunke \\n 6,008,912 A 12/1999 Sato et al. \\n 6,072,496 A 6, 2000 Guenter et al. \\n 6,157.469 A 12/2000 Mestha \\n 6,248,063 B1 6, 2001 Barnhill et al. \\n 6,306,087 B1 10/2001 Barnhill et al. 6,335,985 B1 1/2002 Sambonsugi et al. \\n 6,404,900 B1 6/2002 Qian et al. 6,408.404 B1 6/2002 Ladwig \\n 6,591.224 B1 7/2003 Sullivan et al. \\n 6,604,093 B1 8, 2003 Etzion et al. \\n 6,609, 198 B1 8, 2003 Wood et al. \\n 6,625,315 B2 9/2003 Laumeyer et al. \\n 6,628,829 B1 9, 2003 Chasen \\n 6,697,103 B1 2, 2004 Fernandez et al. \\n 6,707.487 B1 3/2004 Aman et al. \\n 6,711,587 B1 3, 2004 Dufaux \\n 6,757,668 B1 6, 2004 Goebel et al. \\n 6,906,709 B1 6, 2005 Larkin et al. \\n 6,909,745 B1 6, 2005 Puri et al. \\n 6,944,319 B1 9/2005 Huang et al. \\n 6,963,899 B1 1 1/2005 Fernandez et al. 6,968,006 B1 11/2005 Puri et al. 6,970,513 B1 11/2005 Puri et al. 6,970,582 B2 11/2005 Langley 6,976.207 B1 12/2005 Rujan et al. \\n 7,023.913 B1 4/2006 Monroe et al. \\n 7,028,034 B2 4/2006 Wesinger et al. \\n 7,058,204 B2 6, 2006 Hildreth et al. \\n 7,116,353 B2 10/2006 Hobson et al. 7,127.464 B2 10/2006 Wesinger et al. \\n 7,136,524 B1 1 1/2006 Goh et al. \\n 7,187,783 B2 3/2007 Moon et al. \\n 7,196,722 B2 3/2007 White et al. \\n 7,213,174 B2 5/2007 Dahlquist et al. \\n 7,242.423 B2 7, 2007 Lin \\n 7,242,810 B2 7/2007 Chang 7,269,591 B2 9/2007 Wesinger et al. \\n 7,277,485 B1 10/2007 Puri 7.286,687 B2 10/2007 Lindwurm et al. 7,295.228 B2 11/2007 Roberts et al. 7,319,796 B1 1/2008 Sharp \\n 7.366,174 B2 4/2008 MacFaden et al. \\n 7,382.249 B2 6, 2008 Fancella \\n 7,384,590 B2 6/2008 Kelly et al. \\n 7,385,590 B2 6, 2008 Millar et al. 7,412,112 B2 * 8/2008 Yamasaki .......... GO6K9/00785 \\n 345,626 \\n 7,456,727 B2 11/2008 Pinter et al. 7,522, 186 B2 4/2009 Arpa et al. \\n 7,525,576 B2 4/2009 Kannermark et al. \\n 7,596,240 B2 * 9/2009 Ito .................... G08B 13, 196O2 \\n 382,103 \\n 7,746,380 B2 6/2010 Maruya et al. \\n 7,777,783 B1 8, 2010 Chin et al. \\n 8,373,753 B2 * 2/2013 Cheng ................ GO6K9/00771 \\n 348/143 \\n 2001/0048765 A1 12/2001 Yi et al. \\n 2002fOO97322 A1 7/2002 Monroe et al. \\n 2002/0138768 A1 9/2002 Murakami et al. \\n 2002/O1901 19 A1 12/2002 Huffman \\n 2003/0107653 A1 6/2003 Utsumi et al. \\n 2003. O1699.08 A1 9, 2003 Kim et al. \\n 2003/0202102 A1 10, 2003 Shiota et al. \\n 2004/0001149 A1 1/2004 Smith \\n 2004/0022442 A1 2/2004 Kim \\n 2004/0081338 A1 4/2004 Takenaka \\n 2004/0100563 A1 5, 2004 Sablak et al. \\n 2004/01 17638 A1* 6/2004 Monroe ............. GO6K9/00221 \\n T13, 186 \\n 2004/O136574 A1 7/2004 Kozakaya et al. \\n 2005/0052532 A1 3/2005 ElooZ. et al. \\n 2005, 0100209 A1 5/2005 Lewis et al. \\n 2005, 0132414 A1 6/2005 Bentley et al. \\n 2005/022281.0 A1 10, 2005 Jakobson et al. \\n 2005/0265607 A1 12/2005 Chang \\n 2006, OO17807 A1 1/2006 Lee et al. \\n 2006/O112039 A1 5/2006 Wang et al. \\n 2006/0203090 A1 9/2006 Wang et al. \\n 2006/0221 184 A1 10, 2006 Vallone et al. \\n 2006/0279628 A1 12/2006 Fleming 2006/0284978 A1 12/2006 Girgensohn et al. \\n 2007, 0146484 A1 6/2007 Horton et al. \\n 2007. O154088 A1 7/2007 Goh et al. \\n 2008.00684-64 A1 3/2008 Kitagawa et al. \\n 2008.OO795.54 A1 4/2008 Boice \\n 2008, OO887O6 A1 4/2008 Girgensohn et al. \\n 2008/0106597 A1 5/2008 Amini et al. \\n 2008. O130949 A1 6/2008 Ivanov et al. \\n 2008, 0218590 A1 9, 2008 Park et al.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 1}), Document(page_content='2008.OO795.54 A1 4/2008 Boice \\n 2008, OO887O6 A1 4/2008 Girgensohn et al. \\n 2008/0106597 A1 5/2008 Amini et al. \\n 2008. O130949 A1 6/2008 Ivanov et al. \\n 2008, 0218590 A1 9, 2008 Park et al. \\n 2008/0294.588 A1 11/2008 Morris et al. \\n 2009/0322873 A1* 12/2009 Reilly ................ GOS 7,411 \\n 348/143 \\n 2010.0002082 A1 1/2010 Buehler et al. \\n OTHER PUBLICATIONS \\n PCT/US05/43808 International Search Report and Written Opinion, \\n Oct. 10, 2007. \\n PCT/US05/33378 International Search Report and Written Opinion, \\n Apr. 26, 2006. \\n PCT/US05/33750 International Search Report and Written Opinion, \\n May 2, 2007. \\n PCT/US05/16961 International Search Report and Written Opinion, \\n Oct. 17, 2006. \\n VidSys. Inc. Complaint filed in the US District Court for the Eastern \\n district of Virginia on Oct. 19, 2010. \\n Redstone Integrated Solutions Documents, 2003, Dec. 2003. \\n 95/001,525 Reexamination Request for 7,777,783, filed Jan. 21, \\n 2011. \\n Goh et al., “Robust Perceptual Color Identification\\' U.S. Appl. No. \\n 11/229,091, filed Sep. 16, 2005. \\n Dec. 1997, Belhumeur, A., et al. (1997), “Eigenfaces vs. Fisherfaces: recognition using class specific linear projection\\'. \\n IEEE Transactions on Pattern Analysis and Machine Intelligence \\n 19(7): 711-720. \\n Brunei, R. and D. Falavigna (1995), \"Person Identification using \\n multiple clues.” IEEE Transactions on Pattern Analysis and \\n Machine Intelligence 17(10): 955-966, Dec. 1995.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 1}), Document(page_content='US 9,432.632 B2 \\n Page 3 \\n (56) References Cited \\n OTHER PUBLICATIONS \\n Brunelli, R., et al. (1995), “Automatic Person Recognition by Using Acoustic and Geometric Features\\'. Machine Vision and Applica \\n tions 8:317-325, Dec. 1995. Hong, Lin and Anil K. Jain. (1998). “Integrating faces and finger prints for personal identification.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20(12): 1295-1307. International Search Report mailed on Apr. 2006 for PCT Patent Application No. PCT/US05/33378 filed on Sep. 19, 2005, one page. Jain, A.K., et al., (1997). \"On-Line fingerprint Verification\\', IEEE Transactions on Pattern Analysis and Machine Intelligence archive 19{4): 302-314, Dec. 1997. Kittler, J. et al. (1998). “On Combining classifiers\\', IEEE Trans actions on Pattern Analysis and Machine Intelligence 20 (3): \\n 226-239, Dec. 1998. Lu X et al. (2003). “Combing Classifiers for face recognition\\'. \\n IEEE International Conference on Multimedia Systems and Expo, \\n Baltimore, MD, Jul. Dec. 2003. \\n Maio, D. et al. (2002). “FVC2000: fingerprint verification compe \\n tition\\', IEEE Transactions on Pattern Analysis and Machine Intel \\n ligence 24(3): 402-412, Dec. 2002. \\n Phillips, P.J. et al. (2000). “The FERET evaluation methodology for \\n face-recognition algorithms\\'. IEEE Transactions on Pattern Analy \\n sis and Machine Intelligence 22(10): 1090-1104, Dec. 2000. \\n Senior, A. (2001). A combination fingerprint classifier. IEEE Trans \\n actions on Pattern Analysis and Machine Intelligence 23(10): 1165 \\n 1174, Dec. 2001. \\n Turk, A. and A. Pentland. (1991). “Eigenfaces for Recognition\\'. \\n Journal of Cognitive Neuroscience 3 (1): 71-86. \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 2}), Document(page_content='US 9,432,632 B2 Sheet 2 of 8 Aug. 30, 2016 U.S. Patent \\n(u?upy Queuuuou?AuE) Sde N eun6??uOO', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 4}), Document(page_content='US 9,432,632 B2 Sheet 3 of 8 Aug. 30, 2016 U.S. Patent \\neeuw K??unoes uo a[nx] 6u?uo??uOIN ÁIddy', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 5}), Document(page_content='US 9,432,632 B2 Sheet 4 of 8 Aug. 30, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 6}), Document(page_content='US 9,432,632 B2 Sheet 6 of 8 Aug. 30, 2016 U.S. Patent \\n ••?: ( ) … Jessam, \\n ****', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 8}), Document(page_content='US 9,432,632 B2 Sheet 7 of 8 Aug. 30, 2016 U.S. Patent \\n Z ‘61-I \\n–', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 9}), Document(page_content='US 9,432,632 B2 Sheet 8 of 8 Aug. 30, 2016 U.S. Patent \\n ; - & ) {}} }} \\n K', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 10}), Document(page_content='US 9,432,632 B2 \\n 1. \\n ADAPTIVE MULT-MODAL INTEGRATED \\n BIOMETRIC IDENTIFICATION AND \\n SURVELLANCE SYSTEMS \\n CROSS REFERENCE TO RELATED \\n APPLICATIONS \\n This application is a divisional application and claims \\n priority to and the benefit of the filing date of U.S. continu ation patent application Ser. No. 13/738,655, filed Jan. 10, \\n 2013, and entitled \"Adaptive Multi-Modal Integrated Bio \\n metric Identification Detection and Surveillance Systems.” which is a continuation patent application of Ser. No. \\n 13/101,149, filed May 5, 2011, now U.S. Pat. No. 8,373,753, and entitled \"Adaptive Multi-Modal Integrated Biometric \\n Identification Detection and Surveillance Systems\\', which is a divisional patent application of U.S. patent application Ser. \\n No. 1 1/231,353 filed on Sep. 19, 2005, now U.S. Pat. No. 7.956,890 and entitled “Adaptive Multi-Modal Integrated \\n Biometric Identification Detection and Surveillance Sys tems’ which claims the benefit of U.S. Provisional Appli \\n cation No. 60/610,998, filed on Sep. 17, 2004, and entitled “Adaptive Multi-Modal Integrated Biometric Identification Detection Systems, all of which are hereby incorporated by \\n reference in their entirety as if fully set forth herein. \\n BACKGROUND OF THE INVENTION \\n 1. Field of the Invention \\n The invention relates in general to biometric identifica tion, and more particularly, to a Surveillance system using \\n biometric identification. \\n 2. Brief Description of the Related Art \\n The state of the art of applying biometric technologies to authenticate and positively determine the identity of a per \\n son is still faced with several technical challenges. Specifi cally, the challenges can be categories into two aspects: data \\n acquisition and data matching. Data acquisition deals with \\n acquiring biometric data from individuals. Data matching \\n deals with matching biometric data both quickly and accu rately. These challenges can be explained by a port-entry \\n scenario. In Such a setting, it is difficult to obtain certain \\n biometric data such as DNA and voice samples of individu als. For biometric data that can be more easily acquired. Such as face images and fingerprints, the acquired data quality can \\n vary greatly depending on acquisition devices, environmen \\n tal factors (e.g., lighting condition), and individual corpo \\n ration. Tradeoffs exist between intrusiveness of data collec \\n tion, data collection speed, and data quality. \\n Once after the needed data have been acquired, conduct ing matching in a very large database can be very time consuming. It goes without saying that unless a system can \\n acquire and match data both timely and accurately, the system is practically useless in improving public security, \\n where the inconvenience due to the intrusive data-acquisi tion process and the time-consuming matching process \\n ought to be minimized. \\n A biometric system typically aims to address either one of the following issues: 1) Authentication: is the person the one \\n he/she claims to be? 2) Recognition: who a person is? In the first case, data acquisition is Voluntary and matching is done \\n in a one-to-one fashion—matching the acquired data with \\n the data stored on an ID card or in a database. In the second \\n case, individuals may not be cooperating, and the system \\n must conduct searches in a very large repository. \\n The prior art in biometric can be discussed in two parts: \\n single-modal Solutions and multi-modal solutions. Several 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n systems have been built to use one of the following single \\n modal: facial data, voice, fingerprint, iris or DNA. The effectiveness of these single-modal approaches can be evalu \\n ated in three metrics: the degree of intrusiveness, speed and accuracy. From the perspective of a user, acquiring face \\n modal can be the most noninvasive method, when video \\n cameras are mounted in the distance. However, the same convenience nature often compromises data quality. An', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 11}), Document(page_content='modal can be the most noninvasive method, when video \\n cameras are mounted in the distance. However, the same convenience nature often compromises data quality. An \\n intrusive face acquisition method is to acquire frontal face \\n features, which requires corporation from individuals. Voice \\n is another popular modal. However, traditional Voice-rec ognition fails miserable when Voice samples of multiple \\n individuals are simultaneously captured or when back \\n ground noise exists. Even when the acquired Voice data can be \"pure.” existing signal processing and matching tech \\n niques can hardly achieve recognition accuracy of more than \\n 50%. The next popular modal is fingerprint, which can achieve much higher recognition accuracy at the expense of \\n intrusive data acquisition and time-consuming data match \\n ing. Finally, DNA is by far the most accurate recognition technique, and the accompanying inconvenience in data acquisition and the computational complexity are both exceedingly high. Summarizing the single model approach, \\n non-intrusive data-acquisition techniques tend to Suffer from low recognition accuracy, and intrusive data-acquisition \\n techniques tend to Suffer from long computational time \\n As to multimodal techniques, there have been several prior art United States patents and patent applications dis \\n close techniques. However, as will be further discussed \\n below, these disclosures do not provide scalable means to \\n deal with tradeoffs between non-intrusiveness, speed and accuracy requirements. These disclosures may fix their system configuration for a particular application, and cannot \\n adapt to queries of different requirements and of different applications. \\n Wood et al. disclose in U.S. Pat. No. 6,609,198 a security architecture using the information provided in a single sign-on in multiple information resources. Instead of using \\n a single authentication scheme for all information resources, the security architecture associates trust-level requirements \\n with information resources. Authentication schemes (e.g., those based on passwords, certificates, biometric techniques, \\n Smart cards, etc.) are employed depending on the trust-level \\n requirement(s) of an information resource (or information \\n resources) to be accessed. Once credentials have been obtained for an entity and the entity has been authenticated to a given trust level, access is granted, without the need for \\n further credentials and authentication, to information \\n resources for which the authenticated trust level is sufficient. \\n The security architecture also allows upgrade of credentials \\n for a given session. The credential levels and upgrade \\n scheme may be useful for a log-on session; however, such architecture and method of operations do not provide a resolution for high speed and high accuracy applications \\n Such as passenger security check in an airport. \\n Sullivan et al. disclose in U.S. Pat. No. 6,591,224 a \\n method and apparatus for providing a standardized measure \\n of accuracy of each biometric device in a biometric identity authentication system having multiple users. A statistical \\n database includes continually updated values of false accep \\n tance rate and false rejection rate for each combination of \\n user, biometric device and biometric device comparison \\n score. False acceptance rate data are accumulated each time a user Successfully accesses the system, by comparing the \\n user\\'s currently obtained biometric data with stored tem \\n plates of all other users of the same device. Each user is treated as an “impostor with respect to the other users, and', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 11}), Document(page_content=\"US 9,432,632 B2 \\n 3 \\n the probability of an impostor's obtaining each possible \\n comparison score is computed with accumulated data each \\n time a successful access is made to the system. The statis \\n tical database also contains a false rejection rate, accumu lated during a test phase, for each combination of user, \\n biometric device and biometric device comparison score. By \\n utilizing a biometric score normalizer, Sullivan's method and apparatus may be useful for improving the accuracy of \\n a biometric device through acquiring more training data. \\n Murakami et al. disclose in U.S. Pre-Grant Publication \\n 2002-0138768 entitled “Method for biometric authentica \\n tion through layering biometric traits, a portable biometric authentication system having a single technology for mea Suring multiple, varied biological traits to provide individual \\n authentication based on a combination of biological traits. At least one of these biometric traits is a live physiological \\n trait, such as a heartbeat waveform, that is Substantially— but not necessarily completely unique to the population of \\n individuals. Preferably, at least one of the identifying aspects \\n of the biological traits is derived from a measurement taken by reflecting light off the subdermal layers of skin tissue. \\n The Murakami et al. approach is limited by the more \\n intrusive measurement techniques to obtain data such as \\n heartbeat waveform and reflecting light off the subdermal \\n layers of skin tissue. These data are not immediately avail able in a typical security check situation to compare with the \\n biometric data, e.g., heart beat waveforms and reflection light from subdermal layers from the skin of a targeted \\n searching object. Furthermore, the determination or the filtering of persons’ identity may be too time consuming and \\n neither appropriate for nor adaptive to real time applications. \\n Langley discloses in U.S. Pre-Grant Publication 2002 0126881, entitled “Method and system for identity verifi cation using multiple simultaneously scanned biometric images, a method to improve accuracy and speed of bio \\n metric identity verification process by use of multiple simul \\n taneous scans of biometric features of a user, such as multiple fingerprints, using multiple scanners of Smaller size \\n than would be needed to accommodate all of the fingerprints in a single Scanner, and using multiple parallel processors, or a single higher speed processor, to process the fingerprint \\n data more efficiently. Obtaining biometric data from mul \\n tiple user features by use of multiple scanners increases \\n verification accuracy, but without the higher cost and slower processing speed that would be incurred if a single large \\n scanner were to be used for improved accuracy. The meth ods according to Langley may provide the advantages of \\n speed and accuracy improvements. However, the nature of requiring multiple scans makes data acquisition time-con \\n Suming and intrusive. \\n On the academia side, much research effort has been geared toward analyzing data from individual biometric \\n channels (e.g., voice, face, fingerprint, please see the refer ence list for a partial list), less emphasis has been placed on comparing the performance of different approaches or \\n combing information from multiple biometric channels to \\n improve identification. Some notable exceptions are dis \\n cussed below. In Hong Lin, Jain A. K., Integrating faces and fingerprints for personal identification, IEEE Transactions \\n on Pattern Analysis and Machine Intelligence, Vol. 20, No. \\n 12, December 1998, pp. 1295-1307, the authors report an \\n automated person identification system that combines face and fingerprint information. The face recognition method \\n employed is the traditional eigen face approach, M. Turk and A. Pentland, Eigenfaces for Recognition, J. Cognitive Neu \\n roscience Vol. 3, No. 1, 1991, pp. 71-96, which computes a \\n set of orthonormal bases (eigen faces) of the database 10 \\n 15 \\n 25 \\n 30 \\n 35\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 12}), Document(page_content=\"roscience Vol. 3, No. 1, 1991, pp. 71-96, which computes a \\n set of orthonormal bases (eigen faces) of the database 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n images using the principal component analysis. Face images \\n are then approximated by their projection onto the ortho \\n normal Eigen face bases, and compared using Euclidean \\n distances. For fingerprint, the authors extend their previous \\n work, Jain, A. K.; Lin Hong; Bolle, R.; On-line fingerprint \\n verification, Pattern Analysis and Machine Intelligence, Vol. \\n 19, No. 4, April 1997, pp. 302-314, to extract minutiaes from fingerprint images. They then align two fingerprint images \\n by computing the transformation (translation and rotation) \\n between them. Minutiaes are strung together into a string representation and a dynamic programming-based algorithm \\n is used to compute the minimum edit distance between the \\n two input fingerprint strings. Decision fusion is achieved by \\n cross validation of the top matches identified by the two \\n modules, with matching results weighed by their confidence \\n or accuracy levels. The performance of the system is vali \\n dated on a database of about 640 face and 640 fingerprint images. \\n In Phillips, Henson Moon; Rive, S E A.; Russ. The FERRET evaluation methodology for face-recognition algo \\n rithms, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, No. 10, October 2000, pp. 1090-1104, the Michigan State University research group extends their \\n information fusion framework to include more modalities. \\n In particular, images of a Subject's right hand were captured, \\n and fourteen features comprising the lengths of the fingers, \\n widths of the fingers, and widths of the palm at various \\n locations of the hand. Euclidean distance metric was used to \\n compare feature vectors. Simple Sum rules, decision tree and \\n linear discriminant function are used for classification. It is \\n observed that a personal ID system using three modules \\n outperforms that uses only two of the three modules. While this is an interesting experiment, the data set used is Small \\n and there is no accepted universal standard in using hand \\n images in biometrics. In R. Brunelli, D. Falavigna, T. Poggio and L. Stringa, \\n Automatic Person Recognition by Using Acoustic and Geo \\n metric Features, Machine Vision and Applications 1995, Vol.8 pp. 317-325, an automated person recognition system using voice and face signatures is presented. The speaker recognition Subsystem utilizes acoustic parameters (log \\n energy outputs and their first-order time derivatives from 24 triangular band-pass filters) computed from the spectrum of \\n short-time windows of the speech signal. The face recogni tion Subsystem is based on geometric data represented by a \\n vector describing discriminant facial features such as posi \\n tions and widths of the nose and mouth, chin shape, thick ness and shape of the eyebrows, etc. The system captures \\n static images of the test Subjects and the test Subjects are also \\n asked to utter ten digits from Zero to nine for use in the speaker ID Subsystem. Each Subsystem then computes the \\n distances of the test Subject's speech and face signatures \\n with those stored in the databases. Decisions from the two \\n ID modules are combined by computing a joint matching \\n score that is the Sum of the two individual matching scores, weighted by the corresponding variance. Experimental \\n results show that integration of visual and acoustic infor mation enhances both performance and reliability of the separate systems. The above system was later improved \\n upon in Brunelli, R.; Falavigna, D., Person identification using multiple cues, IEEE Transactions on Pattern Analysis \\n and Machine Intelligence, Vol. 17, No. 10, October 1995, \\n pp. 955-966, where multiple classifiers are used in the face recognition Subsystems, and the matching score normaliza \\n tion process is made more robust using robust statistical \\n methods.\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 12}), Document(page_content='US 9,432,632 B2 \\n 5 \\n In Kittler, J.; Hatef, M.; Duin, R. P. W.; Matas, J., On combining classifiers, IEEE Transactions on Pattern Analy \\n sis and Machine Intelligence, Vol. 20, No. 3, March 1998, pp. 226-239, a performance study of various ensemble \\n classification scheme is presented. It is shown that many existing decision aggregation rules are actually simplifica \\n tions based on the more general Bayesian rule. The authors compare the performance of different decision aggregation \\n rules (max, min, median, and majority Voting rule) by \\n performing an experiment in biometrics. Three modules are used: frontal faces, face profiles, and Voiceprints. Simple \\n correlation-based and distance-based matching is performed on frontal faces and face profiles, respectively, by finding a \\n geometric transformation that minimizes the differences in intensity. It is shown that a simple aggregation Scheme by \\n Summing the results from individual classifiers actually perform the best. \\n In Lu X; Wang Y; and Jain A, Combing classifiers for face \\n recognition, IEEE International Conference on Multimedia Systems and Expo, Baltimore, Md., July 2003, three well known appearance-based face recognition methods, namely \\n PCA, M. Turk and A. Pentland, Eigenfaces for Recognition, \\n J. Cognitive Neuroscience Vol. 3, No. 1, 1991, pp. 71-96, \\n ICA, and LDA, Belhumeur, P. N.: Hespanha, J. P.; Krieg man, D. J. Eigenfaces vs. Fisherfaces: recognition using \\n class specific linear projection, IEEE Transactions on Pat tern Analysis and Machine Intelligence, Vol. 19, No. 7, July \\n 1997, pp. 711-720, are used for face image classification. \\n Two combination strategies, the sum rule and RBF network, are used to integrate the outputs from these methods. Experi \\n mental results show that while individual methods achieve \\n recognition rates between 80% and 88%, the ensemble classifier boosts the performance to 90%, using either the \\n sum rule or RBF network. In Senior, A., A combination fingerprint classifier, IEEE Transactions on Pattern Analysis \\n and Machine Intelligence, Vol. 23, No. 10, October 2001, \\n pp. 1165-1174, a similar multi-classifier scheme, this time for fingerprint classification, is proposed. Hidden Markov \\n Models and decision trees are used to recognize ridge structures of the fingerprint. The accuracy of the combina \\n tion classifier is shown to be higher than that of two state-of-the-art systems tested under the same condition. These studies represent encouraging results that validate our \\n multi-modal approach, though only a single biometric chan \\n nel, either face or fingerprint, not a combination of biometric \\n channels, is used in these studies. Maio, D.; Maltoni, D.; Cappelli, R.; Wayman, J. L.; Jain, \\n A. K., FVC2000: fingerprint verification competition, IEEE \\n Transactions on Pattern Analysis and Machine Intelligence, \\n Vol. 24, No. 3, March 2002, pp. 402-412, documents a fingerprint verification competition that was carried out in \\n conjunction with the International Conference on Pattern \\n Recognition (ICPR) in 2000 (a similar contest was held \\n again in 2002). The aim is to take the first step towards the \\n establishment of a common basis to better understand the \\n state-of-the-art and what can be expected from the finger print technology in the future. Over ten participants, includ \\n ing entries from both academia and industry, took part. Four \\n different databases, two created with optical sensors, one with a capacitive sensor, and one synthesized, were used in \\n the validation. Both the enrollment error (if a training image \\n can be ingested into the database or not) and the matching \\n error (if a test image can be assigned the correct label or not) \\n and the average time of enrollment and matching are docu \\n mented. \\n A study, that is similar in spirit but compares the perfor mance of face recognition algorithms, is reported in Phillips, 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 13}), Document(page_content='mented. \\n A study, that is similar in spirit but compares the perfor mance of face recognition algorithms, is reported in Phillips, 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n P. J.; Hyeonjoon Moon; Rizvi, S. A.; Rauss, P. J., The FERET evaluation methodology for face-recognition algo \\n rithms, IEEE Transactions on Pattern Analysis and Machine \\n Intelligence, Vol. 22, No. 10, October 2000, pp. 1090-1104. \\n A subset of the Feret database (a gallery of over 3000 \\n images) was used in the study. Ten different algorithms, \\n using a wide variety of techniques, such as PCA and Fischer \\n discriminant, were tested. Cumulative matching scores as a \\n function of matching ranks in the database are tabulated and used to compare the performance of different algorithms. \\n This study was repeated three times, in August 1994, March \\n 1995, and July 1996. What is significant about this study is \\n that the performance of the face recognition algorithms \\n improved over the three tests, while the test condition \\n became more challenging (with increasingly more images in \\n the test datasets). \\n As can be seen from the above brief survey, multi-modal \\n biometrics holds a lot of promise. It is likely that much more \\n accurate classification results can be obtained by intelli gently fusing the results from multiple biometric channels given performance requirements. While it is important to \\n keep on improving the accuracy and applicability of indi \\n vidual biometric sensors and recognizers, the performance \\n of a biometric system can be boosted significantly by judiciously and intelligently employing and combining mul \\n tiple biometric channels. While there have seen significant research activities in single- and multi-channel biometry over the past decade, the \\n state-of-the-art is still wanting in terms of speed and accu \\n racy. Therefore, a need still exists in the art to provide new and improved methods and system configurations to \\n increase the speed and accuracy of biometric identity veri \\n fication and determinations such that the above-mentioned \\n difficulties and limitations may be resolved. The present \\n invention meets this need. \\n SUMMARY \\n One embodiment of the invention provides a novel sur \\n veillance method. An event sensor Such as, a camera, \\n chemical sensor, motion detector, unauthorized door access \\n sensor, for example, is disposed to sense an occurrence of a \\n potential security breach event. A camera with a view of the \\n area in which an event is sensed gathers biometric informa tion concerning a subject person in the vicinity of the event \\n at about the time the event is sensed. A subject dossier is produced containing biometric information relating to the \\n subject person sensed by the camera with the view of the \\n area. Biometric information of persons captured on one or \\n more other Surveillance cameras in the general vicinity of the event is matched against corresponding biometric infor \\n mation in the Subject dossier. \\n Another embodiment of the invention provides a new Surveillance system. A sensor is disposed in a Surveillance \\n region to sense an occurrence of a security breach event. The \\n system includes a plurality of cameras. At least one camera \\n of the plurality has a view of the security area and can be configured to automatically gather biometric information \\n concerning a Subject person in the vicinity of an area where \\n the event occurred in response to the sensing of the event. \\n One or more of the plurality of cameras can be configured to search for the subject person. The surveillance system also includes a processing system which can be programmed to produce a subject dossier corresponding to the Subject person. The processing system also can be programmed to \\n match biometric information of one or more persons cap', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 13}), Document(page_content='US 9,432,632 B2 \\n 7 \\n tured by one or more of the cameras with corresponding \\n biometric information in the subject dossier. \\n These and other features and advantages of the invention \\n sill be apparent from the following description of embodi \\n ments thereof in conjunction with the drawings. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 is an illustrative showing a map of an airport passenger terminal and its immediate vicinity protected by a \\n surveillance system of one embodiment of the invention and also showing several pop-up views relating to event alerts in \\n accordance With the embodiment. \\n FIG. 2 is another view of the map of FIG. 1 showing Zoom to detail maps of different portions of the overall passenger \\n terminal map. FIG. 3 is an illustrative drawing of example security areas \\n within the surveillance region of FIGS. 1-2 outfitted with \\n event SensOrS. \\n FIG. 4 is an illustrative block level hardware diagram of \\n a Surveillance system in accordance with an embodiment of \\n the invention. \\n FIG. 5 is an illustrative block diagram level drawing of a \\n system architecture of an embodiment of the invention that incorporates the system hardware of FIG. 4. FIG. 6 is an illustrative flow diagram showing gathering \\n and conversion of facial feature data to a facial feature signature. \\n FIG. 7 is an illustrative flow diagram showing gathering \\n and conversion of fingerprint feature data to a fingerprint signature. \\n FIG. 8 is an illustrative flow diagram showing gathering \\n and conversion of DNA data to a DNA signature. One embodiment of the invention may employ a DNA fingerprint \\n for identification purposes. \\n DETAILED DESCRIPTION OF THE \\n INVENTION \\n The following description is presented to enable any \\n person skilled in the art to make and use the invention, and is provided in the context of particular applications and their \\n requirements. Various modifications to the preferred \\n embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied \\n to other embodiments and applications without departing \\n from the spirit and scope of the invention. Moreover, in the following description, numerous details are set forth for the purpose of explanation. However, one of ordinary skill in the \\n art will realize that the invention can be practiced without \\n the use of those specific details. In other instances, well known structures and devices are shown in block diagram \\n from in order not, to obscure the description of the invention with unnecessary detail. Thus, the present invention is not \\n intended to be limited to the embodiments shown, but is to be accorded the widest scope consistent with the principles \\n and features disclosed herein. \\n System Overview \\n One embodiment of the invention involves an intelligent \\n Surveillance system. A plurality of cameras, Some with and \\n some without overlapping fields of view, are distributed throughout a Surveillance region. Intelligent computer soft \\n ware based agents process information captured by one or \\n more of the cameras to produce a subject dossier indicative of the identity of a person whose images have been captured \\n by one or more of the cameras. Information for a subject dossier also may be gathered through other modalities Such 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n as voice recognition, iris Scan, or fingerprint, for example. \\n The system includes multiple event sensors, which may \\n include the cameras, chemical sensors, infrared sensors, or \\n other security alarm sensors that trigger an alert, upon \\n sensing an occurrence of a predetermined category of event \\n requiring heightened vigilance. For example, an alarm may \\n be triggered when a locked door is opened without proper \\n access permission or when an unauthorized person enters a \\n restricted area or when a vehicle is parked in a restricted area. More specifically, a Subject dossier is produced for', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 14}), Document(page_content=\"access permission or when an unauthorized person enters a \\n restricted area or when a vehicle is parked in a restricted area. More specifically, a Subject dossier is produced for \\n individuals in the vicinity of the location of an alarm \\n triggering event. For instance, a Subject dossier may be \\n produced for persons captured in a video camera image at or \\n near a door in the Surveillance region at about the time when \\n an unauthorized opening of the door is detected by an event \\n SSO. \\n A subject dossier may include soft biometric information, \\n also referred to as “soft’ features such as clothing color, \\n estimated height and weight. A subject dossier also may \\n include temporal information, Such as walking speed or \\n direction of travel. In addition, a Subject dossier also may \\n include more permanent information Such as facial features, fingerprint, iris Scan, Voiceprint and DNA. Soft features may \\n be selected to be especially useful for relocating an indi \\n vidual within the Surveillance region, especially in a crowd, \\n for example. For instance, it may be relatively easy to \\n identify individuals based upon clothing color or estimated \\n height and weight. However, soft features have the disad \\n vantage of not being as reliable or permanent over time. If \\n a person takes off his jacket, then an identifying color feature \\n may be lost. If a person sits down, then it may become \\n impossible to use height and weight information to pick that \\n person out of a crowd. \\n System sensors continually monitor the Surveillance \\n region for the occurrence of one or more Suspicious events. \\n In one embodiment, the system directs a live video feed \\n from one or more cameras having the location of an alert \\n triggering event in their field of view to a console in a \\n manned control center. The system also may direct video \\n images captured just before the event to the control center \\n console. Thus, an operator at the console can observe \\n behavior of suspicious individuals at the scene of the event in real time and immediately prior to the event. A subject \\n dossier produced for individuals at the scene of the event can be used to automatically identify and track a suspect indi \\n vidual present at the scene of the event within the surveil \\n lance area after the occurrence of the event. \\n The system may employ information in a subject dossier incrementally. For instance, the system may prioritize infor \\n mation in the Subject dossier. Certain information in the Subject dossier Such as clothing color, estimated height and weight, walking pattern or gait and certain key facial fea \\n tures such as facial shape, facial hair, skin color, or hair color may be used to make an initial estimate of which persons in \\n a camera's field of view are candidates for a match to a \\n Suspicious person identified in response to an alert. Other \\n features from a subject dossier then may be added incre \\n mentally to make a more careful assessment of whether identified candidates actually match the Suspect. Alterna tively, as more information concerning a Suspicious person \\n becomes available, additional features may be added incre mentally to a suspects Subject dossier for that person. This \\n additional information then may be used to more effectively \\n locate and track the individual within the surveillance \\n region.\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 14}), Document(page_content='US 9,432,632 B2 \\n 9 \\n Surveillance Region \\n One embodiment of the invention is configured for use in airport security. In this embodiment, the Surveillance region comprises an airport passenger terminal and the Surrounding passenger ground transport loading/unloading Zone directly \\n outside the terminal and the aircraft parking area adjacent \\n the terminal. FIG. 1 is an illustrative drawing of a map of an airport passenger terminal and its immediate vicinity pro \\n tected by a surveillance system of one embodiment of the \\n invention. The system includes multiple cameras, each with \\n an associated field of view, some of which are overlapping. The Surveillance region has multiple areas including pas senger arrival and departure areas, a passenger departure \\n shops and a terrace. Groups of cameras with overlapping \\n fields of view are deployed to capture images within differ ent regions of the passenger arrival and passenger departure \\n aaS. \\n FIG. 2 is another view of the map of FIG. 1 showing Zoom to detail maps of different portions of the overall passenger \\n terminal map. The illustrative maps of FIGS. 1-2 can be displayed on a control terminal so that an operator can easily \\n correlate an alert to a specific area an airport Surveillance region. For instance, if an alert is triggered in the arrivals region shown in FIG. 1, then an operator may request the \\n left-most Zoom shown in FIG. 2 in order to quickly picture the airport layout in the vicinity of the alert. Additional Zoom \\n maps (not shown) may be provided for numerous locations Such as security gates, check-in counters, airport fairway, \\n parking area, access entrance, check-in counters, etc. Each different area may be associated with a group of cameras and \\n event SensOrS. \\n Event sensors are disposed at selected locations within the surveillance region. FIG. 3 is an illustrative drawing of example security areas within the Surveillance region of \\n FIGS. 1-2 outfitted with event sensors. A first security area comprises a door. The door may be equipped with a sensor, \\n Such as a mechanical sensor, that detects unauthorized opening of the door. A second security area comprises a \\n window. The window may be associated with a mechanical \\n sensor that detects when the window has been broken. A \\n third security represents a threshold to a restricted area. The \\n restricted area may be equipped with motion detectors that \\n detect the presence of persons in a restricted area. Cameras \\n situated throughout the Surveillance region also may serve as event sensors. For example, the system may employ a \\n monitoring rule whereby a camera monitors a particular area \\n of the passenger terminal. If a person is loitering in that area, \\n defined by failing to move beyond a 15 foot radius for more \\n than 60 seconds, then a low level alert is declared, the \\n camera Zooms in, and the face of the loitering person is \\n matched against the faces of persons on a watch list, for example. \\n Landmarks are defined in the security areas for purpose of estimating height and weight and direction and speed of \\n travel of a Suspect individual. For instance, a landmark Such as a countertop may be identified, and processing of a \\n camera image may be calibrated to estimate a person’s \\n height relative to the land marked countertop. A group of multiple structures, such as telephone booths, lounge areas, \\n signs or countertops, within a field of view of one or more of a group of cameras covering a security area may be \\n identified. Processing of camera images from the group of \\n cameras may be used to estimate the direction and speed at which a suspect is moving based upon the sequence and \\n timing of his passing the land marked structures. Although the Surveillance region in this one example is \\n described in terms of an airport passenger terminal, it will be 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n appreciated that the invention is not restricted to an airport', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 15}), Document(page_content='described in terms of an airport passenger terminal, it will be 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n appreciated that the invention is not restricted to an airport \\n terminal. Moreover, the surveillance region need not be a \\n continuous local area. Event sensors and Surveillance cam \\n eras may be disposed over disparate areas and be in com \\n munication with a control center via a network Such as the \\n internet, for example. \\n System Architecture \\n FIG. 4 is an illustrative block level hardware diagram of \\n a Surveillance system in accordance with an embodiment of the invention. The system includes multiple data collection agents, a knowledge server, a local knowledge server data \\n base, an application server, a middle-tier database, web \\n servers, a browser based control console and one or more client applications such as Computer Aided Dispatch sys \\n tem, building management system, access control system, \\n etc. It should be understood that the various components shown are merely illustrative. Each agent may gather infor \\n mation from numerous sources, such as the cameras shown in FIG. 1, distributed throughout a surveillance region. Moreover, for example, the knowledge server and the appli \\n cation server can be implemented across multiple hardware \\n systems or as different processes within a single hardware \\n system. \\n A security agent is a process that spans many tasks to \\n collect information about Subject(s). For example, a security agent may spawn multiple data collection agents include a \\n facial features, fingerprint, DNA, clothing color, Subject \\n gait, Subject height and weight, skin color/tone, hair color/ \\n tone, Subject direction and Voiceprint, for example. Each \\n data collection task produces different information about an individual. More specifically, each produces a signature \\n indicative of some identifying aspect of a person under \\n Surveillance. For instance, a facial features agent uses facial information captured by one or more cameras to produce a \\n signature indicative of an individual’s facial features. Simi larly, for example, a clothing color agent uses clothing color \\n information captured by one or more cameras to produce a \\n signature indicative of the color of an individual’s clothing color. Thus, the multiple agents can produce multiple dif \\n ferent signatures, each indicative of one or more different identifying feature of an individual. The agents provide the signatures to the knowledge \\n server, which aggregates signatures for each given person \\n under surveillance into a subject dossier for that person. The knowledge server indexes the signatures within a given \\n Subject dossier to permit incremental searches for individu als within the search region. The knowledge server also may perform classification and matching. The local knowledge \\n server database stores the digital signatures and correspond \\n ing indexing information. \\n The web services is the component that provides the \\n interfaces via Web Server which is usually part of an operating system. For example, web services provides the \\n interfaces for our internal components or external systems \\n via Web Server (such as Microsoft IIS on Windows, or Apache on Linux). All the interfaces to the system are via HTTP or HTTPS using port 80. Doing so, our system can \\n run across firewall. Basically, the Web Services component \\n just exposes our system interface to the outside world via \\n Web Server. \\n The application server is the component that provides that \\n database access to the user interface component, and per \\n forms session management which includes authentication \\n and authorization. The middle-tier database serves as the \\n local database for the application server. FIG. 5 is an illustrative block diagram level drawing of a \\n system architecture of an embodiment of the invention that', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 15}), Document(page_content='US 9,432,632 B2 \\n 11 \\n incorporates the system hardware of FIG. 4. A user interface \\n (UI) provides an operator of the system with real-time \\n information concerning alert events within the Surveillance \\n region. The UI may provide maps of the entire surveillance \\n region, including Zoom maps. It can display alerts from \\n different sensors including cameras, digital video recorders, \\n access control, bio-chemical detectors, etc. It may display \\n Videos of a security area in which an alert has been triggered, \\n detailed images of Suspect individuals and details of one or \\n more alerts that have been triggered. \\n Referring again to FIG. 1, there is show an example of a \\n UI display Screen in with pop-up display showing various \\n images relating to one or more alerts. In the center of the \\n screen is map of a Surveillance region. The operator can be \\n selectively enlarge, minimize or close each pop-up. A Video \\n Review display provides a video image of the security \\n region at about the time of an alert. An Incident Detection \\n display provides detailed information concerning an alert \\n event. In this example, the alert event involved an individual \\n tailgating at a commuter door. A Suspect Description display \\n provides identifying information concerning an individual \\n under Surveillance based upon information gathered into a \\n Subject dossier produced for the person. A Detailed Images \\n display provides pictures of a suspect individual captured by \\n one or more surveillance cameras. A Potential Identification \\n display provides images of the Suspect together with images \\n of one or more people whose facial features closely match \\n those of the Suspect. The potential matches are based upon \\n a facial feature signature provided by the facial feature \\n agent. Across the bottom of the map, there is a chart listing \\n briefly Summarizing multiple alert situations. The operator \\n may selectively access pop-up screens for these alert situ \\n ation. \\n Thus, the UI advantageously displays a variety of infor \\n mation aggregated in response to one or more alerts. In a typical airport security region, for example, there may be \\n several hundred cameras dispersed throughout a large physi \\n cal area. Moreover, there may be only a few operators monitoring one or more UI consoles. Depending upon the \\n rules for monitoring and declaring alerts, alerts may occur \\n frequently or infrequently. The UI of one embodiment of the \\n invention directs an operator to areas of a Surveillance region that are subject to alert and provides pertinent infor \\n mation concerning the alert So that the operator can effi \\n ciently manage security from a control center. The UI also allows an operator to quickly investigate and simultaneously \\n keep abreast of multiple alert events. \\n Furthermore, as explained more fully below, information \\n from different sensing devices is correlated to facilitate tracking of a suspect within a security region. For instance, \\n Soft biometric information and temporal information is used to locate a suspect as he or she travels within the security region. In one embodiment, a dashed line can be produced on a map on the display showing a path followed by a \\n Suspect within the Surveillance region. Information from \\n different data collection agents may be fused in order to more accurately identify and track an individual. Therefore, \\n the operator can use the UI to evaluate an alert event, to identify and track a suspect. The operator may use this \\n information as a basis to send information to a responder to \\n intercede or deal with an alert incident. \\n Knowledge Services are implemented as an application \\n running on the knowledge server. Knowledge Services cor \\n relate and analyze signature information provided by differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n ent sensory devices (i.e., data gathering agents). The Knowl', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 16}), Document(page_content='relate and analyze signature information provided by differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n ent sensory devices (i.e., data gathering agents). The Knowl \\n edge Services assemble and index subject dossiers, and when appropriate, fuse signature information for improved \\n classification results. The Knowledge Services also gener \\n ate, activate or deactivate rules and send/control rules and instruction to the Rules and Agent Manager. The Rules and Agent Manager also is implemented on the \\n knowledge server. The Rules and Agent Manager manages \\n all other agents and manages rules that can be sent to each \\n agent. It correlates information from agents. It can also \\n escalate an alert if the alert is not acknowledged by an \\n operator within a given timeframe and/or similar alerts happen repeatedly within a given time span (e.g. within 2 \\n hours). Both the Knowledge Service and the Rules and Agent Manager are the primary components for aggregating, \\n categorizing biometric signatures which are parts of object \\n dossiers. It also performs other tasks such as task assign ment/tracking, load balancing tasks among agents, and inter \\n acting with data access components. \\n The following are examples of rules that may be imple \\n mented by the system. \\n Rules: \\n Actor Action \\n Person Walk through lane against \\n direction of traffic \\n Person Tailgating \\n Person Loitering \\n Person Piggyback \\n Person Traveler screening \\n Person Walk in restricted area \\n Vehicle Park overtime \\n Vehicle Park in restricted area \\n The Person-Loitering rule involves the following criteria: \\n Radius 15 foot \\n Duration 20 seconds \\n Alert Severity Low \\n Response Zoom in face to match “watch list \\n The Person-Tailgating Rule involves the following crite \\n ria: \\n Critical \\n Acknowledge Loitering and \\n Tailgating alerts and deliver \\n alarm to operator console Alert severity Response \\n The correlation Monitoring Rule for the occurrence of a Person-Loitering event AND a Person-Tailgating event \\n involving the same person is as follows: \\n Critical \\n Acknowledge Loitering and \\n Tailgating alerts and deliver \\n alarm to operator console Alert Severity Response \\n As described above the UI, may display several categories \\n of information concerning an alert. The Knowledge Service and the Rules and Agent Manager provide the correlation \\n between events and data sources and Subject dossiers that permit an operator to view a map of the location of an alert, \\n soft-biometric data of a suspect and video playback, for example. More particularly, these components provide a link', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 16}), Document(page_content='US 9,432,632 B2 \\n 13 \\n to a map and Zoom stored in the middle tier database, link \\n to video feeds for video view real-time monitoring or \\n playback of recorded video clips and stored in a Digital \\n Video Recorder system and provide the subject dossier \\n information. \\n The Middle Tier Data Access runs on the application \\n server. It controls the database including functions such as \\n query, add, delete, index. Indexing biometric signatures and \\n updating Subject dossiers are done by this component. \\n A (Security) Agent is implemented as an application \\n running on the knowledge server that controls and manages \\n the data gathering sensors. In the case of cameras or DVRs, it can also perform video analytic using Computer Vision \\n technology. Those tasks include background Subtraction, \\n image stabilization, object detection, object classification, \\n object tracking, and object identification. It can also control \\n the movement of Pan/Tilt/Zoom (PTZ) cameras, manage \\n areas of interest within the field of view of the camera \\n (called Mouse/Man Trap), and collect video streams from \\n DVR or cameras. It also has a scheduler that controls when \\n rules or video analytic are performed. \\n A Sensory Device Directory Access and Video Server is \\n implemented as an application that has access to the knowl \\n edge server manages and provides information regarding \\n sensor devices or other Subsystems. Basically, it is a soft \\n ware layer that enables the overall system to handle different \\n makes/models of sensor devices. \\n The Web Services is the component provided by operating \\n systems or web servers. It manages other components, \\n spawns or deletes services as necessary. It can also listen to \\n messages from other systems. The Web Services provides \\n interfaces to the system via Web Services running as part of \\n a Web Server. The system provides a library resided on a \\n specific directory, and the Web Server (which is usually part \\n of the operating system) will use it to interpret interface \\n requests to our system. \\n Tracking, Facial Recognition, Fingerprint recognition, \\n and other biometric identification are done at the (Security) \\n agents. Biometric signatures are collected and generated at the agents, and sent to the Rules-and-Agent Manger. The Knowledge Services and the Rule-and-Agent Manager col \\n lectively collect biometric signatures and object tracking \\n locations, and then generate and manage subject dossiers. A \\n described above, a subject dossier includes information about object (e.g., person) Such as, biometric information/ \\n signatures, soft biometric information (hair color, skin tone/ color, weight or build, height, etc.) and other temporal \\n information (e.g., speed, direction, location, past activities, \\n information that the operator is looking for, etc.). Data fusion is performed by the Knowledge Services and the Rules and Agent Manager. Data required or generated by \\n each of the components are saved and retrievable via the \\n Middle-tier/Data Access component, which in turn utilizes a \\n relational database such as Microsoft SQL Server. \\n Subject Dossier \\n Data gathering agents collect data concerning a subject \\n person from different sources. The Knowledge Services aggregate the data into Subject dossier. The data aggregated \\n into a given dossier may include different digital signatures produced by different data gathering agents. A subject dos \\n sier also may include fused data signatures produced by the \\n fusion of data gathered from multiple data Sources having \\n different data modalities. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n The following is an example of information in a subject \\n dossier. \\n Subject Dossier: \\n Facial Features Signature (e.g., nose shape and size, face \\n width, distance between eye corners, skin color (light, \\n medium, dark), nose angle (profile view) Soft Biometrics Signature (e.g., clothing color, height, weight)', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 17}), Document(page_content='width, distance between eye corners, skin color (light, \\n medium, dark), nose angle (profile view) Soft Biometrics Signature (e.g., clothing color, height, weight) \\n Temporal Information Signature (e.g., direction of travel, speed, past places visited path) Fingerprint Signature \\n Voice Print Signature \\n Iris Scan Signature DNA Analysis Signature \\n Fingerprint Signature Voice Print Signature Iris Scan \\n Signature DNA Analysis Signature. \\n The information in a subject dossier is indexed so that it \\n can be used to more efficiently identify and track Suspects \\n and to avoid false alarms. More particularly, a dossier is \\n indexed so that certain information Such as Soft biometrics \\n can be used to Screen candidates within a Surveillance for \\n closer study and also to predict likely places within a \\n Surveillance region to look for a suspect. For instance, soft \\n biometric information Such as clothing color, height and \\n weight may be employed to select candidates for further investigation. For example, the Knowledge Services may be \\n programmed to cause the Security Agents to search for a \\n match between clothing color in a subject dossier of a Suspect and clothing color of unidentified persons in a \\n surveillance region. If a match is found, then the Knowledge Service may cause the Security Agents to perform an \\n analysis of whether facial features in the subject dossier match facial features of the person with matching color clothing. Moreover, temporal information provided in a \\n Subject dossier Such as direction and speed of travel of a Suspect may trigger the Knowledge Services to alert only \\n certain sensory devices, such as a group of cameras in an area of the Surveillance region where the Suspect is headed, \\n to be on the lookout for the suspect. \\n A Subject dossier may be incremented as more informa tion concerning a suspect is gathered. For example, initially, \\n only soft biometric information Such as clothing color and estimated height and weight might be available. Subse \\n quently, more information Such as a facial feature signature \\n or a voice print may become available and will be added to the subject dossier. Newly received data from these multiple sources may be fused with previous data by the Knowledge \\n Services as it is received. \\n A subject dossier is a record stored in a computer readable medium that can be easily accessed by security agents and \\n a console operator. The dossier is structured to separate soft \\n biometric information and temporal data from other biomet \\n ric information. Soft biometric and temporal information generally can be characterized as being easier to obtain and useful for tracking purpose, but not very reliable for defini \\n tive identification purposes. Other biometric information, Such as fingerprints, voiceprints and an iris Scan are more \\n reliable, but more difficult to obtain. Thus, soft biometric and temporal data can be used advantageously to track an \\n individual until more reliable information, such as detailed facial features or fingerprints can be obtained to provide a \\n more reliable identification. \\n Data Gathering Agents \\n The surveillance system of one embodiment employs \\n multiple streams of data including one or more of facial', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 17}), Document(page_content=\"US 9,432,632 B2 \\n 15 \\n features, vocal, fingerprint, iris Scan, DNA data, Soft bio metric data, temporal data and fused data. FIG. 6 is an illustrative flow diagram showing gathering \\n and conversion of facial feature data to a facial feature \\n signature. Facial feature A comprises a front image of a face \\n that is segmented into a plurality of local areas as an \\n irreducible set of image building elements to extract a set of \\n local features that can be mapped into a mathematical formula. Facial feature B comprises is a side image that is also separated into a set of irreducible image building \\n elements for extracting local features. Facial feature C comprises a side profile curve that is also collected for use \\n in the identity check and authentication processes. Facial \\n features D and E comprise skin color and tone and hair color. \\n These facial feature data are collected from several video \\n key frames taken from a parallax camera. \\n These facial feature data are used to produce a facial features signature. In one embodiment, the Knowledge Services which applies an MPEG-7 descriptor, e.g., a facial recognition descriptor, representing a projection of a face \\n vector onto a set of basis vectors that span the space of possible face vectors and the projection of the face from a side view defined by a profile curve. The face recognition \\n feature sets are extracted from a normalized face image and a normalized profile curve. The normalized face image \\n includes 56 lines with 46 intensity values in each line. The \\n centers of the two eyes in each face image are located on the 24.sup.throw and the 16.sup.th and 30. Sup.st column for the \\n right and left eye respectively. This normalized image is then \\n used to extract the one dimensional face vector that includes \\n the luminance pixel values from the normalized face image \\n arranged into a one dimensional vector using a raster Scan starting at the top-left corner of the image and finishing at \\n the bottom right corner of the image. The face recognition \\n feature set is then calculated by projecting the one-dimen \\n sional face vector onto the space defined by a set of basis vectors. By using the front image, the side image, the profile \\n curve, the skin color and tone and the hair color, the accuracy of identity authentication is significantly improved. A Voiceprint signature also can be produced for identity \\n check and authentication over a telephone, for example. A Voiceprint is particularly useful because it is totally nonin \\n vasive. In one embodiment, a multi-dimensional voice iden tification process may be employed to generate a speaker's \\n Voice signature by processing pitch contour vectors, time \\n signature, beat number vector and Voice shape defined by \\n audio waveforms of the speaker. For example, one embodi \\n ment applies pitch models for different pitch intervals, which \\n are defined to be the difference between the semitones of two \\n adjacent nodes: \\n Pitch Interval=(log(current pitch)-log(previous \\n pitch) log 2.Sup.1/12 \\n FIG. 7 is an illustrative flow diagram showing gathering \\n and conversion of fingerprint feature data to a fingerprint signature. A raw image of a fingerprint is converted into a set \\n of fingerprint codes. The set of codes has a more compact \\n format, e.g., IKENDI Fingerprint Pattern Format, which is based on encoding the friction ridges into a set of direction codes. The coded fingerprint is converted to fingerprint signature in an MPEG-7 descriptor. \\n FIG. 8 is an illustrative flow diagram showing gathering \\n and conversion of DNA data to a DNA signature. One embodiment of the invention may employ a DNA fingerprint \\n for identification purposes. A complete DNA profile includes \\n 13 short tandem repeats (STRs) with repeats of four or five \\n nucleotides in addition to a sex marker. Each STR has 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n various expected length and is located on different chromo\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 18}), Document(page_content='nucleotides in addition to a sex marker. Each STR has 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n various expected length and is located on different chromo \\n Somes or different ends of the same chromosome and each \\n is independently inherited. FIG. 8 show respectively the \\n human chromosomes with STR names and locations and \\n three or four different polymorphisms labeled with each of \\n four fluorescent dyes. The DNAs of different lengths are separated by gel electrophoresis. Since it is desirable to \\n detect all different DNAs in one signal identification pro \\n cess, different colors of dyes are used to mark different DNAS that have same length. Appropriate dyes are \\n employed in a PCR operation with STR primers to separate \\n the DNAs based on length and color to get accurate DNA \\n fingerprint in a single DNA identification process. The DNA profile signature is generated in the present invention by \\n using STRs and STR types, e.g., STR Name, Type}, {STR \\n Name, Type} where STR Names are {TPOX, DSS1358, \\n FGA, D5S818, CSF1PO, D7S820, D8S1179, THO1, VWA, \\n D13S317, D16S539, D18S51, D21S11, SEX, etc. Types are required to make Sure other DNA sequences may use the \\n repeat number of alleles instead of hetero/homozygous, e.g., {Heterozygous, Homozygous. DNA samples for identity \\n check and authentication may include hair, saliva, and blood. Samples are collected and their signatures are stored \\n in a database. New Sample can be collected and analyzed (but not in real time) using DNA arrays/chips, GeneChip, \\n Verigene ID, traditional PCR, or Forensic STR Analysis \\n methods. The result signature will be matched with the signatures in the database. \\n FIG. 8 illustrates genomic barcodes based on a standard Universal Product Codes for identifying retailed products by \\n employing ten alternate numerals at eleven positions to \\n generate one hundred billion unique identifiers. One \\n embodiment of the invention applies the barcode techniques for DNA fingerprint identification process. Special consid \\n erations are focused on the facts that the repeat polymor phisms are found mainly in intergenic (nongene) regions of \\n chromosomes, especially near the centromeres and that the polymorphisms always exist in a pair in this case, one from \\n each cop of chromosome 1. At a polymorphic locus (loca \\n tion), different numbers of a repeated unit create different alleles. Furthermore, repeated sequences of 9-80 nucleotides \\n are referred to as Variable Number Tandem Repeats (VN TRs). This VNTR has a 16 nucleotide repeat. Repeated \\n sequences of 2 to 8 nucleotides are referred to as Short \\n Tandem. Repeats (STRs). This STR has four nucleotide repeat. In a general genomic barcode system, huge number \\n of string of sites are generated with four alternate nucleo tides, i.e., adenine, guanine, cytosine, thymine, at each \\n position. A survey of just fifteen of these nucleotide posi \\n tions would create a possibility of 4.sup.15, i.e., one billion \\n codes. In the present invention, only fourteen STRs and types are employed to generate barcodes that are easier to \\n analyze with much smaller amount of data to process and \\n that can be more conveniently searched with existing search engine, e.g., Google search engine. \\n Soft biometric information, such as clothing color may be \\n captured using cameras calibrated in accordance with a process disclosed in commonly assigned co-pending U.S. \\n patent application Ser. No. Not Yet Known, filed Sep. 16, \\n 2005, entitled “Robust Perceptual Color Identification.” invented by K. Gob, E. Y. Chang and Y. F Wang, which is expressly incorporated by reference in its entirety into this \\n application through this reference. This patent application \\n addresses a problem of camera-based sensors perceiving an article of clothing as having a slightly different color when \\n viewed from different angles or under different lighting', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 18}), Document(page_content=\"US 9,432,632 B2 \\n 17 \\n conditions. The patent application proposes the representing \\n color of an article of clothing using a “robust perceptual \\n color. \\n Data from different modalities may be fused by the Knowledge Services for classification and identification purposes without Suffering the “curse of dimensionality using techniques taught in commonly assigned co-pending \\n U.S. patent application Ser. No. 11/129,090, filed May 13, \\n 2005, entitled, Multimodal High-Dimensional Data Fusion \\n for Classification and Identification, invented by E. Y. \\n Chang, now U.S. Pat. No. 7,242,810 issued Jul. 10, 2007, which is expressly incorporated herein in its entirety by this \\n reference. Data may be incrementally added to a classifica tion and identification process by the Knowledge Services using techniques taught by commonly assigned co-pending \\n U.S. patent application Ser. No. 1 1/230.932, filed Sep. 19, \\n 2005, entitled Incremental Data Fusion and Decision Mak ing, invented by Yuan-Fang Wang, now U.S. Pat. No. \\n 7.467,116 issued Nov. 25, 2008, which is expressly incor porated herein in its entirety by this reference. \\n While the invention has been described with reference to \\n various illustrative features, aspects and embodiments, it will be appreciated that the invention is susceptible of \\n various modifications and other embodiments, other than \\n those specifically shown and described. The invention is therefore to be broadly construed as including all such \\n alternative variations, modifications and other embodiments within the spirit and scope as hereinafter claimed. \\n It is to be understood that the above description is \\n intended to be illustrative, and not restrictive. For example, \\n the above-described embodiments (and/or aspects thereof) \\n may be used in combination with each other. In addition, many modifications may be made to adapt a particular \\n situation or material to the teachings of the invention with out departing from its scope. Dimensions, types of materials, \\n orientations of the various components, and the number and positions of the various components described herein are \\n intended to define parameters of certain embodiments, and are by no means limiting and are merely exemplary embodi \\n ments. Many other embodiments and modifications within the spirit and scope of the claims will be apparent to those of skill in the art upon reviewing the above description. The \\n scope of the invention should, therefore, be determined with reference to the appended claims, along with the full scope \\n of equivalents to which such claims are entitled. In the appended claims, the terms “including and “in which are used as the plain-English equivalents of the respective terms \\n “comprising and “wherein.” Moreover, in the following \\n claims, the terms “first,' 'second,' and “third,' etc. are used \\n merely as labels, and are not intended to impose numerical requirements on their objects. Further, the limitations of the following claims are not written in means—plus-function \\n format and are not intended to be interpreted based on 35 U.S.C. S112, sixth paragraph, unless and until Such claim limitations expressly use the phrase “means for followed by \\n a statement of function void of further structure. \\n What is claimed is: \\n 1. A Surveillance system comprising: multiple data gathering agents disposed throughout a \\n Surveillance region to monitor the Surveillance region, the data gathering agents to provide corresponding signatures identifying aspects of a Subject person in the \\n Surveillance region, the signatures including one or \\n both of biometric information or temporal information; a database storing a map of a layout for the Surveillance region; 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 18 \\n a processing System programmed to aggregate the signa \\n tures associated with the Subject person to produce a \\n subject dossier including one or both of biometric\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 19}), Document(page_content='40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 18 \\n a processing System programmed to aggregate the signa \\n tures associated with the Subject person to produce a \\n subject dossier including one or both of biometric \\n information or temporal information corresponding to the Subject person, the processing system programmed \\n to correlate the data gathering agents, the Subject dos \\n sier and the map; and \\n a control terminal to display the map of the layout for at \\n least a portion of the Surveillance region and produce \\n indicia on the map tracking a path followed by the \\n Subject person within the Surveillance region, the indi \\n cia based on the signatures in the Subject dossier and \\n the data gathering agents. \\n 2. The system of claim 1, wherein the indicia represent a \\n dashed line produced on the map on the display tracking the \\n path followed by the subject person within the surveillance region. \\n 3. The system of claim 1, wherein the processing system \\n configured to correlate different types of the data gathering \\n agents to facilitate tracking of the Subject person and to \\n display a location of the data gathering agents on the layout. \\n 4. The system of claim 1, wherein the data gathering \\n agents include at least one of a camera, digital video \\n recorder, access control or bio-chemical detector. \\n 5. The system of claim 1, wherein the database stores Zoom maps to be displayed by the control terminal, the Zoom \\n maps associated with locations in the Surveillance region, the Zoom maps illustrating at least one of security gates, \\n airport fairway, parking area, access entrance, or check-in \\n counters in the layout of the Surveillance region. \\n 6. The system of claim 1, wherein the data gathering agents monitor a large Surveillance area through which \\n multiple individuals travel. 7. The system of claim 1, wherein the processing system \\n is programmed to index the signatures within the Subject \\n dossier to permit incremental searches for individuals within the Surveillance region. 8. The system of claim 1, wherein the processing system is programmed to perform classification and matching of the signatures. \\n 9. The system of claim 1, wherein the data gathering \\n agents collect information about at least one of a walking pattern or gait of the Subject, the processing system pro \\n grammed to analyze the at least one of a walking pattern or gait of the Subject person. \\n 10. The system of claim 1, wherein the data gathering \\n agents collect information about at least one of an iris Scan and facial features of the Subject person. \\n 11. The system of claim 1, wherein the data gathering \\n agents collect information in connection with multiple indi viduals, the processing system programmed to prioritize the \\n information and analyzing a portion of the information to \\n make an initial estimate of which of the individuals are \\n candidate individuals for a potential match to the subject person, the processing system programmed to incrementally \\n add other portions of the information to the analysis to assess \\n which of the candidate individuals match the subject person. 12. The system of claim 1, wherein the processing system \\n is programmed to apply a monitoring rule wherein at least \\n one of the data gathering agents monitors a particular area in \\n the surveillance region and when an individual loiters in the particular area for more than a select period of time, the processing system declares an alert. \\n 13. The system of claim 1, wherein the processing system \\n is programmed to direct at least one of the data gathering \\n agents to Zoom in on a face of the individual, and the', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 19}), Document(page_content='agents comprise at least one of a camera, chemical sensor, \\n motion detector, door access sensor, capacitive sensor, bio metric sensor, optical sensor, infrared sensor, or security alarm sensor, having a corresponding view of the first or \\n second areas in the surveillance region. \\n ing signatures from different types of the data gathering agents to facilitate tracking of the subject person and dis playing a location of the data gathering agents on the layout. \\n recorder, access control or bio-chemical detector. US 9,432,632 B2 \\n 19 \\n processing system programmed to attempt to match the face \\n of the individual to a watch list. \\n 14. The system of claim 1, wherein at least one of the data \\n gathering agents scans data on at least one of a smart card \\n or an identification (ID) card. 5 15. The system of claim 1, wherein the data gathering agents collect biometric signatures, temporal information and object tracking locations, the biometric signatures including at least one of hair color, skin tone/color, weight, build, or height, the temporal information including at least one of speed, direction, location, past places visited, path, or past activities. 10 \\n 16. The system of claim 1, wherein the data gathering agents comprises first and second data gathering agents that \\n are positioned to monitor different first and second areas in the surveillance region, wherein the subject person travels out of a view of the first data gathering agent and into a view of the second data gathering agent as the subject person \\n travels out of the first area and into the second area. 15 \\n 17. The system of claim 16, wherein the data gathering 20 agents collect multiple data streams including one or more of facial features, vocal, fingerprint, iris scan, DNA data, soft biometric data, temporal data and used data. 18. The system of claim 16, wherein the data-gathering \\n 25 \\n 19. A surveillance method comprising: 30 monitoring a surveillance region with multiple data gath ering agents disposed throughout the surveillance region to provide corresponding signatures identifying \\n aspects of a subject person in the surveillance region, \\n the signatures including one or both of biometric infor mation or temporal information: storing a map of a layout for the surveillance region; aggregating the signatures associated with the subject person to produce a subject dossier including one or \\n both of biometric information or temporal information corresponding to the subject person; \\n correlating the data gathering agents, the subject dossier \\n and the map; and displaying the map of the layout for at least a portion of the Surveillance region and producing indicia on the 45 map tracking a path followed by the subject person \\n within the surveillance region, the indicia based on the signatures in the subject dossier and the data gathering \\n agents. \\n 20. The method of claim 19, wherein the indicia represent a dashed line produced on the map when displayed tracking 35 \\n 40 \\n 50 \\n the path followed by the subject person within the surveil lance region. \\n 21. The method of claim 19, further comprising correlat \\n 55 \\n 22. The method of claim 19, wherein the data gathering agents including at least two of a camera, digital video \\n 60 \\n 23. The method of claim 19, further comprising storing and displaying Zoom maps associated with locations in the 20 \\n Surveillance region, the Zoom maps illustrating at least one of security gates, check-in counters, airport fairway, parking \\n area, access entrance, or check-in counters in the layout of the surveillance region. 24. The method of claim 19, further comprising indexing the signatures within the subject dossier and performing \\n incremental searches for individuals within the surveillance region. \\n 25. The method of claim 19, further comprising perform ing classification and matching of the signatures.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 20}), Document(page_content='incremental searches for individuals within the surveillance region. \\n 25. The method of claim 19, further comprising perform ing classification and matching of the signatures. \\n 26. The method of claim 19, wherein the information in the signatures includes collecting information about at least one of a walking pattern or gait of the subject, the method analyzing the at least one of a walking pattern or gait of the Subject person. \\n 27. The method of claim 19, wherein the information in \\n the signatures includes information about at least one of an iris scan and facial features of the subject person. 28. The method of claim 19, further comprising collecting information in connection with multiple individuals, priori tizing the information, analyzing a portion of the informa \\n tion to make an initial estimate of which of the individuals \\n are candidate individuals for a potential match to the subject person, and incrementally adding other portions of the \\n information to the analysis to assess which of the candidate individuals match the subject person. 29. The method of claim 19, further comprising\\\\applying a monitoring rule wherein at least one of the data gathering agents monitors a particular area in the surveillance region and when an individual loiters in the particular area for more than a select period of time, declaring an alert. 30. The method of claim 19, further comprising directing at least one of the data gathering agents to Zoom in on a face of the individual, and attempting to match the face of the \\n individual to a watch list. \\n 31. The method of claim 19, further comprising scanning \\n data on at least one of a smart card or an identification (ID) \\n card. \\n 32. The method of claim 19, further comprising collecting biometric signatures, temporal information and object track ing locations, the biometric signatures including at least one of hair color, skin tone/color, weight, build, or height, the temporal information including at least one of speed, direc tion, location, past places visited, path, or past activities. 33. The method of claim 19, further comprising position ing first and second data gathering agents, from the multiple \\n data gathering agents, to monitor different first and second areas in the Surveillance region, wherein the subject person travels out of a view of the first data gathering agent and into a view of the second data gathering agent as the subject \\n person travels out of the first area and into the second area. 34. The method of claim 19, wherein the data gathering agents collect multiple data streams including one or more \\n of facial features, Vocal, fingerprint, iris scan, DNA data, soft biometric data, temporal data and used data. 35. The method of claim 19, wherein the data-gathering agents comprise at least one of a camera, chemical sensor, motion detector, door access sensor, capacitive sensor, bio metric sensor, optical sensor, infrared sensor, or security alarm sensor, having a corresponding view of the first or \\n Second areas in the surveillance region. \\n ck ck ck ck ck', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 20})], [Document(page_content='| HAO WANATHA DLA US009971920B2 \\n ( 12 ) United States Patent \\n Derakhshani et al . ( 10 ) Patent No . : US 9 , 971 , 920 B2 ( 45 ) Date of Patent : * May 15 , 2018 \\n ( 54 ) SPOOF DETECTION FOR BIOMETRIC AUTHENTICATION ( 56 ) References Cited \\n U . S . PATENT DOCUMENTS ( 71 ) Applicant : EyeVerify , LLC , Kansas City , KS ( US ) 5 , 291 , 560 A 5 , 303 , 709 A 3 / 1994 Daugman \\n 4 / 1994 Dreher et al . \\n ( Continued ) ( 72 ) Inventors : Reza R . Derakhshani , Shawnee , KS ( US ) ; Casey Hughlett , Lenexa , KS ( US ) ; Jeremy Paben , Prairie Village , \\n KS ( US ) ; Joel Teply , Shawnee , KS ( US ) ; Toby Rush , Roeland Park , KS \\n ( US ) FOREIGN PATENT DOCUMENTS \\n CN CN 101119679 A 2 / 2008 101404059 A 4 / 2009 \\n ( Continued ) ( 73 ) Assignee : EyeVerify LLC , Kansas City , MO ( US ) \\n OTHER PUBLICATIONS ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 \\n U . S . C . 154 ( b ) by 0 days . days . \\n This patent is subject to a terminal dis \\n claimer . Bowyer , K . W . , et al . , “ Image Understanding for Iris Biometrics : A Survey , ” Computer Vision and Image Understanding ; 110 ( 2 ) : 281 \\n 307 ; May 2008 . \\n ( Continued ) \\n ( 21 ) Appl . No . : 14 / 790 , 863 Primary Examiner — Shefali Goradia ( 74 ) Attorney , Agent , or Firm — Goodwin Procter LLP ( 22 ) Filed : Jul . 2 , 2015 \\n ( 65 ) Prior Publication Data \\n US 2016 / 0132735 A1 May 12 , 2016 \\n Related U . S . Application Data \\n ( 63 ) Continuation of application No . 14 / 335 , 345 , filed on Jul . 18 , 2014 , now Pat . No . 9 , 104 , 921 , which is a \\n ( Continued ) ( 57 ) ABSTRACT \\n This specification describes technologies relating to biomet ric authentication based on images of the eye . In general , one aspect of the subject matter described in this specification can be embodied in methods that include obtaining images of a subject including a view of an eye . The methods may \\n further include determining a behavioral metric based on detected movement of the eye as the eye appears in a \\n plurality of the images , determining a spatial metric based \\n on a distance from a sensor to a landmark that appears in a plurality of the images each having a different respective \\n focus distance , and determining a reflectance metric based \\n on detected changes in surface glare or specular reflection \\n patterns on a surface of the eye . The methods may further include determining a score based on the behavioral , spatial , and reflectance metrics and rejecting or accepting the one or \\n more images based on the score . ( 51 ) Int . CI . \\n G06K 9 / 00 ( 2006 . 01 ) ( 52 ) U . S . Cl . CPC . . . . . . . . . . G06K 9 / 00 ( 2013 . 01 ) ; G06K 9 / 00597 \\n ( 2013 . 01 ) ; G06K 9 / 00899 ( 2013 . 01 ) ; G06K 9700906 ( 2013 . 01 ) ( 58 ) Field of Classification Search ??? . . . . . . . . . . . . . . . . . . . . . GO6K 9 / 00597 ; G06K 9 / 00899 See application file for complete search history . 23 Claims , 10 Drawing Sheets \\n 5400 \\n 420 \\n LIGHT SENSOR \\n 4101 \\n 32 \\n 424 \\n 440 \\n AUTHENTICATION \\n MODULE \\n 1450 SECURED DEVICE \\n ACTUATOR \\n 460', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 0}), Document(page_content='US 9 , 971 , 920 B2 Page 2 \\n WO WO - 2008 / 155447 A2 12 / 2008 \\n WO - 2010 / 129074 A111 / 2010 \\n OTHER PUBLICATIONS Related U . S . Application Data \\n continuation of application No . 14 / 216 , 964 , filed on \\n Mar . 17 , 2014 , now Pat . No . 8 , 787 , 628 , which is a continuation of application No . 14 / 059 , 034 , filed on \\n Oct . 21 , 2013 , now Pat . No . 8 , 675 , 925 , which is a continuation of application No . 13 / 888 , 059 , filed on May 6 , 2013 , now abandoned , which is a continuation \\n of application No . 13 / 572 , 097 , filed on Aug . 10 , 2012 , \\n now Pat . No . 8 , 437 , 513 . Bowyer , K . W . , et al . , “ A Survey of Iris Biometrics Research : 2008 - 2010 , ” Handbook of Iris Recognition 2013 ; pp . 15 - 54 ; Jan . \\n 2013 . \\n ( 56 ) References Cited \\n U . S . PATENT DOCUMENTS \\n 5 , 632 , 282 A 5 / 1997 Hay et al . 6 , 095 , 989 A 8 / 2000 Hay et al . \\n 6 , 542 , 624 B1 4 / 2003 Oda \\n 6 , 760 , 467 B1 7 / 2004 Min et al . 6 , 839 , 151 B1 1 / 2005 Andree et al . 7 , 031 , 539 B2 4 / 2006 Tisse et al . 7 , 287 , 013 B2 10 / 2007 Schneider et al . 7 , 327 , 860 B2 2 / 2008 Derakhshani et al . 7 , 336 , 806 B2 2 / 2008 Schonberg et al . 7 , 668 , 351 B1 2 / 2010 Soliz et al . 7 , 801 , 335 B2 9 / 2010 Hanna et al . 7 , 925 , 058 B2 4 / 2011 Lee et al . 8 , 079 , 711 B2 12 / 2011 Stetson et al . \\n 8 , 090 , 246 B2 1 / 2012 Jelinek 8 , 235 , 529 B18 / 2012 Raffle et al . 8 , 251 , 511 B2 8 / 2012 Stetson et al . 8 , 279 , 329 B2 10 / 2012 Shroff et al . 8 , 345 , 935 B2 1 / 2013 Angell et al . 8 , 345 , 945 B2 1 / 2013 Song et al . \\n 8 , 369 , 595 B1 2 / 2013 Derakhshani et al . 8 , 437 , 513 B1 5 / 2013 Derakhshani et al . 8 , 457 , 367 B1 6 / 2013 Sipe et al . 8 , 553 , 948 B2 10 / 2013 Hanna \\n 2005 / 0281440 Al 12 / 2005 Pemer 2005 / 0286801 A112 / 2005 Nikiforov \\n 2006 / 0058682 A1 3 / 2006 Miller et al . 2006 / 0110011 A1 5 / 2006 Cohen et al . 2006 / 0132790 A1 6 / 2006 Gutiin \\n 2006 / 0140460 A16 / 2006 Coutts 2007 / 0110285 Al 5 / 2007 Hanna et al . \\n 2007 / 0286462 Al 12 / 2007 Usher et al . \\n 2007 / 0291277 Al 12 / 2007 Everett et al . 2008 / 0025574 AL 1 / 2008 Morikawa et al . 2008 / 0298642 Al 12 / 2008 Meenen 2010 / 0094262 A1 4 / 2010 Tripathi et al . \\n 2010 / 0128117 A1 5 / 2010 Dyer \\n 2010 / 0142765 Al 6 / 2010 Hamza 2010 / 0158319 Al 6 / 2010 Jung et al . \\n 2010 / 0271471 Al 10 / 2010 Kawasaki et al . 2011 / 0033091 A1 2 / 2011 Fujii et al . \\n 2011 / 0058712 Al 3 / 2011 Sanchez Ramos 2012 / 0163678 A1 6 / 2012 Du et al . 2012 / 0300990 A1 11 / 2012 Hanna et al . 2016 / 0125178 A1 * 5 / 2016 Danikhno . . . . . . . . . . . . . . . GO6F 21 / 32 \\n 726 / 18 Cesar Jr . and Costa , \" Neurcal Cell Classification by Wavelets and Multiscale Curvature , ” Biol Cybern ; 79 : 347 - 360 ; Oct . 1998 . \\n Chen , Z . , et al . , \" A Texture - Based Method for Classifying Cracked Concrete Surfaces From Digital Images Using Neural Networks , ” \\n Proc . of Int \\' l Joint Conference on Neural Networks , IEEE , San Jose , CA ; pp . 2632 - 2637 ; Jul . 31 - Aug . 5 , 2011 . \\n Choras , R . S . , “ Ocular Biometrics - Automatic Feature Extraction From Eye Images , ” Recent Res . in Telecommunications , Informat ics , Electronics and Signal Processing ; pp . 179 - 183 ; May 2011 . \\n Clausi , D . A . and Jernigan , M . E . , “ Designing Gabor Filters for Optimal Texture Separability , ” Pattern Recog ; 33 ( 11 ) : 1835 - 1849 ; \\n Nov . 2000 . Costa , L . d . F . and Cesar Jr , R . M . , \" Shape Analysis and Classifica \\n tion : Theory and Practice , ” CRC Press ; pp . 608 - 615 ; Dec . 2000 . Crihalmeanu , S . , et al . , “ Enhancement and Registration Schemes for Matching Conjunctival Vasculature , ” Proc . of the 3rd IAPR / IEEE International Conference on Biometrics ( ICB ) , Alghero , Italy ; pp . \\n 1247 - 1256 ; Jun . 2009 . \\n Crihalmeanu , S . and Ross , A . , \" Multispectral Scleral Patterns for Ocular Biometric Recognition , ” Pattern Recognition Lett ; \\n 33 ( 14 ) : 1860 - 1869 ; Oct . 2012 .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 1}), Document(page_content='Crihalmeanu , S . and Ross , A . , \" Multispectral Scleral Patterns for Ocular Biometric Recognition , ” Pattern Recognition Lett ; \\n 33 ( 14 ) : 1860 - 1869 ; Oct . 2012 . \\n Crihalmeanu , S . and Ross , A . , \" On the Use of Multispectral \\n Conjunctival Vasculature as a Soft Biometric , ” Proc . IEEE Work shop on Applications of Computer Vision ( WACV ) , Kona , HI ; pp . \\n 204 - 211 ; Jan . 2011 . Derakhshani , R . R . , et al . , “ A New Biometric Modality Based on \\n Conjunctival Vasculature , ” Proc . of the Artificial Neural Networks in Engineering Conference ( ANNIE ) , St . Louis , MO ; pp . 497 - 504 , \\n Nov . 2006 . Derakhshani , R . R . , et al . , \" A Texture - Based Neural Network Clas sifier for Biometric Identification Using Ocular Surface \\n Vasculature , \" Proceedings of International Joint Conference on \\n Neural Networks ( IJCNN ) , Orlando , FL ; Opgs . , Aug . 2007 . Derakhshani , et al . , \" A Vessel Tracing Algorithm for Human Con junctival Vasculature , ” Abstract in Proc . of the Kansas City Life \\n Sciences Institute Research Day Conference , Kansas City , MO ; p . \\n 150 ; Mar . 2006 . Derakhshani , et al . , “ Computational Methods for Objective Assess \\n ment of Conjunctival Vascularity , ” Engineering in Medicine and Biology Society ( EMBC ) , Proc . of the 2012 IEEE Engineering in \\n Medicine and Biology Conference , San Diego , CA ; 4pgs ; Aug . 28 - Sep . 1 , 2012 . \\n Gottemukkula , V . , et al . , \" A Texture - Based Method for Identifica tion of Retinal Vasculature , ” 2011 IEEE International Conference \\n on Technologies for Homeland Security , Waltham , MA ; pp . 434 \\n 439 ; Nov . 15 - 17 , 2011 . Grigorescu , S . E . , et al . , \" Comparison of Texture Features Based on Gabor Filters , ” IEEE Transactions on Image Processing ; 11 ( 10 ) : 1160 - 1167 ; Oct . 2002 . He , X . , et al . , Three - Class ROC Analysis — A Decision Theoretic Approach Under the Ideal Observer Framework ; IEEE Transactions on Med Imaging ; 25 ( 5 ) : 571 - 581 , May 2006 . http : / / appbank . us / healthcare / pulse - phone - put - your - finger - against \\n the - camer - a - to - check - your - pulse - check - your - pulse - every - day - and \\n keep - control - of - your - health , accessed Aug . 8 , 2012 . \\n Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / \\n US2013 / 042889 ; 11pgs . ; dated Oct . 18 , 2013 . Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / US2013 / 043901 ; 11pgs . , dated Aug . 22 , 2013 . Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / \\n US2013 / 043905 ; 9pgs . , dated Nov . 15 , 2013 FOREIGN PATENT DOCUMENTS \\n CN \\n CN \\n CN GB GB \\n WO \\n WO \\n WO \\n WO 101923640 A \\n 102037488 A 102436591 A 2465881 A 2471192 A \\n WO - 01 / 88857 Al WO - 2006 / 119425 A2 WO - 2007 / 127157 A2 \\n WO - 2008 / 030127 AL 12 / 2010 \\n 4 / 2011 \\n 5 / 2012 \\n 6 / 2010 \\n 12 / 2010 11 / 2001 11 / 2006 11 / 2007 \\n 3 / 2008', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 1}), Document(page_content='US 9 , 971 , 920 B2 Page 3 \\n ( 56 ) References Cited \\n OTHER PUBLICATIONS \\n Kalka , N . D . , et al . “ Estimating and Fusing Quality Factors for Iris \\n Biometric Images , ” IEEE Transactions on Systems , Man and Cybernetics , Part A : Systems and Humans ; 40 ( 3 ) : 509 - 524 ; May \\n 2010 . Kirbas , C . and Quek , F . K . H . , “ Vessel Extraction Techniques and Algorithms : A Survey , ” Proc . of the Third IEEE Symposium on Bioinformatics and Bioengineering , Computer Society , Bethesda , \\n MD ; 8pgs ; Mar . 10 - 12 , 2003 . Kollreider , K . , et al . , “ Verifying Liveness by Multiple Experts in Face Biometrics , ” IEEE Comp Soc Conference on Computer Vision and Pattern Recognition Workshops , Anchorage , AK ; pp . 1 - 6 ; Jun . \\n 24 - 26 , 2008 . \\n Li , H . and Chutatape , O . , “ Automated Feature Extraction in Color Retinal Images by a Model Based Approach , ” IEEE Transactions on Biomed Eng ; 51 ( 2 ) : 246 - 254 ; Feb . 2004 . Liu , Z . , et al . , “ Finger Vein Recognition With Manifold Learning , ” J Network and Computer Appl ; 33 ( 3 ) : 275 - 282 ; May 2010 . Liu , C . , et al . , “ Gabor Feature Based Classification Using the Enhanced Fisher Linear Discriminant Model for Face Recognition , \" IEEE Transactions on Image Processing ; 11 ( 4 ) : 467 - 476 ; Apr . 2002 . Owen , C . G . , et al . , \" Optimal Green ( Red - free ) Digital Imaging of Conjunctival Vasculature , ” Opthal Physiol Opt ; 22 ( 3 ) : 234 - 243 ; May 2002 . Proenca , H . , “ Quality Assessment of Degraded Iris Images \\n Acquired in the Visible Wavelength , ” IEEE Transactions on Infor \\n mation Forensics and Security ; 6 ( 1 ) : 82 - 95 ; Mar . 2011 . Randen , T . and Husøy , J . H . , “ Filtering for Texture Classification : A Comparative Study , \" IEEE Transactions on Pattern Analysis and \\n Machine Intell . ; 21 ( 4 ) : 291 - 310 ; Apr . 1999 . Saad , M . A . , et al . , “ Model - Based Blind Image Quality Assessment Using Natural DCT Statistics , ” IEEE Transactions on Image Pro cessing ; X ( X ) : 1 - 12 ; Dec . 2010 . Schwartz , G . and Berry , M . J . , “ Sophisticated Temporal Pattern Recognition in Retinal Ganglion Cells , \" J Neurophysiol ; 99 ( 4 ) : 1787 - 1798 ; Feb . 2008 . Sun , Z . , et al . , “ Improving Iris Recognition Accuracy via Cascaded Classifiers , ” IEEE Transactions on Systems , Man , and Cybernet ics — Part C : Applications and Reviews ; 35 ( 3 ) : 435 - 441 ; Aug . 2005 . Tanaka , T . and Kubo , N . , “ Biometric Authentication by Hand Vein Patterns , \" Soc . of Instrument and Control Engineers ( SICE ) Annual Conference , Sapporo , Japan ; pp . 249 - 253 ; Aug . 4 - 6 , 2004 . Tanaka , H . , et al . , Texture Segmentation Using Amplitude and \\n Phase Information of Gabor Filters , Elec . Comm . in Japan , Part . 3 ; \\n 87 ( 4 ) : 66 - 79 ; Apr . 2004 . Tankasala , S . P . , et al . , “ Biometric Recognition of Conjunctival \\n Vasculature Using GLCM Features , ” 2011 Int \\' l Conference on Image Information Processing ( ICIIP ) , Waknaghat , India ; 6pgs ; \\n Nov . 3 - 5 , 2011 . \\n Tankasala , S . P . et al . , “ Classification of Conjunctival Vasculature Using GLCM Features , ” 2011 Int \\' l Conference on Image Informa \\n tion Processing ( ICIIP ) , Waknaghat , India ; opgs ; Nov . 2011 . Tankasala , S . P . , et al . , “ Visible Light , Bi - Modal Ocular Biometrics , ” \\n Proc . of the 2012 2nd International Conference on Communication Computing and Security ( ICCCS ) , Rourkela , India ; 9pgs ; Oct . 6 - 8 , \\n 2012 . Thomas , N . L . , et al . , \" A New Approach for Sclera Vein Recogni tion , ” Proc . SPIE 7708 , Mobile Multimedia / Image Processing , Security , and Applications 2010 ; 7708 ( 1 ) : 1 - 10 ; Apr . 2010 . Toth , B . , “ Liveness Detection : Iris , ” Encyclo . Biometrics ; 2 : 931 \\n 938 ; Jan . 2009 . Wu , C . and Harada , K . , “ Extraction and Digitization Method of \\n Blood Vessel in Scleraconjunctiva Image , ” Int \\' l J Computer Sci and', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 2}), Document(page_content='938 ; Jan . 2009 . Wu , C . and Harada , K . , “ Extraction and Digitization Method of \\n Blood Vessel in Scleraconjunctiva Image , ” Int \\' l J Computer Sci and \\n Network Security ; 11 ( 7 ) : 113 - 118 ; Jul . 2011 . Zhou , Z . , et al . , “ A Comprehensive Sclera Image Quality Measure , ” \\n 2010 11th Int \\' l . Conf . Control , Automation , Robotics and Vision ( ICARCV ) , Singapore ; pp . 638 - 643 ; Dec . 7 - 10 , 2010 . Zhou , Z . , et al . , \" A New Human Identification Method : Sclera Recognition , ” IEEE Transactions on Systems , Man , and Cybernet ics Part A : Systems and Humans ; 42 ( 3 ) : 571 - 583 ; May 2012 . Zhou , Z . , et al . , “ Multi - angle Sclera Recognition System , ” 2011 IEEE Workshop on Computational Intelligencein Biometrics and Identity Management ( CIBIM ) , Paris ; 6pgs ; Apr . 11 , 2011 . \\n Zhou , Z . , et al . , “ Multimodal Eye Recognition , ” Mobile Multime dia / Image Processing , Security , and Applications 2010 ; 7708 : 1 - 10 ; Apr . 2010 . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 2}), Document(page_content='bols US 9 , 971 , 920 B2 \\n Emman \\n WALTER AR \\n T S01141140109101011001012 \\n A1401411 Sheet 1 of 10 \\n - OLI Wenu APA ZOT May 15 , 2018 \\n WANIUM Wwwwwww \\n W \\n WW . KAN PROPURU \\n 091 OZI OZI \\n M \\n ELEKLE OET \\n OVT \\n OTL \\n OSL U . S . Patent \\n 00L', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 3}), Document(page_content='U . S . Patent May 15 , 2018 Sheet 2 of 10 US 9 , 971 , 920 B2 \\n 210 \\n kannt ZA woor \\n 220 \\n 225 mmmmmmmmmmmmmmmmmmmmmmm FIG . 2 \\n Town wwwwwwwwwwwwww MORENOMENOCH 240 . \\n . ITTELIER \\n - mememmning \\n 002 230', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 4}), Document(page_content='€ \\' O13 US 9 , 971 , 920 B2 \\n * * LEKULLA \\n R \\n I w \\n MATEMATIKENANCE \\n ww . mim immer * \\n HOTEL * * * \\n www \\n var ve ww www . wwww wamewaong \\n A \\n - \\n 22€ € \\n 07€ her Sony \\n w \\n ww \\n \" \\n wy wielu www \\n YOK ww www ww Lebanon WAUWAPENANGANI \\n wys \\n . \\n com 088 Z€€ \\n CASAAAAAAA Sheet 3 of 10 \\nwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww SI DAN KAWA \\n CERNING \\n AU P May 15 , 2018 \\n OSTURE \\n WWWWWWWWWW som many \\n WAND thichchh \\n 01€ U . S . Patent \\n even 00€', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 5}), Document(page_content='400 \\n ????? … ” U . S . Patent \\n 420 \\n 4 - 4 - toc - dette LGT SERS \\n MAMAHA Monorms MAHAMAHANNA \\n ??MATH were 432 May 15 , 2018 \\n 44 f reen 434 \\n ????? My Wii - ? : m - aris Hom PyrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrranPHPWITH \\n ) MAN \\n Airiti 44 \\n MASTERSE 4 4 AMAHAHAHAMAHANNAHAMAHAMAHA Sheet 4 of 10 \\n with the is the in AUTHENTICATION MODULE \\n EELEPHTHERLANTINAIL14 on on \\n HAHAMAHHHHHHHHH 450 \\n SECURED DEVICE ACTUATOR TEL US 9 , 971 , 920 B2 \\n 4466 \\n FG . 4', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 6}), Document(page_content='?? 5302 \\n 5 . ) \" c } { } . \" U . S . Patent \\n 5 16 \\n punsume ven mur ) 4 . 7 3 \\n 8 Ep??? 8 . \\n 524 . 504 ???? •Views ?? ?? ?? ? sta 523 \\n ~ ~ . . SECURE TRANSACTION SERVICE May 15 , 2018 \\npurposuerzburupitizen Network . - . - . \\n ? 535 \\n 526 - » » ~ ~ ~ ~ ~ AUTHENTICATION MODULE | 4 . AA - - - SI - ha . l - ela - . - . - . - 4 . 1 . \\n 1 . A 11 \\n 58 \\n 528 » Sheet 5 of 10 \\n uhs » x ??? » Fre ?K???????????????????????????????????? = 543 \\n ??? 550 . . . . ? » - - l ru . AUTHENTICATION MODULE ???????????????????????????????????? \\n \" + to 1 ~ r ?? ?? ??? \\n i | . . \\n : » \" . \" saw ta - - . \\n • \\n » » \\n » \\n 530 » » AUTHENT CATEG8 | \\n | APPY | \\n » \\n » » - - 114 1 - - - - - - \\n ?? ?? ~ - \\n - » ? 3 ??? xx » ? . ??? ??? ??? ???? . ? . ?? ?? | US 9 , 971 , 920 B2 \\n FIG . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 7}), Document(page_content='U . S . Patent May 15 , 2018 Sheet 6 of 10 US 9 , 971 , 920 B2 \\n 600 WWW Wh 75 602 Obtain image ( s ) of eye \\n 5 604 WWW Determine liveness score for image ( s ) \\n 5 606 BORDERS 616 yes Live eye depicted ? 608 \\n Accept image ( s ) Reject image ( s ) \\n 610 \\n 620 Report spoof attack Segment image ( s ) into regions \\n OOOO \\n Preprocess the images regions \\n Wwwwwwwwwwwwwwwwwww na UHE \\n Determine features for each region \\n Determine match score based on the features and reference features 5 626 \\n 5 628 \\n yes \\n wwwwwwwwwwwwwwww Match ? 6327 25 www Accept Reject Reject \\n Fig . 6', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 8}), Document(page_content='Fig . 7 US 9 , 971 , 920 B2 \\n Return liveness score W \\n WW . \\n MAMAN \\n wwwwwwwww \\n ww liveness metrics Determine liveness score based on \\n VODOOOOOOOOOO wwwxxxwwwwwwwwwwxxx \\n xxxXW Sheet 7 of 10 \\n MUDOVAVAVAATLUND A mananananana Bont Determine reflectance metric \\n Determine spatial metric \\n were \\n Determine behavioral metric May 15 , 2018 \\n wwwwwwwwwwww 914 \\n 714 \\n TIL ST CondoKOFFREDOXXO9FCARDO \\n SESROCK AS a \\nwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww \\n wwww \\n ADAZKORXX SERADECKERXX TOFFORSIKK OASICKOR WIR WKWW . ROMWWWWWWWWWWWWWW \\n * * \\n OK \\n * * \\n * \\n * * * * \\n * * * * * * \\n * * * * * * * * * * * * * * * * * * * * \\n * \\n * \\n * \\n * * * \\n * * \\n * * * \\n * * * \\n * * * * \\n * * * * * Determine liveness metric ( s ) for the image ( s ) OIL ) \\n image ( s ) determination for the Start liveness score ZOL ) \\n 700 U . S . Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 9}), Document(page_content='800 U . S . Patent \\nCOELLOOOOOOOOOOOOOOOOOORGELOODIKSLUOSTELUSSOCIOECENT COLOULOOSIOS S 810 \\n Apply photic stimuli \\n + F TFFFFFFF + 7 + 7 1 \\n Capture sequence of images 2 May 15 , 2018 \\n w w wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww wwwwwww pues 814 \\n Determine diameter of pupil in each image \\n wwwwwwwuuuuuuuuuuuuuuuuwwww anninen M 15 816 \\n Determine motion parameters from sequence of pupil diameters Sheet 8 of 10 \\n int Determine behavioral metric as distance between motion parameters and expected motion parameters \\n W * Fig . 8A US 9 , 971 , 920 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 10}), Document(page_content='U . S . Patent \\n 820 0 . 922020 . O O O . . 27 . 222 . 22 . 22 . 22 . 2 . 4 . 0 . 0 . 24 . 04 . 830 \\n Apply external stimuli 22 \\n wwwwwwwwww Capture sequence of images May 15 , 2018 \\nwwwwwwwwwwwwwwwwwwwwwwwww www 834 \\n Determine location of iris in each image 15836 \\n Determine motion parameters from sequence of iris locations Sheet 9 of 10 \\n ODOOOOOOOOOOOOO O OOOOOOOOOOOOOOOOD moodcocoonozcocacoon Determine behavioral metric as distance between motion parameters and expected motion parameters US 9 , 971 , 920 B2 \\n Fig . 8B', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 11}), Document(page_content='Fig . 9 US 9 , 971 , 920 B2 \\n gsi 7 \\n 796 m 054 \\n 896 - 096 m2 \\n 2480 000 0000 / \\n orientation \\n 60000000000000000 wwwwwwwwwww \\n 0000000000pxogo d o ??????????????? ?????? , ansens meracun seramai meme news season \\n C 996 ON \\nZL186 KWD \\n m \\n ange won \\n plan \\n poso m \\n costo 2000 \\n widoo \\n D www \\n moooooooooooooo o \\n 96€ 7 9707 5968 \\n 9747 972 . 9507 Sheet 10 of 10 \\n C086 \\n MATEKOHALIKIB OUR WWWWWWWWWWW \\n MODO MMMMM \\n 2 \\n 924 \\n - 914 wwwwwwwwwww \\n wwwww \\n w \\n concorso corpo conosconocoooooooooooooowwwwww 00000 \\n 0 \\n00000000000000000000000000000000000000000000000 \\n 000000000000000 \\n 00000600000000000000 \\n 0 . U 1 \\n UEVA YORK EVO \\n * * * * \\n * * * * * * * * * * * * * * 000000 \\n * * * * * * * \\n * * * * * * * 000 \\n 0 \\n 0 00000 \\n 0 0000 May 15 , 2018 \\n V HODIN W \\n OL6 114 youw \\n wwwwwwingo 922 \\n w \\n wwwwwwwwwww rimm ing HY \\n 06 ONDOR \\n ? \\n runninn \\n * * * * * * * * * manning \\n NAMANG AWAY WWWWWwwwwwwwww \\n w wwwwwwwwwwwwwwwWWWWWWWWWWWWWWWWWWwwwwwwwwwwwwwwwww \\n . S \\n port \\n A NAKKOR KERAMIK asiaan \\n www Die \\n wwwNKA \\n 76 \\n 2 706 \\n 6806 cm 806 U . S . Patent \\n 2006 4 006 \\n C 946', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 12}), Document(page_content='US 9 , 971 , 920 B2 \\n SPOOF DETECTION FOR BIOMETRIC a sensor configured to capture two or more images of a \\n AUTHENTICATION subject including a view of an eye , wherein the images collectively include a plurality of focus distances . The \\n CROSS REFERENCE TO RELATED system may further include an illumination element provide \\n APPLICATION 5 photic stimuli in synchronization with the capture of one or more images by the sensor . The system may further include \\n This application is a continuation of , and claims priority a means for determining a behavioral metric based on , at to . patented U . S . patent application Ser . No . 14 / 335 . 345 . least , detected movement of the eye as the eye appears in a \\n filed on Jul . 18 , 2014 , entitled “ Spoof Detection for Bio plurality of the images . The behavioral metric is a measure \\n metric Authentication . ” which is a continuation of , and 10 of deviation of detected movement and timing from claims priority to , patented U . S . patent application Ser . No . expected movement of the eye . The system may further \\n 14 / 216 , 964 , filed on Mar . 17 , 2014 , entitled “ Spoof Detec include a module configured to determine a spatial metric \\n tion for Biometric Authentication , \" which is a continuation based on , at least , a distance from a sensor to a landmark that \\n of , and claims priority to , patented U . S . patent application appears in a plurality of the images each having a different appe Ser . No . 14 / 059 . 034 . filed on Oct . 21 . 2013 . entitled “ Spoof 15 respective focus distance . The system may further include a \\n Detection for Biometric Authentication , ” which is a con module configured to determine a reflectance metric based \\n tinuation of , and claims priority to , abandoned U . S . patent on , at least , detected changes in surface glare or specular application Ser . No . 13 / 888 , 059 , filed on May 6 , 2013 , reflection patterns on a surface of the eye as the eye appears \\n entitled “ Spoof Detection for Biometric Authentication , \" in a plurality of the images , wherein the reflectance metric \\n which is a continuation of , and claims priority to , patented 20 is a measure of changes in glare or specular reflection \\n U . S . patent application Ser . No . 13 / 572 , 097 , filed on Aug . patches on the surface of the eye . The system may further \\n 10 , 2012 , and entitled \" Spoof Detection for Biometric include a module configured to determine a score based on , \\n Authentication . ” The disclosures of the foregoing applica at least , the behavioral , spatial , and reflectance metrics . The system may further include an interface configured to reject tions are incorporated herein in their entirety . 25 or accept the one or more images based on the score . \\n TECHNICAL FIELD In general , one aspect of the subject matter described in this specification can be embodied in a system that includes The present disclosure relates to biometric authentication a data processing apparatus and a memory coupled to the \\n based on images of the eye . data processing apparatus . The memory having instructions \\n 30 stored thereon which , when executed by the data processing \\n BACKGROUND apparatus cause the data processing apparatus to perform \\n operations including obtaining two or more images of a It is often desirable to restrict access to property or subject including a view of an eye , wherein the images resources to particular individuals . Biometric systems may collectively include a plurality of focus distances . The be used to authenticate the identity of an individual to either 35 operations may further include determining a behavioral grant or deny access to a resource . For example , iris scanners metric based on , at least , detected movement of the eye as may be used by a biometric security system to identify an the eye appears in a plurality of the images . The behavioral', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 13}), Document(page_content=\"individual based on unique structures in the individual ' s iris . metric may be a measure of deviation of detected movement \\n and timing from expected movement of the eye . The opera \\n SUMMARY 40 tions may further include determining a spatial metric based \\n on , at least , a distance from a sensor to a landmark that This specification describes technologies relating to bio appears in a plurality of the images each having a different \\n metric authentication based on images of the eye . In general , respective focus distance . The operations may further \\n one aspect of the subject matter described in this specifica include determining a reflectance metric based on , at least , \\n tion can be embodied in a method that includes obtaining 45 detected changes in surface glare or specular reflection two or more images of a subject including a view of an eye , patterns on a surface of the eye as the eye appears in a \\n wherein the images collectively include a plurality of focus plurality of the images , wherein the reflectance metric is a distances . The method may further include determining a measure of changes in glare or specular reflection patches on behavioral metric based on , at least , detected movement of the surface of the eye . The operations may further include \\n the eye as the eye appears in a plurality of the images . The 50 determining a score based on , at least , the behavioral , \\n behavioral metric may be a measure of deviation of detected spatial , and reflectance metrics . The operations may further movement and timing from expected movement of the eye . include rejecting or accepting the one or more images based The method may further include determining a spatial metric on the score . \\n based on , at least , a distance from a sensor to a landmark that In general , one aspect of the subject matter described in \\n appears in a plurality of the images each having a different 55 this specification can be embodied in a non - transient com \\n respective focus distance . The method may further include puter readable media storing software including instructions determining a reflectance metric based on , at least , detected executable by a processing device that upon such execution changes in surface glare or specular reflection patterns on a cause the processing device to perform operations that surface of the eye as the eye appears in a plurality of the include obtaining two or more images of a subject including images , wherein the reflectance metric is a measure of 60 a view of an eye , wherein the images collectively include a \\n changes in glare or specular reflection patches on the surface plurality of focus distances . The operations may further \\n of the eye . The method may further include determining a include determining a behavioral metric based on , at least , score based on , at least , the behavioral , spatial , and reflec detected movement of the eye as the eye appears in a tance metrics . The method may further include rejecting or plurality of the images . The behavioral metric may be a \\n accepting the one or more images based on the score . 65 measure of deviation of detected movement and timing from In general , one aspect of the subject matter described in expected movement of the eye . The operations may further \\n this specification can be embodied in a system that includes include determining a spatial metric based on , at least , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 13}), Document(page_content='US 9 , 971 , 920 B2 \\n distance from a sensor to a landmark that appears in a reliably authenticating individuals . Some implementations plurality of the images each having a different respective may prevent spoofing of an eye biometric based authenti focus distance . The operations may further include deter - cation system using objects that are not a living human eye . \\n mining a reflectance metric based on , at least , detected The details of one or more embodiments of the invention changes in surface glare or specular reflection patterns on a 5 are set forth in the accompanying drawings and the descrip \\n surface of the eye as the eye appears in a plurality of the tion below . Other features , aspects , and advantages of the \\n images , wherein the reflectance metric is a measure of invention will become apparent from the description , the changes in glare or specular reflection patches on the surface drawings , and the claims . of the eye . The operations may further include determining \\n a score based on , at least , the behavioral , spatial , and 10 BRIEF DESCRIPTION OF THE DRAWINGS reflectance metrics . The operations may further include rejecting or accepting the one or more images based on the FIG . 1 is a diagram of the anatomy of a human eye . \\n score . FIG . 2 is a diagram of an example image including These and other embodiments can each optionally include portions showing vasculature of the white of an eye . one or more of the following features . Determining the 15 FIG . 3 is a diagram of an example image that is segmented behavioral metric may include determining an onset , dura - for analysis . tion , velocity , or acceleration of pupil constriction in FIG . 4 is a block diagram of example security system that response to photic stimuli . The photic stimuli may include a is configured to authenticate an individual based in part on flash pulse . The photic stimuli may include a change in the one or more images of the white of an eye . \\n intensity of light output by a display . The determining the 20 FIG . 5 is a block diagram of an example online environ behavioral metric may include determining an onset , dura ment . tion , or acceleration of gaze transition in response to external FIG . 6 is a flow chart of an example process for authen stimuli . The external stimuli may include prompts for ticating an individual based on one or more images of the instructing a user to direct gaze . The external stimuli may white of an eye , where the liveness of the eye in the obtained include an object depicted in a display that moves within the 25 images for authentication is checked . \\n display . The spatial metric may be a measure of deviation of FIG . 7 is a flow chart of an example process for deter the subject from a two - dimensional plane . The spatial metric mining a liveness score for one or more images of an eye . may be a measure of deviation of the subject from an FIG . 8A is a flow chart of an example process for expected three - dimensional shape . Determining the spatial determining a behavioral metric based on constriction of a metric may include determining parallax of two or more 30 pupil in response to photic stimulus . \\n landmarks that appear in a plurality of the images . Half - FIG . 8B is a flow chart of an example process for tones may be detected in an image captured using reduced determining a behavioral metric based on gaze transition of dynamic range and the images may be rejected based at least an iris in response to external stimulus . in part on the half - tones . Determining the behavioral metric FIG . 9 shows an example of a computer device and a may include detecting blood flow of the eye as the eye 35 mobile computer device that can be used to implement the appears in a plurality of the images . Determining the score techniques described here . \\n may include using a trained function approximator to deter', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 14}), Document(page_content=\"may include using a trained function approximator to deter \\n mine the score . The landmark may be a portion of a face DETAILED DESCRIPTION depicted in the images . Determining the reflectance metric may include pulsing a flash to illuminate the subject while 40 Distinctive features of an individual ' s visible vasculature one or more of the images are being captured , detecting the in the whites of the eyes may be used to identify or \\n appearance of glare on the eye from the flash in the images , authenticate the individual . For example , images of the \\n and measuring the time difference between the pulsing of the white of a user ' s eye can be obtained and analyzed to flash and the appearance of a corresponding glare on the eye compare features of the eye to reference record in order to \\n in the images . Determining the reflectance metric may 45 authenticate the user and grant or deny the user access to a \\n include pulsing a flash to illuminate the subject while one or resource . Adversaries or intruders could attempt spoof a \\n more of the images are being captured and detecting fine security system using such an authentication method by three dimensional texture of a white of the eye by measuring presenting something other than a live eye ( e . g . , a picture of \\n uniformity of a pattern of glare on the eye from the flash in an authorized user ' s face or a plastic model of an authorized \\n the images . A sensor setting that controls focus may be 50 user ' s eye ) to the security system ' s light sensor . Some spoof \\n adjusted to a plurality of different settings during capture of attempts may be frustrated by configuring a security system \\n two or more of the images . The images captured with to analyze the obtained images to discriminate images of \\n different focus settings may be compared to determine live eyes from images of props . \\n whether these images reflect their respective focus settings . One or more liveness metrics can be calculated that reflect A sensor setting that controls exposure may be adjusted to a 55 properties a live eye is expected to exhibit that may not be plurality of different settings during capture of two or more exhibited by certain spoof attempts . For example , stimuli of the images . The images captured with different exposure can be applied to a user during the image acquisition process \\n settings may be compared to determine whether these and the response of an eye depicted in the images may be images reflect their respective exposure settings . A sensor quantified with a metric compared to an expected response \\n setting that controls white balance may be adjusted to a 60 of a live eye to those stimuli . In some implementations , the \\n plurality of different settings during capture of two or more obtained images can be checked at a plurality of focus \\n of the images . The images captured with different white distances to determine if the eye depicted in the images is balance settings may be compared to determine whether three dimensional ( e . g . , does it have landmarks that appear these images reflect their respective white balance settings . to be positioned at distances from the sensor that deviated Particular embodiments of the invention can be imple - 65 from a single plane ) . In some implementations , a metric \\n mented to realize none , one or more of the following related to the reflectance of the eye may be determined . A advantages . Some implementations may provide security by live eye has unique reflectance properties caused by its three\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 14}), Document(page_content=\"US 9 , 971 , 920 B2 \\n dimensional shape and its fine surface texture and moisture be preprocessed and segmented to isolate regions of interest \\n that may not be exhibited by many spoof attack props . For within the image and enhance the view of vasculature in the example , a flash device may be used to illuminate the subject whites of the eyes . For example , the regions of interest may during a portion of the image acquisition process and the be tiled portions that form grids covering some or all the \\n timing and quality of the reflection of the flash pulse on the 5 whites of the eyes . A portion 320 of the corresponding to the subject ' s eye may analyzed to determine if it is indeed a live white of the right eye left of the iris may be isolated , for eyeball being imaged in real time . example , by identifying the corneal limbus boundary and the In some implementations , a plurality of liveness metrics edges of the eyelids . Similarly , a portion 322 corresponding may be combined to determine a liveness score or decision to the white of the left eye left of the iris may be isolated . that reflects the likelihood that the images depict a live eye , 10 D Preprocessing may be used to enhance the view of the as opposed to , for example , an image of model or a two vasculature in this region , for example , by selecting a dimensional picture of an eye . For example , a trained component color from the image data that maximizes the function approximator ( e . g . , a neural network ) can be used to determine , based on a plurality of liveness metrics , a contrast between the vasculature and the surrounding white \\n liveness score . The images obtained can then be accepted or 15 por 5 portions of the whites of the eyes . In some implementations , \\n rejected based on the liveness score . In some implementa these portions 320 , 322 of the image may be further seg \\n tions , a spoof attempt may be reported when the liveness mented into tiles forming grids 330 , 332 that divide an \\n score indicates that the images do not depict a live eye . exposed surface area of the whites of the eyes into smaller \\n FIG . 1 is a diagram of the anatomy of a human eye 100 . regions for analysis purposes . Features of the vasculature in The diagram is a cross - section of the eye with a blowup 102 20 these regions of interest may be used for identification , of the anatomy near the corneal limbus boundary of the eye verification , or authentication of an individual . that separates the colored iris 110 from the surrounding FIG . 4 is a block diagram of example security system 400 \\n white of the eye . The white of the eye includes a complex that is configured to authenticate an individual based in part vascular structure which is not only readily visible and on one or more images of the white of an eye 410 . A user of scannable from outside of the eye , but in addition that 25 the security system 400 may present their eye 410 to a light \\n vascular structure is unique and varies between individuals . sensor 420 . In this manner one or more images of the white \\n Thus , these vascular structures of the white of the eye , of the eye 410 may be captured . A digital camera , a \\n mostly due to vasculature of conjunctiva and episclera , can three - dimensional ( 3D ) camera , and a light field sensor are be scanned and advantageously used as a biometric . This examples of light sensors that may be employed . The light biometric can be used to authenticate a particular individual , 30 sensor 420 may employ a variety of technologies , e . g . , or , identify an unknown individual . digital charge - coupled devices ( CCD ) or complementary\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content='The white of the eye has a number of layers . The sclera metal - oxide - semiconductors ( CMOS ) . In some implemen 120 is an opaque , fibrous , protective , layer of the eye tations , the user may be prompted via messages shown on containing collagen and elastic fiber . The sclera 120 is display 424 to make certain poses to expose portions of the covered by the episclera 130 , which has a particularly large 35 white of the eye 410 and facilitate image acquisition . For \\n number of blood vessels and veins that that run through and example , the user may be prompted to direct their gaze in \\n over it . The episclera 130 is covered by the bulbar conjunc order to roll the iris of their eye 410 left , right , up , up - left , tiva 140 , which is a thin clear membrane that interfaces with and roll up - right . In some implementations , not shown , the the eyelid 150 or the environment when the eyelid is opened . user may be prompted to assume poses though messages Blood vessels and veins run through all of these layers of the 40 played through a speaker , through indicator lights ( e . g . white of the eye and can be detected in images of the eye . LEDs ) , or not prompted at all . The eye also includes eyelashes 160 that may sometimes In some implementations , the sensor 420 can be config obscure portions of the white of the eye in an image . ured to detect when the eye 410 has been properly positioned FIG . 2 is a diagram of an example image 200 including in the field of view of the sensor . Alternatively , software or \\n portions showing vasculature of the white of an eye . Such an 45 firmware implemented on a computing device 430 can \\n image 200 may be captured with a sensor ( e . g . , a camera ) analyze one or more images produced by the light sensor \\n that is integrated into a computing device such as , for 420 to determine whether the eye 410 has been properly \\n example , a smart phone , a tablet computer , a television , a positioned . In some implementations , the user may manually laptop computer , or a personal computer . For example , a indicate when the eye 410 is properly positioned through a \\n user may be prompted through a display or audio prompt to 50 user interface ( e . g . , button , keyboard , keypad , touchpad , or look to the left while the image is captured , thus exposing a touch screen ) . \\n larger area of the white of the eye to the right of the iris to An authentication module 440 implemented on the com \\n the view of the sensor . Similarly , a user may be prompted to puting device 430 may obtain one or more images of the look right , up , down , straight , etc . while an image is cap - white of the eye through the light sensor 420 . In some \\n tured . The example image includes a view of an iris 220 with 55 implementations , the computing device 430 is integrated \\n a pupil 210 at its center . The iris 220 extends to the corneal with or electrically coupled to the light sensor 420 . In some \\n limbus boundary 225 of the eye . The white 230 of the eye implementations , the computing device 430 may communi \\n is external to a corneal limbus boundary 225 of the eye . An cate with the light sensor 420 through a wireless interface extensive vasculature 240 of the white of the eye is visible ( e . g . , an antenna ) . in the image 100 . This vasculature 240 may be distinctive for 60 The authentication module 440 processes images an individual . In some implementations , distinctive features obtained through the light sensor 420 to control access to a of the vasculature 240 may be used as a basis for identifying , secured device 450 . For example , the authentication module verifying , or authenticating an individual user . 440 may implement authentication processes described in FIG . 3 is a diagram of an example image 300 , including relation to FIG . 6 . In some implementations , the secured', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content='portions showing vasculature of the whites of two eyes , that 65 device 450 may include an actuator 460 ( e . g . , a locking is segmented for analysis . A captured image 310 may be mechanism ) that affects the access control instructions from obtained in a variety of ways . The captured image 310 may the authentication module 440 . nu', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content=\"US 9 , 971 , 920 B2 \\n The computing device may be integrated with or interface configured to communicate with a first server system 512 \\n with the secured device 450 in a variety of ways . For and / or a second server system 514 over a network 511 . example , the secured device 450 may be an automobile , the Computing devices 502 , 504 , 506 , 508 , 510 have respective light sensor 420 may be a camera integrated in the steering users 522 , 524 , 526 , 528 , 530 associated therewith . The first wheel or dashboard of the automobile , and the computing 5 and second server systems 512 , 514 each include a comput \\n device 430 may be integrated in the automobile and elec - ing device 516 , 517 and a machine - readable repository , or \\n trically connected to the camera and an ignition locking database 518 , 519 . Example environment 500 may include \\n system that serves as the security actuator 460 . A user may many thousands of Web sites , computing devices and serv present views of the whites of their eye to the camera in ers , which are not shown . order to be authenticated as an authorized driver of the 10 Network 511 may include a large computer network , \\n automobile and start the engine . examples of which include a local area network ( LAN ) , In some implementations , the secured device 450 may be wide area network ( WAN ) , the Internet , a cellular network , \\n a real estate lock box , the light sensor 420 may be a camera or a combination thereof connecting a number of mobile integrated with the user ' s mobile device ( e . g . , a smartphone computing devices , fixed computing devices , and server \\n or tablet device ) , and the processing of the authentication 15 systems . The network ( s ) included in network 511 may module 440 may be performed in part by the user ' s mobile provide for communications under various modes or proto device and in part by a computing device integrated with the cols , examples of which include Transmission Control Pro \\n lock box that controls a power locking mechanism . The two tocol / Internet Protocol ( TCP / IP ) , Global System for Mobile computing devices may communicate through a wireless communication ( GSM ) voice calls , Short Electronic mes interface . For example , the user ( e . g . , a realtor giving a 20 sage Service ( SMS ) , Enhanced Messaging Service ( EMS ) , showing of a property ) may use the camera on their mobile or Multimedia Messaging Service ( MMS ) messaging , Eth \\n device to obtain one or more images and submit data based ernet , Code Division Multiple Access ( CDMA ) , Time Divi on the images to the lock box in order to be authenticated as sion Multiple Access ( TDMA ) , Personal Digital Cellular authorized user and granted access to keys stored in the lock ( PDC ) , Wideband Code Division Multiple Access \\n box . 25 ( WCDMA ) , CDMA2000 , or General Packet Radio System In some implementations , the secured device 450 is a gate ( GPRS ) , among others . Communication may occur through or door that controls access to a property . The light sensor a radio - frequency transceiver . In addition , short - range com 420 may be integrated in the door or gate or positioned on munication may occur , e . g . , using a BLUETOOTH , WiFi , or \\n a wall or fence near the door or gate . The computing device other such transceiver system . 430 may be positioned nearby and may communicate 30 Computing devices 502 , 504 , 506 , 508 , 510 enable through a wireless interface with the light sensor 420 and a respective users 522 , 524 , 526 , 528 , 530 to access and to \\n power locking mechanism in the door or gate that serves as view documents , e . g . , web pages included in web sites . For \\n an actuator 460 . In some implementations , the secured example , user 522 of computing device 502 may view a web \\n device 450 may be a rifle and the light sensor 420 may be page using a web browser . The web page may be provided\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"device 450 may be a rifle and the light sensor 420 may be page using a web browser . The web page may be provided \\n integrated with a scope attached to the rifle . The computing 35 to computing device 502 by server system 512 , server \\n device 430 may be integrated in the butt of the rifle and may system 514 or another server system ( not shown ) . electronically connect to the light sensor 420 and a trigger or In example environment 500 , computing devices 502 , hammer locking mechanism that serves as an actuator 460 . 504 , 506 are illustrated as desktop - type computing devices , In some implementations , the secured device 450 may be a computing device 508 is illustrated as a laptop - type com piece of rental equipment ( e . g . , a bicycle ) . 40 puting device 508 , and computing device 510 is illustrated The computing device 430 may include a processing as a mobile computing device . It is noted , however , that device 432 ( e . g . , as described in relation to FIG . 9 ) and a computing devices 502 , 504 , 506 , 508 , 510 may include , machine - readable repository , or database 434 . In some e . g . , a desktop computer , a laptop computer , a handheld \\n implementations , the machine - readable repository may computer , a television with one or more processors embed \\n include flash memory . The machine - readable repository 434 45 ded therein and / or coupled thereto , a tablet computing \\n may be used to store one or more reference records . A device , a personal digital assistant ( PDA ) , a cellular tele \\n reference record may include data derived from one or more phone , a network appliance , a camera , a smart phone , an \\n images of the white of an eye for a registered our authorized enhanced general packet radio service ( EGPRS ) mobile \\n user of the secured device 450 . In some implementations , phone , a media player , a navigation device , an electronic \\n the reference record includes complete reference images . In 50 messaging device , a game console , or a combination of two \\n some implementations the reference record includes features or more of these data processing devices or other appropriate \\n extracted from the reference images . In some implementa data processing devices . In some implementations , a com \\n tions the reference record includes encrypted features puting device may be included as part of a motor vehicle extracted from the reference images . In some implementa - ( e . g . , an automobile , an emergency vehicle ( e . g . , fire truck , \\n tions the reference record includes identification keys 55 ambulance ) , a bus ) . encrypted by features extracted from the reference images . Users interacting with computing devices 502 , 504 , 506 , \\n To create a reference record for a new user and enrollment 508 , 510 can interact with a secure transaction service 523 \\n or registration process may be carried out . An enrollment hosted , e . g . , by the server system 512 , by authenticating \\n process may include the capture of one or more reference themselves and issuing instructions or orders through the images of the white of a new registered user ' s eye . In some 60 network 511 . The secure transactions may include , e . g . , implementations , the enrollment process may be performed e - commerce purchases , financial transactions ( e . g . , online using the light sensor 420 and processing device 430 of banking transactions , credit or bank card transactions , loy \\n authentication system 400 . alty reward points redemptions ) , or online voting . The FIG . 5 is a block diagram showing an example of a secured transaction service may include an authentication \\n network environment 500 on which the techniques described 65 module 525 that coordinates authentication of users from the \\n herein may be implemented . Network environment 500 secured server ' s side of the interaction . In some implemen\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"herein may be implemented . Network environment 500 secured server ' s side of the interaction . In some implemen \\n includes computing devices 502 , 504 , 506 , 508 , 510 that are tations , authentication module 525 may receive image data\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"US 9 , 971 , 920 B2 \\n 10 \\n from a user device ( e . g . , computing devices 502 , 504 , 506 , processing may be transmitted to an authentication module \\n 508 , 510 ) that includes one or more images of the eye of a ( e . g . , authentication modules 525 or 540 ) . In this manner , the \\n user ( e . g . , users 522 , 524 , 526 , 528 , 530 ) . The authentication authentication functions may be distributed between the module may then process the image data to authenticate the client and the server side processes in a manner suited a user by determining if the image data matches a reference 5 particular application . For example , in some implementa \\n record for a recognized user identity that has been previ - tions , the authentication application 550 determines liveness \\n ously created based on image data collected during an scores for captured images and rejects any images with \\n enrollment session . liveness scores that indicate a spoof attack . If a liveness In some implementations , a user who has submitted a score indicates a live eye , image data , based on the accepted \\n request for service may be redirected to an authentication 10 images , may be transmitted to a server side authentication module 540 that runs on separate server system 514 . Authen module ( e . g . , authentication modules 525 or 540 ) for further tication module 540 may maintain reference records for analysis . registered or enrolled users of the secure transaction service In some implementations , the authentication application \\n 523 and may also include reference records for users of other accesses a reference record for a user identity and conducts \\n secure transaction services . Authentication module 540 can 15 a full authentication process , before reporting the result \\n establish secure sessions with various secure transaction ( e . g . , user accepted or rejected ) to a server side authentica services ( e . g . , secure transaction service 523 ) using tion module . encrypted network communications ( e . g . , using a public key The authentication application 550 may be implemented encryption protocol ) to indicate to the secure transaction as software , hardware or a combination of software and \\n service whether the user has been authenticated as a regis - 20 hardware that is executed on a processing apparatus , such as tered or enrolled user . Much like authentication module 525 , one or more computing devices ( e . g . , a computer system as authentication module 540 may receive image data from the illustrated in FIG . 9 ) . requesting user ' s computing device ( e . g . , computing devices FIG . 6 is a flow chart of an example process 600 for \\n 502 , 504 , 506 , 508 , 510 ) and may process the image data to authenticating an individual based on one or more images of \\n authenticate the user . In some implementations , the authen - 25 the white of an eye . A liveness score is determined for the tication module may determine liveness scores for images obtained images and used to accept or reject the images . \\n received from a user and may accept or reject the images When an image of a live eye is detected and accepted , the \\n based on the liveness scores . When an image is rejected as image is further analyzed to determine a match score by \\n a spoof attempt presenting something other than a live eye , extracting features from the image and comparing the fea \\n the authentication module 540 may send network commu - 30 tures to a reference record . The user is then accepted or nication messages to report the spoof attempt to the secure rejected based on the match score . \\n transaction service 523 or a relevant authority . The process 600 can be implemented , for example , by the The authentication module 540 may be implemented as authentication module 440 in the computing device 430 of\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content=\"software , hardware or a combination of software and hard - FIG . 4 . In some implementations , the computing device 430 ware that is executed on a processing apparatus , such as one 35 is a data processing apparatus that includes one or more \\n or more computing devices ( e . g . , a computer system as processors that are configured to perform actions of the \\n illustrated in FIG . 9 ) . process 600 . For example , the data processing apparatus A user device ( e . g . , computing device 510 ) may include may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In an authentication application 550 . The authentication appli some implementations , process 600 may be implemented in cation 550 may facilitate the authentication of the user as a 40 whole or in part by the authentication application 550 that is registered or enrolled user identity for the purpose of access executed by a user computing device ( e . g . , computing ing secured services ( e . g . , secure transaction service 523 ) device 510 ) . For example , the user computing device may be \\n through a network 511 . For example , the authentication a mobile computing device ( e . g . , mobile computing device \\n application 550 may be a mobile application or another type 950 of FIG . 9 ) . In some implementations , process 600 may \\n client application for interacting with a server - side authen - 45 be implemented in whole or in part by the authentication \\n tication module ( e . g . , authentication module 540 ) . The module 540 that is executed by a user server system ( e . g . , authentication application 550 may drive a sensor ( e . g . , a server system 514 ) . In some implementations , the server camera connected to or integrated with a user computing system 514 is a data processing apparatus that includes one device ) to capture one or more images of a user ( e . g . , user or more processors that are configured to perform actions of 530 ) that include views of the white of the user ' s eye . The 50 the process 600 . For example , the data processing apparatus \\n authentication application 550 may prompt ( e . g . , through a may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In display or speakers ) the user to pose for image capture . For some implementations , a computer readable medium can \\n example , the user may be prompted to face the sensor and include instructions that when executed by a computing direct their gaze left or right to expose large portions of the device ( e . g . , a computer system ) cause the device to perform \\n white of an eye to the sensor . 55 actions of the process 600 . In some implementations , the authentication application One or more images of an eye are obtained 602 . The \\n 550 transmits captured image data to an authentication images include a view of a portion of a vasculature of the eye \\n module ( e . g . , authentication modules 525 or 540 ) on a external to a corneal limbus boundary of the eye . The remote server ( e . g . , server systems 512 or 514 ) through the obtained images may be monochrome or represented in \\n network 511 . The collection of image data from user may 60 various color spaces ( e . g . , RGB , SRGB , HSV , HSL , or \\n facilitate enrollment and the creation of a reference record YCbCr ) . In some implementations , an image may be for the user . The collection of image data from user may also obtained using a light sensor ( e . g . , a digital camera , a 3D facilitate authentication against a reference record for a user camera , or a light field sensor ) . The sensor may be sensitive identity . to light in various ranges of wavelength . For example , the In some implementations , additional processing of the 65 sensor may be sensitive to the visible spectrum of light . In image data for authentication purposes may be performed by some implementations , the sensor is paired with a flash or\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content='the authentication application 550 and the results of that torch that can be pulsed to illuminate objects in view of the', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content='11 US 9 , 971 , 920 B2 \\n 12 \\n sensor . The capture of images can be synchronized or with different settings , it may indicate that the sensor has time - locked with pulsing of a flash . In some implementa been bypassed by a spoof attack . For example , sensor tions , the sensor captures a sequence of images that can be configuration settings controlling focus , exposure time , or \\n used to track motion of objects within the field of view of the white balance may be adjusted in this manner . If correspond \\n sensor . The sensor can include one more settings that control 5 ing changes in the obtained image data are not detected , the image capture ( e . g . , focus distance , flash intensity , exposure , obtained images may be rejected 608 . and white balance ) . The images can collectively include a If the liveness score indicates a high likelihood that live plurality of focus distances . For example , a sequence of eye is depicted in the images , the one or more images are images may be captured , each image captured with a dif - accepted 616 and subjected to further analysis to complete \\n ferent focus distance settings for the sensor and / or some 10 the authentication process , \\n sensors ( e . g . , a light field sensor ) can capture an image that The one or more images may be segmented 620 to \\n is focused at a plurality of distances from the sensor . In some identify regions of interest that include the best views of \\n implementations , the one or more images can be obtained vasculature in the white of an eye . In some implementations , 502 by reception through a network interface ( e . g . , a net anatomical landmarks ( e . g . , an iris , its center and corneal work interface of server system 514 ) . 15 limbus boundary , eye corners , and the edges of eyelids ) may \\n A liveness score can then be determined 604 for the one be identified in the one or more images . Regions of interest \\n or more images . In some implementations , image data within the image may be identified and selected based on elements ( e . g . , a voxel , a pixel , a ray , or a red , green or blue their location in relation to the identified anatomical land channel value ) are input directly to a trained function marks . For example , regions of interest may be located in the \\n approximator that outputs a liveness score . The function 20 white of eye to the left , right , above , or below the iris . In approximator can be trained using data corresponding to some implementations , the selected regions of interest are training images of both live eyes and spoof props that are tiled to form a grid covering a larger portion of the white of \\n paired with ideal scores ( e . g . , 1 for live eyes and 0 for spoof the eye . In some implementations , the selected regions of the props ) . The function approximator or classifier models the image are noncontiguous ( e . g . , neighboring regions may \\n mapping from input data ( i . e . , the training image data or 25 overlap or neighboring regions may have space between \\n features ) to output data ( i . e . , the resulting liveness score or them ) . The selected regions of interest may correspond to \\n binary decision ) with a set of model parameters . The model regions of interest selected from a reference image on which \\n parameter values are selected using a training algorithm that data in a reference record is based . is applied to the training data . For example , the function In some implementations , eye corners are found by fitting \\n approximator can be based the following models : linear 30 curves on the detected portions of the eyelid over sclera , and \\n regression , Volterra series , Wiener series , radial basis func - then extrapolating and finding the intersection of those', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content=\"regression , Volterra series , Wiener series , radial basis func - then extrapolating and finding the intersection of those \\n tions , kernel methods , polynomial methods ; piecewise linear curves . If one intersection ( corner ) cannot be found due to models , Bayesian classifiers , k - nearest neighbor classifiers , the fact that the iris was too close ( e . g . , due to gaze \\n neural networks , support vector machines , or fuzzy function direction ) , then a template from the same corner area but approximator . Other models are possible . In some imple - 35 from the opposite gaze direction photo can be derived and \\n mentations , the liveness score may be binary . applied to the problematic corner neighborhood in the image \\n In some implementations , the liveness score is determined at hand , and the maximum correlation location can be tagged \\n 604 based on one or more liveness metrics that in turn are as the corner . determined based on the obtained images . Some examples In some implementations , eyelids are found by adaptive of such a process are described in relation to FIG . 7 . 40 thresholding methods that find the white of the eye from the For example , the liveness score can be determined 604 by image , which border the eyelids . The sclera mask itself can the authentication module 440 , the authentication applica be corrected by morphological operations ( e . g . , convex hull ) tion 550 , authentication module 525 , or the authentication to take out aberrations . \\n module 540 . In some implementations , the limbic boundary is found \\n The liveness score is checked 606 to determine whether 45 from the sclera mask as where the sclera ends due to its \\n the images are likely to include a view of a live eye . In some termination at the iris limbic boundary . implementations , the liveness score can be compared to a In some implementations , the iris center is found through threshold multiple methods . If the eye color is light , the center of the \\n If the liveness score indicates a low likelihood of a live pupil can be found as the iris center . If the iris is too dark , \\n eye and thus a high likelihood of a spoof attack , the one or 50 then the center of the ellipsoid fitted to the limbic boundary \\n more images are rejected 608 . In some implementations , a and its center is found , or it is determined as the focal point \\n spoof attack may then be reported 610 . In some implemen - of normal rays ( i . e . , lines perpendicular to tangents to the \\n tations , the spoof attack is reported 610 through a display or limbic boundary ) converging around the iris center , or a \\n speaker ( e . g . , with an alarm sound or flashing display ) . In combination of the above methods . some implementations , the spoof attack is reported 610 by 55 The image regions may be preprocessed 622 to enhance \\n transmitting one or messages over a network using a net - the view of a vasculature within an image . In some imple \\n work interface . The user may then be rejected 630 and mentations , preprocessing 622 includes Color Image \\n denied access to secured device or service . Enhancement and Contrast Limited Adaptive Histogram In some implementations ( not shown ) , a check may be Equalization ( CLAHE ) which enhances the contrast of the performed to verify that obtained images were captured 60 intensity image . CLAHE operates in small regions of the \\n from a particular sensor and that that the particular sensor image called tiles . Each tile ' s contrast is enhanced such that \\n has not been bypassed by the submission of spoofed image the histogram of the output approximately matches the\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content='has not been bypassed by the submission of spoofed image the histogram of the output approximately matches the \\n data . For example , during image capture , one or more sensor histogram specified by particular distribution ( e . g . , uniform , configuration settings may be adjusted to take on different exponential , or Rayleigh distribution ) . The neighboring tiles settings during capture of two or more of the images . These 65 are then combined using bilinear interpolation to eliminate different settings are expected to be reflected in the obtained the artificially induced boundaries . In some implementa image data . If changes in the image data between images tions , the images may be enhanced by selecting one of the', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content=\"14 US 9 , 971 , 920 B2 \\n 13 \\n red , green or blue color components which has the best images . For example , the kernel parameters may be derived contrast between the vessels and the background . The green for eye images captured at a distance of 6 - 12 centimeters component may be preferred because it may provide the best away from the eye using a particular sensor ( e . g . a back \\n contrast between vessels and background . camera on a smartphone ) and the segmented sclera region In some implementations , preprocessing 622 includes 5 can be resized to a resolution of ( e . g . , 401x501 pixels ) for \\n application of a multi - scale enhancement filtering scheme to the analysis . Visible eye surface vasculature may be spread enhance the intensity of the images thereby facilitating in all the directions on white of the eye . For example , the \\n detection and subsequent extraction features of the vascular Gabor kernels may be aligned across six different angles structure . The parameters of the filter may be determined ( Angle = 0 , 30 , 60 , 90 , 120 , and 150 degrees ) . The phase of \\n empirically so as to account for variations in the girth of the 10 the Gabor - filtered images may vary from - n to tu radians . \\n blood vessels . The algorithm used may have good sensitiv . Phase values above 0 . 25 and below - 0 . 25 radians may \\n ity , good specificity for curves and suppresses objects of correspond to vascular structures . To binarize the phase \\n other shapes . The algorithm may be based on the second image using thresholding , all values of phase above 0 . 25 or \\n derivatives of the image . First , since the second derivatives below - 0 . 25 may be set to one and the remaining values to \\n are sensitive to noise , an image segment is convolved with 15 zero . This may result in a sharp vasculature structure in a Gaussian function . The parameter o of the Gaussian corresponding phase image . This operation can be per function may correspond to the thickness of a blood vessel . formed for images resulting from applications of all six \\n Next , for each image data element , a Hessian matrix may be Gabor kernels at different angles . All the six binarized built and eigenvalues 21 and 22 may be computed . In each images may be added , to reveal a fine and crisp vascular \\n Hessian matrix ridges are defined as points where the image 20 structure . In some implementations , a vector of the elements has an extremum in the direction of the curvature . The of the binarized phase images may be used as a feature direction of the curvature is the eigenvector of the second vector for comparing the image to a reference record . In order derivatives of the image that corresponds to the largest some implementations , differences in textural features \\n absolute eigenvalue à . The sign of the eigenvalue deter - between image regions of interest may be used as a feature \\n mines if it is a local minimum a > 0 or maximum à < 0 . The 25 vector . The sum of all the l ' s in a binarized image area \\n computed eigenvalues are then used to filter the blood vessel divided by the area of region of interest may reflect the line with the equations : extent of the visible vasculature . A match score is determined 626 based on the features and 1 _ line ( 1 , 12 ) = 411 - 1421 if a1 < 0 and 1 _ line ( 11 , 12 ) = 0 corresponding features from a reference record . The refer if N120 30 ence record may include data based at least in part on one or The diameter of the blood vessels varies but the algorithm more reference images captured during an enrollment or assumes the diameter is within an interval , [ do , d1 ] . Gauss - registration process for a user . In some implementations , a \\n ian smoothing filters may be employed in the scale range of match score may be determined 626 as a distance ( e . g . , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 19}), Document(page_content=\"ian smoothing filters may be employed in the scale range of match score may be determined 626 as a distance ( e . g . , a \\n [ d0 / 4 , d1 / 4 ] . This filtering may be repeated N times based on Euclidian distance , a correlation coefficient , modified Haus the smoothing scales : 35 dorff distance , Mahalanobis distance , Bregman divergence , cosine similarity , Kullback - Leibler distance , and Jensen gl = d0 / 4 , 02 = r * 01 , 02 = r ^ 2 * 01 , . . . 02 = Shannon divergence ) between a vector of features extracted 1 ( N - 1 ) * ol = d1 / 4 from the one or more obtained images and a vector of This final output may be the maximum value from the output features from the reference record . In some implementa \\n of all individual filters of N scales . 40 tions , the match score may be determined 626 by inputting \\n Features are determined 624 for each image region that features extracted from the one or more obtained images and reflect structure or properties of the vasculature visible in features from the reference record to a trained function that region of the user ' s eye . In some implementations , approximator . \\n minutia detection methods may be used to extract features of In some implementations , a quality based fusion match \\n the user ' s vasculature . Examples of minutia detection pro - 45 score is determined 626 based on match scores for multiple \\n cesses are described in U . S . Pat . No . 7 , 327 , 860 . images of the same vasculature . In some implementations , In some implementations , features may be determined match scores for multiple images are combined by adding \\n 624 in part by applying a set of filters to the image regions the match scores together in weighted linear combination \\n that correspond to texture features of those image regions . with weights that respectively depended on quality scores \\n For example , features may be determined in part by applying 50 determined for each of the multiple images . Other examples \\n a set of complex Gabor filters at various angles to the image of techniques that may be used to combine match scores for The parameters of the filter can be determined empirically so multiple images based on their respective quality scores as to account for variations in the spacing , orientation , and include hierarchical mixtures , sum rule , product rule , gated girth of the blood vessels . The texture features of an image fusion , Dempster - Shafer combination , and stacked general can be measured as the amount of sharp visible vasculature 55 ization , among others . in the region of interest . This quality can be determined with In some implementations , the match score is determined the ratio of area of sharp visible vasculature to the area of 626 by an authentication module ( e . g . , authentication mod region of interest . The phase of Gabor filtered image , when ule 440 running on computing device 430 ) . \\n binarized using a threshold , may facilitate detection and The match score may be checked 628 to determine \\n reveal sharp visible vasculature . 60 whether there is a match between the one or more obtained The phase of complex Gabor filtered image reflects the images and the reference record . For example the match vascular patterns at different angles when the Gabor filter score may be compared to a threshold . A match may reflect kernel is configured with Sigma = 2 . 5 Pixel , Frequency = 6 ; a high likelihood that the user whose eye is depicted in the and Gamma = 1 . The choice of frequency may be dependent one or more obtained images is the same as an individual \\n on the distance between vessels , which in turn depends on 65 associated with the reference record . the resolution and distance between image acquisition sys - If there is no match , then the user may be rejected 630 . As tem and the subject . These parameters may be invariant to a result , the user may be denied access to a secure device or\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 19}), Document(page_content='15 US 9 , 971 , 920 B2 \\n 16 \\n service ( e . g . , secured device 450 or secure transaction ser - In some implementations , photic stimuli ( e . g . , a flash vice 523 ) . In some implementations , the user may be pulse , a change brightness of an LCD display ) are applied to \\n informed of the rejection 630 through a message that is a subject while the images are being captured . In response \\n shown on a display or played through a speaker . In some to these photic stimuli , a pupil of a live eye is expected to \\n implementations , the rejection may be affected by transmit - 5 constrict to adapt to the change in illumination . Further the \\n ting a message through a network reflecting the status of the pupil is expected to constrict in a certain way over time with , \\n user as rejected . For example , the authentication module an onset time that depends on the reaction time of a user , a 540 , upon rejecting user 530 may transmit a rejection duration of the constriction movement required to reach a \\n message to the secure transaction server 523 using a network new steady state pupil diameter , an average velocity of \\n interface of server system 514 . The authentication module 10 constriction , and a particular acceleration curve for the 540 may also send a rejection message to user computing constriction motion . By examining a sequence of images \\n device 510 in this scenario . captured before and after the start of a photic stimulus , one If there is a match , then the user may be accepted 632 . AS or more parameters of a detected motion may be determined a result , the user may be granted access to a secure device and compared to one or more parameters of the expected \\n or service ( e . g . , secured device 450 or secure transaction 15 motion . A substantial deviation from the expected motion in \\n service 523 ) . In some implementations , the user may be response to the photic stimuli may indicate the subject in \\n informed of the acceptance 632 through a message that is view of the camera is not a live eye and there is spoof attack \\n shown on a display or played through a speaker . In some occurring . An example of this implementation is described \\n implementations , the acceptance may be affected by trans - in relation to FIG . 8A . \\n mitting a message through a network reflecting the status of 20 In some implementations , a behavioral metric may be \\n the user as accepted . For example , the authentication module determined 712 by applying external stimuli ( e . g . , prompts 540 , upon accepting user 530 may transmit an acceptance instructing a user to direct their gaze or a display showing a \\n message to the secure transaction server 523 using a network moving object that user follows with their eyes ) to a subject \\n interface of server system 514 . The authentication module during image capture and tracking the gaze transitions that \\n 540 may also send an acceptance message to user computing 25 may result . In response to these external stimuli , a live eye \\n device 510 in this scenario . is expected to move in a certain way over time . Some FIG . 7 is a flow chart of an example process 700 for parameters of an expected gaze transition motion may \\n determining a liveness score for one or more images of an include an onset time that depends on the reaction time of a \\n eye . One or more liveness metrics are determined 710 for the user , a duration of the gaze transition movement required to \\n images and the liveness score is determined 730 based on the 30 reach a new steady state gaze direction , an average velocity , \\n one or more liveness metrics . and a particular acceleration curve for the gaze transition The process 700 can be implemented , for example , by the motion . By examining a sequence of images captured before', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content=\"authentication module 440 in the computing device 430 of and after the start of an external stimulus , one or more FIG . 4 . In some implementations , the computing device 430 parameters of a detected motion may be determined and \\n is a data processing apparatus that includes one or more 35 compared to one or more parameters of the expected motion . \\n processors that are configured to perform actions of the A substantial deviation from the expected motion in process 700 . For example , the data processing apparatus response to the external stimuli may indicate the subject in may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In view of the camera is not a live eye and there is spoof attack some implementations , process 700 may be implemented in occurring . An example of this implementation is described \\n whole or in part by the authentication application 550 that is 40 in relation to FIG . 8B . executed by a user computing device ( e . g . , computing In some implementations , determining 712 a behavioral device 510 ) . For example , the user computing device may be metric may include detecting flow of blood in a vasculature \\n a mobile computing device ( e . g . , mobile computing device of the white of the eye ( e . g . vasculature in the episclera ) . A 950 of FIG . 9 ) . In some implementations , process 700 may sequence of images may be analyzed to detect changes in \\n be implemented in whole or in part by the authentication 45 hue and changes in visible width of veins and blood vessels \\n module 540 that is executed by a user server system ( e . g . , in the white of the eye that occur over time . The vasculature \\n server system 514 ) . In some implementations , the server of a live eye is expected to exhibit regular changes in vessel \\n system 514 is a data processing apparatus that includes one widths and hue that correspond to a user ' s pulse . A substan or more processors that are configured to perform actions of tial deviation from the expected blood flow pattern may \\n the process 700 . For example , the data processing apparatus 50 indicate the subject in view of the camera is not a live eye \\n may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In and there is spoof attack occurring . some implementations , a computer readable medium can For example , consider a section of vasculature between \\n include instructions that when executed by a computing two branching points or sharp bends . The tubular body of device ( e . g . , a computer system ) cause the device to perform that vessel change shape and color when the heart is pump \\n actions of the process 700 . 55 ing blood through it . In some implementations , 300 frames Process 700 starts 702 when one or more images are or images may be captured over a 10 second period . Image received for processing . For example , the one or more regions may be registered from one capture instance to the \\n images may be encoded as two , three , or four dimensional next . The blood flow may then be measured by comparing arrays of data image elements ( e . g . , a pixel , a voxel , a ray , the physical dimensions ( 2d or 3d ) of points of interest along \\n or a red , green or blue channel value ) . 60 blood vessels over time , as well as the coloration of those One or more liveness metrics may then be determined 710 vessels over time . In this manner , changes consistent with \\n based on the one or more images . In this example , a pulse can be detected . For example if the measure “ pulse ” behavioral metric is determined 712 based on detected signal resembled a square wave that would not be consistent movement of the eye as the eye appears in a plurality of the with a natural circulatory system . If it consisted of spikes \\n images . The behavioral metric can be a measure of deviation 65 ( both vessel dilation and appropriate coloration change ) at\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content='images . The behavioral metric can be a measure of deviation 65 ( both vessel dilation and appropriate coloration change ) at \\n of detected movement and timing from expected movement regular intervals over time within normal range for a human of the eye . user , possibly even for the specific user , then the input is', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content=\"17 US 9 , 971 , 920 B2 \\n 18 \\n likely to correspond to a real live pulse . A distance between dimensional shape . A three - dimensional model including \\n the measure pulse signal and an expected pulse signal may locations of landmarks corresponding to an expected shape \\n be determined to assess the likelihood that the subject is a for a subject including the live eye of a user may be used for \\n live eye rather than a spoof attack . comparison to the detected landmark locations . In some In some implementations , the expected motion param - 5 implementations , the relative positions of landmarks on a \\n eters are specific to a particular user and are determined particular user ' s face may be determined during an enroll \\n during an enrollment session and stored as part of a refer ment session and used generate a three - dimensional model \\n ence record for the particular user . In some implementations , that is stored as part of a reference record . In some imple \\n the expected motion parameters are determined for a popu - mentations , three - dimensional model for a population of \\n lation based on a large collection of user data or oftline 10 users may be determined based on an aggregation of mea \\n studies . surements or studies of a large number of people . Various For example , a behavioral metric may be determined 712 types of metrics can be used as a spatial metric to compare \\n by an authentication module or application ( e . g . , authenti - the detected landmark positions to the expected shape ( e . g . , \\n cation module 440 ) . a Euclidian distance , a correlation coefficient , modified In this example , a spatial metric is determined 714 based 15 Hausdorff distance , Mahalanobis distance , Bregman diver \\n on a distance from a sensor to a landmark that appears in a gence , Kullback - Leibler distance , and Jensen - Shannon plurality of the images each having a different respective divergence ) . focus distance . Focus distance is the distance from a sensor In some implementations , determining 714 the spatial to a point in its field of view that is perfectly in focus . For metric comprises determining parallax of two or more some sensors , the focus distance may be adjusted for dif - 20 landmarks that appear in a plurality of the images . Parallax ferent images by adjusting a focus configuration setting for is the apparent displacement of an observed object due to a the sensor . For example , a landmark ( e . g . , an iris , an eye change in the position of the observer . A plurality of images corner , a nose , an ear , or a background object ) may be taken from different perspectives on the subject may result identified and located in the plurality of images with differ - in landmarks within the images appearing to move by \\n ent focus distances . A landmark ' s representation in a par - 25 different amounts because of differences in their distance \\n ticular image has a degree of focus that depends on how far from the sensor . This parallax effect may be measured and \\n the object corresponding to the landmark is from an in focus used as a spatial metric that reflects the three - dimensional point in the field of view of the sensor . Degree of focus is a nature of a subject in the view of the sensor . If all the \\n measure of the extent to the image of the landmark is blurred landmarks in the images undergo the same apparent dis \\n by optical effects in the light sensor ( e . g . , due to diffraction 30 placement due to relative motion of the sensor , i . e . , the \\n and convolution with the aperture shape ) . The degree of difference in the parallax effect for the landmarks is small , \\n focus for a landmark in a particular image may be estimated then the subject viewed by the camera has higher likelihood by determining the high frequency components of the image of being a two - dimensional spoof attack . In some imple\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content=\"signal in the vicinity of the landmark . When the landmark is mentations , the sensor is moved about the subject during in focus , more high frequency components in its vicinity are 35 image capture to collect image data from different orienta \\n expected . When the degree of focus is low for a landmark , tions relative to the subject . For example , a single camera smaller high frequency components are expected . By com may be rotated or slid slightly or multiple cameras at paring the degree of focus for a landmark in images with different positions may be used for image capture . In some \\n different focus distances , the distance from the sensor to the implementations , a user is prompted to move in order to \\n landmark may be estimated . In some implementations , dis - 40 change the relative orientation of the subject and the sensor . tances from the sensor ( e . g . a camera ) for multiple land - In some implementations , it is assumed that sensor will marks are estimated to form a topological map ( consisting of naturally move relative to the subject . For example , where \\n a set of three - dimensional landmark positions ) of the subject the sensor is a camera in hand - held user device ( e . g . a in the view of the sensor . The positions of these landmarks smartphone or tablet ) the sensor may naturally move relative \\n in the space viewed by the camera may be compared to a 45 to the users face due to involuntary haptic motion . \\n model by determining a spatial metric ( e . g . , the mean square For example , a spatial metric may be determined 714 by \\n difference between the detected location of one or more an authentication module or application ( e . g . , authentication landmarks and the corresponding modeled locations of the module 440 ) . one or more landmarks ) that reflects deviation from the In this example , a reflectance metric is determined 716 \\n model . 50 based on detected change in surface glare or specular \\n In some implementations , the spatial metric is a measure reflection patterns on a surface of the eye as the eye appears of the deviation of the subject from a two - dimensional plane . in a plurality of the images . The reflectance metric may be One possible spoofing strategy is to present a two dimen - a measure of changes in glare or specular reflection patches \\n sional image ( e . g . , a photograph ) of a registered user ' s eye on the surface of the eye . As the illumination of an eye in the to the sensor . However the locations of landmarks ( e . g . , an 55 view of the sensor changes , due to relative motion of the eye \\n eye , nose , mouth , and ear ) in the two dimensional image will and a light source or to changes in a dynamic light source \\n occur in a two dimensional plane , unlike landmarks in and ( e . g . , a flash , LCD screen , or other illumination element ) , the around a real live eye . For example , the locations of multiple glare and specular reflection patterns visible on the eye are landmarks may be fit to the closest two dimensional plane expected to change by appearing , disappearing , growing , and the average distance of the landmarks from this fit plane 60 shrinking , or moving . In some implementations , changes in \\n can be determined as the spatial metric . A high value for this the illumination are induced during image capture by photic spatial metric may indicate a three - dimensional subject and stimuli ( e . g . a flash pulse ) or external stimuli ( e . g . a prompt a higher likelihood that the subject is a live eye , while a low instructing a user to change gaze direction ) . For example , value may indicate a higher likelihood that the subject is a glare , including its boundaries , can be detected by thresh\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content='two - dimensional spoof attack . 65 olding a contrast enhanced image to find the whitest spots . In some implementations , the spatial metric is a measure Detected changes in the glare or specular reflection patterns of the deviation of the subject from an expected three - on the eye in the images may be compared to expected', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content='US 9 , 971 , 920 B2 \\n 19 20 \\n changes in the these patterns by determining 716 a reflec dynamic range for the sensor ( e . g . , a camera ) so that a finer \\n tance metric that measures the deviation of the detected resolution in intensity of detected light is achieved in a range change from an expected change . within which it occurs in the captured images . In this We are looking for changes in the area and shape of this manner , the intensity or color scale can be zoomed in to \\n glare . One can also look at the ratio of circumference to area 5 reveal more subtle changes in the level of the detected image of the glare patch . signal . If the captured images are of a live eye , it is expected \\n In some implementations , a flash may be pulsed to that the range of color or intensity values detected will \\n illuminate the subject while one or more of the images are continue to vary continuously . In contrast , a spoofed image being captured . Glare from the flash may be detected on the ( e . g . a digital photograph presented to the sensor ) may eye as it appears in the images . The pulsing of the flash may 10 exhibit large discontinuous jumps corresponding to half \\n be synchronized with image capture so that the time differ - tones . The extent of halftones in the image may be measured ence between when the flash is pulsed and when the corre - in a variety of ways ( e . g . , as average or maximum eigen \\n sponding glare appears in the images can be measured . The values of a Hessian matrix evaluated in a region of the image \\n reflectance metric may be based on this time difference . or as high frequency components of the image signal ) . In \\n Large deviations from the expected synchronization or time - 15 some implementations , images with a halftone metric above \\n lock of the flash pulse and the onset of a corresponding glare a threshold are rejected . In some implementations , histo or specular reflection may indicate a spoof attack . For grams of gray shades in the image are generated and the example , a replay attack uses pre - recorded video of a uniformity of the distribution between grey level bins ( e . g . , \\n capturing scenario . Glare changes in the pre - recorded video 256 bins ) is measured . \\n are unlikely to be time - locked to a real - time flash event 20 In some implementations , the liveness metrics are deter during the current session . Another example is presenting a mined 710 in parallel . In some implementations , the liveness \\n printed image of an eye to the sensor , in which case glare metrics are determined 710 in series . may spread across the printed image in an unnaturally The liveness score may then be determined 730 based on uniform manner or may not change perceivably due to a lack the one or more liveness metrics . In some implementations , \\n of moisture on the viewed surface . If no corresponding glare 25 the liveness score is determined by inputting the one or more \\n or specular reflection is detected , the reflectance metric may liveness metrics to a trained function approximator . \\n be determined to be a large arbitrary number corresponding The function approximator may be trained using data \\n to poor synchronization or a lack of time - lock between the corresponding to training images of live eyes and various \\n flash and detected glare or specular reflection . spoof attacks that have been correctly labeled to provide a In some implementations , changes in illumination may be 30 desired output signal . The function approximator models the detected as changes as changes in the uniformity of a glare mapping from input data ( i . e . , the training image liveness pattern caused by greater amounts of fine three - dimensional metrics ) to output data ( i . e . , a liveness score ) with a set of texture of a white of the eye being revealed as the intensity model parameters . The model parameter values are selected', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 22}), Document(page_content='of the illumination is increased . For example , a flash may be using a training algorithm that is applied to the training data . \\n pulsed to illuminate the subject at higher intensity while one 35 For example , the function approximator may be based the \\n or more of the images are being captured . Fine three following models : linear regression , Volterra series , Wiener \\n dimensional texture of a white of the eye may be detected by series , radial basis functions , kernel methods , polynomial measuring uniformity of a pattern of glare on the eye in the methods ; piecewise linear models , Bayesian classifiers , \\n images before and after the onset of the flash pulse . For k - nearest neighbor classifiers , neural networks , support vec \\n example , the uniformity of the glare of specular reflection 40 tor machines , or fuzzy function approximator . In some pattern may be measured as the ratio of circumference to the implementations , the liveness score may be binary . area of the glare . The larger this number compared to 2 / R , For example , the liveness score may be determined 730 \\n the more non - circular and non - uniform the glare ( R is the based on one or more liveness metrics by an authentication \\n estimated radius of the glare patch ) . In some implementa - module or application ( e . g . , authentication module 440 ) . tions , a function approximator ( e . g . , a neural network ) is 45 The resulting liveness score may then be returned 740 and \\n trained to distinguish between specular reflection patterns may be used by an authentication system ( e . g . , authentica recorded from live eyeballs vs . synthesized eyeballs , such as tion system 400 ) in variety of ways . For example , the \\n 3D printed eyeballs , using a sensor with an illumination liveness score may be used to accept or reject the one or \\n element ( e . g . , a flash ) . more images . For example , a reflectance metric may be determined 716 50 FIG . 8A is a flow chart of an example process 800 for by an authentication module or application ( e . g . , authenti - determining a behavioral metric based on constriction of a \\n cation module 440 ) . pupil in response to photic stimulus . One or more photic In some implementations ( not shown ) , additional liveness stimuli are applied 810 to the scene viewed by a sensor ( e . g . metrics may be determined 710 . For example , a metric light sensor 420 ) . For example , the photic stimuli may reflecting the extent of saccadic motion of the eye in the 55 include a flash pulse or a change in the brightness of a view of the sensor may be determined . An iris of the eye may display ( e . g . , an LCD display ) . A sequence of images is be landmarked in a sequence of images so that its position captured 812 by the sensor before and after the start of the or orientation may be tracked . This sequence of positions or photic stimuli . For example , the sequence of images may be orientations may be analyzed to determine extent of saccadic captured at regularly spaced times ( e . g . , at 10 , 30 , or 60 Hz ) \\n motion , by filtering for motions at a particular frequency 60 in an interval ( e . g . , 2 , 5 , or 10 seconds ) that includes the start \\n associated with normal saccadic motion . of the photic stimuli . In some implementations , a liveness metric may be deter - In some implementations , a pupil is landmarked in each mined 710 that reflects the extent of halftones in a captured of the captured images and the diameter of the pupil is image . Halftones are artifacts of digital printed images that determined 814 in each captured image . The diameter may \\n may be used in a spoof attack and thus their presence may 65 be determined 814 relative to a starting diameter for the indicate a high likelihood of a spoof attack . For example , pupil that is measured in one or more images captured before one or more images may be captured using a reduced the start of the photic stimuli .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 22}), Document(page_content='21 US 9 , 971 , 920 B2 \\n 22 \\n The resulting sequence of pupil diameters measured in In some implementations , an iris is landmarked in each of response to the photic stimuli may be analyzed to determine the captured images and the position or orientation of the iris 816 one or more motion parameters for the constriction of is determined 834 in each captured image . The position may the pupil in response to the photic stimuli . In some imple be determined 834 relative to a starting position for the iris \\n mentations , motion parameters of the pupil constriction may 5 that is measured in one or more images captured before the \\n include an onset time of the constriction motion relative to start of the external stimuli . the start of the photic stimuli . Onset is the time delay T he resulting sequence of iris positions measured in between the start of the photic stimuli and the start of the response to the external stimuli may be analyzed to deter \\n constriction motion . In some implementations , motion mine 836 one or more motion parameters for the gaze \\n parameters of the pupil constriction may include a duration 10 transition in response to the external stimuli . In some of the constriction motion . Duration is the length of time implementations , motion parameters of the gaze transition between the start of the constriction motion and the end of may include an onset time of the gaze transition motion \\n the constriction motion , when the pupil diameter reaches a relative to the start of the external stimuli . Onset is the time \\n new steady state value ( e . g . , after which the diameter does delay between the start of the external stimuli and the start \\n not change for a minimum interval of time ) . In some 15 of the gaze transition motion . In some implementations , implementations , motion parameters of the pupil constric - motion parameters of the gaze transition may include a tion may include a velocity of pupil constriction . For duration of the gaze transition motion . Duration is the length example , the velocity may be determined as difference in of time between the start of the gaze transition motion and pupil diameters between two points in time divided by the the end of the gaze transition motion , when the iris reaches length of the time interval between them . In some imple - 20 a new steady state position ( e . g . , after which the iris does not mentations , motion parameters of the pupil constriction may move for a minimum interval of time ) . In some implemen include an acceleration of the pupil constriction in different tations , motion parameters of the gaze transition may time segments of constriction period . For example , the include a velocity of gaze transition . For example , the acceleration may be determined as a difference in velocities velocity may be determined as difference in iris positions \\n between two intervals . 25 between two points in time divided by the length of the time The behavioral metric may be determined 818 as a interval between them . In some implementations , motion distance between one or more determined motion param - parameters of the gaze transition may include an accelera \\n eters and one or more expected motion parameters . For tion of the gaze transition . For example , the acceleration \\n example , the behavior metric may include a difference may be determined as a difference in velocities between two \\n between a detected onset time and an expected onset time for 30 intervals . \\n a live eye . For example , the behavior metric may include a The behavioral metric may be determined 838 as a', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content=\"between a detected onset time and an expected onset time for 30 intervals . \\n a live eye . For example , the behavior metric may include a The behavioral metric may be determined 838 as a \\n difference between a detected duration and an expected distance between one or more determined motion param duration of pupil constriction for a live eye . In some eters and one or more expected motion parameters . For implementations , a sequence of pupil diameters is compared example , the behavior metric may include a difference to an expected sequence of pupil diameters by determining 35 between a detected onset time and an expected onset time for \\n a distance ( e . g . , a Euclidian distance , a correlation coeffi - a live eye . For example , the behavior metric may include a \\n cient , modified Hausdorff distance , Mahalanobis distance , difference between a detected duration and an expected Bregman divergence , Kullback - Leibler distance , and duration of pupil constriction for a live eye . In some Jensen - Shannon divergence ) between the two sequences . In implementations , a sequence of iris positions is compared to some implementations , a sequence of pupil constriction 40 expected sequence of iris positions by determining a dis \\n velocities for the constriction motion is compared to an tance ( e . g . , a Euclidian distance , a correlation coefficient , expected sequence of pupil constriction velocities by deter - modified Hausdorff distance , Mahalanobis distance , Breg mining a distance between the two sequences of velocities . man divergence , Kullback - Leibler distance , and Jensen In some implementations , a sequence of pupil constriction Shannon divergence ) between the two sequences . In some \\n accelerations for the constriction motion is compared to an 45 implementations , a sequence of transition velocities for the expected sequence of pupil constriction accelerations by gaze transition motion is compared to expected sequence of determining a distance between the two sequences of accel - transition velocities by determining a distance between the \\n erations . two sequences of velocities . In some implementations , a \\n For example , the process 800 may be implemented by an sequence of gaze transition accelerations for the constriction \\n authentication module or application ( e . g . , authentication 50 motion is compared to an expected sequence of gaze tran \\n module 440 ) controlling a light sensor ( e . g . light sensor 420 ) sition accelerations by determining a distance between the \\n and an illumination element . two sequences of accelerations . FIG . 8B is a flow chart of an example process 820 for For example , the process 820 may be implemented by an determining a behavioral metric based on gaze transition of authentication module or application ( e . g . , authentication an iris in response to external stimulus . One or more external 55 module 440 ) controlling a light sensor ( e . g . light sensor 420 ) stimuli are applied 830 to a user viewed by a sensor ( e . g . and a prompting device ( e . g . , a display , a speaker , or a haptic light sensor 420 ) . For example , the external stimuli may feedback device ) . \\n include prompts instructing a user to direct their gaze ( e . g . , FIG . 9 shows an example of a generic computer device look right , left , up , down , or straight ahead ) during image 900 and a generic mobile computing device 950 , which may \\n capture . Prompts may be visual , auditory , and / or tactile . In 60 be used with the techniques described here . Computing \\n some implementations , the external stimuli can include an device 900 is intended to represent various forms of digital object that moves within in display for user ' s eyes to follow . computers , such as laptops , desktops , workstations , personal\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content='A sequence of images is captured 832 by the sensor before digital assistants , servers , blade servers , mainframes , and and after the start of the external stimuli . For example , the other appropriate computers . Computing device 950 is \\n sequence of images may be captured at regularly spaced 65 intended to represent various forms of mobile devices , such \\n times ( e . g . , at 10 , 30 , or 60 Hz ) in an interval ( e . g . , 2 , 5 , or as personal digital assistants , cellular telephones , smart 10 seconds ) that includes the start of the external stimuli . phones , and other similar computing devices . The compo', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content='23 US 9 , 971 , 920 B2 \\n 24 \\n nents shown here , their connections and relationships , and puting device 900 may be combined with other components \\n their functions , are meant to be exemplary only , and are not in a mobile device ( not shown ) , such as device 950 . Each of \\n meant to limit implementations of the inventions described such devices may contain one or more of computing device \\n and / or claimed in this document . 900 , 950 , and an entire system may be made up of multiple Computing device 900 includes a processor 902 , memory 5 computing devices 900 , 950 communicating with each \\n 904 , a storage device 906 , a high - speed interface 908 other . connecting to memory 904 and high - speed expansion ports Computing device 950 includes a processor 952 , memory \\n 910 , and a low speed interface 912 connecting to low speed 964 , an input / output device such as a display 954 , a com bus 914 and storage device 906 . Each of the components munication interface 966 , and a transceiver 968 , among 902 , 904 , 906 , 908 , 910 , and 912 , are interconnected using 10 other components . The device 950 may also be provided various busses , and may be mounted on a common moth with a storage device , such as a microdrive or other device , erboard or in other manners as appropriate . The processor to provide additional storage . Each of the components 950 , \\n 902 can process instructions for execution within the com - 952 , 964 , 954 , 966 , and 968 , are interconnected using \\n puting device 900 , including instructions stored in the various buses , and several of the components may be \\n memory 904 or on the storage device 906 to display graphi - 15 mounted on a common motherboard or in other manners as \\n cal information for a GUI on an external input / output device , appropriate . such as display 916 coupled to high speed interface 908 . In The processor 952 can execute instructions within the \\n other implementations , multiple processors and / or multiple computing device 950 , including instructions stored in the buses may be used , as appropriate , along with multiple memory 964 . The processor may be implemented as a memories and types of memory . Also , multiple computing 20 chipset of chips that include separate and multiple analog devices 900 may be connected , with each device providing and digital processors . The processor may provide , for \\n portions of the necessary operations ( e . g . , as a server bank , example , for coordination of the other components of the \\n a group of blade servers , or a multi - processor system ) . device 950 , such as control of user interfaces , applications \\n The memory 904 stores information within the computing run by device 950 , and wireless communication by device \\n device 900 . In one implementation , the memory 904 is a 25 950 . \\n volatile memory unit or units . In another implementation , Processor 952 may communicate with a user through the memory 904 is a non - volatile memory unit or units . The control interface 958 and display interface 956 coupled to a memory 904 may also be another form of computer - readable display 954 . The display 954 may be , for example , a TFT \\n medium , such as a magnetic or optical disk . LCD ( Thin - Film - Transistor Liquid Crystal Display ) or an The storage device 906 is capable of providing mass 30 OLED ( Organic Light Emitting Diode ) display , or other \\n storage for the computing device 900 . In one implementa - appropriate display technology . The display interface 956 tion , the storage device 906 may be or contain a computer may comprise appropriate circuitry for driving the display readable medium , such as a floppy disk device , a hard disk 954 to present graphical and other information to a user . The', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 24}), Document(page_content='device , an optical disk device , or a tape device , a flash control interface 958 may receive commands from a user memory or other similar solid state memory device , or an 35 and convert them for submission to the processor 952 . In \\n array of devices , including devices in a storage area network addition , an external interface 962 may be provided in or other configurations . A computer program product can be communication with processor 952 , so as to enable near area \\n tangibly embodied in an information carrier . The computer communication of device 950 with other devices . External program product may also contain instructions that , when interface 962 may provide , for example , for wired commu \\n executed , perform one or more methods , such as those 40 nication in some implementations , or for wireless commu \\n described above . The information carrier is a computer - or n ication in other implementations , and multiple interfaces \\n machine - readable medium , such as the memory 904 , the may also be used . storage device 906 , or a memory on processor 902 , for The memory 964 stores information within the computing example . device 950 . The memory 964 can be implemented as one or The high speed controller 908 manages bandwidth - inten - 45 more of a computer - readable medium or media , a volatile \\n sive operations for the computing device 900 , while the low memory unit or units , or a non - volatile memory unit or units . \\n speed controller 912 manages lower bandwidth - intensive Expansion memory 974 may also be provided and connected operations . Such allocation of functions is exemplary only . to device 950 through expansion interface 972 , which may In one implementation , the high - speed controller 908 is include , for example , a SIMM ( Single In Line Memory coupled to memory 904 , display 916 ( e . g . , through a graph - 50 Module ) card interface . Such expansion memory 974 may ics processor or accelerator ) , and to high - speed expansion provide extra storage space for device 950 , or may also store ports 910 , which may accept various expansion cards ( not applications or other information for device 950 . Specifi shown ) . In the implementation , low - speed controller 912 is cally , expansion memory 974 may include instructions to coupled to storage device 906 and low - speed expansion port carry out or supplement the processes described above , and \\n 914 . The low - speed expansion port , which may include 55 may include secure information also . Thus , for example , \\n various communication ports ( e . g . , USB , Bluetooth , Ether - expansion memory 974 may be provided as a security \\n net , wireless Ethernet ) may be coupled to one or more module for device 950 , and may be programmed with \\n input / output devices , such as a keyboard , a pointing device , instructions that permit secure use of device 950 . In addi \\n a scanner , or a networking device such as a switch or router , tion , secure applications may be provided via the SIMM \\n e . g . , through a network adapter . 60 cards , along with additional information , such as placing The computing device 900 may be implemented in a identifying information on the SIMM card in a non - hackable number of different forms , as shown in the figure . For manner . example , it may be implemented as a standard server 920 , or The memory may include , for example , flash memory \\n multiple times in a group of such servers . It may also be and / or NVRAM memory , as discussed below . In one imple implemented as part of a rack server system 924 . In addition , 65 mentation , a computer program product is tangibly embod \\n it may be implemented in a personal computer such as a ied in an information carrier . The computer program product laptop computer 922 . Alternatively , components from com contains instructions that , when executed , perform one or', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 24}), Document(page_content='US 9 , 971 , 920 B2 \\n 25 26 \\n ne - more methods , such as those described above . The infor - ( e . g . , a mouse or a trackball ) by which the user can provide mation carrier is a computer - or machine - readable medium , input to the computer . Other kinds of devices can be used to \\n such as the memory 964 , expansion memory 974 , memory provide for interaction with a user as well ; for example , \\n on processor 952 , or a propagated signal that may be feedback provided to the user can be any form of sensory \\n received , for example , over transceiver 968 or external 5 feedback ( e . g . , visual feedback , auditory feedback , or tactile \\n interface 962 . feedback ) ; and input from the user can be received in any Device 950 may communicate wirelessly through com - form , including acoustic , speech , or tactile input . \\n munication interface 966 , which may include digital signal The systems and techniques described here can be imple \\n processing circuitry where necessary . Communication inter mented in a computing system that includes a back end \\n face 966 may provide for communications under various 10 component ( e . g . , as a data server ) , or that includes a middle modes or protocols , such as GSM voice calls , SMS , EMS , ware component ( e . g . , an application server ) , or that or MMS messaging , CDMA , TDMA , PDC , WCDMA , includes a front end component ( e . g . , a client computer \\n CDMA2000 , or GPRS , among others . Such communication having a graphical user interface or a Web browser through may occur , for example , through radio - frequency trans which a user can interact with an implementation of the \\n ceiver 968 . In addition , short - range communication may 15 systems and techniques described here ) , or any combination \\n occur , such as using a Bluetooth , WiFi , or other such of such back end , middleware , or front end components . The transceiver ( not shown ) . In addition , GPS ( Global Position components of the system can be interconnected by any ing System ) receiver module 970 may provide additional form or medium of digital data communication ( e . g . , a \\n navigation - and location - related wireless data to device 950 , communication network ) . Examples of communication net \\n which may be used as appropriate by applications running 20 works include a local area network ( “ LAN ” ) , a wide area \\n on device 950 . network ( “ WAN ” ) , and the Internet . Device 950 may also communicate audibly using audio The computing system can include clients and servers . A \\n codec 960 , which may receive spoken information from a client and server are generally remote from each other and user and convert it to usable digital information . Audio typically interact through a communication network . The \\n codec 960 may likewise generate audible sound for a user , 25 relationship of client and server arises by virtue of computer such as through a speaker , e . g . , in a handset of device 950 . programs running on the respective computers and having a \\n Such sound may include sound from voice telephone calls , client - server relationship to each other . \\n may include recorded sound ( e . g . , voice messages , music A number of embodiments have been described . Never files , etc . ) and may also include sound generated by appli - theless , it will be understood that various modifications may \\n cations operating on device 950 . 30 be made without departing from the spirit and scope of the The computing device 950 may be implemented in a invention . number of different forms , as shown in the figure . For In addition , the logic flows depicted in the figures do not example , it may be implemented as a cellular telephone 980 . require the particular order shown , or sequential order , to \\n It may also be implemented as part of a smartphone 982 , achieve desirable results . In addition , other steps may be', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 25}), Document(page_content='It may also be implemented as part of a smartphone 982 , achieve desirable results . In addition , other steps may be \\n personal digital assistant , or other similar mobile device . 35 provided , or steps may be eliminated , from the described \\n Various implementations of the systems and techniques flows , and other components may be added to , or removed described here can be realized in digital electronic circuitry , from , the described systems . Accordingly , other embodi integrated circuitry , specially designed ASICs ( application ments are within the scope of the following claims . \\n specific integrated circuits ) , computer hardware , firmware , What is claimed is : software , and / or combinations thereof . These various imple - 40 1 . A computer - implemented method comprising : mentations can include implementation in one or more capturing , using a light sensor , a plurality of images of a \\n computer programs that are executable and / or interpretable subject including a view of an eye of the subject ; \\n on a programmable system including at least one program determining a reflectance metric by at least one of : mable processor , which may be special or general purpose , measuring a time difference between ( i ) an occurrence \\n coupled to receive data and instructions from , and to trans - 45 of a flash pulse captured by one or more of the \\n mit data and instructions to , a storage system , at least one images and ( ii ) an appearance of a corresponding \\n input device , and at least one output device . glare detected on the eye in one or more of the These computer programs ( also known as programs , images ; and software , software applications or code ) include machine measuring uniformity of a pattern of glare on an outer \\n instructions for a programmable processor , and can be 50 surface of the eye visible in one or more of the implemented in a high - level procedural and / or object - ori images caused by one or more flash pulses captured ented programming language , and / or in assembly / machine by one or more of the images , wherein the uniformity \\n language . As used herein , the terms “ machine - readable of a pattern of glare on the eye is measured as a ratio \\n medium ” and “ computer - readable medium ” refer to any of circumference to an area of the glare ; and computer program product , apparatus and / or device ( e . g . , 55 rejecting or accepting the images based on , at least , the magnetic discs , optical disks , memory , Programmable Logic reflectance metric . \\n Devices ( PLDs ) ) used to provide machine instructions and 2 . The method of claim 1 , further comprising changing or data to a programmable processor , including a machine - one or more parameters at different times during the cap \\n readable medium that receives machine instructions as a turing of the images . machine - readable signal . The term \" machine - readable sig - 60 3 . The method of claim 2 , wherein a particular parameter \\n nal ” refers to any signal used to provide machine instruc - comprises a flash pulse , a focus setting of the light sensor , \\n tions and / or data to a programmable processor . brightness of a display , exposure of the light sensor , white \\n To provide for interaction with a user , the systems and balance of the light sensor , an illumination of the subject , or techniques described here can be implemented on a com - external stimuli . puter having a display device ( e . g . , a CRT ( cathode ray tube ) 65 4 . The method of claim 1 , wherein measuring the time or LCD ( liquid crystal display ) monitor ) for displaying difference comprises synchronizing the flash pulse with the information to the user and a keyboard and a pointing device capturing of one or more of the images .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 25}), Document(page_content='US 9 , 971 , 920 B2 \\n 27 28 \\n 5 . The method of claim 1 , further comprising : 15 . The system of claim 12 , wherein measuring the time determining a spatial metric based on , at least , a distance difference comprises synchronizing the flash pulse with the from the light sensor to a landmark that appears in a capturing of one or more of the images . \\n plurality of the images each having a different respec 16 . The system of claim 12 , wherein the operations further \\n tive focus distance , 5 comprise : \\n wherein the rejecting or accepting the images is further determining a spatial metric based on , at least , a distance \\n based on the spatial metric . from the light sensor to a landmark that appears in a \\n 6 . The method of claim 5 , wherein the spatial metric is a plurality of the images each having a different respec \\n measure of deviation of the subject from an expected tive focus distance , \\n three - dimensional shape . 10 wherein the rejecting or accepting the images is further \\n 7 . The method of claim 5 , wherein determining the spatial based on the spatial metric . \\n metric comprises determining parallax of two or more 17 . The system of claim 16 , wherein the spatial metric is \\n landmarks that appear in a plurality of the images . a measure of deviation of the subject from an expected \\n 8 . The method of claim 1 , further comprising : three - dimensional shape . \\n determining a behavioral metric based on , at least , 15 at 15 18 . The system of claim 16 , wherein determining the \\n detected movement of the eye as the eye appears in a spatial metric comprises determining parallax of two or \\n plurality of the images , wherein the behavioral metric more landmarks that appear in a plurality of the images . \\n is a measure of deviation of detected movement and 19 . The system of claim 12 , wherein the operations further \\n timing from expected movement of the eye , comprise : \\n wherein the rejecting or accepting the images is further 20 her 20 determining a behavioral metric based on , at least , detected movement of the eye as the eye appears in a based on the spatial metric . \\n 9 . The method of claim 8 , wherein determining the plurality of the images , wherein the behavioral metric \\n behavioral metric comprises determining an onset , duration , is a measure of deviation of detected movement and \\n velocity , or acceleration of pupil constriction in response to timing from expected movement of the eye , 25 wherein the rejecting or accepting the images is further photic stimuli . \\n 10 . The method of claim 8 , wherein determining the based on the spatial metric . \\n behavioral metric comprises determining an onset , duration , 20 . The system of claim 19 , wherein determining the \\n or acceleration of gaze transition in response to external behavioral metric comprises determining an onset , duration , \\n stimuli . velocity , or acceleration of pupil constriction in response to \\n 11 . The method of claim 8 , wherein determining the 30 photic stimuli . \\n behavioral metric comprises detecting blood flow of the eye 21 . The system of claim 19 , wherein determining the \\n as the eye appears in a plurality of the images . behavioral metric comprises determining an onset , duration , \\n 12 . A system comprising data processing apparatus pro or acceleration of gaze transition in response to external \\n grammed to perform operations comprising : stimuli . \\n capturing , using a light sensor , a plurality of images of a 35 ces of a 35 22 . The system of claim 19 , wherein determining the \\n subject including a view of an eye of the subject ; behavioral metric comprises detecting blood flow of the eye', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 26}), Document(page_content='subject including a view of an eye of the subject ; behavioral metric comprises detecting blood flow of the eye \\n determining a reflectance metric by at least one of : as the eye appears in a plurality of the images . 23 . A non - transitory computer storage medium encoded measuring a time difference between ( i ) an occurrence with instructions that , when executed by a data processing of a flash pulse captured by one or more of the images and ( ii ) an appearance of a corresponding 40 app onding 40 apparatus , cause the data processing apparatus to perform \\n glare detected on the eye in one or more of the operations comprising : \\n images ; and capturing , using a light sensor , a plurality of images of a \\n measuring uniformity of a pattern of glare on an outer subject including a view of an eye of the subject ; \\n surface of the eye visible in one or more of the determining a reflectance metric by at least one of : \\n images caused by one or more flash pulses captured 45 measuring a time difference between ( i ) an occurrence \\n by one or more of the images , wherein the uniformity of a flash pulse captured by one or more of the images and ( ii ) an appearance of a corresponding of a pattern of glare on the eye is measured as a ratio \\n of circumference to an area of the glare ; and glare detected on the eye in one or more of the \\n rejecting or accepting the images based on , at least , the images ; and \\n reflectance metric . measuring uniformity of a pattern of glare on an outer \\n 13 . The system of claim 12 , wherein the operations further surface of the eye visible in one or more of the \\n comprise changing one or more parameters at different times images caused by one or more flash pulses captured \\n during the capturing of the images . by one or more of the images , wherein the uniformity \\n 14 . The system of claim 13 , wherein a particular param of a pattern of glare on the eye is measured as a ratio \\n eter comprises a flash pulse , a focus setting of the light 55 of circumference to an area of the glare ; and \\n sensor , brightness of a display , exposure of the light sensor , rejecting or accepting the images based on , at least , the \\n white balance of the light sensor , an illumination of the reflectance metric . \\n subject , or external stimuli . * * * * * 50', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 26})], [Document(page_content='US01115139732 \\n ( 12 ) United States Patent ( 10 ) Patent No .: US 11,151,397 B2 \\n ( 45 ) Date of Patent : * Oct . 19 , 2021 Kim et al . \\n ( 54 ) LIVENESS TESTING METHODS AND APPARATUSES AND IMAGE PROCESSING \\n METHODS AND APPARATUSES \\n ( 71 ) Applicant : Samsung Electronics Co. , Ltd. , Suwon - si ( KR ) ( 52 ) U.S. CI . CPC G06K 9/00906 ( 2013.01 ) ; G06K 9/00228 ( 2013.01 ) ; GO6K 9/00268 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search CPC . GO6F 11/0718 ; G06F 21/32 ; G06K 9/00268 ; GO6K 9/00906 ; GO6K 9/4609 ; \\n ( Continued ) ( 72 ) Inventors : Wonjun Kim , Hwaseong - si ( KR ) ; Sungjoo Suh , Seoul ( KR ) ; Jaejoon Han , Seoul ( KR ) ; Wonjun Hwang , \\n Seoul ( KR ) ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS ( 73 ) Assignee : Samsung Electronics Co. , Ltd. , Gyeonggi - do ( KR ) 5,487,172 A * 1/1996 Hyatt B6OR 16/0373 \\n 700/8 \\n G06K 9/0004 \\n 340 / 5.53 7,620,212 B1 * 11/2009 Allen ( * ) Notice : \\n ( Continued ) Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 815 days . \\n This patent is subject to a terminal dis \\n claimer . FOREIGN PATENT DOCUMENTS \\n CN \\n CN ( 21 ) Appl . No .: 15 / 156,847 10-1499 164 A 8/2009 \\n 10-2938144 A 2/2013 \\n ( Continued ) \\n ( 22 ) Filed : May 17 , 2016 \\n OTHER PUBLICATIONS ( 65 ) Prior Publication Data \\n US 2016/0328623 A1 Nov. 10 , 2016 Q is for Quantum - An Encyclopedia of Particle Physics . * \\n ( Continued ) \\n Related U.S. Application Data \\n ( 63 ) Continuation of application No. 14 / 612,632 , filed on Feb. 3 , 2015 , now Pat . No. 9,679,212 . Primary Examiner Santiago Garcia ( 74 ) Attorney , Agent , or Firm — Harness , Dickey & Pierce , P.L.C. \\n ( 30 ) Foreign Application Priority Data \\n May 9 , 2014 \\n Jun . 24 , 2014 ( KR ) ( KR ) 10-2014-0055687 \\n 10-2014-0077333 ( 57 ) ABSTRACT \\n A user recognition method and apparatus , the user recogni tion method including performing a liveness test by extract ing a first feature of a first image acquired by capturing a user , and recognizing the user by extracting a second feature of the first image based on a result of the liveness test , is provided . a \\n ( 51 ) Int . Ci . GO6K 9/00 \\n G06T 7700 ( 2006.01 ) ( 2017.01 ) \\n ( Continued ) 18 Claims , 15 Drawing Sheets \\n 125 \\n 120 120 110 110 \\n 115 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 0}), Document(page_content=\"US 11,151,397 B2 Page 2 \\n FOREIGN PATENT DOCUMENTS ( 51 ) Int . Ci . G06T 7/55 ( 2017.01 ) G06K 9/46 ( 2006.01 ) ( 52 ) U.S. Ci . CPC G06K 9/4661 ( 2013.01 ) ; G06T 770002 ( 2013.01 ) ; G06T 7/55 ( 2017.01 ) ; G06K 9/00 ( 2013.01 ) ; GO6T 2207/10004 ( 2013.01 ) ; GO6T 2207/10016 ( 2013.01 ) ; G06T 2207/10028 ( 2013.01 ) ; G06T 2207/30201 ( 2013.01 ) \\n ( 58 ) Field of Classification Search CPC G06K 9/00899 ; G06K 9/00107 ; G06K \\n 9/00885 ; G06K 9/2018 ; G06K 9/00302 ; \\n G06K 9/00288 ; G06K 9/0004 ; GOOK 9/00228 ; G06K 9/00261 ; G06K 9/00597 ; G06K 9/00604 ; G06K 9/00617 ; G06K 9/00892 ; G06K 9/00912 ; G06T 2207/30201 ; G06T 7/0002 ; G06T 7/0048 ; G06T 770081 ; G06T 770083 ; GOOG 2360/16 ; GO9G 5/02 ; HO4L 9/3231 ; HO4L 63/0861 ; G06Q 20/40145 USPC 382/115 , 116 , 117 , 118 , 203 , 224 , 226 , \\n 382/227 , 274 See application file for complete search history . JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n KR \\n KR \\n KR \\n KR \\n KR \\n KR H8-189819 A \\n H11-339048 A \\n 2002/532807 A 2006-259923 A \\n 2006-259931 A \\n 2009-187130 A \\n 2010-231398 A \\n 2012-069133 A \\n 0421221 \\n 0421221 B1 \\n 2005/0084448 A \\n 0887183 0887183 B1 2011/0092752 A 7/1996 \\n 12/1999 10/2002 \\n 9/2006 \\n 9/2006 \\n 8/2009 10/2010 4/2012 2/2004 \\n 2/2004 \\n 8/2005 \\n 2/2009 \\n 2/2009 \\n 8/2011 \\n OTHER PUBLICATIONS \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 8,503,800 B2 8/2013 Blonk et al . 8,675,926 B2 * 3/2014 Zhang G06K 9/00906 \\n 382/118 \\n 2004/0061777 A1 * 4/2004 Sadok GO8B 17/125 \\n 348/83 \\n 2006/0122834 A1 * 6/2006 Bennett GIOL 15/1822 \\n 704/256 \\n 2007/0268312 A1 11/2007 Marks et al . \\n 2008/0253622 A1 * 10/2008 Tosa ..... G06K 9/00604 \\n 382/117 2010/0097470 A1 * 4/2010 Yoshida GO8B 13/19641 \\n 348/159 2010/0189313 A1 * 7/2010 Prokoski A61B 5/0064 \\n 382/118 2011/0187715 A1 * 8/2011 Jacobs HO4N 13/339 \\n 345/426 2011/0299741 A1 * 12/2011 Zhang G06K 9/00228 \\n 382/117 2012/0013651 A1 * 1/2012 Trayner GO2B 5/32 \\n 345/690 2012/0155725 A1 * 6/2012 Bathe G06T 7/20 \\n 382/128 2013/0212655 A1 * 8/2013 Hoyos G06K 9/00107 \\n 726/5 \\n 2013/0219480 A1 * 8/2013 Bud G06F 21/32 \\n 726/7 2013/0321672 Al 12/2013 Silverstein et al . \\n 2014/0168453 A1 * 6/2014 Shoemake HO4N 5/23206 \\n 348 / 207.11 2014/0201126 A1 * 7/2014 Zadeh G06K 9/627 \\n 706/52 2014/0270404 Al * 9/2014 Hanna G06Q 30/0609 382/116 \\n 2014/0270409 A1 * 9/2014 Hanna G06Q 30/0609 382/118 \\n 2014/0283113 A1 * 9/2014 Hanna G06F 21/32 \\n 726/27 \\n 2015/0124072 A1 5/2015 Wei et al . 2015/0237273 A1 8/2015 Sawadaishi 2015/0324629 A1 * 11/2015 Kim G06K 9/00228 \\n 382/203 \\n 2015/0324993 Al 11/2015 Stein et al . 2016/0085958 A1 * 3/2016 Kang G06F 21/40 \\n 726/19 Notice of Allowance issued in U.S. Appl . No. 14 / 612,632 , dated \\n Jan. 25 , 2017 . \\n U.S. Office Action dated Oct. 17 , 2017 in U.S. Appl . No. 15 / 499,164 . T. Brox , “ A TV flow based local scale measure for texture discrimi nation , ” in Proc . ECCV , May 2004 , 12 pgs . J. Weickert , “ Efficient and reliable schemes for nonlinear diffusion filtering , ” IEEE Trans . Image Process . , vol . 7 , No. 3 , Mar. 1998 , 14 \\n pgs . H. Seo , “ Face verification using the lark representation , ” IEEE Tr . Information Forensics and Security , vol . 6 , No. 4 , Dec. 2011 , 12 pgs . B. Wang , “ Illumination Normalization Based on Weber's Law With Application to Face Recognition ” , IEEE Signal Processing Letters , \\n Aug. 2011 , 4 pgs . , vol . 18 , No. 8 . T. Chen , “ Total Variation Models for Variable Lighting Face Rec ognition ” , IEEE Transactions on Pattern Analysis and Machine Intelligence , Sep. 2006 , vol . 28 , No. 9 , IEEE Computer Society . W. Kim , “ Face Liveness Detection from a Single Image via Non linear Diffusion Speed Model ” , 1 Multimedia Processing Lab . , SAIT , SEC , 130 , 2014 , 2 pgs . Extended European Search Report issued in European Patent Appli\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 1}), Document(page_content='cation No. 15163789 , dated Nov. 20 , 2015 . Tan , et al . , “ Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model ” , Department of Computer Science and Technology , ECCV 2010 , Part VI , LNCS 6316 , pp . 504-517 . Kim , et al . , Face Liveness Detection From A Single Image Via Diffusion Speed Model , IEEE Transactions on Imafe Processing , vol . 24 , No. 8 , Aug. 2015 , pp . 2456-2465 . Scherzer , et al . , “ Variational Methods in Imaging ” , Springer Science & Business Media , Jan. 1 , 2009 , pp . 185-203 . U.S. Office Action dated Jun . 1 , 2017 in U.S. Appl . No. 15 / 499,164 . Final Office Action dated May 17 , 2018 in U.S. Appl . No. 15 / 499,164 . Office Action for corresponding U.S. Appl . No. 14 / 612,632 dated Aug. 30 , 2016 . Pierre Buyssens , Marinette Revenu . Label Diffusion on Graph for Face Identification . ICB , Mar. 2012 , New Delhi , India . pp . 46-51 , 2012 . \\n Office Action for corresponding U.S. Appl . No. 14 / 612,632 dated Mar. 14 , 2016 . Non - Final Office Action dated Oct. 23 , 2018 in U.S. Appl . No. 15 / 499,164 . Office Action dated Nov. 27 , 2018 in Japanese Patent Application \\n No. 2015-056767 . Extended European Search Report dated Sep. 22 , 2020 in European Application No. 20169859.4 . Yu - Jin Zhang , “ Advances in Face Image Analysis : Techniques and Technologies , ” Tsinghua University , Jan. 1 , 2020 , Beijing , China . Joachim Weickert , “ Anisotropic Diffusion in Image Processing , \" Department of Computer Science , University of Copenhagen , Jan. 1 , 1998 , Copenhagen , Denmark . Notice of Allowance dated Mar. 13 , 2019 , in U.S. Appl . No. 15 / 499,164 . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 1}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 1 of 15 US 11,151,397 B2 \\n FIG . 1A \\n 120 110 \\n 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 2}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 2 of 15 US 11,151,397 B2 \\n FIG . 1B \\n 125 120 110 \\n 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 3}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 3 of 15 US 11,151,397 B2 \\n FIG . 2 \\n Oo Oo \\n 225 \\n 215 \\n 210 \\n 220 211', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 4}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 4 of 15 US 11,151,397 B2 \\n FIG . 3 \\n 310 \\n 311 312 \\n INPUT \\n IMAGE RECEIVER TESTER TEST \\n RESULT', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 5}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 5 of 15 US 11,151,397 B2 \\n FIG . 4 \\n 410 420 430 \\n 412 432', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 6}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 6 of 15 US 11,151,397 B2 \\n FIG . 5 \\n 9 \\n # • \\n K \\n 510 520', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 7}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 7 of 15 US 11,151,397 B2 \\n FIG . 6 \\n 600 \\n 613 \\n 611 612 TESTER TEST \\n RESULT INPUT \\n IMAGE RECEIVER DIFFUSER', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 8}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 8 of 15 US 11,151,397 B2 \\n FIG . 7 \\n 710 720 ON \\n 715 \\n 730', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 9}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 9 of 15 US 11,151,397 B2 \\n FIG . 8', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 10}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 10 of 15 US 11,151,397 B2 \\n FIG . 9 \\n 910 920 \\n 913 \\n 911 912 FACE RECOGNITION \\n AND / OR \\n USER VERIFICATION CIRCUIT GENERATOR \\n INPUT \\n IMAGE RECEIVER H DIFFUSER', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 11}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 11 of 15 US 11,151,397 B2 \\n FIG . 10 \\n START \\n 1010 \\n RECEIVE INPUT IMAGE \\n 1020 \\n TEST LIVENESS \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 12}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 12 of 15 US 11,151,397 B2 \\n FIG . 11 \\n START \\n 1110 \\n RECEIVE FIRST IMAGE \\n 1120 \\n GENERATE SECOND IMAGE \\n 1130 \\n GENERATE THIRD IMAGE \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 13}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 13 of 15 US 11,151,397 B2 \\n FIG . 12 \\n START \\n 1210 \\n RECEIVE \\n FIRST IMAGE \\n 1220 \\n GENERATE \\n SECOND IMAGE 1240 \\n CALCULATE DIFFUSION VELOCITY 1230 \\n 1250 GENERATE \\n THIRD IMAGE EXTRACT DIFFUSION \\n VELOCITY BASED \\n STATISTICAL \\n INFORMATION \\n 1270 \\n NO LIVENESS \\n TEST SUCCESSFUL ? \\n YES \\n 1260 \\n PERFORM FACE RECOGNITION AND / OR USER VERIFICATION \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 14}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 14 of 15 US 11,151,397 B2 \\n FIG . 13 \\n 1300 \\n IMAGE \\n SENSOR 1302 \\n IMAGE \\n PROCESSOR \\n 1304 \\n DISPLAY \\n 1308 \\n MEMORY \\n 1306', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 15}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 15 of 15 US 11,151,397 B2 \\n FIG . 14 \\n START \\n 1410 \\n FIRST IMAGE DIFFUSE FIRST IMAGE \\n 1420 \\n NO LIVENESS \\n TEST \\n SUCCESSFUL ? \\n YES 1430 \\n PERFORM USER RECOGNITION \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 16}), Document(page_content='15 \\n a US 11,151,397 B2 \\n 1 2 \\n LIVENESS TESTING METHODS AND corresponding to the diffusion speed greater than or equal to APPARATUSES AND IMAGE PROCESSING the first threshold , among the diffusion speeds ; calculating at \\n METHODS AND APPARATUSES least one of an average or a standard deviation of the diffusion speeds ; or calculating a filter response based on the CROSS - REFERENCE TO RELATED 5 diffusion speeds . \\n APPLICATIONS The extracting of the first feature may include : extracting a first - scale region from the first image based on the diffu This application is a Continuation of and claims priority sion speeds ; and calculating an amount of noise components under 35 U.S.C. $ 120 to U.S. application Ser . No. 14/612 , included in the first - scale region based on a difference 632 , filed Feb. 3 , 2015 , which claims priority under 35 10 between the first - scale region and a result of applying U.S.C. $ 119 to Korean Patent Application No. 10-2014 median filtering to the first - scale region . 0055687 , filed on May 9 , 2014 , and Korean Patent Appli The performing may include : determining whether an cation No. 10-2014-0077333 , filed on Jun . 2014 , in the object included in the image has a planar property or a Korean Intellectual Property Office , the entire contents of which are incorporated herein by reference . three - dimensional ( 3D ) structural property , based on the first feature ; outputting a signal corresponding a failed test in a \\n BACKGROUND case in which the object is determined to have the planar property ; and outputting a signal corresponding to a suc \\n Field cessful test in a case in which the object is determined to One or more example embodiments relate to liveness 20 have the 3D structural property . testing methods , liveness testing apparatuses , image pro The performing may include : calculating a degree of cessing methods , image processing apparatuses , and / or elec- uniformity in a distribution of light energy included in a tronic devices including the same . plurality of pixels corresponding to an object included in the', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content=\"Description of the Related Art image based on the first feature ; outputting a signal corre Biometric technologies may identify a human based on 25 sponding to a failed test in a case in which the degree of unique biometric characteristics of each individual user . uniformity in the distribution of the light energy is greater Among conventional biometric technologies , a face recog- than or equal to a threshold ; and outputting a signal corre nition system may naturally recognize a user based on the sponding to a successful test in a case in which the degree user's face without requiring user contact with a sensor , such of uniformity in the distribution of the light energy is less as a fingerprint scanner or the like . However , conventional 30 than the threshold . face recognition systems may be vulnerable to imperson- The performing may further include at least one of : ations using a picture of a face of a registered target . determining whether the first feature corresponds to a fea ture related to a medium that displays a face or a feature SUMMARY related to an actual face ; outputting a signal corresponding 35 to a failed test in a case in which the first feature corresponds At least one example embodiment provides a user recog- to the feature related to a medium that displays a face ; or nition method including : receiving a first image acquired by outputting a signal corresponding to a successful test in a capturing a user ; performing a liveness test by extracting a case in which the first feature corresponds to the feature first feature of the first image ; and recognizing the user by related to an actual face . extracting a second feature of the first image based on a 40 The user recognition method may further include receiv result of the liveness test . The first image may correspond to ing a user verification request for approving an electronic a first frame in a video , and the new image may correspond commerce payment . The user recognition method may fur to a second frame in the video . ther include approving the electronic commerce payment in According to at least some example embodiments , the a case in which user verification succeeds in the recognizing . user recognition method further include , in a case in which 45 The user recognition method may further include receiv the result of the liveness test corresponds to a failed test : ing a user input which requires user verification . The user receiving a new image ; performing a liveness test by extract- recognition method may further include performing an ing a first feature of the new image ; and recognizing the user operation corresponding to the user input in a case in which by extracting a second feature of the new image based on a user verification succeeds in the recognizing . The user input result of the liveness test on the new image . 50 which requires user verification may include at least one of The performing may include : generating a second image a user input to unlock a screen , a user input to execute a by diffusing a plurality of pixels included in the first image ; predetermined application , a user input to execute a prede calculating diffusion speeds of the pixels based on a differ- termined function in an application , or a user input to access ence between the first image and the second image ; and a predetermined folder or file . extracting the first feature based on the diffusion speeds . The user recognition method may further include receiv The generating may include : iteratively updating value of ing a user input related to a gallery including a plurality of the pixels using a diffusion equation . items of contents . The user recognition method may further The extracting of the first feature may include : estimating include sorting content corresponding to a user among the a surface property related to an object included in the first plurality of items of contents in the gallery in a case in which image based on the diffusion\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='content corresponding to a user among the a surface property related to an object included in the first plurality of items of contents in the gallery in a case in which image based on the diffusion speeds . The surface property 60 the user is identified in the recognizing ; and providing the may include at least one of a light - reflective property of a sorted content to the user .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='surface of the object , a number of dimensions of the surface At least one other example embodiment provides of the object , or a material of the surface of the object . recognition method including : receiving a video acquired by The extracting of the first feature may include at least one capturing a user ; performing a liveness test based on at least of : calculating a number of pixels corresponding to a dif- 65 one first frame in the video ; and recognizing the user based fusion speed greater than or equal to a first threshold , among on at least one second frame in the video based on a result the diffusion speeds ; calculating a distribution of the pixels of the liveness test . 55 \\n user', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='a \\n a \\n ? \\n > US 11,151,397 B2 \\n 3 4 \\n The performing may include at least one of : determining ever , be embodied in many alternate forms and should not be the video to be a live video in a case in which the result of construed as limited to only the embodiments set forth the liveness test corresponds to a successful test ; determin herein . \\n ing the video to be a fake video in a case in which the result Accordingly , while example embodiments are capable of \\n of the liveness test corresponds to a failed test ; or determin- 5 various modifications and alternative forms , the embodi \\n ing the video to be a live video in a case in which the at least ments are shown by way of example in the drawings and will \\n one first frame includes a predetermined number of con be described herein in detail . It should be understood , \\n secutive frames and a result of a liveness test on the however , that there is no intent to limit example embodi \\n consecutive frames corresponds to a successful test . ments to the particular forms disclosed . On the contrary ,', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content='consecutive frames corresponds to a successful test . ments to the particular forms disclosed . On the contrary , \\n The at least one first frame may differ from the at least one 10 example embodiments are to cover all modifications , equivalents , and alternatives falling within the scope of this second frame . For example , the at least one first frame may disclosure . Like numbers refer to like elements throughout include at least one frame suitable for a liveness test , and the the description of the figures . at least one second frame may include at least one frame Although the terms first , second , etc. may be used herein suitable for user recognition . 15 to describe various elements , these elements should not be At least one other example embodiment provides a user limited by these terms . These terms are only used to distin recognition apparatus including a processor configured to guish one element from another . For example , a first element perform a liveness test by extracting a first feature of a first could be termed a second element , and similarly , a second image acquired by capturing a user , and recognize the user element could be termed a first element , without departing by extracting a second feature of the first image based on a 20 from the scope of this disclosure . As used herein , the term result of the liveness test . “ and / or , ” includes any and all combinations of one or more of the associated listed items . BRIEF DESCRIPTION OF THE DRAWINGS When an element is referred to as being “ connected , ” or “ coupled , ” to another element , it can be directly connected Example embodiments will become more apparent and 25 or coupled to the other element or intervening elements may more readily appreciated from the following description of be present . By contrast , when an element is referred to as the example embodiments shown in the drawings in which : being “ directly connected , ” or “ directly coupled , ” to another FIGS . 1A and 1B illustrate a liveness test according to element , there are no intervening elements present . Other example embodiments ; words used to describe the relationship between elements FIG . 2 illustrates a principle of a liveness test according 30 should be interpreted in a like fashion ( e.g. , “ between , \" to an example embodiment ; versus “ directly between , ” “ adjacent , ” versus “ directly adja FIG . 3 illustrates a liveness testing apparatus according to cent , \" etc. ) . an example embodiment ; The terminology used herein is for the purpose of describ FIG . 4 illustrates a diffusion process according to an ing particular embodiments only and is not intended to be example embodiment ; 35 limiting . As used herein , the singular forms “ a , ” “ an , ” and FIG . 5 illustrates an example small - scale region ( SR ) map “ the , ” are intended to include the plural forms as well , unless according to example embodiments ; the context clearly indicates otherwise . It will be further FIG . 6 illustrates a liveness testing apparatus according to understood that the terms “ comprises , \" \" comprising , \" \\n an example embodiment ; “ includes , \" and / or “ including , ” when used herein , specify FIG . 7 illustrates an example input image and example 40 the presence of stated features , integers , steps , operations , images processed according to an example embodiment ; elements , and / or components , but do not preclude the pres FIG . 8 illustrates example changes in an input image as a ence or addition of one or more other features , integers , result of changes in illumination , according to example steps , operations , elements , components , and / or groups \\n embodiments ; thereof .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content='embodiments ; thereof . \\n FIG . 9 illustrates an image processing apparatus accord- 45 It should also be noted that in some alternative imple ing to an example embodiment ; mentations , the functions / acts noted may occur out of the FIG . 10 illustrates a liveness testing method according to order noted in the figures . For example , two figures shown an example embodiment ; in succession may in fact be executed substantially concur FIG . 11 illustrates an image processing method according rently or may sometimes be executed in the reverse order , to an example embodiment ; 50 depending upon the functionality / acts involved . FIG . 12 illustrates an image processing and authentica- Specific details are provided in the following description tion / verification method according to another example to provide a thorough understanding of example embodi \\n embodiment ; ments . However , it will be understood by one of ordinary FIG . 13 is a block diagram illustrating an electronic skill in the art that example embodiments may be practiced system according to an example embodiment ; and 55 without these specific details . For example , systems may be FIG . 14 is a flowchart illustrating a user recognition shown in block diagrams so as not to obscure the example method according to an example embodiment . embodiments in unnecessary detail . In other instances , well known processes , structures and techniques may be shown DETAILED DESCRIPTION without unnecessary detail in order to avoid obscuring 60 example embodiments . Various example embodiments will now be described In the following description , example embodiments will more fully with reference to the accompanying drawings in be described with reference to acts and symbolic represen which some example embodiments are shown . tations of operations ( e.g. , in the form of flow charts , flow Detailed illustrative embodiments are disclosed herein . diagrams , data flow diagrams , structure diagrams , block However , specific structural and functional details disclosed 65 diagrams , etc. ) that may be implemented as program mod herein are merely representative for purposes of describing ules or functional processes include routines , programs , example embodiments . Example embodiments may , how- objects , components , data structures , etc. , that perform par > \\n 2', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content=\"a \\n may be US 11,151,397 B2 \\n 5 6 \\n ticular tasks or implement particular abstract data types and sensor 115 that photographs the face of the user 120. The may be implemented using existing hardware at , for image sensor 115 may also be part of the liveness testing example , existing electronic devices , such as smartphones , apparatus 110 . personal digital assistants , laptop or tablet computers , etc. In one example , as shown in FIG . 1A , the input image is Such existing hardware may include one or more Central 5 generated by photographing the actual face of the user 120 . Processing Units ( CPUs ) , graphics processing units ( GPUs ) , In this example , the liveness testing apparatus 110 deter mines that the face included in the input image corresponds image processors , system - on - chip ( SOC ) devices , digital signal processors ( DSPs ) , application - specific - integrated to a real ( or living ) three - dimensional object , and outputs a \\n circuits , field programmable gate arrays ( FPGAs ) computers signal indicating that the face included in the input image \\n or the like . 10 corresponds to a real three - dimensional object . That is , for \\n Although a flow chart may describe the operations as a example , the liveness testing apparatus 110 tests whether the face included in the input image corresponds to the real ( or sequential process , many of the operations may be per formed in parallel , concurrently or simultaneously . In addi living ) three - dimensional object , and outputs a signal indi cating a successful test since the face in the input image does tion , the order of the operations may be re - arranged . A 15 indeed correspond to the real ( or living ) three - dimensional process may be terminated when its operations are com object . pleted , but may also have additional steps not included in the In another example , as shown in FIG . 1B , the input image figure . A process may correspond to a method , function , is generated by photographing a face displayed on a display procedure , subroutine , subprogram , etc. When a process medium 125 , rather than the actual face of the user 120 . corresponds to a function , its termination may correspond to 20 According to at least this example , the display medium 125 a return of the function to the calling function or the main refers to a medium that displays an object ( e.g. , a face ) in \\n function . two dimensions . The display medium 125 may include , for Reference will now be made in detail to the example example , a piece of paper on which a face of a user is printed embodiments illustrated in the accompanying drawings , ( e.g. , a photo ) , an electronic device displaying a face of a wherein like reference numerals refer to like elements 25 user , etc. In one example scenario , the user 120 may attempt throughout . Example embodiments are described below to to log into an electronic device ( e.g. , a smartphone or the \\n explain the present disclosure by referring to the figures . like ) with another user's account by directing a face dis \\n One or more example embodiments described below played on the display medium 125 toward the image sensor \\n applicable to various fields , for example , smartphones , lap 115. In FIG . 1B , the face displayed on the display medium top or tablet computers , smart televisions ( TVs ) , smart home 30 125 is marked with a broken line to indicate that the face \\n systems , smart cars , surveillance systems , etc. For example , displayed on the display medium 125 is directed toward the \\n one or more example embodiments may be used to test a image sensor 115 , rather than the user 120. In this example , the liveness testing apparatus 110 determines that the face liveness of an input image and / or authenticate a user to log into a smartphone or other device . In addition , one or more 35 dimensional representation of the object , and outputs a included in the input image corresponds to a fake two\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 19}), Document(page_content='example embodiments may be used to test a liveness of an signal indicating that the face included in the input image input image and / or authenticate a user for admission control corresponds to a fake two - dimensional representation of the and / or monitoring in a public area and / or a secured area . object . That is , for example , the liveness testing apparatus 110 tests whether the face included in the input image Liveness Test According to Example Embodiments 40 corresponds to the real ( or living ) three - dimensional object , and outputs a signal indicating a failed test since the face in FIGS . 1A and 1B illustrate a liveness test according to an the input image does not correspond to the real ( or living ) example embodiment . three - dimensional object , but rather the fake two - dimen According to at least some example embodiments , a sional representation of the object . In some cases , the term liveness test refers to a method of testing ( or determining ) 45 fake object may be used to refer to the fake two - dimensional whether an object included in an input image corresponds to representation of the object . a real three dimensional object . In one example , the liveness The liveness testing apparatus 110 may detect a face test may verify whether a face included in an input image region from the input image . In this example , liveness corresponds to ( or is obtained from ) a real three - dimensional testing method and apparatus may be applicable to the face ( 3D ) object ( e.g. , an actual face ) or a fake two - dimensional 50 region detected from the input image . ( 2D ) representation of the object ( e.g. , a picture of a face ) . FIG . 2 illustrates a principle of a liveness test according Through the liveness test , an attempt to verify a face of to an example embodiment . another using a forged and / or falsified picture may be A liveness testing apparatus according to at least this effectively rejected . example embodiment tests a liveness of an object included Referring to FIGS . 1A and 1B , according to at least one 55 in an input image based on whether the object has one or example embodiment , a liveness testing apparatus 110 more characteristics of a flat ( two - dimensional ) surface or a receives an input image including a face of a user 120 , and three - dimensional ( 3D ) structure . tests a liveness of the face included in the received input Referring to FIG . 2 , the liveness testing apparatus distin image . In one example , the liveness testing apparatus 110 guishes between a face 211 displayed on a medium 210 and may be ( or be included in ) a mobile device , for example , a 60 an actual face 220 of a user . The face 211 displayed on the mobile phone , a smartphone , a personal digital assistant medium 210 corresponds to a two - dimensional ( 2D ) flat ( PDA ) , a tablet computer , a laptop computer , etc. In another surface . When an input image is generated by photographing example , the liveness testing apparatus 110 may be ( or be the face 211 displayed on the medium 210 , an object included in ) a computing device , for example , a personal included in the input image has one or more characteristics computer ( PC ) , an electronic product , such as a TV , a 65 of a flat surface . Since a surface of the medium 210 security device for a gate control , etc. The liveness testing corresponds to a 2D flat surface , light 215 incident on the apparatus 110 may receive the input image from an image face 211 displayed on the medium 210 is more uniformly a \\n a a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 19}), Document(page_content='a \\n a US 11,151,397 B2 \\n 7 8 \\n reflected by the surface of the medium 210. Thus , light input image has one or more characteristics of a 3D struc energy is more uniformly distributed on the object included ture . In this case , the liveness testing apparatus determines in the input image . Even if the medium 210 is curved , a that the face of the input image corresponds to a real 3D surface of a curved display may still correspond to , and have object , and outputs a signal indicating the same ( e.g. , characteristics , of a 2D flat surface . 5 indicating a successful test ) . Conversely , the actual face 220 of the user is a 3D In one example , the threshold degree of uniformity may structure . When an input image is generated by photograph- be a value corresponding to a situation in which about 50 % ing the actual face 220 of the user , an object included in the or more of a number of pixels included in an image portion input image has characteristics of a 3D structure . Since the correspond to a face region ( e.g. , more generally , a region in actual face 220 of the user corresponds to a 3D structure 10 which a portion corresponding to a face region is indicated having various 3D curves and shapes , light 225 incident to by a bounding box ) . the actual face 220 of the user is less ( or non- ) uniformly The liveness testing apparatus may test the liveness of an reflected by a surface of the actual face 220 of the user . Thus , object based on a single input image . The single input image light energy is less ( or non- ) uniformly distributed on the may correspond to a single picture , a single image , a still object included in the input image . 15 image of a single frame , etc. The liveness testing apparatus According to at least one example embodiment , a liveness may test a liveness of an object included in a single input testing apparatus tests a liveness of an object included in an image by determining whether the object has one or more input image based on a distribution of light energy in the characteristics of a flat 2D surface or of a 3D structure . In object . In one example , the liveness testing apparatus ana- more detail , for example , the liveness testing apparatus may lyzes the distribution of light energy included in the object 20 test the liveness of the object included in the single input of the input image to determine whether the object included image by calculating a degree of uniformity in the distribu in the input image has characteristics of a flat 2D surface or tion of light energy included in the object . \\n of a 3D structure . FIG . 3 illustrates a liveness testing apparatus 310 accord Still referring to FIG . 2 , in one example , an input image ing to an example embodiment . may be generated by photographing the face 211 displayed 25 Referring to FIG . 3 , the liveness testing apparatus 310', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='on the medium 210. In this case , the liveness testing appa- includes a receiver 311 and a tester 312 . ratus analyzes a distribution of light energy included in the In example operation , the receiver 311 receives an input face of the input image , and determines that the face of the image . The receiver 311 may receive an input image gen input image has one or more characteristics of a flat surface . erated by an image sensor ( not shown ) . The receiver 311 The liveness testing apparatus then determines that the face 30 may be connected to the image sensor using a wire , wire of the input image corresponds to a fake object , and outputs lessly , or via a network . Alternatively , the receiver 311 may a signal indicating the same ( e.g. , indicating a failed test ) . receive the input image from a storage device , such as a In another example , an input image may be generated by main memory , a cache memory , a hard disk drive ( HDD ) , a photographing the actual face 220 of the user . In this case , solid state drive ( SSD ) , a flash memory device , a network the liveness testing apparatus analyzes a distribution of light 35 drive , etc. energy included in a face of the input image , and determines The tester 312 tests a liveness of an object included in the that the face of the input image has one or more character- input image . As discussed above , the tester 312 may test the istics of a 3D structure . The liveness testing apparatus then liveness of the object by determining whether the object has determines that the face of the input image corresponds to a one or more characteristics of a flat 2D surface or of a 3D real 3D object , and outputs a signal indicating the same ( e.g. , 40 structure . In one example , a successful test is one in which indicating a successful test ) . the object is determined to have one or more characteristics According to at least some example embodiments , the of a 3D structure , whereas a failed test is one in which the liveness testing apparatus may determine a liveness of an object is determined to have one or more characteristics of object included in an input image based on a degree of a flat 2D surface . In more detail , for example , the tester 312 uniformity in the distribution of light energy included in the 45 may test the liveness of the object by analyzing a distribution object in the input image . With regard again to FIG . 2 , in one of light energy included in the object of the input image . In example , since the light 215 incident to the face 211 dis- a more specific example , the tester 312 may test the liveness played on the medium 210 is reflected substantially uni- of the object by calculating a degree of uniformity in the formly , light energy included in the face of the input image distribution of the light energy included in the object of the is distributed substantially uniformly . When a degree of 50 input image , and comparing the degree of uniformity of the uniformity in the distribution of the light energy included in distribution of light energy in the object of the input image the face of the input image is greater than or equal to a given with a threshold value . If the determined degree of unifor ( or alternatively , desired or predetermined ) threshold degree mity is greater than or equal to a threshold value , then the of uniformity , the liveness testing apparatus determines that object in the input image is determined to correspond to the face of the input image has one or more characteristics 55 ( have been obtained from ) a flat 2D surface . On the other of a flat 2D surface . In this case , the liveness testing hand , if the determined degree of uniformity is less than or apparatus determines that the face of the input image cor- equal to the threshold value , then the object in the input responds to a fake object , and outputs a signal indicating the image is determined to correspond to ( have been obtained same ( e.g. , indicating a failed test )', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='value , then the object in the input responds to a fake object , and outputs a signal indicating the image is determined to correspond to ( have been obtained same ( e.g. , indicating a failed test ) . from ) a 3D structure . In another example with regard to FIG . 2 , since the light 60 According to at least some example embodiments , the 225 incident on the actual face 220 of the user is less ( or tester 312 may filter a plurality of pixels corresponding to non- ) uniformly reflected , light energy included in the face the object included in the input image to analyze the of the input image has a light distribution that is less ( or distribution of the light energy included in the plurality of non- ) uniform . When a degree of uniformity in the distri- pixels corresponding to the object included in the input bution of the light energy included in the face of the input 65 image . In one example , the tester 312 may filter the plurality image is less than the given threshold degree of uniformity , of pixels using a diffusion process . In this example , the tester the liveness testing apparatus determines that the face of the 312 may diffuse a plurality of pixels corresponding to the a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='a \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='a \\n 1 \\n a US 11,151,397 B2 \\n 9 10 \\n object included in the input image to analyze the distribution According to at least some example embodiments , the of the light energy included in the plurality of pixels liveness testing apparatus may reduce the final iteration corresponding to the object included in the input image . An count L using the AOS scheme to solve Equation 1. When example diffusion process will be described in more detail the AOS scheme is used , the reliability of ut , which is the below with reference to FIG . 4 . 5 value of the finally diffused pixel , may be sufficiently high Although example embodiments may be discussed in although a time step t of a given size is used . The liveness detail with regard to a diffusion process , it should be testing apparatus may increase efficiency of an operation for understood that any suitable filtering processes may be used the diffusion process using the AOS scheme to solve the in connection with example embodiments . In one example , diffusion equation . The liveness testing apparatus may per example embodiments may utilize bilateral filtering . 10 form the diffusion process using a relatively small amount of Because bilateral filtering is generally well - known , a processor and / or memory resources . detailed description is omitted . Moreover , any suitable fil- The liveness testing apparatus may effectively preserve a tering that preserves an edge region and blurs a non - edge texture of the input image using the AOS scheme to solve the region , in a manner similar or substantially similar to diffusion equation . The liveness testing apparatus may effec diffusion and bilateral filtering , may be used in connection 15 tively preserve the original texture of the input image even with example embodiments discussed herein . in relatively low - luminance and backlit environments . FIG . 4 illustrates a diffusion process according to an Referring to FIG . 4 , image 410 corresponds to an input example embodiment . image , image 420 corresponds to an intermediate diffusion According to at least some example embodiments , a image , and image 430 corresponds to a final diffusion image . liveness testing apparatus may diffuse a plurality of pixels 20 In this example , the final iteration count L is set to “ 20 ” . The corresponding to an object included in an input image . The image 420 was acquired after values of pixels included in the liveness testing apparatus may iteratively update values of input image were iteratively updated five times based on the plurality of pixels using a diffusion equation . In one Equation 3. The image 430 was acquired after the values of example , the liveness testing apparatus may diffuse the the pixels included in the input image were iteratively plurality of pixels corresponding to the object included in 25 updated 20 times based on Equation 3 . the input image according to Equation 1 shown below . According to at least this example embodiment , the \\n 26 + 1 = + * + div ( d ( 1Vukl ) Vu \" ) liveness testing apparatus may use a diffusion speed to [ Equation 1 ] determine whether the object included in the input image has In Equation 1 , k denotes an iteration count , uk denotes a one or more characteristics of a flat surface or of a 3D value of a pixel after a k - th iteration , and uk + 1 denotes a 30 structure . The diffusion speed refers to a speed at which each value of a pixel after a ( k + 1 ) -th iteration . A value of a pixel pixel value is diffused . The diffusion speed may be defined in the input image is denoted uº . as shown below in Equation 4 . Still referring to Equation 1 , V denotes a gradient opera tor , div ( ) denotes a divergence function , and d ( ) denotes a s ( x , y ) = \\\\ u + ( x , y ) -4 ° ( x , y ) [ Equation 4 ]', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 21}), Document(page_content=\"diffusivity function . In Equation 4 , s ( x , y ) denotes a diffusion speed of a pixel The diffusivity function do ) may be given ( or alterna- at coordinates ( x , y ) , uº ( x , y ) denotes a value of the pixel at tively , desired or predetermined ) function . In one example , coordinates ( x , y ) in an input image , and u ' ( x , y ) denotes a the diffusivity function may be defined as shown below in value of the pixel at coordinates ( x , y ) in a final diffusion Equation 2 . image . As shown in Equation 4 , as a difference between a \\n d ( Vul ) = 1 / ( \\\\ Vul + B ) [ Equation 2 ] 40 pixel value before diffusion and a pixel value after diffusion increases , a calculated diffusion speed also increase , In Equation 2 , B denotes a relatively small positive whereas when the difference between the pixel value before number ( e.g. , a minute value , such as about 10-6 ) . When the diffusion and the pixel value after diffusion decreases , the diffusivity function defined as shown above in Equation 2 is calculated diffusion speed decreases . used , a boundary of an object may be preserved relatively More broadly , the liveness testing apparatus may deter well during the diffusion process . When the diffusivity mine whether the object included in the input image has one function is a function of pixel gradient Vu as shown in or more characteristics of a flat surface or of a 3D structure Equation 2 , the diffusion equation is a nonlinear diffusion based on a magnitude of the change in pixel values in the equation . image after L number of iterations of the above - discussed The liveness testing apparatus may apply an additive 50 filtering process . operator splitting ( AOS ) scheme to solve Equation 1 , and the A face image may be divided into a small - scale region and liveness testing apparatus may diffuse the plurality of pixels a large - scale region . The small - scale region may refer to a corresponding to the object included in the input image region in which a feature point or a feature line is present . according to Equation 3 shown below . In one example , the small - scale region may include the eyes , 55 the eyebrows , the nose , and the mouth of the face . The large - scale region may refer to a region in which a relatively uk + 1 [ Equation 3 ] = ( -2 ( ( 1 – 27Ax ( ed ' ) * + ( 1 – 2tAy ( uck ) A Jecke large portion is occupied by skin of the face . In one example , the large - scale region may include the forehead and cheeks \\n of the face . \\n In Equation 3 , I denotes a value of a pixel in an input 60 Diffusion speeds of pixels belonging to the small - scale image , Ac denotes a horizontal diffusion matrix , A , denotes region may be greater than diffusion speeds of pixels belong a vertical diffusion matrix , T denotes a time step . A final ing to the large - scale region . Referring back to the example iteration count L and the time step t may be given ( or shown in FIG . 4 , a pixel 411 corresponding to eyeglass alternatively desired or predetermined ) . In general , when the frames in the image 410 differs from neighboring pixels time step t is set to be relatively small and the final iteration 65 corresponding to skin , and thus , a value of the pixel 411 may count L is set to be relatively large , a reliability of u ? change substantially ( e.g. , relatively greatly ) as a result of denoting a value of a finally diffused pixel may increase . diffusion . The value of the pixel 411 in the image 410 may 35 \\n a \\n 45 \\n a a \\n a \\n 1 \\n a \\n a\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 21}), Document(page_content='15 US 11,151,397 B2 \\n 11 12 \\n be updated to a value of a pixel 431 in the image 430 by differs from the SR map 520 acquired when the actual face diffusion . Conversely , a pixel 412 corresponding to a cheek of the same user is photographed . In the SR map 510 and the in the image 410 is similar to neighboring pixels , and thus , SR map 520 , portions marked with black color correspond a value of the pixel 412 may change less than the value of to pixels satisfying SR ( x , y ) = 1 , whereas portions marked the pixel 411 ( e.g. , relatively slightly ) by diffusion . The 5 with white color correspond to pixels satisfying SR ( x , y ) = 0 . \\n value of the pixel 412 in the image 410 may be updated to In this example , in the SR map 510 and the SR map 520 , the \\n a value of a pixel 432 in the image 430 as a result of black portions have relatively fast diffusion speeds , and the \\n diffusion . white portions have relatively slow diffusion speeds . \\n A difference in diffusion speeds may also result from a According to at least one example embodiment , the distribution of light energy in an image . When light energy 10 liveness testing apparatus may test a liveness of a face in an in the image is more uniformly distributed , a relatively small image by analyzing an SR map . For example , the liveness \\n diffusion speed may be calculated . Moreover , when the light testing apparatus may test the liveness of the face in the \\n energy is more uniformly distributed , the probability of image by extracting various features from the SR map . neighboring pixels having similar pixel values may be When an actual face of a user is photographed , various \\n higher ( e.g. , relatively high ) . Conversely , when light energy light reflections may occur due to curves on the actual face \\n of the user . in the image is less ( or non- ) uniformly distributed , a relatively high diffusion speed may be observed . Moreover , When light energy in the image is less ( or non- ) uniformly \\n when the light energy is less ( or non- ) uniformly distributed , distributed , a relatively large number of pixels having a pixel \\n the probability of neighboring pixels having different pixel value of “ 1 ” may be included in the SR map . In this example , \\n values may be relatively high . 20 the liveness testing apparatus may determine the liveness of \\n According to at least some example embodiments , a the face in the image based on Equation 6 shown below . \\n liveness testing apparatus may calculate a degree of unifor mity in the distribution of the light energy in the image based on statistical information related to diffusion speeds . The 1 , if N ( SR ( x , y ) = 1 ) < 6 [ Equation 6 ] \\n liveness testing apparatus may test a liveness of an object in the image based on the statistical information related to otherwise \\n diffusion speeds . To calculate the statistical information related to diffusion speeds , the liveness testing apparatus In Equation 6 , N ( SR ( x , y ) = 1 ) denotes a number of pixels may extract a small - scale region from an image according to 30 satisfying SR ( x , y ) = 1 , and denotes a threshold value , which Equation 5 shown below . may be given , desired , or alternatively preset . The liveness testing apparatus may determine that the face in the image \\n 1 , if s ( x , y ) > u to corresponds to a fake object when [ Equation 5 ] SR ( x , y ) 0 , otherwise 25 Fake = : ( x , y ) \\n 0 , \\n = { : 35 \\n ( x , y ) \\n a \\n Fake = { ( x , y )', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 22}), Document(page_content='1 , if s ( x , y ) > u to corresponds to a fake object when [ Equation 5 ] SR ( x , y ) 0 , otherwise 25 Fake = : ( x , y ) \\n 0 , \\n = { : 35 \\n ( x , y ) \\n a \\n Fake = { ( x , y ) \\n a N ( SR ( x , y ) = 1 ) < 6 In Equation 5 , SR ( x , y ) is an indicator indicating whether a pixel at coordinates ( x , y ) belongs to a small - scale region . In this example , when a value of SR ( x , y ) corresponds to “ 1 ” , is satisfied . the pixel at the coordinates ( x , y ) belongs to the small - scale 40 In another example , when light energy in the image is less region , whereas when the value of SR ( x , y ) corresponds to ( non- ) uniformly distributed , a relatively large amount of “ O ” , the pixel at the coordinates ( x , y ) does not belong to the small - scale region . noise components may be included in the SR map . In this case , the liveness testing apparatus may determine the The value of SR ( x , y ) may be determined based on a liveness of the face in the image based on Equation 7 shown diffusion speed for the pixel at coordinates ( x , y ) . For 45 below . example , when the diffusion speed s ( x , y ) is greater than a given ( or alternatively , desired or predetermined ) threshold value , the value of SR ( x , y ) is determined to be “ 1 ” . Other wise , the value of SR ( x , y ) is determined to be “ O ” . The 1 , if | SR ( x , y ) – SRM ( x , y ) ] < & [ Equation 7 ] \\n threshold value may be set based on an average y of the 50 0 , otherwise entire image and a standard deviation a of the entire image . The average u of the entire image may correspond to an average of diffusion speeds of pixels included in the entire In Equation 7 , SRx ( x , y ) denotes a value of a pixel at image , and the standard deviation a of the entire image may coordinates ( x , y ) in an image acquired by applying median correspond to a standard deviation of the diffusion speeds of 55 filtering to an SR map , and denotes a threshold value , which the pixels included in the entire image . may be given , desired , or alternatively preset . As the amount Hereinafter , an image in which a value of a pixel at of noise components increases , a number of pixels having a coordinates ( x , y ) corresponds to SR ( x , y ) will be referred to difference between a value of SR ( x , y ) and a value of SR M ( x , as a small - scale region ( SR ) map . Since each pixel included y ) increase . In this example , when in the SR map may have a value of “ O ” or “ 1 ” , the SR map 60 may also be referred to as a binary map . The SR map may effectively represent an underlying structure of a face in various illumination environments . SR ( x , y ) – SRM ( x , y ) < & \\n FIG . 5 illustrates two example SR maps according to example embodiments . Referring to FIG . 5 , as shown , the SR map 510 acquired is satisfied , the face in the image is determined to be a fake when a medium displaying a face of a user is photographed object . a \\n a \\n a a \\n ( x , y ) \\n 65', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 22}), Document(page_content='Live = ( x , y ) \\n ( x 20 \\n ( x , y ) \\n otherwise US 11,151,397 B2 \\n 13 14 \\n Equations 6 and 7 are provided only as examples . The the extracted features and the learned parameters , the clas liveness testing apparatus may test the liveness of the object sifier may output a signal indicating whether the object in the image based on a variety of diffusion speed based included in the input image corresponds to a real object or statistical information . In one example , the liveness testing a fake object \\n apparatus may use a distribution of pixels having diffusion 5 FIG . 6 illustrates a liveness testing apparatus 600 accord speeds greater than or equal to a given , desired or alterna- ing to an example embodiment . tively predetermined , threshold value . Referring to FIG . 6 , the liveness testing apparatus 600 \\n In more detail , the liveness testing apparatus may deter includes : a receiver 611 , a diffuser 612 ; and a tester 613. The \\n mine the liveness of the face in the image based on Equation receiver 611 may correspond to the receiver 311 shown in \\n 8 shown below . 10 FIG . 3 . In example operation , the receiver 611 receives an input image , and outputs the input image to the diffuser 612 and \\n the tester 613 . 1 , if N ( SR ( x , y ) = 1 ) 26 [ Equation 8 ] Although element 612 is referred to as a diffuser and the 15 example embodiment shown in FIG . 6 will be described 0 , otherwise with regard to a diffusion operation , the element 612 may be more generally referred to as a filter or filter circuit 612 . \\n The liveness testing apparatus may determine that the face Moreover , any suitable filtering operation may be used as \\n in the image corresponds to a live 3D object when E ( x , y ) N desired . \\n ( SR ( x , y ) = 125 is satisfied . The diffuser 612 diffuses a plurality of pixels correspond \\n In another example , the liveness testing apparatus may ing to an object included in the input image by iteratively \\n determine the liveness of the face in the image based on updating values of the plurality of pixels corresponding to \\n Equation 9 shown below . the object included in the input image based on a diffusion equation . In one example , the diffuser 612 may diffuse the 25 plurality of pixels corresponding to the object included in \\n 1 , if I ( SR ( x , y ) – SRM ( x , y ) 25 [ Equation 9 ] the input image using Equation 1 discussed above .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='Live = The diffuser 612 may iteratively update the values of the plurality of pixels corresponding to the object included in 0 , the input image by applying an AOS scheme to the diffusion 30 equation . In one example , the diffuser 612 may diffuse the In this example , when ISR ( x , y ) -SR ( x , y ) l2 is sat- plurality of pixels corresponding to the object included in isfied , the face in the image is determined to be a live 3D the input image using Equation 3 discussed above . The object . diffuser 612 may output a diffusion image generated when The liveness testing apparatus may also use diffusion the plurality of pixels are diffused . speed based statistical information without using an SR 35 Still referring to FIG . 6 , the tester 613 tests a liveness of map . In this example , the liveness testing apparatus may use the object included in the input image based on diffusion respective values of diffusion speeds of all pixels , an average speeds of the plurality of pixels . In one example , the tester of the diffusion speeds of all the pixels , and a standard 613 may test the liveness of the object by estimating a deviation of the diffusion speeds of all the pixels . The surface property related to the object based on the diffusion liveness testing apparatus may also use a filter response 40 speeds . The surface property refers to a property related to based on diffusion speeds . The liveness testing apparatus a surface of the object , and may include , for example , a may use a result of applying median filtering to diffusion light - reflective property of the surface of the object , a speeds of all pixels . number of dimensions of the surface of the object , and / or a According to at least some example embodiments , the material of the surface of the object . liveness testing apparatus may extract various features based 45 The tester 613 may analyze a distribution of light energy on diffusion speed based statistical information , and learn included in the input image to estimate the surface property the extracted features . The liveness testing apparatus may related to the object included in the input image . In one calculate diffusion speed based statistical information from example , the tester 613 may analyze the distribution of the various training images , and enable a classifier to learn the light energy included in the input image to determine features extracted from the statistical information , in a 50 whether the object included in the input image has a surface learning stage . The training images may include images of property ( one or more characteristics ) of a medium display a live 3D object and images of a fake 2D object . ing a face ( e.g. , a 2D flat surface ) or a surface property ( one A simply structured classifier may obtain a distance or more characteristics ) of an actual face of a user ( e.g. , a 3D between vectors ( e.g. , a Euclidian distance ) or a similarity structure ) . ( e.g. , a normalized correlation ) , and compare the distance 55 When the object included in the input image is determined between the vectors or the similarity to a threshold value . A to have a surface property of a medium displaying a face neural network , a Bayesian classifier , a support vector ( e.g. , a 2D flat surface ) , the tester 613 may output a signal machine ( SVM ) , or an adaptive boosting ( AdaBoost ) learn- corresponding to a failed test . That is , for example , when the ing classifier may be used as a more elaborate classifier . tester 613 determines that the object included in the input The liveness testing apparatus may calculate diffusion 60 image has a surface property of a medium displaying a face speed based statistical information from an input image , and ( e.g. , a 2D flat surface ) , the tester 613 may output a signal extract features from the statistical information using a indicative of a failed test . When the object included in the given , desired , or alternatively predetermined ,', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='613 may output a signal extract features from the statistical information using a indicative of a failed test . When the object included in the given , desired , or alternatively predetermined , method . The input image is determined to have a surface property of an method may correspond to the method used in the learning actual face of a user ( e.g. , a 3D structure ) , the tester 613 may stage . The liveness testing apparatus may input the extracted 65 output a signal corresponding to a successful test . That is , for features and learned parameters into the classifier to test a example , when the tester 613 determines that the object liveness of the object included in the input image . Based on included in the input image has a surface property of an ?', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='a \\n a \\n a \\n I = W : v US 11,151,397 B2 \\n 15 16 \\n actual face of a user ( e.g. , a 3D structure ) , the tester 613 may tially unaffected by illumination on an object when the input output a signal indicative of a successful test . image is obtained . One or more example embodiments may In another example , the tester 613 may test the liveness of provide technology that may generate an image less suscep the object by calculating statistical information related to the tible ( e.g. , impervious ) to changes in illumination , thereby diffusion speeds . As discussed above , a 2D object and a 3D 5 increasing reliability of face recognition and / or user verifi object have different light - reflective properties . And the cation , and / or reducing computational complexity of face different light - reflective properties between the 2D object recognition and / or user verification . and the 3D object may be modeled based on diffusion Still referring to FIG . 7 , the input image 710 includes an speeds . illumination component 715 and a non - illumination com In this example , the tester 613 may calculate a diffusion 10 ponent . In this example , the illumination component 715 speed based on the input image from the receiver 611 and the refers to a component , from among components constituting diffusion image from the diffuser 612. In one example , the pixel values , that is affected ( e.g. , substantially affected ) by tester 613 may calculate a diffusion speed for each pixel external illumination . The non - illumination component using Equation 4 discussed above . To calculate statistical refers to a component , from among components constituting information related to diffusion speeds , the tester 613 may 15 pixel values , that is substantially unaffected by external extract a small - scale region using Equation 5. The extracted illumination . The image processing apparatus may separate small - scale region may be represented as an SR map . The the illumination component 715 from the input image 710 to tester 613 may then determine the liveness of the object generate an image less susceptible ( e.g. , impervious ) to included in the input image using Equation 6 , 7 , 8 or 9 . changes in illumination . The tester 613 may test the liveness of the object included 20 The image processing apparatus may detect a face region in the input image based on a variety of diffusion speed from an input image . In this example , example embodiments based statistical information . The tester 613 may use a may be applicable to the face region detected from the input distribution of pixels having diffusion speeds greater than or image . Hereinafter , the term “ face image ” refers to an input equal to a given , desired , or alternatively predetermined , image including a face , or a face region extracted from the threshold value . The tester 613 may also use diffusion speed 25 input image . based statistical information without using an SR map . A face image may be expressed based on an illumination When the calculated statistical information corresponds to component and a non - illumination component . The face statistical information related to the medium displaying the image may be based on a Lambertian model , as shown face , the tester 613 may output a signal corresponding to a below in Equation 10 .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='failed test . When the calculated statistical information cor- 30 [ Equation 10 ] responds to statistical information related to the actual face of the user , the tester 613 may output a signal corresponding In Equation 10 , I denotes a face image , w denotes an to a successful test . That is , for example , when the calculated illumination component , and v denotes a non - illumination statistical information is indicative of a medium displaying component . With regard to the example shown in FIG . 7 , I the face , the tester 613 may output a signal indicating a 35 corresponds to the input image 710 , w corresponds to the failed test , whereas when the calculated statistical informa- image 720 related to the illumination component , and v tion is indicative of an actual face of the user , the tester 613 corresponds to the image 730 related to the non - illumination may output a signal indicating a successful test . component . \\n The liveness testing apparatus 600 may test the liveness of The image 720 related to the illumination component may the object based on a single input image . The single input 40 include the illumination component 715 , whereas the image image may correspond to a single picture , a single image , or 730 related to the non - illumination component may not a still image of a single frame . include the illumination component 715. Thus , the image 730 related to the non - illumination may be an image less Image Processing According to Example susceptible ( e.g. , impervious ) to changes in illumination .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='Embodiments 45 The image 730 related to the non - illumination component may also be referred to as a canonical image . FIG . 7 is a flow diagram illustrating image processing The illumination component 715 may have a relatively according to example embodiments . FIG . 8 illustrates high probability of being distributed in a large - scale region example changes in an input image depending on illumina- of the image . Thus , the image 720 related to the illumination tion according to example embodiments 50 component may be an image corresponding to a large - scale Referring to FIG . 7 , an input image 710 includes a face of region . The illumination component 715 may have a rela a user . The face of the user included in the input image 710 tively low probability of being distributed in a small - scale is affected ( e.g. , greatly or substantially affected ) by illumi- region . Thus , the image 730 related to the non - illumination nation . For example , referring to FIG . 8 , although a face of component may be an image corresponding to a small - scale the same user is photographed , different images may be 55 region . generated depending on illumination . When an input image The image processing apparatus may generate the image is vulnerable to changes in illumination , reliability of face 730 related to the non - illumination component based on the recognition and / or user verification may decrease ( e.g. , input image 710. In one example , the image processing considerably or substantially decrease ) , and / or a computa- apparatus may receive the input image 710 , and generate the tional complexity may increase ( e.g. , substantially or con- 60 image 720 related to the illumination component based on siderably increase ) . the input image 710. The image processing apparatus may An image processing method and / or apparatus according calculate the image 730 related to the non - illumination to one or more example embodiments may generate an component based on the input image 710 and the image 720 image less susceptible ( e.g. , impervious ) to changes in related to the illumination component using Equation 10 illumination from an input image . An image processing 65 shown above . method and / or apparatus according to one or more example According to at least one example embodiment , the image embodiments may also generate an image that is substan- processing apparatus may diffuse the input image 710 to a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='? \\n + \\n a \\n + 1 \\n u a \\n 30 US 11,151,397 B2 \\n 17 18 \\n generate the image 720 related to the illumination compo- In Equation 13 , I denotes a value of a pixel in the input nent . Diffusion speeds of pixels belonging to the small - scale image 710 , Ac denotes a horizontal diffusion matrix , A , region may be greater than diffusion speeds of pixels belong- denotes a vertical diffusion matrix , and t denotes a time step . ing to the large - scale region . The image processing appara- The final iteration count L and the time step t may be given , tus may separate the small - scale region and the large - scale 5 desired , or alternatively predetermined . In general , when the region based on a difference in diffusion speeds . The image time step t is set to be relatively small and the final iteration processing apparatus may diffuse a plurality of pixels count L is set to be relatively large , a reliability of u? , which included in the input image 710 a number of times corre denotes a value of a final diffused pixel , may increase . sponding to a given , desired , or alternatively predetermined , iteration count ( e.g. , about 20 ) to generate the image 720 10 the image processing apparatus to reduce the final iteration Using the AOS scheme to solve Equation 11 may enable \\n related to the illumination component corresponding to the count L. When the AOS scheme is used , the reliability of the large - scale region . According to at least one example embodiment , the image final diffused pixel ut may be sufficiently high although the \\n processing apparatus may iteratively update values of the time step t of a given , desired , or alternatively predeter plurality of pixels using a diffusion equation . In one 15 mined , size is used . Image processing apparatuses according example , the image processing apparatus may diffuse the to one or more example embodiments may increase an plurality of pixels corresponding to the face included in the efficiency of operations for diffusion processes using the input image 710 using Equation 11 shown below . AOS scheme to solve diffusion equations . \\n Ux + 1 = + * + div ( d ( \" Vukl ) Vuk ) [ Equation11 ] The image processing apparatus may generate the image \\n 20 730 related to the non - illumination component based on the In Equation 11 , k denotes an iteration count , u denotes a input image 710 and the image 720 related to the illumina value of a pixel after a k - th iteration , uk + 1 denotes a value of a tion component . In one example , the image processing a pixel after a ( k + 1 ) -th iteration , and uk corresponds to uk ( x , y ) , which is a value of a pixel at coordinates ( x , y ) in an apparatus may generate the image 730 related to the non \\n image after k diffusions . The value ukal corresponds to 25 in Equation 10 corresponds to ué , Equations 14 and 15 may illumination component using Equation 14 or 15. Since \\' w \\' \\n ok + 1 ( x , y ) , which is a value of a pixel at coordinates ( x , y ) in an image after ( k + 1 ) diffusions . In this example , uº denotes be derived from Equation 10 . \\n a value of a pixel in the input image 710. When a final v = irut iteration count corresponds to “ L ” , ut denotes a value of a [ Equation 14 ] \\n pixel in the image 720 related to the illumination compo \\n nent . log v = log 1 - log u [ Equation 15 ] \\n As before , V denotes a gradient operator , div ( ) denotes a In Equations 14 and 15 , I denotes a face image , and may divergence function , and d ( ) denotes a diffusivity function . correspond to , for example , the input image 710. The face The diffusivity function may be given , desired , or alterna tively predetermined . In one example , the image processing 35 ut denotes a large - scale region , and may correspond to , for image I may also correspond to uº . The final diffused pixel', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 25}), Document(page_content=\"apparatus may define the diffusivity function as shown example , the image 720 related to the illumination compo below in Equation 12 . nent . Still referring to Equations 14 and 15 , v denotes a d ( iVul ) = 1 / ( IVul + B ) [ Equation 12 ] small - scale region , and may correspond to , for example , the \\n In Equation 12 , ß denotes a small positive number . When the image 730 related to the non - illumination component . diffusivity function defined in Equation 12 is used , a bound- 40 FIG . 9 illustrates an image processing apparatus 910 ary of a face may be preserved relatively well during the according to an example embodiment . Also shown in FIG . diffusion process . When the diffusivity function corresponds 9 is a face recognition and / or user verification circuit 920 , \\n to a function of pixel gradient Vu as shown in Equation 12 , which will be discussed in more detail later . \\n the diffusion equation is nonlinear . Herein , an image gen Referring to FIG . 9 , the image processing apparatus 910 erated by diffusion is referred to as a diffusion image . When 45 includes : a receiver 911 ; a diffuser 912 ; and a generator 913 . the diffusivity function is nonlinear , an image generated by As with FIG . 6 , although element 912 in FIG.9 is referred \\n diffusion is referred to as a nonlinear diffusion image . to as a diffuser and the example embodiment shown in FIG . Equation 12 is provided as an example of the diffusivity 9 will be described with regard to a diffusion operation , the function , however , example embodiments may utilize other element 912 may be more generally referred to as a filter or diffusivity functions . For example , one of a plurality of 50 filter circuit 912. Moreover , as mentioned above , the filter candidate diffusivity functions may be selected based on an utilize any suitable filtering operation , rather than \\n input image . the diffusion process discussed here . Moreover , although example embodiments are discussed In example operation , the receiver 911 may receive an with regard to diffusivity functions , other filter functions input image . In a more specific example , the receiver 911 \\n may also be used , as mentioned above . 55 may receive an input image generated by an image sensor According to at least some example embodiments , the ( not shown ) . The receiver 911 may be connected to the image processing apparatus may apply an AOS scheme to image sensor using a wire , wirelessly , or via a network . solve Equation 11. In one example , the image processing Alternatively , the receiver 911 may receive the input image apparatus may diffuse the plurality of pixels corresponding from a storage device , such as , a main memory , a cache to the face included in the input image 710 using Equation 60 memory , a hard disk drive ( HDD ) , a solid state drive ( SSD ) , \\n 13 shown below . a flash memory device , a network drive , etc. The diffuser 912 may diffuse a plurality of pixels corre sponding to an object included in the input image . In one \\n [ Equation 13 ] example , the object may correspond to a face of a user . The 4 + 1 litt = ' ' all1 – 2TAz ( ed ) + + ( 1 – 2tAy ( uck ) Pjecte 65 diffuser 912 may iteratively update values of the plurality of pixels corresponding to the object included in the input image based on a diffusion equation . In one example , the 912 may \\n 1 \\n =\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 25}), Document(page_content='a \\n 2 US 11,151,397 B2 \\n 19 20 \\n diffuser 912 may diffuse the plurality of pixels correspond- With regard to the liveness testing apparatus 310 shown ing to the object included in the input image according to in FIG . 3 , for example , at operation 1010 , the receiver 311 Equation 11 . receives the input image , and the tester 312 tests a liveness The diffuser 912 may iteratively update the values of the of the object included in the received input image at opera plurality of pixels corresponding to the object included in 5 tion 1020. The tester 312 may test the liveness of the object the input image by applying an AOS scheme to the diffusion included in the input image received at the receiver 311 equation . In one example , diffuser 912 may diffuse the based on whether the object in the input image has one or plurality of pixels corresponding to the object included in more characteristics of a flat surface or a 3D structure . The the input image using Equation 13. The diffuser 912 may details of the operation performed by the tester 312 are output a diffusion image generated when the plurality of 10 discussed above with regard to FIG . 3 , and thus , a detailed pixels are diffused . The diffusion image may correspond to discussion is not repeated here . an image related to an illumination component ( e.g. , 720 in FIG . 7 ) . With regard to the liveness testing apparatus 600 shown \\n Still referring to FIG . 9 , the generator 913 may generate in FIG . 6 , for example , at operation 1010 the receiver 611 an output image based on the input image and the diffusion 15 receives the input image . At operation 1020 , the diffuser 612 image . The generator 913 may generate the output image diffuses a plurality of pixels corresponding to the object \\n using Equation 14 or 15. The output image may correspond included in the input image received at the receiver 611 , and \\n to an image related to a non - illumination component ( e.g. , the tester 613 tests the liveness of the object based on', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='to an image related to a non - illumination component ( e.g. , the tester 613 tests the liveness of the object based on \\n 730 in FIG . 7 ) . The generator 913 may output the output diffusion speeds of the plurality of pixels . The details of the image to the face recognition and / or user verification circuit 20 operation performed by the diffuser 612 and the tester 613 920. The face recognition and / or user verification circuit 920 are discussed above with regard to FIG . 6 , and thus , a may perform any well - known face recognition and / or user detailed discussion is not repeated here . verification circuit 920 operation as will be discussed in FIG . 11 is a flow chart illustrating an image processing some detail later . Alternatively , the generator 913 may method according to an example embodiment . For example output the output image to a memory ( not shown ) . 25 purposes , the image processing method shown in FIG . 11 According to at least some example embodiments , the will be discussed with regard to the image processing image processing apparatus 910 may generate the output apparatus shown in FIG . 9 . image based on a single input image . The single input image Referring to FIG . 11 , at operation 1110 , the image pro may correspond to a single picture , a single image , or a still cessing apparatus receives a first image . At operation 1120 , image of a single frame . Still referring to FIG . 9 , in one 30 the image processing apparatus generates a second image , example the face recognition and / or user verification circuit and at operation 1130 the image processing apparatus gen 920 may recognize a face included in the input image based erates a third image . on the output image from the generator 913. The output The first image may correspond to an input image ( e.g. , image may correspond to an image related to the non- 710 in FIG . 7 ) , the second image may correspond to an illumination component and less susceptible ( e.g. , impervi- 35 image related to an illumination component ( e.g. , 720 in ous ) to changes in illumination . The face recognition and / or FIG . 7 ) , and the third image may correspond to an image user verification circuit 920 may recognize the face included related to a non - illumination component ( e.g. , 730 in FIG . in the input image based on an image less susceptible ( e.g. , 7 ) . \\n impervious ) to changes in illumination . Thus , accuracy In more detail , with regard to FIGS . 9 and 11 , for and / or a reliability of the face recognition may increase . 40 example , at operation 1110 the receiver 911 receives a first When an image less susceptible ( e.g. , impervious ) to ( input ) image . changes in illumination is used , a performance of an align- At operation 1120 , the diffuser 912 generates a second ment operation may increase in a relatively low - luminance image based on the first ( input ) image . In this example , the', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='environment . second image is an image related to an illumination com In another example , the face recognition and / or user 45 ponent ( e.g. , 720 in FIG . 7 ) . verification circuit 920 may verify the user based on the At operation 1130 , the generator generates a third ( output ) output image from the generator 913. The image processing image based on the first ( input ) image and the second image apparatus 910 may verify the user by recognizing the face of generated by the diffuser 912. In this case , the third ( output ) the user based on the output image . The output image may image is an image related to a non - illumination component correspond to an image related to the non - illumination 50 ( e.g. , 730 in FIG . 7 ) . The details of the operations performed component and less susceptible ( e.g. , impervious ) to by the diffuser 912 and the generator 913 are discussed changes in illumination . The image processing apparatus above with regard to FIG . 9 , and thus , a detailed discussion 910 may verify the user based on an image less susceptible is not repeated here . ( e.g. , impervious ) to a change in illumination . Thus , accu- More generally , the descriptions provided with reference racy and / or a reliability of the user verification may increase . 55 to FIGS . 1A through 9 may be applicable to operations of FIGS . 10 and 11 , and thus , more detailed descriptions are Flowchart According to Example Embodiments omitted for conciseness . FIG . 12 illustrates an image processing method according FIG . 10 is a flow chart illustrating a liveness testing to another example embodiment . method according to an example embodiment . In some 60 The example embodiment shown in FIG . 12 combines the cases , the flow chart shown in FIG . 10 will be discussed with liveness testing method of FIG . 10 with the image process regard to the liveness testing apparatus shown in FIGS . 3 ing method of FIG . 11. For example purposes , the method \\n and 6 . shown in FIG . 12 will be described with regard to the image Referring to FIG . 10 , at operation 1010 the liveness processing apparatus shown in FIG . 9. The details of the testing apparatus receives an input image . At operation 65 operations described with regard to FIG . 12 are provided 1020 , the liveness testing apparatus tests a liveness of an above with regard to , for example , FIGS . 3 , 6 , 9 , 10 and 11 , object included in the received input image . and thus , a detailed discussion is not repeated here .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='10 \\n 15 US 11,151,397 B2 \\n 21 22 \\n Referring to FIG . 12 , at operation 1210 the receiver 911 Referring to FIG . 13 , the electronic system includes , for of the image processing apparatus 910 receives a first image . example : an image sensor 1300 , an image signal processor The first image may correspond to an input image including ( ISP ) 1302 , a display 1304 and a memory 1308. The image a face of a user . The receiver 911 outputs the first image to sensor 1300 , the ISP 1302 , the display 1304 and the memory the diffuser 912 and the generator 913 . 5 1308 communicate with one another via a bus 1306 . At operation 1220 , the diffuser 912 generates a second The image sensor 1300 may be the image sensor 115 image based on the first image received at the receiver 911 . described above with regard to FIGS . 1A and 1B . The image The diffuser 912 generates the second image by diffusing the sensor 1300 is configured to capture an image ( also referred first image from the receiver 911. The second image may be to as image data ) in any well - known manner ( e.g. , by an image related to an illumination component . converting optical images into electrical signals ) . The image At operation 1240 , the generator 913 calculates a diffu- is output to the ISP 1302 . sion speed for each pixel based on the first image and the The ISP 1302 may include one or more of the apparatuses second image . The diffusion speed for each pixel may be and / or may perform one more of the methods discussed calculated based on a difference between a pixel value in the above with regard to discussed above with regard to FIGS . second image and a corresponding pixel value in the first 1A through 12. The ISP 1302 may also include the face image . recognition and / or user verification circuit 920 to perform At operation 1250 , the generator 913 extracts statistical face recognition and / or user verification operations dis information based on diffusion speeds . For example , the cussed above with regard to FIGS . 1A through 12. In a more generator 913 calculates a number of pixels having diffusion 20 specific example , the ISP 1302 may include the liveness speeds greater than a given , desired , or alternatively prede- testing apparatus 310 shown in FIG . 3 , the liveness testing termined , threshold value . apparatus 610 shown in FIG . 6 , the image processing At operation 1270 , the generator 913 performs a liveness apparatus 910 shown in FIG . 9 and / or the face recognition test based on the diffusion speed based statistical informa- and / or user verification circuit 920 shown in FIG . 9. The tion . In one example , the generator 913 determines whether 25 memory 1308 may store images captured by the image the input image corresponds to a real 3D object based on the sensor 1300 and / or generated by the liveness testing appa number of pixels having diffusion speeds greater than the ratus and / or the image processing apparatus . The memory', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='threshold value . 1308 may be any suitable volatile or non - volatile memory . If the generator 913 determines that the input image does The display 1304 may display images captured by the image not correspond to a real 3D object ( the liveness test fails ) , 30 sensor 1300 and / or generated by the liveness testing appa then the face recognition and / or user verification circuit 920 ratus and / or the image processing apparatus . does not perform face recognition and / or user verification at The ISP 1302 may also be configured to execute a operation 1260 , and the process terminates . program and control the electronic system . The program Returning to operation 1270 , if the generator 913 deter- code to be executed by the ISP 1302 may be stored in the mines that the input image does correspond to a live 3D 35 memory 1308 . object ( the liveness test succeeds ) , then face recognition The electronic system shown in FIG . 13 may be con and / or user verification is performed . In this example , an nected to an external device ( e.g. , a personal computer or a image less susceptible ( e.g. , impervious ) to changes in network ) through an input / output device ( not shown ) and illumination may be generated for use in face recognition may exchange data with the external device . and / or user verification operations . The electronic system shown in FIG . 13 may embody Still referring to FIG . 12 , at operation 1230 the generator various electronic systems including : a mobile device , such 913 generates a third image based on the first image and the as a mobile phone , a smartphone , a personal digital assistant second image . In one example , the generator 913 calculates ( PDA ) , a tablet computer , a laptop computer , etc .; a com the third image based on a ratio of the first image to the puting device , such as a personal computer ( PC ) , a tablet PC , second image as discussed above with regard to Equation 14 45 a netbook ; or an electronic product , such as a television ( TV ) or a difference between the first image and the second image or smart TV , a security device for a gate control , etc. in a log domain as discussed above with regard to Equation FIG . 14 is a flowchart illustrating a user recognition 15. The third image may be an image related to a non- method according to an example embodiment . Each opera illumination component and impervious to a change in tion of the user recognition method of FIG . 14 may be illumination . 50 performed by a user recognition apparatus . The user recog At operation 1260 , the face recognition and / or user veri- nition apparatus may be implemented using a software fication circuit 920 performs face recognition and / or user module , a hardware module , or a combination thereof . verification based on the third image . In the example Referring to FIG . 14 , at operation 1410 , the user recog embodiment shown in FIG . 12 , the face recognition and / or nition apparatus extracts a first feature of a first image . For user verification circuit 920 performs the face recognition 55 example , at operation 1410 , the user recognition apparatus and / or user verification operations in operation 1260 only extracts the first feature by diffusing the first image . The first when the input image does correspond to a real 3D object image is an image acquired by capturing a user , and may ( the liveness test succeeds ) . In this example , the face rec- include a face of the user . The first feature of the first image ognition and / or user verification may be performed based on may be a diffusion - based feature such as a diffusion speed , the third image corresponding to the image less susceptible 60 for example . ( e.g. , impervious ) to changes in illumination . At operation 1420 , the user recognition apparatus per Details of the descriptions provided with reference to forms a liveness test based on the first feature of the first FIGS . 1A through 11 may be applicable to operations of', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='recognition apparatus per Details of the descriptions provided with reference to forms a liveness test based on the first feature of the first FIGS . 1A through 11 may be applicable to operations of image . The descriptions provided with reference to FIGS . 1 FIG . 12 , and thus , duplicated descriptions are omitted for through 13 may be applicable to operations 1410 and 1420 , conciseness . 65 and thus more detailed descriptions are omitted for concise FIG . 13 is a block diagram illustrating an electronic ness . For example , operation 1420 may correspond to opera system according to an example embodiment . tion 1270 of FIG . 12 . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='a \\n a \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='9 \\n a \\n a US 11,151,397 B2 \\n 23 24 \\n If the liveness test succeeds , the user recognition appa- in which payment information is input from the user at the ratus extracts a second feature of the first image , and point in time at which the actual payment is needed , user performs user recognition based on the second feature of the verification may be requested for payment approval . first image at operation 1430. The user recognition may In this example , the terminal may acquire an input image include verification to determine whether the user in the first 5 including a face of the user using a camera in response to image matches a pre - registered user , or identification to reception of a request for an electronic commerce payment determine a user corresponding to the user in the first image , from the user . When a liveness test and / or user verification among a plurality of users . is performed based on the input image , the electronic The second feature of the first image may be determined commerce payment may be performed based on a corre in various ways . For example , the second feature of the first 10 sponding result . image may be a feature extracted by processing the first A security module may be implemented to be separate image to recognize a face of the user , a feature extracted from a payment module . The payment module may request from an image generated by diffusing the first image , or a user verification from the security module at a point in time combination thereof . at which a payment approval is needed . The security module The user recognition of operation 1430 may be performed 15 may be executed in a safety region in an operating system . only when the liveness test of operation 1420 succeeds . In response to reception of the user verification request from Although not shown in FIG . 14 , if the liveness test fails , the the payment module , the security module may receive the user recognition apparatus may receive a new image and input image from the camera via a security channel , and perform a liveness test with respect to the new image . If the perform a liveness test and / or user verification with respect liveness test with respect to the new image succeeds , the 20 to the input image . The security module may return a user recognition apparatus may perform user recognition verification result to the payment module . The payment using the new image . In an example , the first image and the module may approve or reject a payment based on the new image may correspond to different frames in a video . verification result . Although not shown in FIG . 14 , in another example , the In another example , the examples may be applicable to first image may be a video . In this example , the liveness test 25 user verification to unlock a screen , to execute a predeter may be performed based on at least one first frame in the mined application , to execute a predetermined function in an video , and user recognition may be performed based on at application , or to access a predetermined folder or file . In', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='least one second frame in the video based on a result of the response to a request for unlocking the screen , the terminal liveness test . The at least one first frame and the at least one may acquire an input image including a face of a user , and second frame may correspond to consecutive frames in the 30 perform a liveness test and / or user verification to determine video . The at least one frame and the at least one second whether the screen is to be unlocked . In response to a request frame may be the same or different frames . for executing a predetermined application , executing a pre Various criteria may be determined for a liveness test with determined function in an application , or accessing a pre respect to the video based on the at least one first frame . If determined folder or file , the terminal may acquire an input any one of the at least one first frame included in the video 35 image including a face of a user , and perform a liveness test passes the liveness test , the video may be determined to be and / or user verification to determine whether a correspond a live video . If any one of the first frame included in the ing operation is to be allowed . video fails the liveness test , the video may be determined to The examples described with respect to FIGS . 1 through be a fake video . If a predetermined number of consecutive 14 may be applicable to user identification in various fields . first frames , for example , three consecutive first frames , pass 40 For example , the examples may be applied to user identi the liveness test , the video may be determined to be a live fication to sort content associated with a user among a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='video . plurality of items of contents stored in a gallery . In response When the video is determined to be a live video through to a request for an access to the gallery , the terminal may the liveness test performed based on the at least one first acquire an input image including a face of a user using the frame , user recognition may be performed based on the at 45 camera , and identify the user by performing a liveness test least one second frame in the video . In this example , the at and / or user identification based on the input image . When least one second frame may be the same as or different from the user is identified , the terminal may sort content associ the at least one first frame . For example , a portion of frames ated with the identified user among the plurality of items of included in the video may be suitable for a liveness test , but contents stored in the gallery , and provide the sorted content unsuitable for user recognition . Conversely , a portion of the 50 to the user . In another example , rather than automatically frames included in the video may be suitable for user sorting content in response to the access to the gallery , recognition , but unsuitable for a liveness test . Through the content associated with the user may be sorted and provided example provided above , the liveness test may be performed by an explicit request of the user , for example , a user input . based on at least one first frame more suitable for the In the above examples , the terminal may include various liveness test , among the plurality of frames included in the 55 computing devices such as a smartphone , a portable phone , video , and the user recognition may be performed based on a tablet computer , a laptop computer , a desktop computer , a at least one second frame more suitable for the user recog- wearable device , a smart vehicle , and a smart home appli nition . The examples described with respect to FIGS . 1 through One or more example embodiments ( e.g. , liveness testing 14 may be applicable to user verification in various fields . 60 apparatuses , image processing apparatuses electronic sys For example , the examples may be applied to user verifi- tems , etc. ) described herein may be implemented using cation for electronic commerce . Information related to a hardware components and software components . For payment method of a user , for example , a credit card , may example , the hardware components may include micro be registered in a terminal in advance , and a payment may phones , amplifiers , band - pass filters , audio to digital con be easily performed using the pre - registered payment 65 vertors , and processing devices . A processing device may be method through user verification at a point in time at which implemented using one or more special purpose computers , an actual payment is needed . In another example , in a case such as , for example , a processor , a controller and an a \\n a \\n a \\n ance . \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='a \\n a US 11,151,397 B2 \\n 25 26 \\n arithmetic logic unit , application - specific - integrated - circuit , a compiler , and files containing higher level code that may system - on - chip device , a digital signal processor , a micro- be executed by the computer using an interpreter . The computer , a field programmable array , a programmable logic above - described devices may be configured to act as one or unit , a microprocessor or any other device capable of more software modules in order to perform the operations of responding to and executing instructions in a defined man- 5 the above - described example embodiments , or vice versa . ner . The processing device may run an operating system A number of examples have been described above . Nev ( OS ) and one or more software applications that run on the ertheless , it should be understood that various modifications OS . The processing device also may access , store , manipu- may be made . For example , suitable results may be achieved late , process , and create data in response to execution of the if the described techniques are performed in a different order software . For purpose of simplicity , the description of a 10 and / or if components in a described system , architecture , processing device is used as singular ; however , one skilled device , or circuit are combined in a different manner and / or in the art will appreciated that a processing device may replaced or supplemented by other components or their include multiple processing elements and multiple types of equivalents . Accordingly , other implementations are within processing elements . For example , a processing device may the scope of the following claims . include multiple processors or a processor and a controller . 15 What is claimed is : In addition , different processing configurations are possible , 1. A user recognition method comprising : such a parallel processors . receiving a first image acquired by capturing a user ; Furthermore , example embodiments may be implemented performing a liveness test by extracting a first feature of by hardware , software , firmware , middleware , microcode , the first image ; and hardware description languages , or any combination thereof . 20 recognizing the user by extracting a second feature of the When implemented in software , firmware , middleware or first image in response to a successful result of the microcode , the program code or code segments to perform liveness test , the second feature being different from the the necessary tasks may be stored in a machine or computer first feature , wherein readable medium such as a computer readable storage the liveness test verifies whether an object corresponds medium . When implemented in software , a processor or 25 to a real three - dimensional ( 3D ) object or a fake processors will perform the necessary tasks . two - dimensional ( 2D ) representation of the object , A code segment may represent a procedure , function , and subprogram , program , routine , subroutine , module , software the first feature includes a degree of uniformity in a package , class , or any combination of instructions , data distribution of light energy included in a plurality of structures or program statements . A code segment may be 30 pixels corresponding to an object included in the first coupled to another code segment or a hardware circuit by image .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 29}), Document(page_content='passing and / or receiving information , data , arguments , 2. The user recognition method of claim 1 , further com parameters or memory co nts . Information , arguments , prising , in response to a failed result of the liveness test : parameters , data , etc. may be passed , forwarded , or trans- receiving a second image ; mitted via any suitable means including memory sharing , 35 performing a liveness test by extracting a first feature of message passing , token passing , network transmission , etc. the second image ; and The software may include a computer program , a piece of recognizing the user by extracting a second feature of the code , an instruction , or some combination thereof , to inde- second image based on a result of the liveness test with pendently or collectively instruct or configure the processing respect to the second image . device to operate as desired . Software and data may be 40 3. The user recognition method of claim 2 , wherein the embodied permanently or temporarily in any type of first image corresponds to a first frame in a video , and the machine , component , physical or virtual equipment , com- second image corresponds to a second frame in the video . puter storage medium or device , or in a propagated signal 4. A user recognition method , comprising wave capable of providing instructions or data to or being receiving a first image acquired by capturing a user ; interpreted by the processing device . The software also may 45 performing a liveness test by extracting a first feature of be distributed over network coupled computer systems so the first image ; that the software is stored and executed in a distributed recognizing the user by extracting a second feature of the fashion . The software and data may be stored by one or more first image based on a result of the liveness test ; and non - transitory computer readable recording mediums . wherein the performing includes Example embodiments described herein may be recorded 50 generating a second image by diffusing a plurality of in non - transitory computer - readable media including pro pixels included in the first image , gram instructions to implement various operations embod calculating diffusion speeds of the plurality of pixels ied by a computer . The media may also include , alone or in based on a difference between the first image and the combination with the program instructions , data files , data second image , and structures , and the like . The program instructions recorded 55 extracting the first feature based on the diffusion speeds on the media may be those specially designed and con of the plurality of pixels . structed for the purposes embodied herein , or they may be 5. The user recognition method of claim 4 , wherein the of the kind well - known and available to those having skill in generating comprises : the computer software arts . Examples of non - transitory iteratively updating values of the plurality of pixels using computer - readable media include magnetic media such as 60 a diffusion equation . hard disks , floppy disks , and magnetic tape ; optical media 6. The user recognition method of claim 4 , wherein the such as CD ROM discs and DVDs ; magneto - optical media extracting the first feature comprises : such as optical discs ; and hardware devices that are specially estimating a surface property related to an object included configured to store and perform program instructions , such in the first image based on the diffusion speeds of the as read - only memory ( ROM ) , random access memory 65 plurality of pixels , ( RAM ) , flash memory , and the like . Examples of program wherein the surface property comprises at least one of a instructions include both machine code , such as produced by light - reflective property of a surface of the object , a a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 29}), Document(page_content='10 \\n 15 \\n 20 \\n 2 \\n 25 US 11,151,397 B2 \\n 27 28 \\n number of dimensions of the surface of the object , or a determining whether the first feature corresponds to a material of the surface of the object . feature related to a medium that displays a face or a \\n 7. The user recognition method of claim 4 , wherein the feature related to an actual face ; \\n extracting the first feature comprises at least one of : outputting a signal corresponding to a failed result of the \\n calculating a number of pixels corresponding to a diffu- 5 liveness test in a case in which the first feature corre \\n sion speed greater than or equal to a first threshold , sponds to the feature related to the medium that dis plays a face ; or among the diffusion speeds of the plurality of pixels ; outputting a signal corresponding to the successful result calculating a distribution of the pixels corresponding to of the liveness test in a case in which the first feature the diffusion speed greater than or equal to the first corresponds to the feature related to an actual face . threshold , among the diffusion speeds of the plurality of 12. The user recognition method of claim 1 , further pixels ; comprising : calculating at least one of an average or a standard receiving a user verification request for approving an \\n deviation of the diffusion speeds ; or electronic commerce payment ; and \\n calculating a filter response based on the diffusion speeds approving the electronic commerce payment in a case in \\n of the plurality of pixels . which the recognizing the user is successful . \\n 8. The user recognition method of claim 4 , wherein the 13. The user recognition method of claim 1 , further \\n extracting the first feature comprises : comprising : \\n extracting a first - scale region from the first image based receiving a user input which requires user verification ; and on the diffusion speeds of the plurality of pixels ; and calculating an amount of noise components included in performing an operation corresponding to the user input \\n the first - scale region based on a difference between the in a case in which the recognizing the user is success ful . first - scale region and a result of applying median filtering to the first - scale region . 14. The user recognition method of claim 13 , wherein the \\n 9. The user recognition method of claim 1 , wherein the user input which requires user verification comprises at least \\n performing comprises : one of : \\n determining whether the object has a planar property or a a user input to unlock a screen , \\n three - dimensional ( 3D ) structural property , based on a user input to execute an application , \\n the first feature ; a user input to execute a function in an application , or \\n outputting a signal corresponding to a failed result of the a user input to access a folder or file . \\n liveness test in a case in which the object is determined 15. The user recognition method of claim 1 , further \\n to have the planar property ; and comprising : \\n outputting a signal corresponding to the successful result receiving a user input related to a gallery including a \\n of the liveness test in a case in which the object is plurality of items of content ; \\n determined to have the 3D structural property . sorting content corresponding to the user among the \\n 10. A user recognition method comprising : plurality of items of content in the gallery in a case in \\n receiving a first image acquired by capturing a user ; which the recognizing the user is successful ; and \\n performing a liveness test by extracting a first feature of providing the content to the user . \\n the first image ; and 16. A non - transitory computer - readable medium compris \\n recognizing the user by extracting a second feature of the ing a program that , when executed on a computer device , \\n first image in response to a successful result of the causes the computer device to perform the method of claim', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 30}), Document(page_content='first image in response to a successful result of the causes the computer device to perform the method of claim \\n 1 . liveness test , the second feature being different from the first feature , 17. A user recognition apparatus comprising a processor \\n wherein the liveness test verifies whether an object cor configured to : \\n responds to a real three - dimensional ( 3D ) object or a perform a liveness test by extracting a first feature of a \\n fake two - dimensional ( 2D ) representation of the object , first image acquired by capturing a user ; \\n and recognize the user by extracting a second feature of the \\n wherein the performing includes first image based on a result of the liveness test ; and \\n calculating a degree of uniformity in a distribution of wherein the processor is further configured to \\n light energy included in a plurality of pixels corre generate a second image by diffusing a plurality of \\n sponding to an object included in the first image pixels included in the first image , \\n based on the first feature , calculate diffusion speeds of the plurality of pixels \\n outputting a signal corresponding to a failed result of based on a difference between the first image and the \\n the liveness test in a case in which the degree of second image , and \\n uniformity in the distribution of light energy is extract the first feature based on the diffusion speeds of the plurality of pixels . greater than or equal to a threshold , and outputting a signal corresponding to the successful 18. The user recognition method of claim 1 , wherein the \\n result of the liveness test in a case in which the object is a face included in the first image ; and the liveness test verifies whether the face corresponds to degree of uniformity in the distribution of light energy is less than the threshold . a real three - dimensional ( 3D ) face or a fake two \\n 11. The user recognition method of claim 1 , wherein the dimensional ( 2D ) representation of the face . \\n performing further comprises at least one of : 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 30})], [Document(page_content='US009060688B2 \\n (12) United States Patent (10) Patent No.: US 9,060,688 B2 \\n ROWe (45) Date of Patent: *Jun. 23, 2015 \\n (54) BIOMETRICS BASED ON LOCALLY A61B5/6838 (2013.01); A61 B 5/726 (2013.01); CONSISTENT FEATURES A61B 2562/0233 (2013.01); A61B 2562/046 \\n (2013.01); G06K9/00046 (2013.01); G06K (71) Applicant: LUMIDIGM, INC., Albuquerque, NM 9/28 (2013.01); G06K 2009/0006 (2013.01); (US) G06K 2009/00932 (2013.01); G07C 9/00158 (2013.01); A61 B 5/7264 (2013.01) \\n (72) Inventor: Robert K. Rowe, Corrales, NM (US) (58) Field of Classification Search USPC ......... 382/100, 115, 117, 124, 125, 128, 116, \\n (73) Assignee: HID GLOBAL CORPORATION, 382/191, 127, 149, 162, 312 Austin, TX (US) See application file for complete search history. \\n (*) Notice: Subject to any disclaimer, the term of this (56) References Cited \\n patent is extended or adjusted under 35 U.S.C. 154(b) by 0 days. U.S. PATENT DOCUMENTS \\n This patent is Subject to a terminal dis- 5,291.560 A * 3/1994 Daugman ..................... 382,117 \\n claimer. 5,812,252 A * 9/1998 Bowker et al. .................. 356/71 6,005,963 A * 12/1999 Bolle et al. ... ... 382,124 \\n 6,018,586 A * 1/2000 Kamei ....... ... 382,125 (21) Appl. No.: 13/624.361 6,041,410 A * 3/2000 Hsu et al. ... ... 713, 186 \\n 6,052.474. A * 4/2000 Nakayama . ... 382,124 (22) Filed: Sep. 21, 2012 6,175.407 B1 * 1/2001 Sartor ............ 356,71 \\n 6,356,649 B2 * 3/2002 Harkless et al. ... ... 382,115 \\n (65) Prior Publication Data 6.421,453 B1* 7/2002 Kanevsky et al. . ... 382,115 \\n 7,545,963 B2 * 6/2009 Rowe ................. ... 382,124 US 2013/OO22248A1 Jan. 24, 2013 7,809,168 B2 * 10/2010 Abiko et al. .................. 382,115 \\n (Continued) Related U.S. Application Dat e pplication Uata Primary Examiner — Vu Le \\n (63) Continuation of application No. 12/051,173, filed on Assistant Examiner — Aklilu Woldemariam \\n Mar. 19, 2008, now Pat. No. 8,285,010. (74) Attorney, Agent, or Firm — Marsh Fischmann & \\n (60) Provisional application No. 60/896,063, filed on Mar. Breyfogle LLP: Daniel J. Sherwinter \\n 21, 2007. (57) ABSTRACT \\n (51) Int. Cl. Systems, devices, methods, and software are described for \\n G06K 9/00 (2006.01) biometric sensors that permit a reduction in the size of the \\n A6 IB5/00 (2006.01) sensing area without significant reduction in biometric func \\n Ok 5.2. 7 3:08: tionality of the sensor. A skin site of an individual is illumi nated, and light scattered from the skin site is received. An GO7C 9/OO (2006.01) image of a locally consistent feature of the skin site is formed \\n (52) U.S. Cl. from the received light. The locally consistent feature is ana CPC ................ A61B5/0059 (2013.01); G06K 9/00 lyzed to perform a biometric function. \\n (2013.01); A61B5/0062 (2013.01); A61B 5/1172 (2013.01); A61 B 5/6826 (2013.01); \\n 100 N 24 Claims, 16 Drawing Sheets \\n 102 \\n 104', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 0}), Document(page_content='US 9,060.688 B2 \\n Page 2 \\n (56) References Cited 2007/0086624 A1* 4/2007 Breed et al. ................... 382/104 \\n 2007/0206842 A1* 9, 2007 Hamid ......... ... 382,125 \\n U.S. PATENT DOCUMENTS 2007/0242858 A1* 10/2007 Aradhye et al. ... 382,115 \\n 2008.0025579 A1 1/2008 Sidlauskas et al. ........... 382,124 \\n 7,835,554 B2 * 1 1/2010 Rowe ............................ 382,124 2008.0025580 A1 1/2008 Sidlauskas et al. ........... 382,124 \\n 2003/0044051 A1* 3/2003 Fujieda ... 382,124 2008/0273768 A1* 11/2008 Dennis et al. ... ... 382,124 \\n 2003/O16371.0 A1* 8, 2003 Ortiz et al. ..... 713, 186 2008/0298.642 A1* 12/2008 Meenen .......... ... 382,115 \\n ck 2009/0046903 A1 2/2009 Corcoran et al. ... 382,124 2004/OOO3295 A. 1/2004 Elderfield et al. . 713,202 2009, O154792 A1* 6, 2009 Sun et al. ..... ... 382,154 \\n 2004/0208343 A1* 10, 2004 Golden et al. . 382,110 2009,0245591 A1* 10, 2009 R. ck owe et al. .. ... 382,115 2004/02407 12 A1* 12/2004 Rowe et al. ................... 382,124 ck 2010/0177937 A1 7/2010 Zhang et al. . ... 382,115 2006/0173256 A1* 8, 2006 Ridder et al. 600/316 2012/0114251 A1* 5, 2012 Solem et all 382,195 \\n 2006/0210120 A1* 9, 2006 Rowe et al. . 382,115 . . . . . . . . . . . . . . . . . . \\n 2006/0274921 A1* 12/2006 Rowe ............................ 382,124 * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 1}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 1 of 16 US 9,060,688 B2 \\n 102 \\n 104 \\n Fig. 1', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 2}), Document(page_content='US 9,060,688 B2 Sheet 2 of 16 Jun. 23, 2015 U.S. Patent \\n S. \\n C C S \\n C s SS \\n S. KX S. \\n 106', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 3}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 3 of 16 US 9,060,688 B2 \\n Fig. 3', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 4}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 4 of 16 US 9,060,688 B2 \\n Fig. 4', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 5}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 5 of 16 US 9,060,688 B2 \\n Fig. 5', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 6}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 6 of 16 \\n 600 \\n 608 606 \\n 602 604 \\n Fig. 6 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 7}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 7 of 16 US 9,060,688 B2 \\n Fig. 7', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 8}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 8 of 16 US 9,060,688 B2 \\n 719', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 9}), Document(page_content='US 9,060,688 B2 Sheet 9 of 16 \\n Fig. 9A \\n saw A Jun. 23, 2015 U.S. Patent \\n Fig. 9B', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 10}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 10 of 16 US 9,060,688 B2 \\n 1000 1008 \\n N- N1/ 27 C5/777777,777,770 YY \\n 719', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 11}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 11 of 16 US 9,060,688 B2 \\n 25 \\n 2 O \\n 1 5 \\n 10 \\n 350 450 550 650 750 850 950 1050 \\n Wavelength (nm) \\n Fig.11B', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 12}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 12 of 16 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 13}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 13 of 16 US 9,060,688 B2 \\n Fig. 13', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 14}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 14 of 16 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 15}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 15 of 16 US 9,060,688 B2 \\n 1500 y 151Ob \\n Computer \\n Readable Storage \\n Media \\n 1502 1504 1506 1508 \\n 1510a \\n Input Output Storage Real St. e Device(s) Device(s) Device(s) Media E. \\n 526 \\n 1520 \\n Communications Processing Working \\n System Acceleration Memory \\n L \\n Operating \\n Biometric 1518 \\n Sensor Other Code (Programs) \\n Fig. 15', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 16}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 16 of 16 \\n illuminate Skin Site \\n Receive Light Scattered From Skin Site \\n Generate Local Feature Profile from the \\n Received Light \\n Analyze local Feature Profile by Comparing \\n Local Feature Profile to Reference Feature \\n Profile \\n Perform Biometric Function based on \\n Analysis of Local Feature Profile \\n Fig. 16 1604 \\n 1608 \\n 1612 \\n 1616 \\n 1620 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 17}), Document(page_content=\"US 9,060,688 B2 \\n 1. \\n BOMETRICS BASED ON LOCALLY \\n CONSISTENT FEATURES \\n CROSS REFERENCES \\n This application is a continuation of, and claims the benefit of the filing date of U.S. patent application Ser. No. 12/051, \\n 173, entitled “BIOMETRICS BASED ON LOCALLY CON \\n SISTENT FEATURES.” filed Mar. 19, 2008 by Robert K. Rowe, which is a nonprovisional of, and claims the benefit of \\n the filing date of U.S. Provisional Patent Application No. \\n 60/896,063, filed Mar. 21, 2007, entitled “BIOMETRICS \\n BASED ON MULTISPECTRAL SKINTEXTURE Each of \\n the these applications is hereby incorporated by reference, as \\n if set forth in full in this document, for all purposes. This application is related to each of the following commonly \\n assigned applications, the entire disclosure of which is incor porated herein by reference for all purposes: U.S. patent \\n application Ser. No. 1 1/219,006, entitled “COMPARATIVE \\n TEXTURE ANALYSIS OF TISSUE FOR BIOMETRIC \\n SPOOF DETECTION, filed Sep. 1, 2005 by Robert K. Rowe; and U.S. patent application Ser. No. 1 1/458,619, \\n entitled “TEXTURE-BIOMETRICS SENSOR, filed Jul. \\n 19, 2006 by Robert K. Rowe. \\n BACKGROUND \\n The present invention relates to biometrics in general and, in particular, to biometrics based on multispectral skin tex \\n ture. \\n Fingerprint-based biometric sensors are used across a \\n broad range of applications, from law enforcement and civil \\n identification to commercial access control. They are even \\n used in Some consumer devices such as laptops and cellular telephones. In at least Some of these applications, there is a \\n general need in the art to reduce the overall size of the sensor \\n in order to reduce the area of the device that the sensor \\n occupies, as well as to reduce the overall cost of the sensor. Most fingerprint sensors work by imaging a fingerprint and \\n comparing the image to one or more stored images in a \\n database. As such, when a large area of the fingerprint is imaged, more data may be compared and more discriminating \\n results may be obtained. Conversely, the performance of \\n these types of fingerprint sensors may degrade as their sizes \\n decrease. \\n One way to accommodate this concern may be to produce long, narrow fingerprint sensors that simulate a larger-area \\n sensor by combining a series of narrow images collected \\n while the user Swipes a finger across the sensor Surface. \\n Another, similar way to accommodate this concern may be to piece together a set of Smaller images of a fingerprint to collectively form a larger fingerprint image (called “mosaik \\n ing'). Such configurations may reduce the sensor size, but \\n they may also place additional burdens on the user (e.g., requiring the user to learn how to Swipe a fingerprint or \\n requiring precise locating of the finger on the sensor) and may \\n limit the applications in which Such a sensor may be employed. \\n It may be desirable, therefore, to provide a reduced-size biometric sensor that may be highly usable and employed in a wide variety of applications without a significant degrada \\n tion in functionality or usability. \\n SUMMARY \\n Among other things, methods, systems, and devices are \\n described for biometric sensors that permit a reduction in the 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n size of the sensing area without significant reduction in bio metric functionality of the sensor. \\n Embodiments of the invention achieve a reduction in sens \\n ing area of a biometric sensor by measuring a property of the \\n skin that is locally consistent while still being distinct between different people. Because the property is locally \\n consistent, measurements at an enrollment skin location and at a measurement site may be meaningfully compared even if \\n the enrollment and measurement sites are different. In some \\n embodiments, the property comprises an optical property of \\n the skin. In particular, certain embodiments use a range of spatiospectral imaging techniques with Small-area sensors.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 18}), Document(page_content=\"embodiments, the property comprises an optical property of \\n the skin. In particular, certain embodiments use a range of spatiospectral imaging techniques with Small-area sensors. \\n Matching may be done using spatial, spectral, and/or textural \\n descriptors of the imaging data combined with a classification \\n methodology that applies characteristics of the data that are locally consistent. \\n In some embodiments, spatial information is detected. For example, different spacings may be provided between light \\n Sources and imagers to look for spatial illumination signa \\n tures (e.g., “roll-off). In other embodiments, spectral infor mation is detected. For example, at a fixed spacing between the light source and the imager, different frequency compo \\n sitions may be obtained by using various illumination wave lengths, polarizations, filters, and other characteristics of illu \\n mination. Further, the spectral information may be detected and/or used in many different ways, including pixel-by-pixel, \\n on average over a certain window of pixels, as Summarized by various frequency decomposition methods, etc. In still other \\n embodiments, textural information is detected. In some con figurations, the textural information may indicate optical fea tures (e.g., lumpiness, ridge spacing, etc.); while in other \\n configurations, the textural information may indicate features of the spatial and/or spectral data (e.g., the distribution of \\n coefficients derived from a Fourier decomposition of one or more multispectral image regions). In yet other embodi \\n ments, multiple types of information are detected simulta neously or in series, and are used in conjunction for a variety \\n of situations. In even other embodiments, the sensor or other components may be operable to “learn' over time. For \\n example, each time a user is identified, the biometric infor mation may be processed, along with some or all of the \\n previous biometric information gathered from that user. The \\n combined information may be used to generate a more dis criminatory biometric profile for that user. \\n One set of embodiments provides a method of performing \\n a biometric function. The method includes illuminating a \\n Small-area purported skin site of an individual with illumina tion light, wherein the Small-area purported skin site is in \\n contact with a surface; receiving light scattered from the \\n Small-area purported skin site, wherein the light is received Substantially in a region that includes the Surface; generating \\n a local feature profile from the received light, wherein the \\n local feature profile identifies a feature of the small-area purported skin site, the feature of the Small-area purported \\n skin site being of a type predetermined to exhibit substantial local consistency; and analyzing the generated local feature \\n profile to perform the biometric function. Analyzing the gen \\n erated local feature profile includes comparing the generated \\n local feature profile with a reference local feature profile, \\n wherein the reference local feature profile was generated \\n from light scattered from a small-area reference skin site, and the small-area purported skin site is substantially different \\n from the reference Small-area skin site. \\n Another set of embodiments provides a biometric sensor. \\n The biometric sensor includes a Surface adapted for contact with a purported skin site of an individual; an illumination Subsystem disposed to illuminate the purported skin site; a\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 18}), Document(page_content='US 9,060,688 B2 \\n 3 \\n detection Subsystem disposed to receive light scattered from \\n the purported skin site, wherein the light is received substan \\n tially in a region that includes the Surface; and a computa \\n tional unit interfaced with the detection subsystem. The com putational unit has instructions for forming an image from the 5 received light; instructions for generating an image-texture \\n measure from the image; and instructions for analyzing the generated image-texture measure to perform the biometric \\n function. \\n 10 \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n A further understanding of the nature and advantages of the \\n present invention may be realized by reference to the follow ing drawings. In the appended figures, similar components or 15 \\n features may have the same reference label. Further, various components of the same type may be distinguished by fol \\n lowing the reference label by a dash and a second label that distinguishes among the similar components. If only the first \\n reference label is used in the specification, the description is 20 applicable to any one of the similar components having the \\n same first reference label irrespective of the second reference \\n label. \\n FIG. 1 shows a simplified perspective view of an embodi \\n ment of an exemplary Small-area sensor, according to various 25 \\n embodiments of the present invention. \\n FIG. 2 shows a simplified cross-sectional view of a sensor head, like the sensor head shown in FIG. 1, according to \\n various embodiments of the invention. \\n FIG.3 shows a simplified top view of a sensor head having 30 \\n a number of light sources arranged to be equidistant from a \\n detector, according to various embodiments of the invention. FIG. 4 shows a simplified top view of a sensor head having \\n a number of light sources arranged around a common detec \\n tor, according to various embodiments of the invention. 35 FIG. 5 shows a simplified top view of a sensor head having a number of light sources arranged with respect to multiple \\n detector elements, according to various embodiments of the \\n invention. \\n FIG. 6 shows a simplified top view of a sensor head having 40 \\n a number of light sources arranged around a detector array, \\n according to various embodiments of the invention. \\n FIG. 7 shows a simplified illustration of an embodiment of a multi-spectral biometric sensor using direct illumination, \\n according to various embodiments of the invention. 45 \\n FIG. 8 shows a simplified illustration of an embodiment of a multi-spectral biometric sensor using TIR imaging, accord \\n ing to various embodiments of the invention. FIG.9A illustrates nine exemplary images captured during \\n a single finger placement using an embodiment of an MSI 50 \\n biometric sensor, according to various embodiments of the \\n invention. \\n FIG.9B shows an exemplary result of applying a compos iting algorithm to two placements of the same finger, accord \\n ing to various embodiments of the invention. 55 FIG. 10A shows a simplified perspective view of an MSI \\n sensor having a number of light sources arranged around a \\n detector array, according to various embodiments of the \\n invention. \\n FIG. 10B shows a simplified side view of an embodiment 60 \\n of an MSI sensor, like the one in FIG. 10A, where the imager is substantially in contact with the skin site, according to \\n various embodiments of the invention. \\n FIG. 10C shows a simplified side view of an embodiment \\n of an MSI sensor, like the one in FIG. 10A, where the imager 65 is displaced from the skin site, according to various embodi \\n ments of the invention. 4 \\n FIG. 11A shows an illustration of an exemplary Bayer \\n color filter array in which filter elements correspond to a set of primary colors and are arranged in a Bayer pattern. \\n FIG. 11B shows an illustrative color response curve for an exemplary Bayer filter, like the one in FIG. 11A. \\n FIG. 12 shows an illustrative datacube form of storing or analyzing a body of spatio-spectral data.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 19}), Document(page_content='FIG. 12 shows an illustrative datacube form of storing or analyzing a body of spatio-spectral data. \\n FIG. 13 shows an embodiment of a personal spectral bio metric system in the configuration of an electronic key fob, \\n according to various embodiments of the invention. FIG. 14 shows an embodiment of a personal spectral bio metric system in the configuration of a watch, according to \\n various embodiments of the invention. \\n FIG. 15 shows an exemplary computational system for \\n implementing biometric sensors and related functionality \\n according to various embodiments of the invention. FIG.16 provides a flow diagram of exemplary methods for \\n using multispectral sensor structures according to various \\n embodiments of the invention. \\n DETAILED DESCRIPTION OF THE INVENTION \\n Systems, devices, methods, and software are described for \\n biometric sensors that permit a reduction in the size of the sensing area without significant reduction in biometric func tionality of the sensor. It may be desirable to provide a small \\n area sensor for a number of reasons. For example, many applications may be limited by size (e.g., by form factor), by power consumption (e.g., by battery life), by cost, or by some \\n other limitation. It will further be appreciated that providing \\n a small, but reliable, sensor may allow the sensor to be used in many applications, including laptops, cell phones, car keys, \\n garage door openers, locks, industrial machine Switches, or \\n any other application where it may be desirable to obtain \\n biometric information or to restrict access. \\n One way to reduce the sensing area while maintaining a \\n simple, single-touch user interface may be to measure a prop \\n erty of the skin that is locally consistent while still being \\n distinct from person to person. In this way, a small-area \\n sensor may be able to perform a biometric match using a skin location never previously enrolled as long as the optical prop \\n erties of the enrolled and tested skin sites were “similar \\n enough.” \\n Embodiments of the invention provide methods and sys tems that allow for the collection and processing of a variety \\n of different types of biometric measurements, including inte \\n grated, multifactor biometric measurements in some embodi ments. These measurements may provide strong assurance of a person’s identity, as well as of the authenticity of the bio \\n metric sample being taken. \\n Skin composition and structure is very distinct, very com plex, and varies from person to person. By performing optical \\n measurements of spatio-spectral properties of skin and under \\n lying tissue, a number of assessments may be made. For example, a biometric-identification function may be per formed to identify or verify whose skin is being measured, a \\n liveness function may be performed to assure that the sample \\n being measured is live and viable skin and not another type of material, estimates may be made of a variety of physiological parameters such as age, gender, ethnicity, and other demo \\n graphic and anthropometric characteristics, and/or measure \\n ments may be made of the concentrations of various analytes and parameters including alcohol, glucose, degrees of blood \\n perfusion and oxygenation, biliruben, cholesterol, urea, and \\n the like. \\n The complex structure of skin may be used in different \\n embodiments to tailor aspects of the methods and systems for', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 19}), Document(page_content='US 9,060,688 B2 \\n 5 \\n particular functions. The outermost layer of skin, the epider mis, is Supported by the underlying dermis and hypodermis. \\n The epidermis itself may have identified sub-layers that \\n include the stratum corneum, the stratum lucidum, the Stra tum granulosum, the stratum spinosum, and the stratum ger \\n minativum. Thus, for example, the skin below the top-most \\n stratum corneum has some characteristics that relate to the \\n Surface topography, as well as Some characteristics that change with depth into the skin. While the blood supply to \\n skin exists in the dermal layer, the dermis has protrusions into the epidermis known as \"dermal papillae, which bring the \\n blood supply close to the surface via capillaries. In the volar \\n surfaces of the fingers, this capillary structure follows the pattern of the friction ridges and Valleys on the Surface. In \\n some other locations on the body, the structure of the capillary \\n bed may be less ordered, but is still characteristic of the particular location and person. \\n As well, the topography of the interface between the dif ferent layers of skin is complex and characteristic of the skin \\n location and the person. While these sources of subsurface structure of skin and underlying tissue may represent a sig \\n nificant noise Source for non-imaging optical measurements \\n of skin for biometric determinations or analyte measure \\n ments, the structural differences may be manifested by spa tiospectral features that can be compared through embodi \\n ments of the invention. \\n In some instances, inks, dyes, and/or other pigmentation may be present in portions of the skin as topical coating or \\n Subsurface tattoos. These forms of artificial pigmentation \\n may or may not be visible to the naked human eye. However, if one or more wavelengths used by the apparatus of the \\n present invention is sensitive to the pigment, the sensor can be used in some embodiments to Verify the presence, quantity, \\n and/or shape of the pigment in addition to other desired mea \\n Surement tasks. \\n In general, embodiments of the present invention provide \\n methods and systems that collect spatio-spectral information \\n that may be represented in a multidimensional data structure that has independent spatial and spectral dimensions. In cer \\n tain instances, the desired information is contained in just a \\n portion of the entire multidimensional data structure. For example, estimation of a uniformly distributed, spectrally \\n active compound may require just the measured spectral char \\n acteristics, which may be extracted from the overall multidi mensional data structure. In Such cases, the overall system design may be simplified to reduce or eliminate the spatial \\n component of the collected data by reducing the number of \\n image pixels, even to a limit of a single pixel. Thus, while the \\n systems and methods disclosed are generally described in the context of spatio-spectral imaging, it will be recognized that \\n the invention encompasses similar measurements in which the degree of imaging is greatly reduced, even to the point \\n where there is a single detector element. Some embodiments of the invention use multispectral \\n imaging (MSI) of skin to provide information about both the surface and subsurface (“multispectral\\') characteristics of \\n the skin tissue. In addition to the potential of MSI revealing \\n significant information from below the surface of the skin, the plurality of wavelengths, illumination angles, and optical \\n polarization conditions may yield additional information beyond that available through simple surface reflectance \\n measurements. These multispectral characteristics may be \\n described as \"textures” and used with a classification meth \\n odology that seeks to find characteristics of the data that are locally consistent (e.g., to determine the identity of an indi vidual). Moreover, the plurality of wavelengths, illumination', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content=\"odology that seeks to find characteristics of the data that are locally consistent (e.g., to determine the identity of an indi vidual). Moreover, the plurality of wavelengths, illumination \\n angles, and optical polarization conditions used in this inves 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n tigation yields additional information beyond that available \\n through simple Surface reflectance measurements More broadly, data collected under a plurality of optical \\n conditions, whether they be collected simultaneously or sequentially, is referred to herein as “multispectral data. A more complete description of aspects of multispectral data is \\n described in co-pending, commonly assigned U.S. patent \\n application Ser. No. 1 1/379,945, entitled “MULTISPEC \\n TRAL BIOMETRIC SENSORS filed Apr. 24, 2006, the entire disclosure of which is incorporated herein by reference \\n for all purposes. The distinct optical conditions may include \\n differences in polarization conditions, differences in illumi nation angle, differences in imaging angle, differences in illumination wavelength, and the like. Spatio-spectral data \\n may thus be considered to be a subset of certain types of multispectral data that includes spatial information, e.g., \\n where the different multispectral illumination conditions are \\n recorded with a detector that provides for at least a pair of measurements at Substantially the same illumination condi tion and different spatial or angular positions. Alternatively, \\n spatio-spectral data may also be derived from a single detec \\n tor that measures the optical response of the sample at 2 or \\n more illumination conditions that differ by their spatial or \\n angular orientation with respect to the detector. Also, as used \\n herein, “small-area' sensors refers invarious embodiments to sensors having an active area less than 1 cm, with certain specific embodiments using sensors having an active area less than 0.75 cm, less than 0.5 cm, less than 0.25 cm, less than 0.1 cm, less than 0.05 cm, or less than 0.01 cm. In one embodiment, the sensor is substantially a point sensor. \\n It will be appreciated that the devices, systems, and meth \\n ods described herein may be applied to many types of bio \\n metric identification. Particularly, locally consistent features \\n may be identified in a number of different regions of the body. \\n For example, various Volar Surfaces of the skin may include \\n locally consistent features which may be identified with Small-area sensors according to the invention, including the Volar Surfaces of the palm, fingers, joints, knuckles, etc. As \\n such, while many embodiments of the invention are described with reference to fingerprints, it will be appreciated that the \\n embodiments may also apply to other biometric sites, and should not be taken as limiting in any way. \\n There are a number of ways of using texture matching of fingerprints. In one example, a local texture analysis using \\n Gabor filters may be applied to tessellated regions around the core point. In another example, a Fourier analysis fingerprint \\n texture may be used as the basis for biometric determinations. In still another example, wavelet analyses may be applied to \\n fingerprint images to distinguish between them. While these \\n examples differ in basis functions and other methodological \\n matters, they are all based on conventional fingerprint images. In other words, the observed textural pattern is \\n extracted from a single image, which may limit the informa tion content and may be adversely affected by artifacts due to \\n effects such as dry skin, poor contact between the skin and sensor, and other operationally important matters. \\n Some embodiments of the invention use multiple images \\n taken with a robust form of imaging. The images contain \\n information about both the surface and subsurface character\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content='Some embodiments of the invention use multiple images \\n taken with a robust form of imaging. The images contain \\n information about both the surface and subsurface character \\n istics of the skin. In certain embodiments, the data plane in the MSI stack that most closely matches conventional optical fingerprinting images is removed from the texture analysis in \\n order to avoid the presence of spurious effects (e.g., dry skin, \\n poor contact between the skin site and the sensor platen, etc.). It will be appreciated that there are a number of ways to perform biometric matching using Small-area fingerprint sen \\n sors. One approach may be to build up a large enrollment', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content='US 9,060,688 B2 \\n 7 \\n image by piecing together a series of small fingerprint images \\n taken over multiple placements of the finger (known as \\n \"mosaiking\\'). Another approach may be to combine minutiae \\n information instead of combining the images themselves. One limitation to these approaches is that they may require \\n precise enrollment of a skin site to take accurate measure ments. For example, to provide accurate results with these types of sensors, a user may have to precisely locate his finger \\n on the sensor, avoid any movements of the skin during mea \\n Surement, etc. \\n AS Such, it may be desirable instead to sense Small-area features of the skin site that are locally consistent over other \\n areas of the skin site. Some embodiments of the invention find \\n characteristics of the skin site that are locally consistent. Using locally consistent characteristics may allow an enroll \\n ment measurement to be made at one skin site and Success \\n fully verified at a different (e.g., nearby) skin site. This may \\n minimize certain types of errors inherent in many Small-area sensors (e.g., slight movements of the skin site during sens ing, etc.) and may improve usability and reliability. \\n To capture information-rich data about the Surface and \\n Subsurface features of a skin site (e.g., the skin of a finger), an MSI sensor may collect multiple images of the skin site under a variety of optical conditions. The raw images may be cap \\n tured using different wavelengths of illumination light, dif \\n ferent polarization conditions, and/or different illumination \\n orientations. Each raw image may then contain somewhat different, but complementary, information about the skin site. The different wavelengths may penetrate the skin to different \\n depths and be absorbed and scattered differently by various \\n chemical components and structures in the skin. The different polarization conditions may change the degree of contribu \\n tion of Surface and Subsurface features to the raw image. \\n Further, different illumination orientations may change the \\n location and degree to which surface features are accentuated. \\n Embodiments of the MSI sensors according to the inven \\n tion are configured to detect textural information from a skin site. “Texture\\' may generally refer to any of a large number of \\n metrics that describe some aspect of a spatial distribution of \\n tonal characteristics of an image, some of which were \\n described above. For example, Some textures, such as those commonly found in fingerprint patterns or wood grain, are \\n flowlike and may be well described by metrics such as an \\n orientation and coherence. For textures that have a spatial regularity (at least locally), certain characteristics of the Fou \\n rier transform and the associated power spectrum are impor tant Such as energy compactness, dominant frequencies and \\n orientations, etc. Certain statistical moments such as mean, \\n variance, skew, and kurtosis may be used to describe texture. \\n Moment invariants may be used, which are combinations of \\n various moments that are invariant to changes in scale, rota tion, and other perturbations. Gray-tone spatial dependence \\n matrices may be generated and analyzed to describe image \\n texture. The entropy over an image region may be calculated \\n as a measure of image texture. Various types of wavelet trans forms may be used to describe aspects of the image texture. \\n As mentioned above, steerable pyramids, Gabor filters, and \\n other mechanisms of using spatially bounded basis functions \\n may be used to describe the image texture. These and other \\n Such measures of texture known to one familiar in the art may be used individually or in combination in embodiments of the \\n invention. \\n Image texture may thus be manifested by variations in \\n pixel intensities across an image, which may be used in \\n embodiments of the invention to perform biometric func', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='invention. \\n Image texture may thus be manifested by variations in \\n pixel intensities across an image, which may be used in \\n embodiments of the invention to perform biometric func \\n tions. In some embodiments, additional information may be extracted when such textural analysis is performed for differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n ent spectral images recorded under different illumination \\n wavelengths and/or polarization conditions and extracted \\n from a multispectral data set, producing a chromatic textural \\n description of the skin site. These embodiments may enable biometric functions to be performed by capturing only a \\n Small-area portion of an image of a skin site. The texture \\n characteristics of the skin site are expected to be approxi \\n mately consistent over the skin site, permitting biometric \\n functions to be performed with measurements made at differ \\n ent portions of the image site. In many instances, it may not \\n even be required that the portions of the skin site used in \\n different measurements overlap with each other. \\n Exemplary Sensor Embodiments \\n This ability to use different portions of the skin site pro \\n vides considerable flexibility in the structural designs that \\n may be used. This is, in part, a consequence of the fact that \\n biometric matching may be performed Statistically instead of \\n requiring a match to a deterministic spatial pattern. The sen \\n Sor may be configured in a compact manner because it need \\n not acquire an image over a specified spatial area. The ability \\n to provide a small sensor also permits the sensor to be made more economically than sensors that need to collect complete \\n spatial information to perform a biometric function. In dif \\n ferent embodiments, biometric functions may be performed with purely spectral information, while in other embodi \\n ments, spatio-spectral information is used. FIG. 1 shows a simplified perspective view of an embodi \\n ment of an exemplary Small-area sensor, according to some \\n embodiments of the present invention. The sensor assembly \\n 100 consists of a series or plurality of light sources 104 \\n arranged in a selected manner on a sensor head 102, which also contains one or more detectors 106. The sensor assembly 100 may also include power conditioning electronics (not \\n shown), which Supply power to the light Sources 104 and may \\n also include signal processing electronics (not shown) which amplify the resulting signal from the detector 106. A multi \\n conductor cable 108 may be provided to power the sensor \\n head and to communicate with a system (e.g., a microproces sor or computer) for processing the detected signals. \\n The light sources 104 may be light emitting diodes (LEDs), \\n laser diodes, vertical cavity surface emitting lasers (VC SELS), quartz tungsten halogen incandescent bulbs with or \\n without optical pass-band filters and with or without optical \\n shutters, or a variety of other optical sources known in the art. The light sources 104 may each have the same wavelength \\n characteristics or can be comprised of sources with different center wavelengths in the spectral range from about 350 nm to \\n about 2500 nm. In general, the collection of light sources 104 \\n may include Some sources that have the same wavelengths as \\n others and some sources that are different. In one embodi \\n ment, the light sources 104 include sets of LEDs, laser diodes, VCSELs, or other solid-state optoelectronic devices with dif fering wavelength characteristics that lie within the spectral \\n range from about 350 nm to about 1100 nm. In some cases, the detector array may include an optical filter array to limit \\n the wavelengths of light seen by certain array elements. \\n The detector 106 may be a single element or it may be a \\n one- or two-dimensional array of elements. The detector type \\n and material are chosen to be appropriate to the source wave lengths and the measurement signal and timing requirements.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='one- or two-dimensional array of elements. The detector type \\n and material are chosen to be appropriate to the source wave lengths and the measurement signal and timing requirements. \\n For example, the detectors may include PbS, PbSe, InSb, \\n InGaAs, MCT, bolometers and/or micro-bolometer arrays. In \\n one embodiment where the light sources 104 are solid-state optoelectronic devices operating in the spectral range from \\n about 350 nm to about 1100 nm, the detector material is \\n silicon.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='US 9,060,688 B2 \\n 9 \\n The light sources 104 may be sequentially illuminated and \\n extinguished to measure the tissue properties for each Source \\n by turning power to each of them on and off. Alternatively, \\n multiple light sources 104 may be electronically modulated \\n using encoding methods that may be known in the art. These \\n encoding patterns include, for example, Fourier intensity \\n modulation, Hadamard modulation, random modulation, and \\n other modulation methods. \\n It is worth noting that the configuration shown in FIG. 1 \\n includes a number of light sources 104 and a single detector 106, effectively providing variable source-detector spacings. \\n This configuration may be applicable, for example, where a \\n small number of light sources 104 with different wavelength \\n characteristics are available. In these cases, providing vari \\n able source-detector spacings may be useful in gathering \\n additional optical information from tissue. \\n FIG. 2 shows a simplified cross-sectional view of a sensor \\n head, like the sensor head 100 shown in FIG. 1, according to \\n some embodiments of the invention. Also shown is the tissue \\n 210 in contact with the face 209 of the sensorhead 102 and the \\n mean optical paths 212, 214, 216, 218, 220, and 222 of the \\n light traveling from each light source 211,213,215, 217, 219, \\n and 221, respectively, to the detector 106. In acquiring tissue \\n spectral data, measurements can be made in at least two different sampling modes. The optical geometry illustrated in \\n FIG. 2 is known as diffuse reflectance sampling geometry \\n where the light sources and detector lie on the same side of the \\n tissue. An alternative method is known as transmission Sam \\n pling, wherein light enters a thin tissue region Such as an \\n earlobe or a fingertip on one side and then is detected by a \\n detector located on the other side of the tissue. Although light \\n in Such regions as the silicon-region can penetrate tissue to \\n significant depths of one centimeter or more, depending upon \\n the wavelength, transmission sampling of the tissue limits the \\n region of the body that can be used. Thus, while either mode of sampling may be applicable to the present invention, and especially to analysis utilizing light in the silicon-region, \\n many embodiments utilize sampling methods based on \\n reflected light. \\n Referring to FIG. 2, when the tissue is illuminated by a \\n particular light source 211, the resulting signal detected by \\n detector 106 contains information about the tissue optical properties along a path between the source 211 and detector 106. The actual path of any given photon may be highly \\n erratic due to effects of optical scattering by the tissue, but the \\n mean optical path 212 may be a more regular and Smooth \\n curve, as shown in the Figure. The mean optical path (e.g., 212) may, in general, be dif \\n ferent for different source-detector separation differences. If \\n another light source 221 is located at the same distance from \\n the detector 106 as light source 211, and the two light sources have the same wavelength characteristics, the resulting sig \\n nals may be combined (e.g., to increase the resulting signal \\n to-noise ratio of the measurement) or may be used as inde pendent spatio-spectral measurements. Iflight source 221 has \\n a different wavelength characteristic from light source 211, the resulting signals may provide information about optical \\n properties of the tissue 210, especially as they relate to bio \\n metric determinations, and should be analyzed as distinct data \\n points. In a similar manner, if two light sources have the same wavelength characteristics and are positioned at different dis \\n tances from the detector 106 (for example light sources 211 \\n and 213), then the resulting information in the two signals is \\n different and the measurements may be recorded and ana lyzed as distinct data points. Differences in both wavelength 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content='different and the measurements may be recorded and ana lyzed as distinct data points. Differences in both wavelength 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n characteristics and source-detector separation may provide \\n additional information about optical characteristics of the \\n tissue 210. \\n In some embodiments, the detector 106 is located either in \\n the center of the sensor head 102 or offset to one side of the \\n sensor head 102 (e.g., to provide for varying source-detector \\n separation distances). The sensor head 102 may be various shapes including oval, square, or rectangular. The sensor head \\n 102 may also have a compound curvature on the optical \\n surface to match the profile of the device in which it is \\n mounted, or to match the profile of the skin site intended to \\n touch the sensor. \\n Light that reflects from the topmost layer of skin may not \\n contain significant information about the deeper tissue prop \\n erties. In fact, reflections from the top surface of tissue (known as “specular or “shunted light) may sometimes be \\n detrimental to optical measurements. For this reason, FIG. 2 \\n illustrates a sensor head 102 geometry wherein the detector 106 is recessed from the sensor surface 209 in optically opaque material 207 that makes up the body of the sensor \\n head 102. The recessed placement of detector 106 minimizes \\n the amount of light that can be detected after reflecting off the first (e.g., epidermal) Surface of the tissue. It can be seen that the same optical blocking effect may be produced by recess \\n ing each of the light sources 211,213,215, 217, 219, and 221, and/or the detector 106. Of course, other optical blocking \\n methods, such as the use of different polarization filters, are possible according to the invention. \\n It will be appreciated that many other configurations of \\n light sources and detectors are possible without departing \\n from the invention. In one embodiment, as shown in FIG.3, a \\n sensor head 102 has a number of light sources 104 arranged to be equidistant from a detector 106. This configuration may be \\n useful, for example, where each light source 104 is a different wavelength and sufficient light sources 104 may be obtained \\n to achieve desired accuracy results from the system. An \\n example of this may occur where individual light Sources result from combining optical filters with one or more broad \\n band (e.g., incandescent) light sources. In this case, many unique wavelength bands may be defined and each of the \\n sources 104 may be placed equidistant from the central detec tor 106. Alternatively, each of the light sources 104 may be \\n Substantially the same and the resulting set of measurements \\n are used as a measure of the spatial differences of the skin at a particular wavelength. \\n In another embodiment, as shown in FIG. 4, a number of light sources 401, 404, 407,410 are arranged around a com \\n mon detector 413. Four different light sources 401, 404, 407, \\n 410 are shown for illustration but fewer or more can be used \\n in a particular embodiment. Each of the light sources 401, 404, 407, 410 is optically coupled to a different optical \\n waveguide 402, 405, 408, 411. Each waveguide 402, 405, \\n 408,411 has individually controllable electronic or mechani cal optical shutters 403, 406, 409, 412. These optical shutters \\n 403, 406, 409, 412 can be individually controlled to encode the light by allowing light to enter the tissue from a waveguide \\n 402,405, 408,411 at a predetermined position or positions. In \\n certain embodiments, the optical shutters 403, 406, 409, 412 include micro-electromechanical systems (“MEMS) struc \\n tures. The light sources 401, 404, 407, 410 may include \\n different LEDs, laser diodes,VCSELs, or other types of illu mination sources. Alternatively, one or more incandescent sources with different optical filters may be used to generate \\n light of different wavelength characteristics to couple into \\n each of the waveguides 402, 405, 408, 411.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content='light of different wavelength characteristics to couple into \\n each of the waveguides 402, 405, 408, 411. \\n In yet another embodiment, multiple source-detector dis \\n tances may be achieved by using more than one detector', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content=\"US 9,060,688 B2 \\n 11 \\n element, as shown in FIG. 5. In the illustrated embodiment, \\n each of three different light sources 502, 504, 506 is posi \\n tioned relative to three detectors 501, 503, 505 such that the \\n spacing between a given light source and each of the detectors is different. For example, the source-detector spacing for a \\n light source 502 is shortest with respect to detector 501 and longest with respect to detector 505. By turning on the light \\n sources 502,504,506 in a sequential or encoded pattern and measuring the response at each of the three detectors 501, \\n 503,505, tissue characteristics for some or all of the available Source-detector separations at Some or all of the wavelengths \\n may be measured. \\n Some embodiments configure multiple detector elements and multiple illumination Sources using a detector array. FIG. \\n 6 illustrates a simplified top view of a sensor 600 using a \\n detector array according to some embodiments of the inven \\n tion. In this embodiment, multiple light sources 602, 604, 606, 608 are placed at the perimeter of a detector array 609. \\n The signal detected at each of the array elements may then represent a different Source-detector separation with respect \\n to the light from a given light Source. Many variants on this \\n configuration may exist, including the use of one dimensional \\n (“1-D), two-dimensional (2-D), or three-dimensional (3- D') arrays, and placing sources within the array as well as on the periphery. \\n Other embodiments are configured differently to detect \\n similar or different optical data. For example, many different types of multi-spectral imaging (“MSI) sensors are possible. \\n One embodiment of an MSI biometric sensor is depicted with the schematic diagram of FIG.7, which shows a front view of \\n the MSI sensor using direct illumination. The MSI biometric sensor 701 comprises an illumination subsystem 721 having \\n one or more light sources 703 and a detection subsystem 723 \\n with an imager 715. The figure depicts an embodiment in which the illumination subsystem 721 comprises a plurality \\n of illumination subsystems 721a and 721b, but the invention \\n is not limited by the number of illumination subsystems 721 or detection subsystems 723. For example, the number of illumination subsystems 721 may conveniently be selected to \\n achieve certain levels of illumination, to meet packaging \\n requirements, and to meet other structural constraints of the \\n MSI biometric sensor 701. \\n Illumination light passes from the source 703 through illu \\n mination optics 705 that shape the illumination to a desired form, such as in the form of flood light, light lines, light \\n points, and the like. The illumination optics 705 are shown for convenience as consisting of a lens but may more generally \\n include any combination of one or more lenses, one or more mirrors, and/or other optical elements. The illumination optics 705 may also comprise a scanner mechanism (not \\n shown) to scan the illumination light in a specified one \\n dimensional, or two-dimensional, or three-dimensional pat tern. The light source 703 may comprise a point source, a line \\n Source, an area source, or may comprise a series of Such \\n Sources in different embodiments. In various embodiments, one or more light sources 703 may be configured to illuminate \\n a skin site 719 (e.g., the skin of a finger) at different wave lengths, different polarization conditions, and/or different \\n illumination orientations. \\n In some embodiments, the light source 703 may comprise one or more quasimonochromatic sources in which the light \\n is provided over a narrow wavelength band. Such quasi \\n monochromatic sources may include Such devices as light emitting diodes, laser diodes, or quantum-dot lasers. Alterna \\n tively, the light source 703 may comprise abroadband source \\n Such as a white-light LED, an incandescent bulb, or a glow \\n bar. In the case of a broadband source, the illumination light 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content=\"Such as a white-light LED, an incandescent bulb, or a glow \\n bar. In the case of a broadband source, the illumination light 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n may pass through a bandpass filter 709 to narrow the spectral \\n width of the illumination light. In one embodiment, the band \\n pass filter 709 comprises one or more discrete optical band \\n pass filters. In another embodiment, the bandpass filter 709 \\n comprises a continuously variable filter that moves rotation \\n ally or linearly (or with a combination of rotational and linear \\n movement) to change the wavelength of illumination light. In \\n still another embodiment, the bandpass filter 709 comprises a \\n tunable filter element such as a liquid-crystaltunable filter, an \\n acousto-optical tunable filter, a tunable Fabry-Perot filter, or \\n other filter mechanism known to one knowledgeable in the \\n art. \\n In other embodiments, white light is used. As used herein, “white light” refers to light that has a spectral composition \\n amenable to separation into constituent wavelength bands, \\n which in some cases may comprise primary colors. The usual \\n primary colors used to define white light are red, green, and \\n blue, but other combinations may be used in other instances, \\n as will be known to those of skill in the art. For clarity, it is emphasized that “white light’ as used herein might not appear \\n white to a human observer and might have a distinct tint or \\n color associated with it because of the exact wavelength dis \\n tribution and intensity of the constituent wavelength bands. In \\n other cases, the white light may comprise one or more bands \\n in the ultraviolet or infrared spectral regions. In some cases, \\n the white light might not even be visible at all to a human \\n observer when it consists of wavelength bands in the infrared and/or ultraviolet spectral regions. A portion of the light scat \\n tered by the skin and/or underlying tissue exits the skin and is \\n used to form an image of the structure of the tissue at and \\n below the surface of the skin. Because of the wavelength \\n dependent properties of the skin, the image formed from each \\n wavelength of light comprised by the white light may be \\n different from images formed at other wavelengths. In some \\n embodiments, an optical filter or filter array may be incorpo \\n rated with the detector array to separate the white light into a \\n set constituent wavelengths. For example a color filter array \\n comprised of red, green and blue filter elements arranged in a \\n Bayer pattern may be used to separate the white light, as \\n known to one familiar in the art. \\n Various embodiments of the illumination subsystem 721 and detection subsystem 723 are configured to operate in a variety of optical regimes and at a variety of wavelengths. \\n One embodiment uses light sources 703 that emit light sub stantially in the region of 350-1 100 nanometers. In this \\n embodiment, the detector 715 may be based on silicon detec \\n tor elements or other detector material knownto those of skill \\n in the art as sensitive to light at Such wavelengths. In another \\n embodiment, the light sources 703 may emit radiation at wavelengths that include the near-infrared regime of 1.0-2.5 \\n microns, in which case the detector 715 may include elements \\n made from InGaAs, InSb, PbS, MCT, and other materials \\n known to those of skill in the art as sensitive to light at such wavelengths. \\n In the embodiment shown in FIG. 7, the first illumination subsystem 721a includes a first light emitting diode (“LED') \\n 703a (i.e., the light source) and an illumination polarizer 707. \\n Light from the first LED 703a passes through the illumination polarizer 707 before illuminating a finger 719 (i.e., the skin \\n site) as it rests on a sensor platen 717. The illumination polarizer 707 may include, for example, a linear polarizer or \\n a circular polarizer. Light interacts with the finger 719 and a\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content='site) as it rests on a sensor platen 717. The illumination polarizer 707 may include, for example, a linear polarizer or \\n a circular polarizer. Light interacts with the finger 719 and a \\n portion of the light is directed toward the detection subsystem \\n 723. Other light which does not reflect directly back toward the detection Subsystem 723 may undergo refractions, scat', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content='US 9,060,688 B2 \\n 13 \\n tering, other reflections, and other optical events. Some of this \\n other light may ultimately reflect toward the detection sub \\n system 723. \\n The detection Subsystem 723 includes an imaging polar \\n izer 711. The imaging polarizer 711 is oriented with its optical \\n axis to be orthogonal to the axis of the illumination polarizer \\n 707, such that light with the same polarization as the illumi nation light is Substantially attenuated by the imaging polar \\n izer 711. This may significantly reduce the influence of light \\n reflected from the surface of the skin and emphasize light that has undergone multiple optical scattering events after pen \\n etrating the skin. \\n The second illumination subsystem 721b includes a second \\n LED 703b, but no illumination polarizer 707. When the sec \\n ond LED 703b is illuminated, the illumination light may be \\n randomly polarized. The surface-reflected light and the deeply penetrating light may both able to pass through the \\n imaging polarizer 711 in equal proportions due to the random \\n polarization. As such, the image produced from this unpolar \\n ized second LED 703b may contain stronger influence from \\n surface features of the finger. \\n It is worth noting that the direct-illumination sources, the polarized first LED 703a and the unpolarized second LED \\n 703b, as well as the imaging system, may be arranged to avoid \\n critical-angle phenomena at platen-air interfaces. This may \\n provide certainty that each illuminator will illuminate the \\n finger 719 and that the imager 715 will image the finger 719 \\n (e.g., regardless of whether the skin is dry, dirty, or even in \\n good contact with the sensor). \\n In some embodiments, the sensor layout and components \\n may advantageously be selected to minimize the direct reflec \\n tion of the illumination into the detection optics 713. In one \\n embodiment, such direct reflections are reduced by relatively \\n orienting the illumination subsystem 721 and detection sub \\n system 723 such that the amount of directly reflected light \\n detected is minimized. For instance, optical axes of the illu mination subsystem 721 and the detection subsystem 723 \\n may be placed at angles Such that a mirror placed on the platen \\n 717 does not direct an appreciable amount of illumination \\n light into the detection subsystem 723. In addition, the optical \\n axes of the illumination and detection subsystems 721 and 723 may be placed at angles relative to the platen 717 such that the angular acceptance of both Subsystems is less than the \\n critical angle of the system; such a configuration avoids \\n appreciable effects due to total internal reflectance (“TIR) \\n between the platen 717 and the skin site 719. In other embodiments, the detection subsystem 723 may \\n incorporate detection optics that include lenses, mirrors, and/ \\n or other optical elements that forman image of the region near \\n the platen surface 717 onto the detector 715. The detection optics 713 may also include a scanning mechanism (not shown) to relay portions of the platen region onto the detector \\n 715 in sequence. It will be appreciated that there are many \\n ways to configure the detection subsystem 723 to be sensitive \\n to light that has penetrated the surface of the skin and under gone optical scattering within the skin and/or underlying \\n tissue before exiting the skin. \\n In some embodiments, in addition to or instead of the direct \\n illumination illustrated in FIG. 7, the MSI sensor 701 also integrates a form of TIR imaging. For example, the polariza \\n tion effects discussed with respect to FIG.7 may be effective \\n when non-contact sensors are used, while TIR and other \\n techniques may be more effective with contact sensors. FIG. 8 provides an exemplary illustration of an MSI sensor 801 \\n using TIR imaging. Like the direct-illumination MSI sensor 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 24}), Document(page_content='using TIR imaging. Like the direct-illumination MSI sensor 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n 701 (FIG. 7), the TIR-based MSI sensor 801 includes one or more light sources 803 and a detection subsystem 723 with an imager 715. \\n In the TIR illumination mode, one or more light sources 803 (e.g., LEDs) illuminate the side of the platen 717. A portion of the illumination light propagates through the platen \\n 717 by making multiple TIR reflections at the platen-air inter \\n faces. At points where the TIR is broken by contact with the skin (e.g., 810), light enters the skin and is diffusely reflected. \\n A portion of this diffusely reflected light is directed toward the imaging system and passes through the imaging polarizer \\n (since this light is randomly polarized), forming an image for \\n this illumination state. Unlike all the direct illumination \\n states, the quality of the resulting raw TIR image may be \\n dependent on having skin of Sufficient moisture content and cleanliness making good optical contact with the platen, just \\n as is the case with conventional TIR sensors. \\n In practice, many MSI sensors may contain multiple direct \\n illumination LEDs of different wavelengths. For example, the \\n Lumidigm J1 10 MSI sensor has four direct-illumination wavelength bands (430, 530, and 630 nanometers, as well as a white light), in both polarized and unpolarized configura \\n tions. When a finger is placed on the sensor platen, eight \\n direct-illumination images are captured along with a single \\n T1R image. The raw images are captured on a 640x480 image \\n array with a pixel resolution of 525 ppi. All nine images are captured in approximately 500 mSec. \\n FIG.9A illustrates nine exemplary images captured during \\n a single finger placement using an embodiment of an MSI \\n biometric sensor. The upper row 902 shows raw images for unpolarized illumination wavelengths of blue (430 nanom \\n eters), green (530 nanometers), and red (630 nanometers), as well as white light. The middle row 904 shows images, cor responding to those in the upper row 902, for a cross-polar \\n ized case. The single image on the bottom row 906 shows a TIR image. The grayscale for each of the raw images has been expanded to emphasize the features. \\n It can be seen from FIG. 9 that there are a number of \\n features present in the raw data including the textural charac \\n teristics of the Subsurface skin, which appear as mottling that is particularly pronounced under blue and green illumination \\n wavelengths. As well, the relative intensities of the raw \\n images under each of the illumination conditions is very \\n indicative of the spectral characteristics (e.g., color) of the finger or other sample. It is worth noting that the relative \\n intensities have been obscured in FIG. 9 to better show the \\n comparative details of the raw images. \\n The set of raw images shown in FIG.9 may be combined together to produce a single representation of the fingerprint \\n pattern. In some embodiments, this fingerprint generation \\n relies on a wavelet-based method of image fusion to extract, \\n combine, and enhance those features that are characteristic of a fingerprint. In one embodiment, the wavelet decomposition \\n method that is used is based on the dual-tree complex wavelet transform (“DTCWT). Image fusion may occur by selecting \\n and compiling the coefficients with the maximum absolute magnitude in the image at each position and decomposition \\n level. \\n An inverse wavelet transform may then be performed on the resulting collection of coefficients, yielding a single, com posite image. An example of the result of applying the com positing algorithm to two placements of the same finger is \\n shown in FIG. 9B. The two composite fingerprint images \\n (*0952 and 954) may then be used with conventional finger print matching Software. \\n In some embodiments, the DTCWT process used to gen erate composite fingerprint images is also used to provide the', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 24}), Document(page_content='US 9,060,688 B2 \\n 15 \\n spectral-textural features of the multispectral data. In one \\n embodiment, the coefficients from a 3rd level of the DTCWT decomposition of the multispectral image Stack are used as \\n features for the texture analysis. Since the strength and qual ity of the raw TIR image plane may be highly variable (depen \\n dent on skin moisture, good contact, etc), certain embodi ments omit the TIR image plane from the multispectral \\n texture analysis. \\n In some embodiments, an inter-image product, P. is defined \\n as the conjugate product of coefficients at Some direction, d. and decomposition level, k, generated by any two of the raw multispectral images, iand, in a multispectral image stack at \\n location X.y, as defined by: \\n In this equation, C(x, y, k) is the complex coefficient for image iat decomposition levelk, direction d, and locationX.y. and C (x, y, k) is the conjugate of the corresponding complex value for image j. The conjugate products may represent the \\n fundamental features of the image while remaining insensi \\n tive to translation and some amount of rotation. \\n In one embodiment, all real and imaginary components of all the conjugate products generated from each unique image \\n pair are compiled as a feature vector. For eight raw image \\n planes, this results in a 384-element vector (28 conjugate \\n products per direction, six directions, two Scalar values (i.e., real and imaginary) per product for iz; plus eight conjugate \\n products per direction, six directions, one scalar value (i.e., real only) for it). In addition, the isotropic magnitudes of the \\n coefficients are added to the feature vector, where the isotro pic magnitude is the sum of the absolute magnitudes over the \\n six directional coefficients. Finally, the mean DC values of each of the raw images over the region of analysis are added \\n to the feature vector. Concatenating all of these values results \\n in a 400-element feature vector at each element location. \\n In addition to the direct illumination and TIR sensors of \\n FIGS. 7 and 8, respectively, some embodiments of MSI sen \\n sors use illuminated arrays. FIG. 10A shows an illustration of an exemplary embodiment of an array-based MSI sensor. The \\n sensor 1000 includes a number of light sources 1004 and an imager 1008. In some embodiments, the light sources 1004 include white-light Sources, although in other embodiments, the light sources 1004 include quasi-monochromatic sources. \\n Similarly, the imager 1008 may include a monochromatic or color imager. \\n In one example, the imager 1008 has a Bayer color filter array in which filter elements corresponding to a set of pri mary colors are arranged in a Bayer pattern. An example of \\n Such a pattern is shown in FIG. 11A for an arrangement that \\n uses red 1104, green 1112, and blue 1108 color filter ele ments. In some instances, the imager 1008 may additionally \\n include an infrared filter or other filter, e.g., disposed to \\n reduce the amount of infrared light detected. FIG. 11B shows an illustrative color response curve for an exemplary Bayer filter. As shown, there may generally be \\n Some overlap in the spectral ranges of the red 1124, green \\n 1132, and blue 1128 transmission characteristics of the filter \\n elements. As evident particularly in the curves for the green \\n 1132 and blue 1128 transmission characteristics, the filter \\n array may allow the transmission of infrared light. This may \\n be avoided with the inclusion of an infrared filteras part of the detector subsystem. In other embodiments, the infrared filter \\n may be omitted and one or more light sources 1004 that emit infrared light may be incorporated. In this way, all color filter \\n elements 1104, 1108, and 1112 may allow the light to sub stantially pass through, resulting in an infrared image across \\n the entire imager 1008. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n Returning to FIG. 10A, in some embodiments, the sensor', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 25}), Document(page_content='the entire imager 1008. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n Returning to FIG. 10A, in some embodiments, the sensor \\n 1000 is a “contact sensor, because the image is collected substantially in the region of the skin site 719 being mea \\n sured. It is possible, however, to have different configurations \\n for operating the sensor, some with the imager 1008 substan \\n tially in contact with the skin site 719 and some with the imager 1008 displaced from the region of the skin site 719. \\n In the embodiment of FIG. 10B, the imager 1008 of the sensor 1000-1 is substantially in contact with the skin site \\n 719. Light from the sources 1004 propagates beneath the \\n tissue of the skin site 719. This may permit light scattered \\n from the skin site 719 and in the underlying tissue to be detected by the imager 1008. \\n An alternative embodiment in which the imager 1008 is displaced from the skin site 719 is shown schematically in \\n FIG. 10C. In this drawing the sensor 1000-2 includes an optical arrangement 1012 that translates an image at the \\n region of the skin site 719 to the imager 1008. In one embodi ment, the optical arrangement 1012 includes a number of \\n optical fibers, which translate individual pixels of an image by \\n total internal reflection along the fiber without substantially loss of intensity. In this way, the light pattern detected by the imager 1008 is substantially the same as the light pattern \\n formed at the region of the skin site 719. The sensor 1000-2 may thus operate in Substantially the same fashion as the \\n sensor 1000-1 shown in FIG. 10B. That is, light from the sources 1004 is propagated to the skin site, where it is reflected and scattered by underlying tissue after penetrating \\n the skin site 719. Because information is merely translated Substantially without loss, the image formed by the imager \\n 1008 in such an embodiment may be substantially identical to \\n the image that would beformed with an arrangement like that \\n in FIG. 10A. \\n In embodiments where purely spectral information is used to perform a biometric function, spectral characteristics in the \\n received data may be identified and compared with an enroll \\n ment database of spectra. The resultant tissue spectrum of a particular individual includes unique spectral features and \\n combinations of spectral features that can be used to identify \\n individuals once a device has been trained to extract the \\n relevant spectral features. Extraction of relevant spectral fea tures may be performed with a number of different tech niques, including linear and quadratic discriminant analysis, \\n genetic algorithms, simulated annealing, and other such tech niques. While not readily apparent in visual analysis of a spectral output, such analytical techniques can repeatably \\n extract unique features that can be discriminated to perform a \\n biometric function. Classification may be performed using a variety of methods including Support vector machines, K \\n nearest neighbors, neural networks, and other well known classification methods. Examples of specific techniques are \\n disclosed in commonly assigned U.S. Pat. No. 6.560,352. \\n entitled APPARATUS AND METHOD OF BIOMETRIC \\n IDENTIFICATION OR VERIFICATION OF INDIVIDU \\n ALS USING OPTICAL SPECTROSCOPY: U.S. Pat. No. \\n 6,816,605, entitled “METHODS AND SYSTEMS FOR \\n BIOMETRIC IDENTIFICATION OF INDIVIDUALS \\n USING LINEAR OPTICAL SPECTROSCOPY: U.S. Pat. \\n No. 6,628,809, entitled “APPARATUS AND METHOD FOR \\n IDENTIFICATION OF INDIVIDUALS BY NEAR-INFRA \\n RED SPECTRUM: U.S. Pat. No. 7,203,345, entitled \\n APPARATUS AND METHOD FOR IDENTIFICATION \\n OF INDIVIDUAL BY NEAR-INFRARED SPECTRUM, \\n filed Sep. 12, 2003 by Robert K. Rowe et al.; and U.S. patent \\n application Ser. No. 09/874,740, entitled “APPARATUS \\n AND METHOD OF BIOMETRIC DETERMINATION \\n USING SPECIALIZED OPTICAL SPECTROSCOPYSYS', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 25}), Document(page_content='US 9,060,688 B2 \\n 17 \\n TEM filed Jun. 5, 2001 by Robert K. Rowe et al. The entire disclosure of each of the foregoing patents and patent appli \\n cations is incorporated herein by reference in its entirety. \\n Many of the methods taught by the foregoing disclosures are readily applicable to spatio-spectral data. In particular, all 5 \\n or part of the spatio-spectral data may be analyzed using \\n techniques such as wavelets, Fourier decomposition, Steer \\n able pyramids, Gabor filters, and other decomposition meth \\n ods known in the art. The coefficients derived from these \\n decompositions may then be concatenated together into a \\n vector of values as described elsewhere in this disclosure. The \\n resulting vector may then be analyzed in similar ways as a \\n vector of spectral values can be. \\n The ability to perform biometric functions with image \\n texture information, including biometric identifications, may \\n exploit the fact that a significant portion of the signal from a \\n living body is due to capillary blood. For example, when the \\n skin site 719 comprises a finger, a known physiological char \\n acteristic is that the capillaries in the finger follow the pattern 20 \\n of the external fingerprint ridge structure. Therefore, the con \\n trast of the fingerprint features relative to the illumination wavelength is related to the spectral features of blood. In particular, the contrast of images taken with wavelengths \\n longer than about 580 nanometers may be significantly 25 \\n reduced relative to those images taken with wavelengths less than about 580 nanometers. Fingerprint patterns generated \\n with non-blood pigments and other optical effects (e.g., \\n Fresnel reflectance) may have a different spectral contrast. Light scattered from a skin site 719 may be subjected to 30 variety of different types of comparative texture analyses in \\n different embodiments. Some embodiments make use of a \\n form of moving-window analysis of image data derived from \\n the collected light to generate a figure of merit, and thereby \\n evaluate the measure of texture or figure of merit. In some 35 embodiments, the moving window operation may be replaced \\n with a block-by-block or tiled analysis. In some embodi \\n ments, a single region of the image or the whole image may be \\n analyzed at one time. \\n In one embodiment, fast-Fourier transforms are performed 40 on one or more regions of the image data. An in-band contrast \\n figure of merit C is generated in Such embodiments as the ratio of the average or DC power to in-band power. Specifi \\n cally, for an index i that corresponds to one of a plurality of wavelengths comprised by the white light, the contrast figure 45 \\n of merit is: 10 \\n 15 \\n XX, F6, n) 50 i \\n In this expression, F.(S. m) is the Fourier transform of the image f(x, y) at the wavelength corresponding to index i. \\n where Xandy are spatial coordinates for the image. The range 55 defined by R. and R, represents a limit on spatial fre quencies of interest for fingerprint features. For example, \\n R may be approximately 1.5 fringes/mm in one embodi \\n ment and R, may be 3.0 fringes/mm. In an alternative formulation, the contrast figure of merit may be defined as the 60 ratio of the integrated power in two different spatial frequency \\n bands. The equation shown above is a specific case where one of the bands comprises only the DC spatial frequency. \\n In another embodiment, moving-window means and mov \\n ing-window standard deviations are calculated for the col- 65 lected body of data and used to generate the figure of merit. In this embodiment, for each wavelength corresponding to 18 \\n index i. the moving-window meanu and the moving-window \\n standard deviation O, are calculated from the collected image \\n f(x,y). The moving windows for each calculation may be the \\n same size and may conveniently be chosen to span on the order of 2-3 fingerprint ridges. Preferably, the window size is', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 26}), Document(page_content='f(x,y). The moving windows for each calculation may be the \\n same size and may conveniently be chosen to span on the order of 2-3 fingerprint ridges. Preferably, the window size is \\n sufficiently large to remove the fingerprint features but suffi ciently small to have background variations persist. The fig \\n ure of merit C, in this embodiment is calculated as the ratio of the moving-window standard deviation to the moving-win \\n dow mean: \\n In still another embodiment, a similar process is performed but a moving-window range (i.e., max(image values)-min \\n (image values)) is used instead of a moving-window standard \\n deviation. Thus, similar to the previous embodiment, a mov ing-window mean LL and a moving-window range Ö, are cal \\n culated from the collected image f(x, y) for each wavelength \\n corresponding to index i. The window size for calculation of the moving-window mean is again preferably large enough to \\n remove the fingerprint features but Small enough to maintain \\n background variations. In some instances, the window size \\n for calculation of the moving-window mean is the same as for calculation of the moving-window range, a suitable value in \\n one embodiment spanning on the order of two to three fin gerprint ridges. The figure of merit in this embodiment is \\n calculated as the ratio of the moving-window mean: \\n This embodiment and the preceding one may be consid ered to be specific cases of a more general embodiment in which moving-window calculations are performed on the \\n collected data to calculate a moving-window centrality mea Sure and a moving-window variability measure. The specific \\n embodiments illustrate cases in which the centrality measure comprises an unweighted mean, but may more generally \\n comprise any other type of statistical centrality measure Such \\n as a weighted mean or median in certain embodiments. Simi \\n larly, the specific embodiments illustrate cases in which the variability measure comprises a standard deviation or a range, \\n but may more generally comprise any other type of statistical \\n variability measure Such as a median absolute deviation or \\n standard error of the mean in certain embodiments. \\n In another embodiment that does not use explicit moving window analysis, a wavelet analysis may be performed on \\n each of the spectral images. In some embodiments, the wave let analysis may be performed in a way that the resulting \\n coefficients are approximately spatially invariant. This may be accomplished by performing an undecimated wavelet decomposition, applying a dual-tree complex wavelet \\n method, or other methods of the sort. Gabor filters, steerable pyramids and other decompositions of the sort may also be \\n applied to produce similar coefficients. Whatever method of decomposition is chosen, the result is a collection of coeffi cients that are proportional to the magnitude of the variation corresponding to a particular basis function at a particular \\n position on the image. To perform biometric spoof detection, \\n the wavelet coefficients, or some derived summary thereof, may be compared to the coefficients expected for generated \\n from an appropriate reference dataset samples. In the case', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 26}), Document(page_content=\"US 9,060,688 B2 \\n 19 \\n where the biometric matching function is a determination of a person’s identity, the appropriate reference dataset is one \\n collected from the target person at an earlier time. These \\n enrollment data may be collected from the same skin site or a nearby, locally-consistent skin site and processed in the man \\n ner described. The results of the processing are stored in a \\n database and then retrieved to be used to compare to a similar measurement taken at a later time. If the comparison shows that the results are sufficiently close, the sample is deemed \\n match and identity is established or confirmed. Otherwise, the sample is determined not to match (e.g., to be a spoof). In a \\n similar manner, the coefficients may also be used for biomet ric verification by comparing the currently measured set of \\n coefficients to a previously recorded set from a set of genuine measurements taken on a representative population of people. \\n Various embodiments described above may produce a body of spatio-spectral data, which may be used in various \\n biometrics applications. The invention is not limited to any particular manner of storing or analyzing the body of spatio \\n spectral data. For purposes of illustration, it is shown in the \\n form of a datacube in FIG. 12. \\n The datacube 1201 is shown decomposed along a spectral \\n dimension with a plurality of planes 1203, 1205, 1207, 1209, \\n 1211, each of which corresponds to a different portion of the light spectrum and each of which include spatial information. In some instances, the body of spatio-spectral data may \\n include additional types of information beyond spatial and \\n spectral information. For instance, different illumination con \\n ditions as defined by different illumination structures, differ ent polarizations, and the like may provide additional dimen \\n sions of information. \\n In an embodiment where illumination takes place under white light, the images 1203, 1205, 1207, 1209, and 1211 might correspond, for example, to images generated using \\n light at 450 nm, 500 nm, 550 nm, 600 nm, and 650 nm. In another example, there may be three images that correspond \\n to the amount of light in the red, green, and blue spectral \\n bands at each pixel location. Each image represents the opti cal effects of light of a particular wavelength interacting with \\n skin. Due to the optical properties of skin and skin compo nents that vary by wavelength, each of the multispectral \\n images 1203, 1205, 1207,1209, and 1211 will be, in general, \\n different from the others. The datacube may thus be expressed \\n as RCXs, Ys, X, Y, w) and describes the amount of diffusely reflected light of wavelength seen at each image point X, Y, \\n when illuminated at a source point X, Ys. Different illumi nation configurations (flood, line, etc.) can be summarized by Summing the point response over appropriate source point \\n locations. A conventional non-TIR fingerprint image F(X, Y) can loosely be described as the multispectral data cube for a given wavelength, W, and Summed over all source posi \\n tions: \\n F(X,Y) =XX R(Xs, Ys, X, Y, lo). YS X's \\n Conversely, the spectral biometric dataset S(0) relates the measured light intensity for a given wavelength w to the \\n difference D between the illumination and detection loca \\n tions: \\n The datacube R is thus related to both conventional finger print images and to spectral biometric datasets. The datacube 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n R is a superset of either of the other two data sets and contains \\n correlations and other information that may be lost in either of the two separate modalities. \\n The light that passes into the skin and/or underlying tissue is generally affected by different optical properties of the skin \\n and/or underlying tissue at different wavelengths. Two opti \\n cal effects in the skin and/or underlying tissue that are affected differently at different wavelengths are scatter and\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 27}), Document(page_content='and/or underlying tissue at different wavelengths. Two opti \\n cal effects in the skin and/or underlying tissue that are affected differently at different wavelengths are scatter and \\n absorbance. Optical scatter in skin tissue is generally a Smooth and relatively slowly varying function wavelength. \\n Conversely, absorbance in skin is generally a strong function \\n of wavelength due to particular absorbance features of certain components present in the skin. For example blood, melanin, \\n water, carotene, biliruben, ethanol, and glucose all have sig nificant absorbance properties in the spectral region from 400 \\n nm to 2.5 Lim, which may sometimes be encompassed by the white-light sources. \\n The combined effect of optical absorbance and scatter causes different illumination wavelengths to penetrate the \\n skin to different depths. This effectively causes the different spectral images to have different and complementary infor \\n mation corresponding to different Volumes of illuminated tissue. In particular, the capillary layers close to the Surface of \\n the skin have distinct spatial characteristics that can be imaged at wavelengths where blood is strongly absorbing. Because of the complex wavelength-dependent properties of \\n skin and underlying tissue, the set of spectral values corre sponding to a given image location has spectral characteris \\n tics that are well-defined and distinct. These spectral charac teristics may be used to classify the collected image on a pixel-by-pixel basis. This assessment may be performed by generating typical tissue spectral qualities from a set of quali \\n fied images. For example, the spatio-spectral data shown in \\n FIG. 12 may be reordered as an Nx5 matrix, where N is the number of image pixels that contain data from living tissue, rather than from a surrounding region of air. An eigen-analy \\n sis, Fisher linear discriminant analysis, or other factor analy sis performed on this set matrix produces the representative \\n spectral features of these tissue pixels. The spectra of pixels in a later data set may then be compared to such previously \\n established spectral features using metrics Such as Mahalano \\n bis distance and spectral residuals. If more thana Small num ber of image pixels have spectral qualities that are inconsis \\n tent with living tissue, then the sample is deemed to be non genuine and rejected, thus providing a mechanism for \\n incorporating anti-spoofing methods in the sensor based on \\n determinations of the liveness of the sample. The foregoing analysis framework can also be used to \\n determine identity. In this case, the earlier reference data are taken from a single target individual rather than a represen tative user population. A Subsequent Successful comparison \\n to a particular person’s reference data may then establish both \\n the identity and the genuineness of the measurement (e.g., \\n assuming the earlier reference measurement is ensured to be genuine). \\n Alternatively, textural characteristics of the skin may alone \\n or in conjunction with the spectral characteristics be used to determine the authenticity of the sample. For example, each spectral image may be analyzed in Such a way that the mag \\n nitude of various spatial characteristics may be described. \\n Methods for doing so include wavelet transforms. Fourier \\n transforms, cosine transforms, gray-level co-occurrence, and the like. The resulting coefficients from any such transform \\n described an aspect of the texture of the image from which \\n they were derived. The set of such coefficients derived from a set of spectral images thus results in a description of the \\n chromatic textural characteristics of the multispectral data.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 27}), Document(page_content='US 9,060,688 B2 \\n 21 \\n These characteristics may then be compared to similar char \\n acteristics of known samples to perform a biometric determi \\n nation Such as spoof or liveness determination. Alternatively, \\n the characteristics may be compared to those developed from \\n data taken on reputedly the same person at an earlier time to \\n establish identity. Methods for performing such determina \\n tions are generally similar to the methods described for the \\n spectral characteristics above. Applicable classification tech \\n niques for Such determinations include linear and quadratic \\n discriminant analysis, classification trees, neural networks, \\n and other methods known to those familiar in the art. \\n Similarly, in an embodiment where the sample is a volar \\n Surface of a hand or finger, the image pixels may be classified \\n as “ridge.” “valley, or “other based on their spectral quali \\n ties or their chromatic textural qualities. This classification \\n can be performed using discriminant analysis methods such \\n as linear discriminant analysis, quadratic discriminant analy \\n sis, principal component analysis, neural networks, and oth \\n ers known to those of skill in the art. Since ridge and valley pixels are contiguous on a typical Volar Surface, in some \\n instances, data from the local neighborhood around the image pixel of interest are used to classify the image pixel. In this \\n way, a conventional fingerprint image may be extracted for \\n further processing and biometric assessment. The “other category may indicate image pixels that have spectral quali \\n ties that are different than anticipated in a genuine sample. A \\n threshold on the total number of pixels in an image classified \\n as “other may be set. If this threshold is exceeded, the sample may be determined to be non-genuine and appropriate indi \\n cations made and actions taken. \\n In a similar way, multispectral data collected from regions \\n Such as the Volar Surface of fingers may be analyzed to \\n directly estimate the locations of “minutiae points.” which are \\n defined as the locations at which ridges end, bifurcate, or undergo other Such topographic change. For example, the \\n chromatic textural qualities of the multispectral dataset may \\n be determined in the manner described above. These qualities may then be used to classify each image location as “ridge \\n ending.” “ridge bifurcation, or “other in the manner \\n described previously. In this way, minutiae feature extraction may be accomplished directly from the multispectral data \\n without having to perform computationally laborious calcu \\n lations such as image normalization, image binarization, image thinning, and minutiae filtering, techniques that are \\n known to those familiar in the art. \\n Biometric determinations of identity may be made using the entire body of spatio-spectral data or using particular portions thereof. For example, appropriate spatial filters may \\n be applied to separate out the lower spatial frequency infor mation that is typically representative of deeper spectrally \\n active structures in the tissue. The fingerprint data may be extracted using similar spatial frequency separation and/or \\n the pixel-classification methods disclosed above. The spec \\n tral information can be separated from the active portion of \\n the image in the manner discussed above. These three por tions of the body of spatio-spectral data may then be pro \\n cessed and compared to the corresponding enrollment data \\n using methods known to one familiar in the art to determine the degree of match. Based upon the strength of match of \\n these characteristics, a decision can be made regarding the \\n match of the sample with the enrolled data. Additional details regarding certain types of spatio-spectral analyses that may \\n be performed are provided in U.S. Pat. No. 7,147,153, \\n entitled “MULTISPECTRAL BIOMETRICSENSOR, filed', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content=\"be performed are provided in U.S. Pat. No. 7,147,153, \\n entitled “MULTISPECTRAL BIOMETRICSENSOR, filed \\n Apr. 5, 2004 by Robert K. Rowe et al., the entire disclosure of which is incorporated herein by reference for all purposes. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 22 \\n It will now be appreciated that the various sensor embodi \\n ments may be used to detect locally consistent features of the \\n skin site in a number of ways. Various embodiments may \\n detect spatial, spectral, textural, and/or other information, \\n alone or in combination, in series or in parallel. It will further be appreciated that many configurations of sensors may be \\n used, according to the invention, some of which are described \\n below. \\n Exemplary Application Embodiments \\n Small-area biometric sensors, such as those discussed above, may be embedded in a variety of systems and appli \\n cations according to the invention. In some embodiments, the sensor is configured as a dedicated System that is connected to a PC or a network interface, an ATM, Securing an entryway, or allowing access to a particular piece of electronics Such as a \\n cellular phone. In these embodiments, one or more people may be enrolled in the biometric system and use a particular \\n reader to gain access to a particular function or area. In other embodiments, the sensor is configured as a per \\n sonal biometric system that confirms the identity of the sale \\n person authorized to use the device, and transmits this autho rization to any properly equipped PC, ATM, entryway, or \\n piece of electronics that requires access authorization. In one \\n embodiment, the personal biometric system transmits an \\n identifying code to a requesting unit and then uses a biometric \\n signal to confirm authorization. This may imply that the sys \\n tem needs to perform a verification task rather than a poten \\n tially more difficult identification task. Yet, from the user's perspective, the system may recognize the user without an \\n explicit need to identify himself or herself. Thus, the system \\n may appear to operate in an identification mode, which may \\n be more convenient for the user. \\n An advantage of a personal biometric system may be that, \\n if an unauthorized person is able to defeat the personal bio metric system code for a particular biometric system-person \\n combination, the personal biometric system may be reset or \\n replaced to use a new identifying code and thus re-establish a \\n secure biometric for the authorized person. This capability may be in contrast to multi-person biometric systems that \\n base their authorization solely on a biometric signature (e.g., spatio-spectral information from a fingerprint). In this latter \\n case, if an intruder is able to compromise the system by \\n Somehow imitating the signal from an authorized user, there may be no capability to change the biometric code when it is based solely on a fixed physiological characteristic of a per \\n SO. \\n FIG. 13 shows one embodiment of a personal spectral \\n biometric system 1300 in the configuration of an electronic key fob 1302. While an equidistant sensor configuration is \\n shown, it will be appreciated that any type of sensor according \\n to the invention may be used. In some embodiments, an illumination system 1304 and a detection system 1306 are \\n built into the fob 1302, as is a means to collect and digitize the spectral information (not shown). In one embodiment, short range wireless techniques 1303 (e.g., based upon RF signals) \\n are transmitted to communicate between the fob and a corre \\n sponding reader (not shown), e.g., to allow access to a PC, \\n entryway, etc. In another embodiment, an infrared optical \\n signal is used to transmit information between the fob 1302 \\n and the reader. In yet another embodiment, a direct electrical \\n connection is established between the fob 1302 and the \\n reader. \\n Actual processing of the detected biometric information\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content='and the reader. In yet another embodiment, a direct electrical \\n connection is established between the fob 1302 and the \\n reader. \\n Actual processing of the detected biometric information \\n may be made either within the fob 1302 or at the reader. In the former case, logical operations necessary to perform the com \\n parison may be done within the fob 1302, and then a simple \\n confirmation ordenial signal may be transmitted to the reader.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content=\"US 9,060,688 B2 \\n 23 \\n In the latter case, the detected biometric information may be \\n transmitted from the fob 1302 to the reader (either all at once, serially, or in any other useful way), and the comparison and \\n decision may be performed at the reader or at a host to which \\n the reader is connected. In either case, the communication \\n between the fob 1302 and the reader may be performed in a secure manner, e.g., to avoid interception and unauthorized \\n use of the system. Methods for ensuring secure communica \\n tion between two devices are well known to one of ordinary \\n skill in the art. \\n A second embodiment of a personal spectral biometric \\n system 1400 is depicted in FIG. 14. In this embodiment, the \\n biometric reader 1411 is built into the case of a watch 1412 \\n and operates based upon signals detected from the skin proxi \\n mate to the location of the watch (e.g., around the wrist). In certain embodiments, operation of this system may be iden \\n tical to the operation described with respect to FIG. 13. \\n In addition to the watch or fob, similar biometric capability may be built into other personal electronic devices, including, \\n for example, personal digital assistants (“PDAs) and cellular telephones. In each case, the personal biometric system may \\n provide user authorization to access both the device in which \\n it is installed, as well as authorization for mobile commerce \\n (“M-Commerce') or other wireless transactions that the device may be capable of performing. Small-area sensors may also be put into firearms, commercial equipment, power tools, or other potentially dangerous devices or systems, e.g., \\n to prevent unauthorized or unintended usage. For example, a \\n biometric sensor may be placed in the handgrip of a firearm to sense tissue properties while the gun is being held in a normal \\n a. \\n Further embodiments provide the ability to identify people who are to be explicitly excluded from accessing protected \\n property as well as determining those who are authorized to access the property. This capability may, for example, \\n improve the biometric performance of the system with \\n respect to those unauthorized people who are known to attempt to use the device, which could be particularly impor \\n tant in certain cases (e.g., the case of a personal handgun). In particular, parents who own a biometrically enabled handgun \\n may enroll themselves as authorized users and also can enroll their children as explicitly unauthorized users. In this way, \\n parents may have further insurance that children who are \\n known to be in the same household as a gun will not be able \\n to use it. \\n Even further embodiments use the explicit-denial capabil \\n ity of a biometric system in a fixed installation Such as a home, \\n place of business, oran automobile. For example, a biometric system installed at the-entryway of a place of business can be \\n used to admit authorized employees and temporary workers. If an employee is fired or the term of the temporary employee \\n expires, then their enrollment data can be shifted from the \\n authorized to the unauthorized database, and an explicit check is made to deny access to the former employee if he or she attempts to enter. It will be appreciated that the applications described herein are only examples, and that many other applications may be \\n possible. For example, many spatio-spectral features may be \\n indicative of living tissue, allowing some sensors to be used to \\n detect the “liveness” of a sample. This may deter certain types \\n of circumvention attempts, like the use of latex or wax “sur rogates.” or dead or excised tissue. In some applications. Such \\n as Internet access authorization, it may be useful to be able to Verify the sex and/or age of the person using the spectral \\n biometric System. Because ages and sexes may manifest in different ways in skin structure and composition, the optical spectra may also change in Systematic and indicative ways, 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 29}), Document(page_content='10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 24 \\n Such that age and sex may be estimated using the biometric data. Additional details regarding estimation of certain per \\n Sonal characteristics from biometric measurements are pro \\n vided in U.S. Pat. No. 7,623,313, entitled “METHODS AND \\n SYSTEMS FORESTIMATION OF PERSONAL CHAR \\n ACTERISTICS FROM BIOMETRIC MEASUREMENTS \\n filed Dec. 9, 2004 by Robert K. Rowe, the entire disclosure of which is incorporated herein by reference for all purposes. \\n In various embodiments, a biometric sensor of any of the types described above may be operated by a computational \\n system to implement biometric functionality. FIG.15 broadly \\n illustrates how individual system elements may be imple \\n mented in a separated or more integrated manner. The com putational device 1500 is shown comprised of hardware ele \\n ments that are electrically coupled via bus 1526, which is also \\n coupled with the biometric sensor 1556. The hardware ele ments include a processor 1502, an input device 1504, an output device 1506, a storage device 1508, a computer-read \\n able storage media reader 1510a, a communications system \\n 1514, a processing acceleration unit 1516 such as a DSP or special-purpose processor, and a memory 1518. The com \\n puter-readable storage media reader 1510a is further con nected to a computer-readable storage medium 1510b, the \\n combination comprehensively representing remote, local, \\n fixed, and/or removable storage devices plus storage media for temporarily and/or more permanently containing com \\n puter-readable information. The communications system \\n 1514 may comprise a wired, wireless, modem, and/or other type of interfacing connection and permits data to be \\n exchanged with external devices. The computational device 1500 also comprises software \\n elements, shown as being currently located within working memory 1520, including an operating system 1524 and other \\n code 1522. Such as a program designed to implement methods \\n of the invention. It will be apparent to those skilled in the art \\n that Substantial variations may be used in accordance with specific requirements. For example, customized hardware \\n might also be used and/or particular elements might be imple \\n mented in hardware, Software (including portable software, \\n Such as applets), or both. Further, connection to other com puting devices such as network input/output devices may be employed. \\n It will be appreciated that these units of the device may, individually or collectively, be implemented with one or more Application Specific Integrated Circuits (ASICs) adapted to \\n perform some or all of the applicable functions in hardware. Alternatively, the functions may be performed by one or more \\n other processing units (or cores), on one or more integrated \\n circuits. In other embodiments, other types of integrated cir \\n cuits may be used (e.g., Structured/Platform ASICs, Field Programmable Gate Arrays (FPGAs), and other Semi-Cus \\n tom ICs), which may be programmed in any manner known in \\n the art. The functions of each unit may also be implemented, \\n in whole or in part, with instructions embodied in a memory, \\n formatted to be executed by one or more general or applica tion-specific processors. \\n Exemplary Methods \\n FIG.16 provides a flow diagram of exemplary methods for using multispectral sensor structures according to embodi \\n ments of the invention. While the drawing shows a number of steps performed in a particular order, this is intended to be \\n illustrative rather than limiting. In other embodiments, the steps may be performed in a different order, further steps may \\n be added, and/or some steps identified specifically may be \\n omitted. \\n At block 1604, a skin site of an individual is illuminated. In \\n Some embodiments, the skin site is illuminated under a plu', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 29}), Document(page_content=\"US 9,060,688 B2 \\n 25 \\n rality of distinct optical conditions, e.g., including a plurality \\n of wavelengths, polarizations, and/or source-detector spac \\n ings. Light scattered from the skin site is received at block \\n 1608. An image is then formed of the skin site from the received light (e.g., by generating images of the skin site 5 \\n corresponding to the distinct optical conditions) to generate a \\n local feature profile at block 1612. The local feature profile \\n may characterize a locally consistent feature of the skin site being imaged. \\n At block 1616, the local feature profile may be analyzed. In \\n Some embodiments, this analysis includes comparing the \\n local feature profile to a reference feature profile. In certain \\n embodiments, the reference feature profile includes image \\n data relating to locally consistent features of the individual \\n using the sensor. For example, the data may come from pre \\n vious uses of the sensor and/or from a database of previously \\n collected images. Further, in some embodiments, the sensor may be capable of “learning over time by enhancing its \\n analysis algorithm. For example, genetic algorithms, image 20 \\n averaging, neural networks, and other techniques may be \\n used. \\n Based on the analysis of the local feature profile, a biomet \\n ric function may be performed at block 1620. In some \\n embodiments, the biometric function comprises an identity 25 function. For example, the local feature profile may be com \\n pared with stored reference feature profiles to identify the \\n individual or to confirm the identity of the individual. In other \\n embodiments, the biometric function includes demographic, anthropomorphic, liveness, analyte measurement, and/or 30 \\n other biometric functions. \\n This description provides example embodiments only, and \\n is not intended to limit the scope, applicability or configura \\n tion of the invention. Rather, the ensuing description of the \\n embodiments will provide those skilled in the art with an 35 enabling description for implementing embodiments of the \\n invention. Various changes may be made in the function and arrangement of elements without departing from the spirit \\n and Scope of the invention. Also, it should be emphasized that technology evolves and, thus, many of the elements are 40 examples and should not be interpreted to limit the scope of \\n the invention. \\n Thus, various embodiments may omit, Substitute, or add various procedures or components as appropriate. For \\n instance, it should be appreciated that in alternative embodi- 45 \\n ments, the methods may be performed in an order different \\n from that described, and that various steps may be added, \\n omitted, or combined. Also, features described with respect to \\n certain embodiments may be combined in various other \\n embodiments. Different aspects and elements of the embodi- 50 \\n ments may be combined in a similar manner. \\n Also, it is noted that the embodiments may be described as a process which is depicted as a flow diagram or block dia gram. Although each may describe the operations as a sequential process, many of the operations can be performed 55 \\n in parallel or concurrently. In addition, the order of the opera \\n tions may be rearranged. A process may have additional steps \\n not included in the figure. \\n Moreover, as disclosed herein, the term “memory” or “memory unit may represent one or more devices for storing 60 \\n data, including read-only memory (ROM), random access memory (RAM), magnetic RAM, core memory, magnetic \\n disk storage mediums, optical storage mediums, flash \\n memory devices, or other computer-readable mediums for \\n storing information. The term “computer-readable medium'' 65 \\n includes, but is not limited to, portable or fixed storage \\n devices, optical storage devices, wireless channels, a sim 10 \\n 15 26 \\n card, other Smart cards, and various other mediums capable of storing, containing, or carrying instructions or data. \\n Furthermore, embodiments may be implemented by hard\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 30}), Document(page_content='15 26 \\n card, other Smart cards, and various other mediums capable of storing, containing, or carrying instructions or data. \\n Furthermore, embodiments may be implemented by hard \\n ware, Software, firmware, middleware, microcode, hardware \\n description languages, or any combination thereof. When \\n implemented in Software, firmware, middleware, or micro \\n code, the program code or code segments to perform the \\n necessary tasks may be stored in a computer-readable \\n medium Such as a storage medium. Processors may perform \\n the necessary tasks. \\n Having described several embodiments, it will be recog \\n nized by those of skill in the art that various modifications, \\n alternative constructions, and equivalents may be used with \\n out departing from the spirit of the invention. For example, \\n the above elements may merely be a component of a larger \\n system, wherein other rules may take precedence over or \\n otherwise modify the application of the invention. Also, a \\n number of steps may be undertaken before, during, or after \\n the above elements are considered. Accordingly, the above description should not be taken as limiting the scope of the \\n invention, which is defined in the following claims. \\n What is claimed is: \\n 1. A method of performing a biometric function, the method comprising: \\n illuminating a small-area purported skin site of an indi \\n vidual with illumination light; receiving light scattered from the Small-area purported \\n skin site; generating a local feature profile from the received light, \\n wherein the local feature profile identifies a feature of the Small-area purported skin site of a type predeter \\n mined to exhibit Substantial local consistency across non-overlapping skin sites of the individual; and analyzing the generated local feature profile to perform the \\n biometric function, the biometric function selected from a group consisting of an identity function, a demo \\n graphic function, and an anthropometric function, wherein analyzing the generated local feature profile com prises comparing the generated local feature profile with \\n a reference local feature profile generated from light \\n scattered from a small-area reference skin site of the \\n individual that was previously captured during biomet \\n ric enrollment of the individual and does not overlap with the Small-area purported skin site. \\n 2. The method recited in claim 1, wherein generating the local feature profile comprises: \\n forming an image from the received light; generating spatially-distributed multispectral data from the \\n image; and processing the spatially-distributed multispectral data to \\n generate the local feature profile. \\n 3. The method recited in claim 1, wherein generating the local feature profile comprises: \\n forming an image from the received light; \\n generating an image-texture measure from the image; and processing the generated image-texture measure to gener \\n ate the local feature profile. \\n 4. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n determining an identity of the individual from the gen \\n erated local feature profile. \\n 5. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n estimating a demographic characteristic of the indi \\n vidual from the generated local feature profile.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 30}), Document(page_content='US 9,060,688 B2 \\n 27 \\n 6. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n estimating an anthropometric characteristic of the indi \\n vidual from the generated local feature profile. \\n 7. The method recited in claim 1 wherein illuminating the Small-area purported skin site comprises illuminating the \\n Small-area purported skin site under a plurality of distinct \\n optical conditions. \\n 8. The method recited in claim 7 wherein illuminating the \\n Small-area purported skin site under the plurality of distinct optical conditions comprises illuminating the Small-area pur \\n ported skin site with light under a plurality of distinct polar \\n ization conditions. \\n 9. The method recited in claim 7 wherein illuminating the \\n Small-area purported skin site under the plurality of distinct \\n optical conditions comprises illuminating the Small-area pur \\n ported skin site with light under a plurality of distinct wave lengths. \\n 10. The method recited in claim 7 wherein generating the local feature profile from the received light comprises: \\n generating a plurality of images, each of the plurality of \\n images corresponding to one of the distinct optical con \\n ditions; and applying a compositing algorithm to the plurality of \\n images. \\n 11. A biometric sensor comprising: \\n an illumination Subsystem disposed to illuminate a small \\n area purported skin site of an individual with illumina tion light; \\n a detection Subsystem disposed to receive light scattered \\n from the Small-area purported skin site; and \\n a computational unit interfaced with the detection sub system and having: \\n instructions for generating a local feature profile from \\n the received light, wherein the local feature profile \\n identifies a feature of the small-area purported skin site of a type predetermined to exhibit substantial local consistency across non-overlapping skin sites of \\n the individual; and instructions for analyzing the generated local feature \\n profile to perform a biometric function selected from a group consisting of an identity function, a demo \\n graphic function, and an anthropometric function, wherein analyzing the generated local feature profile comprises comparing the generated local feature pro \\n file with a reference local feature profile generated \\n from light scattered from a small-area reference skin site of the individual that was previously captured \\n during biometric enrollment of the individual and does not overlap with the Small-area purported skin \\n site. \\n 12. The biometric sensor recited in claim 11, wherein the instructions for generating the local feature profile comprise: 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 28 \\n forming an image from the received light; generating spatially-distributed multispectral data from the image; and processing the spatially-distributed multispectral data to \\n generate the local feature profile. \\n 13. The biometric sensor recited in claim 11, wherein the instructions for generating the local feature profile comprise: forming an image from the received light; generating an image-texture measure from the image; and processing the generated image-texture measure to gener \\n ate the local feature profile. \\n 14. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise determining an identity of the individual from the generated local feature profile. \\n 15. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise estimating a demographic characteristic of the \\n individual from the generated local feature profile. \\n 16. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise estimating an anthropometric characteristic of \\n the individual from the generated local feature profile. \\n 17. The biometric sensor recited in claim 11 wherein the', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 31}), Document(page_content='the individual from the generated local feature profile. \\n 17. The biometric sensor recited in claim 11 wherein the \\n illumination Subsystem is disposed to illuminate the Small area purported skin site under a plurality of distinct optical \\n conditions. \\n 18. The biometric sensor recited in claim 17 wherein the \\n plurality of distinct optical conditions comprises a plurality of \\n distinct polarization conditions. \\n 19. The biometric sensor recited in claim 17 wherein the \\n plurality of distinct optical conditions comprises light under a plurality of distinct wavelengths. \\n 20. The biometric sensor recited in claim 17 wherein the \\n instructions for generating the local feature profile from the received light comprise: \\n instructions for generating a plurality of images, each of the plurality of images corresponding to one of the dis \\n tinct optical conditions; and instructions for applying a compositing algorithm to the plurality of images. \\n 21. The method recited in claim 1, wherein the feature of the Small-area purported skin site comprises artificial pig \\n mentation. \\n 22. The method recited in claim 21, wherein analyzing the generated local feature profile comprises verifying a pres \\n ence, quantity, and/or shape of the artificial pigmentation. \\n 23. The biometric sensor recited in claim 11, wherein the \\n feature of the Small-area purported skin site comprises artifi cial pigmentation. \\n 24. The biometric sensor recited in claim 23, wherein the instructions for analyzing the generated local feature profile comprise verifying a presence, quantity, and/or shape of the \\n artificial pigmentation. \\n k k k k k', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 31})], [Document(page_content='US010956732B2 \\n ( 12 ) United States Patent \\n Le Henaff ( 10 ) Patent No .: US 10,956,732 B2 \\n ( 45 ) Date of Patent : Mar. 23 , 2021 \\n ( 56 ) ( 54 ) SYSTEM AND METHOD FOR DETECTING THE AUTHENTICITY OF PRODUCTS References Cited \\n U.S. PATENT DOCUMENTS \\n ( 71 ) Applicant : Guy Le Henaff , Montreal ( CA ) 5,267,332 A 5,673,338 A * 11/1993 Walch et al . 9/1997 Denenberg ( 72 ) Inventor : Guy Le Henaff , Montreal ( CA ) G06K 9/3233 \\n 382/209 \\n ( Continued ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 130 days . FOREIGN PATENT DOCUMENTS \\n ( 21 ) Appl . No .: 15 / 527,907 CN CN 102576394 7/2012 103310256 9/2013 \\n ( Continued ) ( 22 ) PCT Filed : Nov. 23 , 2015 \\n PCT / CA2015 / 051216 OTHER PUBLICATIONS ( 86 ) PCT No .: \\n $ 371 ( c ) ( 1 ) , ( 2 ) Date : May 18 , 2017 Secure fragile watermarking - capabilities , Sergio Bravo - Solorio et \\n al . , Elsevier , 2011 , pp . 728-739 ( Year : 2011 ) . * \\n ( Continued ) \\n ( 87 ) PCT Pub . No .: WO2016 / 077934 \\n PCT Pub . Date : May 26 , 2016 Primary Examiner Jayesh A Patel ( 74 ) Attorney , Agent , or Firm - Hamre , Shcumann , Mueller & Larson , P.C. - \\n ( 65 ) Prior Publication Data \\n US 2018/0349695 A1 Dec. 6 , 2018 \\n Related U.S. Application Data \\n ( 60 ) Provisional application No. 62 / 082,939 , filed on Nov. 21 , 2014 . \\n ( 51 ) Int . Ci . GO6K 9/60 ( 2006.01 ) \\n G06K 9/00 ( 2006.01 ) \\n ( Continued ) \\n ( 52 ) U.S. CI . CPC G06K 9/00577 ( 2013.01 ) ; G06F 16/583 \\n ( 2019.01 ) ; G06K 9/00 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search CPC G06K 9/00577 ; G06K 9/00 ; G06K 9/2063 ; GOOK 9/3233 ; G06K 9/6201 ; \\n ( Continued ) ( 57 ) ABSTRACT \\n System and method for detecting the authenticity of prod ucts by detecting a unique chaotic signature . Photos of the products are taken at the plant and stored in a database / server . The server processes the images to detect for each authentic product a unique authentic signature which is the result of a manufacturing process , a process of nature etc. To detect whether the product is genuine or not at the store , the user / buyer may take a picture of the product and send it to the server ( e.g. using an app installed on a portable device or the like ) . Upon receipt of the photo , the server may process the receive image in search for a pre - detected and / or pre - stored chaotic signature associated with an authentic product . The server may return a response to the user indicating the result of the search . A feedback mechanism may be included to guide the user to take a picture at a specific location of the product where the chaotic signature may exist . \\n 26 Claims , 16 Drawing Sheets', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 0}), Document(page_content='US 10,956,732 B2 Page 2 \\n ( 51 ) Int . Ci . \\n GO6F 16/583 ( 2019.01 ) \\n GO6K 9/20 ( 2006.01 ) \\n G06T 5/40 ( 2006.01 ) \\n G06T 7/44 ( 2017.01 ) G06T 5/00 ( 2006.01 ) \\n G06T 7/42 ( 2017.01 ) \\n G06T 7770 ( 2017.01 ) \\n GO6K 9/32 ( 2006.01 ) \\n G06K 9/62 ( 2006.01 ) GO6K 19/06 ( 2006.01 ) \\n G06T 7700 ( 2017.01 ) \\n ( 52 ) U.S. Ci . CPC G06K 9/2063 ( 2013.01 ) ; GOOK 9/3233 ( 2013.01 ) ; G06K 9/6201 ( 2013.01 ) ; G06K 19/06037 ( 2013.01 ) ; GO6T 5/006 ( 2013.01 ) ; GO6T 5/40 ( 2013.01 ) ; G06T 7/0002 ( 2013.01 ) ; G06T 7/42 ( 2017.01 ) ; G06T 7/44 ( 2017.01 ) ; G06T 7/70 ( 2017.01 ) ; G06K 2009/0059 ( 2013.01 ) ; G06K 2209/25 ( 2013.01 ) ; G06T 2207/20061 ( 2013.01 ) ; GOOT 2207/20064 ( 2013.01 ) ; G06T 2207/20076 ( 2013.01 ) ( 58 ) Field of Classification Search CPC ..... G06K 19/06037 ; G06K 2009/0059 ; G06K 2209/25 ; G06T 7/0002 ; G06T \\n 2207/20076 See application file for complete search history . \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 6,101,265 A * 8/2000 Bacus 2006/0013486 A1 * 1/2006 Burns G06T 7/001 \\n 382/195 \\n 2006/0100964 A1 5/2006 Wilde et al . \\n 2006/0290136 A1 * 12/2006 Alasia G07D 7/128 \\n 283/72 \\n 2007/0150829 A1 * 6/2007 Eschbach G06F 40/103 \\n 715/781 \\n 2007/0158434 A1 * 7/2007 Fan H04N 1/32144 \\n 235/487 \\n 2007/0274585 A1 * 11/2007 Zhang G06F 19/321 \\n 382/132 \\n 2007/0292034 A1 * 12/2007 Tabankin HO4N 5/232 \\n 382/232 \\n 2008/0095465 A1 * 4/2008 Mullick G06T 7/35 \\n 382/284 2008/0201305 A1 * 8/2008 Fitzpatrick G06Q 30/02 2008/0219503 A1 * 9/2008 Di Venuto GO6K 9/6857 \\n 382/103 \\n 2008/0310765 A1 * 12/2008 Reichenbach G06K 7/1443 \\n 382/312 \\n 2009/0080695 A1 * 3/2009 Yang GO6K 9/3241 \\n 382/103 \\n 2009/0302101 A1 * 12/2009 Poizat G06K 9/00577 \\n 235/375 \\n 2010/0067691 A1 * 3/2010 Lin HO4L 9/3247 \\n 380/55 \\n 2010/0128964 A1 * 5/2010 Blair G06K 9/2027 \\n 382/135 2010/0195894 A1 * 8/2010 Lohweg G07D 7/2016 \\n 382/135 \\n 2010/0296583 A1 * 11/2010 Li HO4N 19/162 \\n 375 / 240.24 \\n 2011/0007935 Al * 1/2011 Reed H04N 1/32309 \\n 382/100 \\n 2011/0135160 A1 * 6/2011 Sagan G07D 7/12 \\n 382/108 \\n 2011/0142302 A1 * 6/2011 Leung G06T 1/005 \\n 382/128 \\n 2012/0104097 A1 * 5/2012 Moran G06K 19/06037 \\n 235/449 2013/0142440 A1 * 6/2013 Hirayama B42D 25/305 \\n 382/212 \\n 2013/0173383 Al 7/2013 Sharma et al . \\n 2013/0287267 A1 * 10/2013 Varone G06K 19/18 \\n 382/115 \\n 2014/0006101 A1 * 1/2014 Andrade G06Q 30/02 705 / 7.29 \\n 2014/0037129 A1 * 2/2014 Reed GO6K 9/00 \\n 382/100 \\n 2014/0147046 A1 * 5/2014 Massicot G07D 7/128 \\n 382/182 \\n 2014/0185882 A1 * 7/2014 Masuura HO4N 5/23267 \\n 382/107 2014/0201094 Al * 7/2014 Herrington G06Q 30/018 705/317 \\n 2014/0304122 A1 * 10/2014 Rhoads G06T 19/006 \\n 705 / 27.2 \\n 2014/0341374 A1 * 11/2014 Thozhuvanoor HO4L 9/0869 \\n 380/28 2015/0310601 A1 * 10/2015 Rodriguez G07G 1/0072 \\n 348/150 \\n 2016/0055398 A1 * 2/2016 Ishiyama G06K 9/036 \\n 382/190 \\n 2016/0063302 A1 * 3/2016 Doerr G06K 9/0053 \\n 382/154 \\n 2016/0071101 A1 * 3/2016 Winarski G06Q 20/3829 705/71 2016/0140420 A1 * 5/2016 Di Venuto Dayer , V \\n G06K 9/00577 \\n 382/103 \\n 2016/0171744 A1 * 6/2016 Rhoads G06K 9/6255 \\n 345/419 \\n 2016/0259947 A1 * 9/2016 Negrea GO6F 21/31 \\n 2016/0259976 A1 * 9/2016 Halasz GO6K 19/086 6,396,507 B1 * 5/2002 Kaizuka \\n 7,443,508 B1 * 10/2008 Vrhel \\n 7,891,565 B2 * 2/2011 Pinchen \\n 8,027,509 B2 * GOIN 15/1475 \\n 345/665 G06T 3/0025 \\n 345/660 \\n GO1J 3/10 \\n 348/195 GO6K 19/086 \\n 235/385 \\n G06T 1/0042 \\n 382/100 \\n H04L 9/001 \\n 713/176 \\n GO6F 21/64 \\n 726/32 G06K 9/00671 \\n 382/189 \\n HO4N 19/124 \\n 382/233 \\n G06K 9/00577 \\n 382/190 9/2011 Reed \\n 8,090,952 B2 * 1/2012 Harris \\n 8,171,567 B1 * 5/2012 Fraser \\n 8,731,301 B1 * 5/2014 Bushman \\n 8,908,980 B2 * 12/2014 Hong \\n 8,989,500 B2 * 3/2015 Boutant \\n 9,208,394 B2 * 12/2015 Di Venuto Dayer \\n 9,269,022 B2 * 2/2016 Rhoads 9,401,153 B2 * 7/2016 Sharma 9,800,360 B2 * 10/2017 Yamada 9,947,015 B1 * 4/2018 Vildosola 10,482,471 B2 * 11/2019 Herrington 2002/0025084 Al * 2/2002 Yang \\n 2003/0174863 A1 * 9/2003 Brundage G06K 9/6857 \\n G06F 16/583 \\n GIOL 19/018 \\n HO4H 60/80 G06Q 30/0185 G06Q 30/018 HO4N 1/3875', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 1}), Document(page_content='2003/0174863 A1 * 9/2003 Brundage G06K 9/6857 \\n G06F 16/583 \\n GIOL 19/018 \\n HO4H 60/80 G06Q 30/0185 G06Q 30/018 HO4N 1/3875 \\n 382/299 B42D 25/29 \\n 382/100 G07D 7/0032 \\n 382/100 A61B 8/465 \\n 600/407 \\n G07D 7/128 \\n 382/100 \\n G06K 9/36 \\n 382/132 2004/0001604 A1 * 1/2004 Amidror \\n 2004/0068170 A1 * 4/2004 Wang \\n 2004/0258274 A1 * 12/2004 Brundage \\n 2006/0013464 A1 * 1/2006 Ramsay', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 1}), Document(page_content='US 10,956,732 B2 \\n Page 3 \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 2016/0321531 A1 * 11/2016 Lau \\n 2018/0107915 A1 * 4/2018 Toedtli \\n 2020/0027106 A1 * 1/2020 Kendrick G06K 19/06103 \\n G06K 19/06028 \\n G06K 9/6201 \\n FOREIGN PATENT DOCUMENTS Supplementary European Search Report issued in European Appli cation No. 15860424.9 dated Mar. 19 , 2018 ( 19 pages ) . “ Template Matching ” ; Wikipedia , XP055433175 , 2014 , Retrieved from the Internet , https://en.wikipedia.org/w/index.php?title=Template_ matching & oldid = 615694604 . ( 6 pages ) . Wiegand et al .: “ Motion Estimation for Video Coding \" , 2012 , XP055433170 , Retrieved from the Internet , https : //web.stanford . edu / class / ee398a / handouts / lectures / EE398a_MotionEstimation_ 2012. ( 33 pages ) . European Search Report issued in European Application No. 15860424 dated Dec. 19 , 2017 ( 18 pages ) . “ Template Matching ” ; Wikipedia , XP055433175 , 2014 , 6 pages , retrieved from the Internet , https://en.wikipedia.org/w/index.php ? title = Template_matching & oldid = 615694604 . Wiegand et al .: “ Motion Estimation for Video Coding \" ; 2012 , 33 pages , Retrieved from the Internet : https://web.stanford.edu/class/ ee398a / handouts / lectures / EE398a_MotionEstimation_2012.pdf . Qiu Chen et al . , “ A Modified Way of Image Matching Based on Grid Searching , \" Image Processing and Multimedia Technology , 2009 , CN CN \\n GB \\n WO \\n WO \\n WO \\n WO 103702217 4/2014 104123537 10/2014 \\n 2464723 4/2010 \\n 2013173408 11/2013 \\n WO - 2013173408 A1 * 11/2013 \\n 2013191281 12/2013 \\n WO - 2013191281 A1 * 12/2013 G06K 9/3216 \\n HO4N 7/18 \\n OTHER PUBLICATIONS pp . 56-60 . \\n International Search Report issued in International Application No. \\n PCT / CA2015 / 051216 dated Mar. 1 , 2016 ( 5 pages ) . Written Opinion issued in International Application No. PCT / CA2015 / 051216 dated Mar. 1 , 2016 ( 6 pages ) . First Office Action and Search Report issued for Chinese Patent Application No. 201580074242.8 , dated Mar. 19 , 2020 , 38 pages including English translation of the Office Action . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 2}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 1 of 16 US 10,956,732 B2 \\n ? \\n 0 \\n ? \\n 112', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 3}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 2 of 16 US 10,956,732 B2 \\n 122 \\n 1 \\n 125 \\n 7 \\n FIC 3C', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 4}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 3 of 16 US 10,956,732 B2 \\n FIG 4a \\n 1992 \\n FIG 46 FIG 40', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 5}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 4 of 16 US 10,956,732 B2 \\n 136a \\n 136 \\n KW \\n SLIM FIT 138 \\n 1386 FIG 5a \\n SLIME S_INT ST \\n a \\n } FOLDER 1 \\n FOLDER 2 \\n } \\n FOLDER 3 \\n ? \\n ? . \\n FOLDER \\n FIG 6 6', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 6}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 5 of 16 US 10,956,732 B2 \\n Image 1 Sub - image 1 \\n Image 2 Sub - image 2 \\n 1 I \\n 1 \\n 1 \\n Imagen Sub - imagen \\n FIG 8a', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 7}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 6 of 16 US 10,956,732 B2 \\n FIG 8C \\n 156', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 8}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 7 of 16 US 10,956,732 B2 \\n 162 \\n FIG 9a \\n FIG 9b', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 9}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 8 of 16 US 10,956,732 B2 \\n Take picture \\n 170 App \\n passes minimal requirements ? \\n 174 \\n Transmit picture \\n Determine category ( optional ) \\n Extract set of features from received image \\n Identity ( Search set of features in database ) \\n Declare Unknown \\n 184 \\n Authenticate a ROI \\n Server \\n side Suggest new ROI \\n Aggregated probability beyond a certain \\n treshold ? \\n Deliver results', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 10}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 9 of 16 US 10,956,732 B2 \\n 1 \\n 3000 \\n ODOS \\n ve \\n O \\n Soc Doo \\n 00 \\n FIG 116', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 11}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 10 of 16 US 10,956,732 B2 \\n 1022 \\n Q \\n WWW Won \\n * Y \\n X cy \\n FIG 16 FIG 17', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 12}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 11 of 16 US 10,956,732 B2 \\n1 Best Boots Wars00 \\n Move to Side \\n Please Paracetamol 500mg Tablets XXX \\n 2. W ***** WWWXXX ***** ***** \\n0 \\n FIG 19 \\n Boots Sera 530 \\n close um \\n ***** 20.10.13 22 2.0.X EZEGO & 888888888888 2021 8000H 9090 \\n FIG 21 \\n FIG 23', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 13}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 12 of 16 US 10,956,732 B2 \\n * Us \\n us \\n 3011 \\n FIG 24 \\n FIG 25 \\n Close up \\n 3020 \\n 3021 \\n FIG 26 \\n FIG 27 \\n Even 3060 \\n FIG 29 FIG 28', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 14}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 13 of 16 US 10,956,732 B2 \\n 1 \\n } - 144 14 \\n 3 \\n para \\n 144 \\n FIG 30-1 ? \\n . \\n 2 3 \\n F # . 1 \\n 17 FIG 30-2 \\n } \\n 3 \\n ?? \\n .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 15}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 14 of 16 US 10,956,732 B2 \\n } \\n FIG 30-3 \\n 1 \\n } \\n 2 \\n 3 \\n * Kit FIG 30-4 \\n 1 \\n }', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 16}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 15 of 16 US 10,956,732 B2 \\n 250 \\n using an image capturing device , capturing images of authentic products \\n 252 processing the captured images including detecting , for each authentic product , a unique chaotic signature in \\n one or more images associated with that authentic product \\n receiving , from a remote computing device , an authenticity request for a given product , the authenticity request comprising an image of the given product \\n performing a search for a pre - detected chaotic signature 256 \\n associated with one of the authentic products , within the received image of the given product \\n determining the authenticity of the given product based \\n FIG 31', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 17}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 16 of 16 US 10,956,732 B2 \\n 260 \\n receiving captured images of authentic products \\n 262 processing the captured images including detecting , for \\n each authentic product , a unique chaotic signature in \\n one or more images associated with that authentic product \\n 264 receiving , from a remote computing device , an authenticity request for a given product , the authenticity request comprising an image of the given product \\n performing a search for a pre - detected chaotic signature 266 associated with one of the authentic products searching within the received image of the given product ; \\n 268 determining the authenticity of the given product based \\n FIG 32', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 18}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 1 2 \\n SYSTEM AND METHOD FOR DETECTING in fact requires adding this lens to the capturing device and \\n THE AUTHENTICITY OF PRODUCTS also requires the user to know where to check for the signature part of the product ) . However , natural chaos of \\n RELATED APPLICATIONS matter already exists in most of the product categories that 5 need authentication , which is the fundamental concept This application claims priority from U.S. Provisional behind human fingerprint . Accordingly , the embodiments Application No. 62 / 082,939 filed on Nov. 21 , 2014 , the aim at capturing the existing natural chaos and dealing with specification of which is incorporated herein by reference in the difficulties that exist to generalize the process when the its entirety . end user is either not expecting to see any addition of material ( Art , luxury ) or increase in cost ( Drug manufac BACKGROUND turing ) . \\n Accordingly , a method is described which allows for ( a ) Field detecting the natural chaos and aggregating analysis from \\n The subject matter disclosed generally relates to systems various areas of the product ( hard or soft ( flexible ) ) using \\n and methods for detecting the authenticity of products . In only photos captured by portable computing devices . In \\n particular , the subject matter relates to a system and method more simplified words , the identification process acts as a \\n for uniquely identifying items so as to be able to distinguish method for detecting a unique and chaotic virtual serial \\n genuine items from counterfeit items . number , as well as a way for confirming existence or 20 non - existence of the virtual serial number .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='genuine items from counterfeit items . number , as well as a way for confirming existence or 20 non - existence of the virtual serial number . \\n ( b ) Related Prior Art In a non - limiting example of implementation , a user guided method is described which allows for a powerful Counterfeiting is a hugely lucrative business in which algorithm to be use , which algorithm , allows for a guided criminals rely on the continued high demand for cheap progressive elimination / decimation of suspicion to enable a goods coupled with low production and distribution costs . 25 vision - based authentication system that serves the purpose . Counterfeiting of items such as luxury goods , food , In other words , a fingerprint authentication system is exem alcoholic beverages , materials , art work ( paintings , sculp- plified which is generalized to cover natural chaos in prod ture ) , drugs and documents defrauds consumers and tar- ucts and goods . The system does not always allow for a nishes the brand names of legitimate manufacturers and simultaneous identification but may require first to know providers of the genuine items . Additionally , the counterfeit 30 where to search for the fingerprint . items can often endanger the public health ( for example , In an aspect , there is provided a method for determining when adulterated foods and drugs are passed off as genuine ) . the authenticity of products , the method comprising : using The Organization for Economic Cooperation and Devel- an image capturing device , capturing images of authentic opment ( OECD ) estimated the value of counterfeiting to be products ; processing the captured images including detect in the region of $ 800 billion per year worldwide , including 35 ing , for each authentic product , a unique chaotic signature in 250 billion in drugs and medical . This imposes a real burden one or more images associated with that authentic product ; on world trade estimated to 2 % of world trade for 2007 . receiving , from a remote computing device , an authenticity Anti - counterfeiting measures have included serial num- request for a given product , the authenticity request com bers , machine readable identifiers ( e.g. , barcodes and two- prising an image of the given product ; performing a search dimensional codes ) , “ tamper - proof ” security labels ( e.g. , 40 for a pre - detected chaotic signature associated with one of holograms and labels that change state or partly or com- the authentic products , within the received image of the pletely self - destruct on removal ) , and remotely detectable given product ; and determining the authenticity of the given tags ( e.g. , radio - frequency identification tags ) applied to product based on a result of the search . items directly or to tags , labels , and / or packaging for such In another aspect , there is provided a method for deter items . The principle behind the most common approach is to 45 mining the authenticity of products , the method comprising try to increase the difficulties to reproduce a specific item / receiving captured images of authentic products ; processing tag / object affixed on the goods being purchased . the captured images including detecting , for each authentic However , such measures have themselves been counter- product , a unique chaotic signature in one or more images', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='feited . associated with that authentic product ; receiving , from a Some methods exist which affix an additional label that 50 remote computing device , an authenticity request for a given contains some sort of signature of a non - reproducible mate- product , the authenticity request comprising an image of the rial , in the form of a hologram or in the form of grains of given product ; performing a search for a pre - detected cha certain colors ( like Stealth Mark ) . Some other methods exist otic signature associated with one of the authentic products which heat up a polymer to create a bubble to purposely searching within the received image of the given product ; introduce a chaotic pattern . 55 and determining the authenticity of the given product based \\n Therefore , there remains a need in the market for a more on a result of the search . secure system and method for detecting the authenticity of In a further aspect , there is provided a memory device the products being purchased . having recorded thereon non - transitory computer readable instructions for determining the authenticity of products ; the \\n SUMMARY 60 instructions when executed by a processor cause the pro cessor to process images of authentic products including The present embodiments describe such system and detecting , for each authentic product , a unique chaotic \\n method . signature in one or more images associated with that authen As discussed above , existing approaches aim at affixing a tic product ; receive , from a remote computing device , an controlled material of a certain aspect onto some hard 65 authenticity request for a given product , the authenticity surface of the product to allow for a simple signature request comprising an image of the given product ; search for detection when looked at through a magnifying lens ( which a pre - detected chaotic signature associated with one of the', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='US 10,956,732 B2 \\n 3 4 \\n authentic products within the received image ; and determine FIG . 3b is dose up view of an ROI region in FIG . 3a ; the authenticity of the given product based on a result of the FIG . 3c is an image transform of the image of FIG . 3b \\n search . using a gradient over a low pass filtered image , in accor In yet a further aspect , there is provided a computing dance with a non - limiting example of implementation ; device having access to a memory having recorded thereon 5 FIG . 3d show a high pass filtering of the image of FIG . 36 computer readable code for determining the authenticity of once luminance had been neutralized ; products , the code when executed by the processor of the FIGS . 4a to 4c illustrate images of a different product ; computing device causes the computing device to process FIGS . 5a to 5c illustrate images of yet a further product images of authentic products including detecting , for each and show how the chaotic aspect may be produced as a result authentic product , a unique chaotic signature in one or more 10 of the manufacturing process ; images associated with that authentic product ; receive , from FIG . 6 illustrates an example of a database including a a remote computing device , an authenticity request for a plurality of folders 1 - n , each folder containing images of given product , the authenticity request comprising an image genuine products of the same product series ; of the given product ; search for a pre - detected chaotic signature associated with one of the authentic products 15 FIG . 7 illustrates an exemplary configuration of a folder \\n within the received image ; and determine the authenticity of associated with a given product series ; \\n the given product based on a result of the search . FIGS . 8a - 8e illustrate a non - limiting example of imple \\n In yet another aspect , there is provided a memory device mentation of the guiding process ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content='In yet another aspect , there is provided a memory device mentation of the guiding process ; \\n having recorded thereon non - transitory computer readable FIG . 9a illustrates an example of a QR data matrix instructions for installing on a portable computing device 20 surrounding a chaotic signature on a cork of a bottle ; comprising an image capturing device , the instructions when FIG . 9b illustrates an example of a package including a executed by a processor causes the computing device to : QR data matrix in accordance with an embodiment ; capture an image of a given product ; send the image of the FIG . 10 is a general flowchart of the workflow between given product to a remote server for verification ; receive the app and the server , in accordance with an embodiment ; from the remote server a request to take a close - up image of 25 FIG . 11a illustrates a chaotic aspect of paper of a label the given product , and location information identifying a within vicinity and under a bar code provided on a bottle ; region of interest ( ROI ) for the close - up image ; identify the FIG . 11b is a gradient image of FIG . 11a over a small location of the ROI on the given product ; display a visual amount of pixels in which luminance is neutralized to indicator of the ROI on a display device associated with the enhance high frequencies ; computing device to allow a user to zoom over the ROI and 30 FIGS . 12 to 29 illustrate different methods for determin take the close - up picture . ing the ROI on different products ; In yet another aspect , there is provided a memory device FIGS . 30-1 to 30-4 illustrate an example of a method for having recorded thereon non - transitory computer readable determining the authenticity of a product by progressively instructions for installing on a computing device , the com- detecting a lattice defining a web comprising a plurality of puter readable instructions comprising : images of authentic 35 paths in an image ; products , each image comprising a chaotic signature which FIG . 31 is flowchart of a method for determining the is specific to an authentic product , and / or data representing authenticity of products , in accordance with an embodiment ; the chaotic signatures associated with the authentic prod- FIG . 32 is flowchart of a method for determining the ucts ; and executable instructions which when executed by authenticity of products , in accordance with another the computing device cause the computing device to : cap- 40 embodiment . ture or receive a first image of a first product ; process the It will be noted that throughout the appended drawings , first image to determine an authenticity of the first product , like features are identified by like reference numerals . wherein processing of the first image comprises detecting a presence or lack of presence of a pre - recorded chaotic DETAILED DESCRIPTION', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content='signature in the first image . Features and advantages of the subject matter hereof will Embodiments of the invention describe a system and become more apparent in light of the following detailed method for detecting the authenticity of products by detect description of selected embodiments , as illustrated in the ing a unique chaotic signature that should be intimately part accompanying figures . As will be realized , the subject matter of the product itself . As a non - limiting example , leather disclosed and claimed is capable of modifications in various 50 pore , fiber in Xray of metal goods , wood , fiber in paper . respects , all without departing from the scope of the claims . Photos of the products are taken at the plant and stored in a Accordingly , the drawings and the description are to be database / server which is accessible via a telecommunica regarded as illustrative in nature , and not as restrictive and tions network . The server processes the images to detect for the full scope of the subject matter is set forth in the claims . each authentic product a unique authentic signature which is 55 the result of a manufacturing process or a process of nature BRIEF DESCRIPTION OF THE DRAWINGS or any other process that may leave a unique signature that can be used as a unique identification of the product . To Further features and advantages of the present disclosure detect whether the product is genuine or not at the store , the will become apparent from the following detailed descrip- user may take a picture of the product and send it to the tion , taken in combination with the appended drawings , in 60 server ( e.g. using an app installed on a portable device or the which : like ) . Upon receipt of the photo , the server may process the FIG . 1 describes an example of the production registration receive image in search for a pre - detected and / or pre - stored phase in accordance with an embodiment ; chaotic signature associated with an authentic product . The FIG . 2 illustrates an example of the verification request server may return a response to the user indicating the result phase , in accordance with an embodiment ; 65 of the search . A feedback mechanism may be included to FIG . 3a illustrates an example of an image taken at the guide the user to take a picture at a specific location of the authentication phase on a Smartphone ; product where the chaotic signature may exist . 45', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content=\"US 10,956,732 B2 \\n 5 6 \\n The authentication method may include three main turing device 102 is operably installed to take photos of the phases : 1 ) a product registration phase which usually occurs bags 104 as the bags 104 pass down the production chain . at the plant where the genuine products are manufactured , Pictures of the bags are stored on a database / server 106 whereby , images of the genuine products are taken and which is accessible via a communications network 108 . stored . 2 ) The second phase is the verification request phase 5 The image capturing device 102 can , without limitation , which occurs at the store where the product is being sold , be a smartphone equipped with a good quality camera and whereby , a user who wants to check the authenticity of a a direct telecommunication system a WiFi , Bluetooth or given product may take a picture of the product and send the internet enabled camera or any camera with wired or wire picture to the server for verification . This phase can involve less remote transmission capabilities . an Augmented Reality experience in order to guide the user 10 FIG . 2 illustrates an example of the verification request toward an area of the product that are best suited for phase , in accordance with an embodiment . As shown in FIG . authentication ( and which include one or more ROIs as 2 , when the user wants to verify the authenticity of a given described below ) . 3 ) The third phase is the comparison product 110 , the user may take a picture of the product 110 phase whereby the server compares the received image with using a portable computing device 112 and send the picture pre - stored images of genuine products and outputs the 15 of the telecommunications network 108 to the server 106 for comparison results for viewing on the user's computing comparison . \\n device . In an embodiment , an application may be provided for Phase 2 ) may involve three sub - phases : Phase 2.1 ) per- installing on the portable computing device 112 for taking a forms an identification of the product , phase 2.2 ) is a picture of the product 110 and sending the picture to the dialogue between the system and the user based on phase 2.1 20 server 106. The application may include the IP address of the to guide user toward a Zone Of Authentication ( aka Region server and may be configured to transmit the picture to the of Interest ROI ) as ( 122 ) , ( 128 ) , ( 154 ) ( 156 ) , ( 1041 ) , ( 2012 ) server once the picture is taken . The application may also be for dose range capture for capturing an image with sufficient configured to perform local image processing and compress resolution for the identification process . Phase 2.3 ) com- ing before sending the image to the server 106. In another prises capturing of the dose range image if determined to be 25 embodiment , the application may include an interactive wide enough within acceptable tolerance for slant and aspect to interact with the user and ask for additional rotation ( a purified Current Transformation Matrix ( x ' = ax pictures in specific areas of the product to improve the x + cxy + e y ' = bxx + dxy + f , where a , b , c , and d express a analysis . This will be described in further detail herein traditional rotation matrix with a zoom coefficient and e , f below . translation ) with “ a ” and “ d ” coefficient equal and “ e and “ b ” 30 Image Processing and Search at the Server coefficient dose to zero ) “ e ” and f ” coefficient being as Once the image is locally processed and / or compressed it expected for targeting the ROI . Sub Phase 2.3 may be is sent to the server for further processing and comparison performed automatically by the app when the portable with pictures of genuine products .\", metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 21}), Document(page_content='device detects that a sufficient resolution of the ROI has been In an embodiment , the server 106 may be configured to reached ( sufficient increase in size and / or number of pixels ) . 35 locally process the image of a genuine product taken in the The third phase includes reporting to the user the result of product registration phase to determine one or a plurality of the search . This phase performs the authentication , working region of interests ( ROI ) within the image these ROI on high precision analysis . The result can be done by includes a potential chaotic signature . The embodiments aggregation of probability of analysis either of various describe a non - limiting example of an image processing methods applied to same image or of various attempts to 40 method for the sake of illustration and understanding . How authenticate from different views of same object as it can be ever , the embodiments are not limited to this method and seen in FIG . 13. For example three probabilities of 50 % for may be implemented with other image processing methods three different ROI can result in a pretty high combined or a combination of the method described herein and other probability that a positive match has occurred confirming the methods . The region of interest may be chosen based on an authenticity of the product . A Neyman - Pearson lemma algo- 45 increased frequency in a given area as compared to the rest rithm allows to calculate the final probability using all of the image . In an embodiment , three histograms of fre probabilities and their associated qualities . The main quencies may be used : a global 2D , a vertical 1D , and a 1D embodiment may report the final probability as a bar graph . horizontal histogram of frequencies . A search is done on If the report delivers a probability that is beyond doubt then peak and valley of frequencies to find all relative peaks in the display is an OK message , otherwise the result takes the 50 the frequency range 1/500 of image dimension to 1/2000 of form of a bar from red to green allowing the user to evaluate image dimension . Search uses a Kalman filtering of the if they need another method of authentication or to restart histogram then extracts differences in the range of an the analysis in better environmental condition ( generally due adjusted percentage of the difference between peaks and to lighting ) valleys , for example 30 % is an acceptable range . Accord It must be noted that the 2nd phase may be done with a 55 ingly a signal that drops less than 30 % between two relative repetition of image pickup and analysis if a doubtful con- maximums will disqualify the two relative maximums as dition is determined by the machine or if the product does being peaks . This determines the coordinates of area of not have sufficient chaotic aspect on a single place ( single interest where a certain regularity exists ( high pass filter ) but ROI ) . This is exemplified on the case of a bag shown in where the higher frequencies exhibits irregularities without FIGS . 8B and 8D which show two different requests for a 60 being in the range of the noise of the image or the surface close range pickup allowing top aggregate probability out of of the good / product at registration time ( typically in a 2 the authentication process . pixels range ) . Referring now to the drawings , FIG . 1 describes an When more than one product is available , the apparatus example of the production registration phase in accordance may use a succession of operation to find commonality with an embodiment . In the present example , the product 65 between images . This is done by looking for a medium registration phase is exemplified as being a plant for manu- frequency . The server may use a Hough transform to adjust facturing luxury bags . As shown in FIG . 1 an image cap- orientation of the object and adjust scale . For example ,', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 21}), Document(page_content='US 10,956,732 B2 \\n 7 8 \\n without limitation it is possible to focus on the label of the signature . As the Mask contains local information about product , the stitches , and other areas where a unique signa- frequency contrast in an area or interest it is of interest to ture may be present . Without being mandatory , this helps the consider an histogram of the Mask itself as well as an asserting recognition of chaos in ROI which give a safer histogram of Mask frequencies allowing to characterize a determination of the identity of chaos in respective Area of 5 factor based on ratio of medium frequencies versus all interests ( ROI ) as well as giving a first set of clue as the others , that allow evaluation of the capabilities the apparatus image processing parameters to use for the signing part have to determine signature using only one area or need to analysis as it allows the analysis of chaos to work closer to aggregate over a couple of others . The more narrow is the a confirmation mode than an estimation mode . In particular frequency histogram , the more a specific area is good place the relative positioning to anchor point . This allows for a 10 to find signing chaotic pattern . simpler identification of relation between sub areas of the FIG . 3a illustrates an example of an image taken at the image that exhibit at least some signing capabilities . The registration phase . The exemplary image 120 represents a other interest of such rough common analysis is to determine hand bag . The server 106 may process the image 120 as a minimal feature set that should appear . This is crucial to discussed above to determine a ROI 122 which includes the allow a better determination of cases where the system will 15 chaotic signature . FIG . 3b is close up view of the ROI region assert that it cannot be said that the object is a genuine one , 122 in FIG . 3a . As shown in FIG . 3b , the region 122 includes but without reporting it by negation as being not genuine . It a plurality of irregularities that can be used as anchor points is then said as Undetectable . 124 as compared to the rest of the areas . The other irregu The single or multiple ROIs are characterized by a higher larities shown are identified as 125 and these constitute the frequency than the Anchor points , a noise estimation is done 20 features set . The difference between anchor points and first to determine the noise dispersion and the signal / noise features set is that the anchor points exist in all the products ratio . This allows to determine a cut off frequency for in a given line of products while the features set 125 frequencies of interest . The interesting areas for ROI are constitute the chaotic signature and may be the result of a characterized by a high contrast of frequency on relatively natural process , a manufacturing process , or a natural evo wide area ( 4 to 8 time the pixel distance of the low 25 lution once a manufacturing process is done , like a fermen frequency ) . All areas that exhibit such pattern of frequencies tation process of mildew that will have its own chaotic are grouped by connectivity and create a Mask of the ROI . structure and can worn out a copy of this structure as a worn The values used in the mask reveal the contrast for the out on a material too smooth to be analyzed , or any surrounding neighbor . This allow for weighting the quality combination of the above .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content=\"of the authentication that can be done on the ROI at a later 30 FIG . 3c Is an image transform of the image 122 using a stage . In an embodiment , the Mask is computed on every low pass filtered image using a pixels level adjustment using object individually then an intersection of every Mask or a local luminance around each pixels weighting an image ROI is done . The intersection of the Mask takes the form of gradient over this pixels . On such image a Hough transform an addition of contrast . Additionally the first Mask for a first is applied that allow to search for major line in Hough space object can profitably be enriched from experience learned 35 that intersect . Such crossing is considered as anchor point from computation of other new objects as they arrive and the quality of such crossing of Hough line is increased without requiring to re - compute it for all others . This Mask when regularly found over other sampled of production has the higher value where the contrast is not only rather goods when they exhibit same crossing in nearly same high but also very common . This creates a mix of common- vicinities . FIG . 3d show a high pass filtering of the image ality of the ROI with also the characteristics of it . This Mask 40 122 once luminance had been neutralized , another Hough is a key component to know where to consider ROIs in scan transform allow to search for commonality of lines but in that will occur during the image analysis in real time . The this case it is the complementary that are taken in account as apparatus focuses its search on areas where the Mask or ROI the feature set of chaotic points that will be metered between is high , gradually suggesting areas where the mask have them , or more generally to an anchor point . This only lower and lower values . Each degree or recognition found 45 exemplify a method for analyzing irregularities over goods for each ROI are considered in a final result , each can be that qualify a signature , But other method can be used either weighted with different factor to aggregate their similarity in statistical analysis and classification or non - parametric clas a consistent manner . Under a certain threshold value ( typi- sification of pattern , that allow at later time a simpler search . cally Number of Object * 3 % of contrast swing ) the Mask Other Examples are Provided Below . areas of no interest are nullified to avoid a scan of ROI on 50 For example , FIG . 4a illustrates an image of another areas that definitively have no reason to show similarity . So product . There is at least 3 directions of view that allow to that when the object is not genuine and all previous ROI take a picture allowing authentication . Direction 440 is scan failed individually or by cumulated weighted aggrega- typically used during the sale process and exemplified in tion to deliver an acceptable combined probability of simi- FIGS . 4b and 4c . Direction 441 is aimed toward another area larity , the system can determine that the product is not 55 of the product where sufficient chaotic aspect exists in a genuine . place that is expected to be worn out less easily during the In some cases , the location of the ROI may be substan- life of a product . This can even be used for analyzing tially the same for all products in a given product line . For passage of person either with a camera looking upward example , if the highest chaotic signature is around the ( provided that focus plane and image pre - processing location of button on the bag due the fact that the button is 60 respects people's privacy ) . It can even be used sideway installed manually , then the location of the ROI may be using the grain of the leather as a signature . The image 126 substantially the same for all the products in this series , and shown in FIG . 4b includes an area 128 which is rich with these cases are faster to search since the server does not have irregularities . When processing the image 126 , the\", metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content='in this series , and shown in FIG . 4b includes an area 128 which is rich with these cases are faster to search since the server does not have irregularities . When processing the image 126 , the server to search in other locations when the image is received from would identify the area 128 as a ROI which includes the the user . However , in most cases the location of the ROI of 65 chaotic signature . FIG . 4b is a close up view of the ROI 128 choice or set or ROIs are different for each product of the of FIG . 4a . As shown in FIG . 4b , the ROI 128 includes same series depending on where the server finds the chaotic Anchor points 131. These Anchor points are determined as', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content='US 10,956,732 B2 \\n 9 10', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='features which exist across production ( de facto if not fore , if the received image does not have a sufficient quality existing it should warn a trigger that product is defective ) , that allows for searching for the chaotic signature , the server the other set involving more chaotic pattern is made of at may send a notification back to the portable device asking least a few of the leather pores 132 that are amongst several for another picture . The notification may be accompanied chaotic aspects including the cut 130 made in the fabric . 5 with a certain message to display to the user on the portable Combined , these irregularities define a chaotic signature device such as for example : not sufficient lighting , or too which is unique to the product associated with that picture . much lighting , or change position etc. Such chaotic signature constitutes a unique identification In an embodiment , and in order to expedite the identifi aspect which is impossible to forge reproduce , let alone to cation process and make it acceptable to reduce the waiting detect where the chaotic signature exists on the product . The 10 time for the client ( user ) who is waiting for a response , the signature may be strengthened further by creating relation- identification process only delivers one to many ( < 100 ) ships between the different chaotic aspects such as but not plausible candidates to a final processing stage for final limited to : number of chaotic aspects , distance between a comparison with the received image . In other words , if there certain chaotic aspect and another one etc. are 100,000 registered products in the database , hence at A further example is shown in FIGS . 5a and 56 which 15 least 100,000 images ( each image representing one authentic shows how the chaotic aspect may be produced as a result product ) , the identification process may chose a smaller of the manufacturing process . FIG . 5a is a top view of a subset of images e.g. 1/1000 to 1/10000 of the entire set of product . FIG . 5b is dose up view of the ROI of the product images for comparing them with the received image . The shown in FIG . 5a and FIG . 5c is dose up view of the ROI selection of this subset is based on the features set . In in an image associated with a different product from the 20 particular , the server may chose the images having features same series of the product of FIG . 5a . set that are the closest to the received image to then compare In particular , FIG . 5a illustrates a picture 136 of a shirt by each one of those images to the received image using an the brand Perry Ellis® . The picture 136 includes a ROI 138 . authentication process that is more thorough and subse FIG . 5b is close up view of the ROI 138a of the shirt of FIG . quently more CPU intensive . In an example , the server may 5a , and FIG . 5b is a close up view of the ROI 138b of 25 only chose 100 images maximum for this final comparison another shirt but in the same location . As shown at 140a the step . Therefore , the final decision by the server may be area beside the sticker “ SLIM FIT ” in FIG . 5b includes three rendered in about 10 sec not counting communication delays squares that are spaced apart from the sticker . By contrast , between the user and the server . FIG . 5c shows two squares that are partially provided under The purpose of this stage is to certify beyond a reasonable the sticker and another square that is just in touch with the 30 doubt that one or none of the identified sets offers sufficient sticker . This is an example of another type of chaotic similarity to the received image so that the outcome prob signatures that the server may detect in the pictures and look ability that the object is genuine can be asserted . The reason for in the images received from the user . of such 2 strokes action is due to the long processing time In an embodiment , the server may , based on the shape , that can require an Authentication process for all', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='images received from the user . of such 2 strokes action is due to the long processing time In an embodiment , the server may , based on the shape , that can require an Authentication process for all the recoded color and other aspects of the product , classify the products 35 products in the database which depending on the type of belonging to the same series ( aka line of products ) in a given product may reach millions . The analysis of chaotic area can folder for facilitating the search when receiving images from be either completely pattern dependent , looking for a pattern the user . For example , as shown in FIG . 6 , the database may at any place using an algorithm like a GIST , SURF or a BOF include a plurality of folders 1 - n each folder containing ( Bag Of Features ) to generate descriptors working on a images of genuine products of the same product series e.g. 40 pre - processed image . The preprocessing may be done in the PERRY FLUS® slim fit shirts , or the Monogram Delight- portable device but the GIST analysis is preferably done at ful® series by Louis Vuitton® etc. Analysis of such a the server . characteristic image can be done using either an imbedded Then the server side analyzer enters the authentication solution using a GIST kind of method or a ORBS or SURF process working on the very limited subset but using a fine method to extract descriptors feeding a LSH ( Locally Sen- 45 pattern analysis mode . The identification process uses an sitive Hashing ) tree , but can also be done using online algorithm that deliver a plausible CTM ( coordinates trans solution like the QUALCOMM Vuforia SURF implemen- formation matrix ) allowing to transform the perspective tation or MoodStock search engine implementation which distorted image that also very frequently exhibits rotation or offer solutions that are essentially able to give a rough even slant . In one embodiment the authentication process identification of the product for Phase 2.1 . 50 can use algorithm methodologies derived from fingerprint In a further embodiment , the server may associate with authentication processing taking the smears and curves out each image of a genuine product a sub - image representing of the oriented gradient processing . The fingerprint algo one of the ROI of that image as exemplified in FIG . 7. FIG . rithm delivers reasonable success and mainly very low level 7 illustrates an exemplary configuration of a folder associ- of mistakes . The penalty is that it frequently delivers a ated with a given product series . The sub - images may be 55 higher level of undetermined situations ( neither a yes nor a used for expediting the comparison process whereby the no ) . This may be reported to user who in turn should be server may compare the pixels in the received image with guided to take another snapshot . In another embodiment , it each sub - image representing each stored chaotic signature of is possible to use a specific algorithm made of speculative a genuine product until a match occurs or until the entire pattern matching in a wavelet converted image . This gives a folder is searched without any match . 60 certain flexibility to compensate for orientation beyond the Accordingly , when receiving the image from the useriapp , CTM delivered by the identification phase . the identification process begins . The server may first deter- However , an enhanced embodiment exists whereby the mine the product line , and when the product line is deter- identification process is advantageously helped by making a mined the server may start searching in the folder associated certain number of position within the image available as with that product line in order to find whether or not a 65 anchor points that can be used to determine the position previously photographed genuine product exists which has where to search for chaotic salient point . This is exemplified the same chaotic signature as in the received picture . There- in FIG . 3b at 124', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='photographed genuine product exists which has where to search for chaotic salient point . This is exemplified the same chaotic signature as in the received picture . There- in FIG . 3b at 124 , and FIG . 4b at 131 where the anchor point', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='US 10,956,732 B2 \\n 11 12', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='( or beacon ) are determined from points that exist and are scan to a decent minimum . The interest of combining the AR common in nearly all manufactured goods for each product , with Authentication method is to allow this high precision line as part of the design and that have no or very few chaotic position to be determined with the help of the AR system and comportment . generally asking the user to focus on some very specific area Anchor points are considered as points of interest , for 5 at very close range , This allows to find safer anchor points example constellation of peaks out of the gradient image , that allow for a simpler scanning of the image to find if a which have a common geographical location . An example of decent amount of the expected patterns from the original a criteria that can be used for the tolerance of commonality image exist in the submitted sample . of geographical location evaluated during the Anchor point The benefit of the AR recognition phase is that it can be determination can be the Chi - square of position dispersion . 10 repeated at various scales over the same object to gradually This dispersion of position is kept as a parameter that will qualify the object at closer and closer range still keeping characterize the anchor point . Anchor points sets are used to previous probability of being genuine for a progressive characterize more easily the object and optionally identify a aggregation until it reach a sufficient level or all ROI are batch or a manufacturing method . When anchor points are scanned and it failed to deliver a conform probability or available ( determined by processing the batch of images for 15 being genuine . To exemplify this , FIG . 12 shows an object a class of same product line ) then the analysis of chaotic area ( Art ) as FIG . 14 at long range recognized by the AR system . or interest can occur more easily . The relative position of The white spot like 1021 represents anchor points as found chaotic salient point of chaotic area ( ROI ) is then metered to by the recognition system . They allow to estimate a decently a reasonable number of all surrounding anchor points . The precise CTM ( current transformation matrix ) and extract value kept is the distance , the angle versus a relative horizon 20 from the AR database the Asset that must be associated ) with and the quality of dispersion of the anchor point . This this candidate to be displayed atop the Art , this suggests to information is stored in the central database . The number of the user to close up on areas of interest for Authentication Anchor points within a radius of a chaotic point determines ( ROI ) , like the area 1031 shown in FIG . 17. FIG . 15 shows a statistical minimum and maximum allowing to decode the the same object but with the closer range pickup . It had also constellation during reading and asserts a quality of recog- 25 been recognized by the AR identifier and anchor points nition based on the number of points found versus all the determined as shown by bright white spots . It quickly point that had been stored during the registration . appears that the signature of the artist is a structure that are It is also possible sometimes that Anchor points them- of a medium entropy , means are regular enough to allow a selves exhibit sufficient chaotic dispersion so that it should good chance that feature points ( descriptors ) like ORBS can be very unlikely that the positions of anchors point between 30 be found and efficiently used ( the bright white spots alike themselves are sufficient for a unique discrimination of the 1021 ) , as well as giving enough of these feature points with goods . While being true on some goods . It did appear that a rather high precision so that a pattern matching can start this direct method is setting constraints on the algorithms with a decent load on CPU because the number of tries that can be used to search for these signatures made of the needed is', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='can start this direct method is setting constraints on the algorithms with a decent load on CPU because the number of tries that can be used to search for these signatures made of the needed is thankfully limited by the high precision of posi differences of distances . While using anchor points and 35 tioning which decrease the need to try various shift and scale additionally using a pattern that should be at a certain and even rotation . bearing of the anchor points increase the flexibility offered This capability to guide user is of high interest as it is for these fuzzy searches . Too many parameters are uncon- shown on FIGS . 14 to 15. The device is started , the AR trolled while being involved in the whole chain of analysis . system performs a first recognition which allows to know For example , the moisture in the atmosphere where the 40 that at least this artwork is known by the system , then it goods are stored and also the way the goods are stored or suggests a predefined area of interest in the form a certain stacked may distorted it slightly within acceptable tolerance number of areas where the apparatus would like the user to for the end user , while it definitively changes the geometries focus on . It then displays a bullseye and an arrow to guide of the galaxy of the features set . Therefore this 2 levels the user to take a closer picture of the area that is indicated approach allows the pattern detection of chaotic features to 45 on the screen display . When a recognition is established , this be done using a smaller area for the ROI , which is better for in itself is first good sign of existence of similar products in dealing with a distorted final product , and simultaneously the database ( the initial probability of the feature set is based the presence of anchor points allows this smaller pattern area for example on the number of goods which have similar in the ROI to still be easy enough to position and then descriptors the sets for goods that are different ) . Subse correlate with the content of the central image database . 50 quently , the purpose of the next action is to qualify a certain The difficulties of the art lies in the tradeoff necessary pattern and / or intersection of patterns such as for example between the low precision position delivered by the capture the line of the signature with the texture of the paper , this system with rotation , scaling and even natural perspective natural alliance of chaos cannot be reproduced at least using distortions , compared to the high precision position needed known technics as of today . by a safe authentication system . These distortions are a 55 Once the user takes a closer picture of area 1021 then the consequence of the coarse nature of the image pickup process starts the image transform and pattern analysis . FIG . device , generally hand held device which is held without 16 shows a first level of image transform done through a great concern by consumer at time of pickup . A minimum gradient localized in a direction , this make the structure of guidance is needed and offered , as it can be seen on FIG . 12 the paper appear . The AR system will allow to help user and FIG . 13 where the bracket like 1022 does guide the user 60 focusing on this area with their own tracking mechanism and to have a picture as compatible with a 90 degrees modulus the assets here are the arrow and the bullseye ( see FIG . 13 ) . as possible while decreasing shear , slant and maximizing Then either in real time over the video feed or trough action image size . However without additional guidance either by of the camera , a picture of higher resolution will be taken on the computer using computer vision CV to establish correc- a more critical area ROI , in this particular example the ROI tion factor , or more efficiently using capabilities offered by 65 is shown as being area 1041 . the Augmented Reality Paradigm and tools , this precision of Another example is depicted by', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='the ROI tion factor , or more efficiently using capabilities offered by 65 is shown as being area 1041 . the Augmented Reality Paradigm and tools , this precision of Another example is depicted by FIGS . 18 to 23 where the pickup is largely insufficient to narrow down the samples to full process is explained . FIG . 18 shows the view from the', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='US 10,956,732 B2 \\n 13 14', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='smartphone side before being identified , then FIG . 19 shows and creates the authentication area that uniquely identifies the response once the package has been identified . As it is the product or gives a strong indication that the product is known to have better ROI on a side ( this will be FIG . 22 ) , authentic . the user is driven to take another view of the package as in Such numerical analysis to matching can be expected in FIG . 26. Once identified the AR embarks again and suggests 5 a simple and 1/1 relationship on some products that have a to have a close view of a ROI in FIG . 21. This allows a high serial number . Analyzing the chaotic pattern naturally or resolution image to be taken as shown in FIG . 22. FIG . 23 purposely added onto the bank note allows for an absolute depicts the image once processed and shows the anchor identification of genuine banknote in a 1/1 mode . points ( rectangles ) and the area of pattern search ( ellipse ) . The concept of anchor point allows to reduce the com The anchor points had been determined as a constant pattern 10 putational problem to avoid analysis using a topography from image to image during take - up and as maximizing the method such as the triangulation method , close to astro usage of Vertical and horizontal shape that are less likely to nomical position analysis , in which the apparatus first uses be chaotic , therefore , they can be expected to be present in a detector to identify a first object and its weighted center , almost every image of the same product line . This is also then establishes the evolution of the first object based on the exemplified on FIG . 11b where the printed barcode itself 15 metrics determined by the position acquired by triangulation naturally offers anchor points . As well as on FIG . 4c with of two other objects known to exist and which form a label and stitches . triangle qualified by angles with the first object . However , Hence the interest there is to focus on barcode , not for the many differences between the embodiments and the angular code itself but also for the alignment it provides which can based triangulation method . For example , if the other two be used as a reference for metering the pattern . In FIG . 22 20 objects are not detected the triangulation method determines it can also be seen that the manufacturer had to affix that the first object also does not exist or does not qualify . production information that allows a drastic reduction in the However , in the present embodiments very many relevant number of pattern to check against . It can be seen that objects may exist and very any fake irrelevant / fake objects patterns are expected at some specific place so that it is not may also exist at the relevant place ( ROI ) . Therefore , there of interest to scan every place for every pattern but rather use 25 exists a necessity to assemble a multitude of signature sets a pattern matching and displacement analysis . and compare them to a reference database . Accordingly , a For example , in the pharmaceutical industry , lot numbers statistical analysis is used to measure the distance between are usually in the ranges of 1000 to 100,000 while the the anchor points and the ROIs which may then create a product manufactured can include millions of units . In other plurality of probabilities associated with the different sets of words , 1000-100,000 units can have the same lot number on 30 ROIs , each set being associated with a different product . The them . Therefore , it is possible to narrow down the search plurality of probabilities may then be combine into a final from millions to 100,000 sets of patterns which can be probability . reasonably explored even with a crude enumerating process For example , a product may have 100 different patterns at ( pattern size is typically 16x16 and a CPU can compare 1 some specific ROI , and another product may have another Millions / second so that 10 second over 100 000 set allow', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='100 different patterns at ( pattern size is typically 16x16 and a CPU can compare 1 some specific ROI , and another product may have another Millions / second so that 10 second over 100 000 set allow to 35 set of patterns at a different ROI . During authentication have 10 patterns / set check at 10 position , which appears to phase , the first result of pattern matching may deliver be more than sufficient as generally there is always at least patterns coming from the first set and may very well 6 patterns that differs between products made with a sub- determine patterns coming from the second set . Therefore , strate like leather , paper , wood , clothe , embossed plastics , the misplacement of pattern should be resolved using the and in general all high chaotic substrate . It can be noted that 40 statistical analysis to authenticate the most relevant set of a tree organization of patterns allow to drastically speed up pattern as being the one having the highest number of even such enumeration process . patterns in the given ROI and the best match between pattern Another scenario is to look for the serial number ( if any ) and candidate image in each ROI found to be at the proper and search for the chaotic signature in the ROI for this distance between anchor points and pattern . product . However , not all products have serial numbers , 45 Other attempts have been made to perform authentication which necessitates the methods discussed above for narrow- using pattern matching . One of these methods uses analysis ing down the search . Other cases of unique products include of a cross section of pattern ( as if the pattern is like a fuzzy art work such as original paintings which are by default barcode ) using a classification based on weak coincidence of unique . code which is strengthened with the number of decently Another exemplification is depicted in FIGS . 24 to 29 50 close match of code . Other methods use classification meth which deal with the blister of the drug . FIGS . 24 and 26 ods based on the number of patterns that have a decent represent two different blisters and the Identification suc- image match while being simultaneously in immediate cessfully embarks on both pictures asking for a close up of vicinity of a relative other pattern matching in same condi a ROI ( 3010 ) and ( 3020 ) . The full size gradient shown on tion . However , the present embodiments do not consider the FIGS . 19 and 27 are for the sake of general understanding 55 amount of relationship between patterns in immediate vicin but the apparatus may focus on ROIs only . The information ity , but rather use classification through analysis of distance on the side of the blister ( 3011 ) and ( 3021 ) are of interest to to at least one reference point i.e. the anchor points . be decoded with a glyph recognition ( Support Vector Other methods exists which are color / intensity based Machine ( SVM ) which is a typical first stage of an OCR ) to approaches which look at a substrate to identify colors and determine if some alpha - numerical information would allow 60 their intensities in different areas on a document . These a reduction in the number of pattern to cross analyze . The method do not use the pattern identification approach per se , ROI of FIG . 28 and FIG . 29 show anchor points ( rectangles ) but rather a merged pattern intensity in which the loci of and pattern area ( ellipse ) . It must be noted that 3050 and analysis must be precise to avoid influence of nearby print 3060 are emphasizing the difference that exists between the ing . The side effect here is to decrease the requirement of two blisters . An extreme difference exists here but it is an 65 precision of location . Typically these methods are mainly exemplification of the natural manufacturing chaos occur- used with very regular products like banknotes , but do not ring during the processing that , which chaos is rare in nature properly work on productions that have wide', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='manufacturing chaos occur- used with very regular products like banknotes , but do not ring during the processing that , which chaos is rare in nature properly work on productions that have wide varieties of', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 15 16 \\n aspects like paint or variations during production , like drugs Accordingly , a decrease of constraints for qualification blisters . Some additional approaches exist which are keeps the operational quality of the authentication above the labelled as being pattern based but which in fact look at needed requirement . patterns as shapes that must exist , and not as pixels sets . Pattern searching and identification of pattern from a These shapes are looked for during authentication without 5 database is difficult to put in a generalized tree like method performing a reconstruction which requires a high quality that would allow a scan through very many pattern sets . This image pickup . The main problem with these approaches is is due to the amount of combination of patterns that create that they decrease the probability to successfully authenti- a set and the similarity between the patterns of different sets . cate if the image intake is of bad quality or not - normed . Yet In some case the recognition of the object allow to focus on an additional approach exists which is based on pattern only one reference image like in example on FIG . 14 for an analysis and which requires high magnification which inher- artwork which is unique , or as exemplified here of a lithog ently allow seizing more of chaotic nature of substrate which raphy in limited quantity , and numbered ( as exemplified by greatly simplify the issue because the variety of pattern is area 1011 which explicit the sample printed ) these cases are high enough to look for a one / one relation ( a pattern have near perfect cases as the set of pattern to check against is very small to none chance to belong to another product ) . The de - facto limited in size . But in some other case like the drugs identification of the pattern area become crucial and apply box on FIG . 18 or on the blister on FIG . 24 , the number of mainly in cases where the product itself can be checked reference images can be huge ( size of a batch lot ) , so that using a method that allows a physical analysis of the even if the apparatus narrows down the number of refer position . This approach optimizes the case of false positive 20 ences to scan through various mechanism , like the batch because the severity of the process is too high . In other number ( 2010 ) or ( 3010 ) on the blister , there is still a need words , the number of cases where a product is identified as to scan many reference images . being genuine when in fact it is not are extremely dimin- While the LSH tree allows for some organization of ished , but the penalty is the increase in the number of cases pattern for fuzzy search , another approach using a reverse where genuine products are not identified as such , which in 25 principle where for each pattern the associated data are the fact is very bad from the standpoint of the manufacturer who list of products that may contain it , is more efficient in the is generally paying for the service . main embodiment . When the amount of reference image is In a non - limiting example of implementation , the prin very high the sortation approach described above for finding ciple may accumulate the distance to anchor points , sub tracted from expected distance , thus establishing a score 30 tions ) can be of a cardinality sized to have a geometrical the best pattern ( 10 pattern / set tried for match at 10 posi \\n based on the sum of delta of distances . When a distance is beyond reasonable values , it is completely discarded . The progression with samples and requires minutes of CPU for \\n Sum Of Square differences ( SSD ) or alike norma of match processing time . Then the pattern search can benefit to create \\n ing of difference between pattern and target , are com and “ a posterior ” principle because using square of 16x16 pounded in this analysis . As an example for values used in 35 this deliver enough case where nearly each combination of', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 26}), Document(page_content='a main embodiment . If the SSD is 30 % of maximum match 16x16 square may be used , either within the same reference \\n for the analysis session then the tolerance on distance before image at various locations , or across all the reference \\n being discarded is 20 % if the pattern matching SSD is higher images . It is then of interest to use a memory based indexing \\n than 66 % then the tolerance on distance is lower in the range mechanism as this 16 * 16 square can be coded as 16 + 16 bits of 5 % . The counter - intuitive principle being that a strong 40 which would lead to 32 bits to index every combination of matching with a crude method like SSD is rather an oddity patterns . This allows to create for each of those 23224 Billion and should not be bonified . While the embodiments use the patterns a list of reference image index and their ROI SSD method , it must be noted that the embodiments are not locations which allows to speed up the search by quickly limited to this method and that other pattern matching decimating patterns that are not existing and then filter the methods are available for example in CV library like 45 commonality of position and index of the reference . The OpenCV and can be used with such approach . The maxi- position to an anchor point is a key component of the final mum for the analysis session being determined with the determination . The embodiments can properly operate using analysis of the dispersion of SSD , the regularity of high 1K square patterns to analyses ( 64x16 ) patterns , each pattern score being a strong indicator of the quality of the match set being 16 * 16 pixels over an image of 2K * 1K pixels once analysis . While a dispersion that may even include some 50 areas of no interest are eliminated . The distance of pattern to very strong matching is a negative indication of a matching anchor point is in itself a sufficient factor for classification authentication . This information may be compounded using because the aggregation of found distances subtracted from a Neural Network with a training set done using various the expected distance deliver a final score that involves 1000 substrates . It must be noted that preliminary to or during the distances which if coded over 4 bits deliver a 4K bits capturing of images of the reference images , there generally 55 precision for the index . The implementation of the main exists the possibility to inform the system about the nature embodiment does not allow a high precision because it had of the substrate in the object , which allows a user to match been evaluated from the rules in industry that 128 bits are it to a predefined category ( Wood , Leather , Cardboard , sufficient to qualify the uniqueness of an object . Accord paper , plastics , skin , woven fabrics , agglomerated grains , ingly , the embodiments only consider patterns that appear in etc ... ) so that the Trained Set used at classification for final 60 the ROI at a distance of maximum 4 pixels from the anchor aggregation of all scores , can be chosen based on this point . specificity allowing greater discrimination which allows to However , for the case where we have millions of products increase tolerance to change of shape of the object versus the to search against , even optimized pattern matching such as reference image . This is typically a more critical problem the methods discussed above , can be expensive in term of when analyzing object made of leather or cloth , while being 65 CPU bandwidth / usage to allow for a decent response time . a less critical problem because this kind of objects substrates And knowing a - priori precisely ( pixel precision ) where to naturally exhibits more randomness of the natural chaos . search for a match of pattern of a set will dramatically speed', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 26}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 17 18 \\n up the pattern matching . The main embodiment uses a where path intersects the more path lead to a place the more specific method accelerator in this case to reduce the interesting this intersection is and should be consider as an response time . entry point for the search . The principle of this accelerator relies on a subpart of the The radius of search is typically equal to 1 square i.e. H264 compression encoding , by deriving the pattern match- 5 covers 32x32 pixels . ing from the motion estimator used in the H264 compression The computation is organized by first doing the image algorithm ( http://web.stanford.edu/class/ee398a/handouts/ processing needed to extract edges of gradient , then con lectures / EE398a_MotionEstimation2012.pdf ) . This allows verted to a skeleton of single pixels applying a kernel to use hardware assisted systems and feeding a deeply convolution to erode the paths . When the skeleton convo modified version of the encoder , keeping only the analyzer , lution is done , a process maybe started to join “ interrupted ” and where the supplied image from the Device is the lattice paths , based on a tangential approach , ( at last 3 pixels \" moving ” image and where the reference image act as a aligned allow to bridge another lattice sub path if same 3 \" previous ” frame . This allows to aggregate the vector and pixels tangent exist on the other side and the bridge doesn\\'t direction of vector to determine the accuracy of the match- need to cover more than a couple of pixels . This compen ing . sates for disappearance of continuity in the sample image This method dramatically speeds up the search and is also mainly due to poor lighting condition . Incidentally this improved by reducing the amount of tile to check focusing allows a certain analysis of the quality of the image as the on a subset determined at registration time . This set is based number of bridges needed gives an indication of the differ on a histogram of frequencies and the tile which exhibit the 20 ence of quality with the image pickup done on the reference best homogenous histogram ( modeled after a search for a image . “ flat line ” ) is the most interesting . Then path too close ( 1 pixels apart ) are joined and eroded A4Kx4K encoder working on a subset made of 1K square again . of 16x16 , allow to compute ( 4000/1000 ) * ( 4000/16 ) * 30 This complex processing is justified by the interest there Comparison / seconds which is in the range of 30000 vector 25 is to feed a massively multithreaded architecture like the one of displacement of pattern / sec , the organization of search available for CPU assistance by GPU like the NVidia GPU can also be improved with tree organized search prequali- architecture in CUDA . So that these massive amounts of fying certain aspects of pattern using a method as explained processors can all work in parallel doing the path analysis as above , this gives a factor 100 to the efficiency toward a brute well as reconsider node numbering as described above . force scan , so that a machine can compare an average of 10 30 The image processing part can also be done in the GPU . Million references images in a bit over 3 seconds . As it is This approach allows for a strong confirmation of authen more interesting to try slight changes in position of the ticity while preserving a flexibility on the aspect of the sample image ( 4 different shift and 2 scale ) this decreases image which is compatible with the poor pickup image . the throughput to an average of change 1.1 Millions / sec The performance is improved by an organization of node', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 27}), Document(page_content='which is a good high mark limits of the number of reference 35 between reference images which allow to create a tree of to scan once the AR experience allows to decrease the field similar locality of node across reference images using a very of product to explore ( a drug batch size of a common drugs similar approach as the one described above for similarity of are in the 100,000 sample range for blisters . ) square 16x16 when many reference exists . If the pattern matching method fails to deliver a suffi- An example is described in FIGS . 30-1 to 30-4 . FIGS . ciently high score at proper positions of anchors point , or in 40 30-1 to 30-4 illustrate an example of a method for deter general if a confirmation is requested using another mining the authenticity of a product by progressively detect approach the later decision can become a need due to ing a lattice defining a web comprising a plurality of paths lighting conditions , then another confirmation method in an image . The lattice may be taken out of a reference embarks using as much as possible of the information from image and FIG . 30-2 represents a lattice taken out of a the previous one in particular about positioning of the 45 candidate image . As clearly shown in FIG . 4-2 a disconti sample image . nuity exists in the line joining the nodes 1 and 3 , when This method uses the fact that most chaotic aspect out of compared to FIG . 30-1 . Also as shown in FIGS . 30-1 and the image transformation using frequency gives way to a 30-2 , three nodes exists which are numbered 1-3 between lattice like pattern . Otherwise described as a labyrinths . The the three different lines that define the lattice . The embodi search will take some lattice node and dive in the lattice of 50 ments may chose the nodes as an entry point in order to the reference image at some specific entry point . This in turn circulate the lattice to determine the authenticity of FIG . allow to aggregate a score made of the total of length of path 30-2 with respect to FIG . 30-1 . Assuming that the system that do match between the 2 lattices . chooses node 3 as entry point and mistakenly considers node The interest of this method is to simplify the search of the 3 as being node 2 due to some similarity of the pixels around best insertion point , If the insertion point for the search is 55 the two nodes . In the present case , the system may proceed badly chosen then rapidly the amount of length covered by with the processing of the lattice by choosing a path that the similarity of path become small however at each inter- starts with node 3 expecting to end up at node 1 as secting within a limited distance , the search may restart exemplified in FIG . 30-3 . However , after an expected dis using already computed path but considering them as part of tance has been travelled on the line , the system may detect the same reference node but inserted at different place in the 60 that it does not find node 1. In which case , the system may sample image . This allow to rapidly allow a re - adjustment of try a different direction as exemplified in FIG . 30-4 until it the analysis . finds node 2 and re - apply the same strategy to find node 1 , At each major crossing points the computation can be which when accomplished can make the system re - asset its done starting another thread that will re - evaluate the perti- initial travel path with a great benefit of being able to use all nence of this point as a better entry point . 65 the computations that are already pre - done by requalifying The insertion point can be such as putting grid of squares the end points and re - using the expensive computations of 16x16 then within each squares sort places if the lattice the path .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 27}), Document(page_content='US 10,956,732 B2 \\n 19 20', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='Processing at the App server to identify the potential location of the ROI may As discussed above , since the location of the chaotic include one or more of : dimensions of the ROI , surface ratio signature is not always the same for all the genuine products , between the ROI and the surface of the product on which the the server may have to search different areas within the ROI is located , and location information identifying the received image to detect the presence of a previously 5 location of the ROI within the product . The app may be recorded / photographed chaotic signature . configured to perform basic image processing to isolate the In one embodiment , the server may apply a progressive product from its background to locate the identified ROI targeting to find the area of interest in the received image . using the received information to then guide the user to take This also allows pinpointing known specific location of closer pictures thereof . interest where the frequency dispersion is optimal ( e.g. using 10 In an embodiment , in addition to the visual indicator 154 the same criteria used to detect the chaotic signature in the the app may be adapted to produce one or more audible images of the authentic products ) . Using such guidance sounds , including voice , that help guiding the user in the allows going from coarse analysis that delivers probability right orientation and / or direction until a decent picture of the amounts to target a final very precise area that exhibit a ROI is available at the screen . unique identity . This process can be done either within the 15 In an embodiment , the app may be configured to display transmitted images by looking at subsets of the image one or more visual indicators on the screen to guide the user establishing a pattern matching of a rather generic area until take a closer picture of the identification location that may a subset exhibits sufficient similarity in the sense of a Norma include a potential ROI . A non - limiting example of imple like sum of square of difference , applied either on the image mentation of the guiding process is provided in FIGS . 8a - 8e . or a transformation thereof , or can be done by requiring the 20 As shown in FIG . 8a , the user takes a picture 150 of a device 152 to adjust the pickup to a specific area . product 151 using a smartphone 152. The picture 150 may Typically the suggested area used to assist the pattern then be sent to the server for processing . If the server does analyzer use the value of the Mask of ROI computed as not find any match to a pre - recorded chaotic signature , the explained above . The aspect of such Mask exhibits an server may select based on the statistics associated with that irregular shape with peaks and valleys with a noise back- 25 line of products another location for the ROI and send the ground . Peaks are directly used as suggestion for the ROI to info to the app . The app may use the information to detect consider . This human guided method is applied very simi- the location of the ROI on the photographed product . larly within the server to consider a succession of ROI . For example , as shown in FIG . 8b , the app may highlight However , because it is executed in the server by an automat an area 154 on that need to be photographed closer . The user going from an ROI to another ROI this occurs in a period of 30 may then bring the device 152 closer to take a closer picture milliseconds for aggregating results of ROI analysis , while of the identified area as exemplified in FIG . 8c . When the using manual operation asking the user to manually take a app detects a resolution level which is equal to or above a better snapshot of the ROI , but this time it will occur the given threshold the app may then allow the user to take a rate in which the user will supply the image . However the second picture ( e.g. by activating the picture taking button ) speed in this case is not the issue as the user', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='allow the user to take a rate in which the user will supply the image . However the second picture ( e.g. by activating the picture taking button ) speed in this case is not the issue as the user understands 35 and send it to the server . As shown in FIG . 8c the surface naturally that the process requires more areas for certifica- area of the visual indicator may enlarge as the device 152 tion and at the contrary the server is not expected to approaches the ROI . The device 152 may send the close up consider multiple ROIs within its own analysis . Selection of picture 155 to the server . If no match exists , the server may a User guided focus on ROI or on a server automatic guided request another picture in a different location 156 as exem approach on ROI is based on the quality of picture taken for 40 plified in FIGS . 8d and 8e . The process may continue until analysis . Below a certain level of quality the system must a final determination is made . compensate by asking user for a closer look but on various The final determination may be one of several cases areas of the object . This also could be needed to compensate including : for poor quality picture due to poor lighting conditions Genuine : this is the case where an image was found in the In another embodiment , the server may also generate 45 database that matches the received image . statistics related to the ROIs in a given folder of pictures of Unknown : Not genuine but not certain as being faked . genuine products . Such statistics may relate to the dimen Meaning no images in the database match the received sions and locations of the ROIs for a given product series . image ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='Therefore , and as explained above , if the server could not Undetectable : the received image does not meet the detect a potential ROI in the image received from the user , 50 minimal criteria for image content , typically the the server may send information to the app installed on the expected frequency distribution . In other words , the phone to guide the user to take a closer picture of a potential received image cannot be used to make proper deter location where the ROI may exist . Examples of the type of mination ; information that the server may send to the app include : Not Genuine : there is a certain number of cases where the size / dimensions of the ROI relative to the image of the 55 good can be either replicated for same serial number product , the location of the ROI within the image , shape of including enough of the chaotic aspect , or the good is the ROI , boundaries of the ROI etc. known from other sources as being a non - registered In addition to the functions discussed above such as one . It can come from external sources and can be taking the picture and sending it to the server etc. , the app ( without limitation ) detected at 2 different places simul may be adapted to determine whether or not the image meets 60 taneously , or being sold for a first time at a place that the basic requirements of clarity and resolution e.g. avoid is not an authorized dealer , or being explicitly regis obvious cases where picture is blurry or dark etc. tered as faked if destruction of good is not the appro In addition , the app may be equipped with the intelligence priate method , or registered but not yet offered . that allows it to use the information received from the server If the image is qualified as “ Unknown ” another embodi to identify / detect the identified location ( potential ROI loca- 65 ment can submit the picture as it was taken over the web to tion ) on the product being photographed . In a non - limiting a processing center where human specialist may act as example of implementation , the information sent by the referee and do a human interpretation of the uncertainties', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='5 US 10,956,732 B2 \\n Šll????tch?éh? . > É??ch?ETzú?', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='either in an absolute manner or using help for comparison or not because any dislocation of the features defining the with the set of images suspected to be the goods analyzed chaotic signature would result in a negative match . The same but that cannot be confirmed . principle apply to other goods like those delivered in pre The server may then send the final determination back to filled syringe . the portable device for display in the app . The QR data embodiment facilitates the search process at In another embodiment , a local version limited to a certain the server because the size in pixels and the location of the number of digest of the signed image , like a GIST data set ROI would be known . Furthermore , the QR data allows for and pattern set , may be embedded for some specialized associating meta - information with the product . For example , check within the app used for the authentication as discussed the label if can very conveniently include or display a serial below . In the latter case , the portable device may be con- 10 number . This number will go through an OCR and consid figured to perform the entire analysis locally without send- erably eases the recognition process by asking for an image ing the image to the remote server for processing . In a based authentication , rather than image based identification . non - limiting example of implementation , the app may have The QR code can also contains additional focus informa access to the features set of a subset of the registered tion aimed at easing the camera pickup , in particular the products . The subset may be specific to a certain manufac- 15 focal plane of the object 164 can be at a very different depth turer or line of products . The subset may also be dependent than the QR code . Typically the camera will first auto focus on geographical constraints in the sense that users in a given on the easiest part of the picture , generally the high contrast . country will not need access to registered products that are Then the QR code can be read and determined the necessary shipped or scheduled to be sold or distributed in a different de - focus that would allow the camera to enforce the maxi country or different continent etc. 20 mum quality of a small part of the field of view , the aperture The latter embodiment is of particular interest when the 162. So that the quality of the signing chaotic camera pickup objects to authenticate are in limited range . This include is maximized . auctions where offered goods have pretention as for the The presence of a QR data also eases considerably the origin , inventories of luxury goods in warehouse or at analysis of horizontality of the picture at the verification borders , the transiting product between two places , e.g. 25 phase , so that pattern authentication can use algorithm of a servicing of plane engine parts etc. In these cases the amount more speculative nature . of products that are genuine candidates are limited ( 100 to Often , the product is provided in a package , and thus , it 10000 ) then the handheld device may be downloaded in would be difficult to open the package to test the authenticity advance with all the features set and authentication pattern of the product before purchasing it . In an embodiment , the necessary for authentication so that a totally offline experi- 30 QR data matrix 160 may be provided on the package and a ence can be conducted using handheld CPU for the whole visual access may be provided through the aperture 162 to processing . Methods applied for authenticating many not an area of the product defining the chaotic signature as allow to use gigantic amount of memory nor powerful GPU exemplified in FIG . 9b . FIG . 9b illustrates an example of a but on the other side the amount of pattern to check against package including a QR data matrix in accordance with an is very limited ( 1 Million squares 16x16 average ) so that a 35 embodiment . As shown in FIG . 9b , the QR data', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='of pattern to check against package including a QR data matrix in accordance with an is very limited ( 1 Million squares 16x16 average ) so that a 35 embodiment . As shown in FIG . 9b , the QR data matrix 162 good result can be achieved within a reasonable amount of provides visual access to the product included in the package time . Eventually removing reference that are check to speed 166 , thus , providing access to the chaotic signature of the later usage as the check session occurs . product without having to open the package before purchas In an embodiment , the features set may be provided in a library that the app can access for verification purposes . The 40 As exemplified in FIG . 11a the structure of the signing features set in the library may be updated periodically as pattern can be found in an area of choice like a barcode . In new products are registered and / or have reason to be made this case the structure of the paper contains sufficient dif part of the library . For example , if the product was manu- ferences from label to label so that it can be used as a signing factured and registered but not yet released . factor . The richness of such structure can be understood In an embodiment , the app may be specific to a certain 45 from FIG . 11b which is an image process that neutralizes manufacturer or line of products , or even a certain badge of luminance and enhances high frequencies trough a gradient products . In other cases , the app may be used for determin- process over a small amount of pixels . The Anchor point can ing authenticity of different and unrelated products e.g. be taken out of the edge of the Barcode , through a Hough luxury bags , and vaccination syringes . transform looking at crossing of lines , while the chaotic QR Code and Chaotic Seal 50 features are taken out of high frequencies that also show a In an embodiment , it is possible to use a QR data matrix dispersion of characteristics in other word taking a fre in addition to the chaotic seal . For example , a QR data quency range just below the highest ray found in the matrix may be provided which defines an empty space ! frequency histogram ) . aperture to place over the chaotic seal such that the chaotic The usage of barcode as a help to guide correction of seal would be surrounded by the QR data matrix . The 55 horizon is exemplified on FIG . 21 where the user is first chaotic seal may be provided in the form of a piece of wood guided with AR to find one place of interest for authentica or another object having a unique shape / signature . The QR tion , here the bar code area . It is also shown on the example data matrix may be provided on a label to be placed over the that if the system fails to authenticate the drug pack with the chaotic signature . An example is provided in FIG . 9 . barcode , before reporting to user a low probability of FIG . 9a illustrates an example of a QR data matrix 60 genuine origin , the apparatus may guide the user toward surrounding a chaotic signature on a cork of a bottle . As another are of interest like 2021 where the batch code shown in FIG.9a , AQR data matrix 160 defining an aperture appears . This can greatly help reducing the amount of 162 is provided on a bottle 164. The aperture 162 allows a reference image to compare to . visual access to a chaotic signature defined by the shape and In the case of bar code area , the bar code is an additional lines of cork provided in the bottle . Such embodiment not 65 source of information as it can identify the product uniquely only allows for identifying the authenticity of the bottle 164 reducing dramatically the search for identification in the but also to detect whether the bottle was previously opened database , simultaneously the orientation and aspect of bar up ing it .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='any kind . US 10,956,732 B2 \\n 23 24 \\n code allow to correct the image hence readjust the distribu- FIG . 32 is flowchart of a method for determining the tion of frequencies on an image that are distorted like the authenticity of products , in accordance with another barrel effect shown on a bottle . This is helpful to readjust embodiment . The method comprises receiving captured image pickup perspective and field taken image pickup . It images of authentic products at step 260. Step 262 comprises can be noted on FIG . 11b that in 1126 the chaos that are part 5 processing the captured images including detecting , for each of the glass molding irregularities are of high interest as they authentic product , a unique chaotic signature in one or more \\n can be part of the chaotic feature set hence certify that not images associated with that authentic product . Step 264 \\n only the label is genuine but belonging to the proper content . comprises receiving , from a remote computing device , an \\n The picture in FIG . 11b is taken with a smartphone Samsung authenticity request for a given product , the authenticity \\n Galaxy SIV Mini® with a standard camera and no add - on of 10 request comprising an image of the given product . Step 266 comprises performing a search for a pre - detected chaotic', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='Authorized Dealer Adjustments signature associated with one of the authentic products searching within the received image of the given product . In an embodiment , it is possible to grant licensed and Step 268 comprises determining the authenticity of the given trusted dealer the authority to test the products before 15 product based on the result of the search . displaying them for sale and the authority to add pictures of While preferred embodiments have been described above the product should the server return a negative response or and illustrated in the accompanying drawings , it will be a positive response which is inconsistent . evident to those skilled in the art that modifications may be The re - registration can then be organized locally under made without departing from this disclosure . Such modifi surveillance from manufacturer , otherwise the goods may be 20 cations are considered as possible variants comprised in the returned to the plant for verification and / or registration . scope of the disclosure . FIG . 10 is a general flowchart of the workflow between For example , the process described above is not limited to the app and the server , in accordance with an embodiment . chaotic signatures and can be applied to characterize human Steps 170-174 take place at the app ( portable computing applied signatures that can be of a non - chaotic nature but device ) . Step 170 comprises taking a picture of the product 25 rather extremely difficult to reproduce , example of such can that need to be verified . At step 172 the app assesses the be inclusion of metallic structures buried within another minimal requirements of the picture taken at 170. If they are metallic object , where X ray may show the pattern . In which ok the picture is transmitted to the server at step 174 case , the manufacturing process may be complexified to a otherwise a new picture is taken ( with or without guidance ) . level where it becomes deterrent to produce a fake product . Step 176 and up take place at the server side . At step 176 30 When the chaotic nature is difficult to assert then it can be the server determines the category ( product line ) of the created on purpose and even contain a method that allow an product shown in the received picture . The received may identification of the product , an example of a technology that then be processed to extract a set of features . In an embodi- can be combined is offered by “ Stealth Mark ” which delivers ment , the set of features may be extracted using the same a product where the dispersion of grain is chaotic , whereas rules and algorithms used at the product registration step 35 the detection of the product does not involve an authenti when the images of the authentic products are processed to cation but rather an identification . extract / find the chaotic signature . At step 180 the extracted feature set is searched in the database to find a match . At step The invention claimed is : 182 the results of searching the different features are iden- 1. A computer - implemented method for determining the tified through their features set ( image descriptors like GIST 40 authenticity of products , the method comprising : this is Phase 2.1 . Then user pickup , or server focus itself on a registration phase that includes : a ROI for authentication at step 184 . receiving images of authentic products , at a server ; If , at step 186 , the combined probability is beyond a detecting anchor points present in one or more of the certain threshold the results are delivered at step 190 . images of every one of the authentic products , the Otherwise , the server may request a new picture at step 188 45 anchor points existing at common geographical loca and suggest a different location for the potential ROI and tions between the images of the authentic products in send the same back to the app for taking the new picture at a given category of the authentic products ; the specified location . The', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='ROI and tions between the images of the authentic products in send the same back to the app for taking the new picture at a given category of the authentic products ; the specified location . The experience is repeated if neces- processing the images at the server including searching sary at step 188 , result of step 184 being kept for aggregation for and detecting , for each authentic product , a until all location of the set of known place of ROI for this 50 unique chaotic signature in one or more of the identified product are scanned , or earlier if the aggregated images associated with each authentic product , the probability reached the required threshold of accuracy . server configured to analyze the images of the FIG . 31 is flowchart of a method for determining the authentic products in search for the unique chaotic authenticity of products , in accordance with an embodiment . signature that uniquely identifies the authentic prod The method begins at step 250 by capturing images of 55 uct with respect to the other authentic products , the authentic products using an image capturing device . Step unique chaotic signature naturally existing in the 252 comprises processing the captured images including authentic product without adding or affixing material detecting , for each authentic product , a unique chaotic onto the authentic product ; signature in one or more images associated with that authen- for each authentic product , measuring relative positions tic product . Step 254 comprises receiving , from a remote 60 between the unique chaotic signature and one or computing device , an authenticity request for a given prod more of the anchor points , the relative positions uct , the authenticity request comprising an image of the including a distance and an angle between the unique given product . Step 256 comprises performing a search for chaotic signature and each anchor point ; a pre - detected chaotic signature associated with one of the storing the anchor points and the unique chaotic sig authentic products , within the received image of the given 65 natures associated with the authentic products and product . Step 258 comprises determining the authenticity of the relative positions associated with each unique the given product based on the result of the search . chaotic signature in a database ; and', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='5 \\n 10 \\n 15 US 10,956,732 B2 \\n 25 26 \\n an authentication phase that includes : searching , within the image of the given product , for the receiving at the server , an authenticity request for a unique chaotic signature associated with authentic given product from a remote computing device , the products pertaining only to the detected product - line . authenticity request comprising an image of the 11. The method of claim 1 , further comprising : given product ; providing a QPR matrix defining an aperture on the given performing a search within the image of the given product ; and product for one or more of the anchor points and the providing visual access to the unique chaotic signature of unique chaotic signature within the relative positions the given product through the aperture of the QPR of the one or more anchor points ; matrix such that the QPR matrix surrounds the unique determining that the given product is authentic if the chaotic signature of the given product . unique chaotic signature found in the image of the 12. The method of claim 1 , further comprising : given product matches one of the chaotic signatures providing a QPR matrix defining an aperture on a package \\n stored in the database . containing the given product , wherein the package 2. The method of claim 1 , wherein determining that the contains a visual access to an area of the given product given product is authentic comprises comparing the unique containing the unique chaotic signature of the given chaotic signature of the given product to the unique chaotic product ; and signatures of a sub - group of the authentic products stored in providing visual access to the unique chaotic signature of \\n the database . the given product through the aperture of the QPR 3. The method of claim 1 , wherein performing the search 20 matrix such that the QPR matrix surrounds the unique within the image of the given product comprises using a chaotic signature of the given product . \\n hardware accelerator built with a pattern analyzer embedded 13. The method of claim 1 , wherein the unique chaotic with a hardware video encoder . signatures are the result of a manufacturing process , a 4. The method of claim 3 , further comprising aggregating process of nature , or both thereof . motion estimation vectors to create a final score to determine 25 14. The method of claim 1 , wherein the registration phase the authenticity of the given product based on a sum of occurs at a plant where the authentic products are manufac normals of the vectors of the motion estimator . tured , or at a licensed dealer . 5. The method of claim 1 , further comprising finding , in 15. The method of claim 1 , wherein detecting the unique a frequency transformation of a given image of an authentic chaotic signature of each authentic product comprises per product designated nodes of a first lattice found in the given 30 forming a progressive targeting of at least one of the images image of the authentic product ; to find an area of interest where a frequency dispersion is finding an entry point in a second lattice found in the optimal . given image , travelling a given path of the second lattice for a given 16. A computer - implemented method for determining the \\n distance ; authenticity of products , the method comprising : \\n if a node is not found within the given distance , a direction a registration phase that includes : \\n of travel is changed to explore other parts of the second receiving , at a server , captured images of authentic \\n lattice until a number of nodes is found in the given products ; \\n image which match with the designated nodes of the detecting anchor points present in one or more of the \\n given image of the authentic product in position and 40 captured images of every one of the authentic prod', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 31}), Document(page_content='given image of the authentic product in position and 40 captured images of every one of the authentic prod \\n length of path between nodes . ucts , the anchor points existing at common geo 6. The method of claim 1 , further comprising requesting graphical locations between the images of the another image of the given product from the remote com authentic products in a given category of the authen puting device . tic products ; \\n 7. The method of claim 1 , further comprising : storing the detected anchor points ; identifying at least one region of interest ( ROI ) within the processing the captured images at the server including image of the given product ; detecting and storing in a database , for each authen requesting a close - up image of the ROI from the remote tic product , a unique chaotic signature in one or more computing device ; and of the images associated with each authentic product , searching within the close - up image for the unique cha- 50 the server configured to analyze the captured images otic signature associated with the given product . of the authentic products in search for the unique 8. The method of claim 7 , wherein identifying the ROI chaotic signature that uniquely identifies the authen comprises : tic product with respect to the other authentic prod processing the image of the given product using a set of ucts , the unique chaotic signature naturally existing rules used to find the unique chaotic signature in the 55 in the authentic product without adding or affixing images stored in the database ; and material onto the authentic products ; estimating a potential location of the unique chaotic for each authentic product , measuring relative posi signature of the given product , the potential location tions between the unique chaotic signature and representing the ROI . two or more of the anchor points , the relative 9. The method of claim 7 , wherein identifying the ROI is 60 positions including at least one of a distance and done based on statistical information relating to locations of an angle between the unique chaotic signature and the unique chaotic signatures in a given line of products . each anchor point ; and 10. The method of claim 1 , further comprising : an authentication phase that includes : classifying the images of the authentic products into one receiving , at the server , an authenticity request for a or more product - lines ; given product from a remote computing device , detecting a product - line associated with the given product the authenticity request comprising an image of shown in the image of the given product ; and the given product ; 35 \\n 45 \\n 65', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 31}), Document(page_content='US 10,956,732 B2 \\n 27 28 \\n performing a search within the image of the given unique chaotic signature in one or more images product for the anchor points and the unique associated with that authentic product , the comput chaotic signature within the relative positions of ing device configured to analyze the images of the the anchor points ; authentic products in search for the unique chaotic determining that the given product is authentic if the 5 signature that uniquely identifies the authentic prod unique chaotic signature found in the image of the uct with respect to the other authentic products , the given product matches one of the unique chaotic unique chaotic signature naturally existing in the signatures stored in the database . authentic product without adding or affixing material 17. The method of claim 16 , wherein detecting the unique onto the authentic product ; chaotic signature of each authentic product comprises per- 10 measure relative positions between the unique chaotic forming a progressive targeting of at least one of the images to find an area of interest where a frequency dispersion is signature and two or more anchor points , the relative \\n optimal . positions including at least one of a distance and an \\n 18. A memory device having recorded thereon non angle between the unique chaotic signature and each \\n transitory computer readable instructions for determining 15 anchor point ; \\n the authenticity of products , the instructions when executed store the anchor points , the unique chaotic signatures of \\n by a computer cause the computer to : the authentic products and the relative positions perform a registration phase that includes : associated with each unique chaotic signature and \\n receive and process images of authentic products the anchor points in a database ; and including detecting , for each authentic product , 20 perform an authentication phase that includes : anchor points present in one or more of the images receive , from a remote computing device , an authen of every one of the authentic products , the anchor ticity request for a given product , the authenticity points existing at common geographical locations request comprising an image of the given product ; between the images of the authentic products in a search within the image of the given product for the given category of the authentic products , and a 25 anchor points and unique chaotic signatures within unique chaotic signature in one or more of the the relative positions of the anchor points ; and images associated with each authentic product , the determine that the given product is authentic if the computer configured to analyze the images of the unique chaotic signature found in the image of the authentic products in search for the unique chaotic given product matches one of the unique chaotic signature that uniquely identifies the authentic prod- 30 signatures stored in the database . uct with respect to the other authentic products , the unique chaotic signature naturally existing in the 20. The computing device of claim 19 , wherein the \\n authentic product without adding or affixing material computing device is adapted to request another image of the \\n onto the authentic product ; given product from the remote computing device . \\n measure relative positions between the unique chaotic 35 21. The computing device of claim 19 , wherein the \\n signature and two or more anchor points , the relative computing device is adapted to : \\n positions including at least one of a distance and an identify a region of interest ( ROI ) within the image of the \\n angle between the unique chaotic signature and each given product ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 32}), Document(page_content='positions including at least one of a distance and an identify a region of interest ( ROI ) within the image of the \\n angle between the unique chaotic signature and each given product ; \\n anchor point ; request a close - up image of the ROI from the remote store the anchor points , the unique chaotic signatures of 40 computing device ; the authentic products and the relative positions search within the close - up image for the unique chaotic associated with each unique chaotic signature and signature associated with the given product . the anchor points in a database ; and 22. The computing device of claim 21 , wherein the perform an authentication phase that includes : computing device is adapted to process the image of the receive , from a remote computing device , an authen- 45 given product using a set of rules used to find the unique ticity request for a given product , the authenticity chaotic signature in the images stored in the database ; and request comprising an image of the given product ; estimate a potential location of the unique chaotic signature search within the image of the given product for the of the given product , the potential location representing the anchor points and unique chaotic signatures within ROI . the relative positions of the anchor points ; and 23. The computing device of claim 21 , wherein the ROI determine that the given product is authentic if the is identified based on statistical information relating to unique chaotic signature found in the image of the locations of the unique chaotic signatures in a given line of given product matches one of the unique chaotic products . signatures stored in the database . 24. The computing device of claim 19 , wherein the 19. A computing device having access to a memory 55 computing device is adapted to : having recorded thereon computer readable code for deter- classify the images of the authentic products into one or mining the authenticity of products , the code when executed more product - lines ; by a processor of the computing device causes the comput- detect a product - line associated with the given product ing device to : shown in the image of the given product ; and perform a registration phase that includes : search , within the image of the given product , for the receive and process images of authentic products unique chaotic signature associated with authentic including detecting , for each authentic product , products pertaining only to the detected product - line . anchor points present in one or more of the images 25. The computing device of claim 19 , wherein the of every one of the authentic products , the anchor computing device is adapted to search for the unique chaotic points existing at common geographical locations 65 signature of the given product in a QPR matrix defining an between the images of the authentic products in a aperture providing visual access to the unique chaotic sig given category of the authentic products , and a nature of the given product . 50 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 32}), Document(page_content='30 US 10,956,732 B2 \\n 29 \\n 26. The computing device of claim 19 , wherein detecting the unique chaotic signature of each authentic product comprises performing a progressive targeting of at least one of the images to find an area of interest where a frequency dispersion is optimal . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 33})], [Document(page_content=\"US010504073B2 \\n United States Patent \\n Atsmon et al . ( 10 ) Patent No .: US 10,504,073 B2 \\n ( 45 ) Date of Patent : Dec. 10 , 2019 \\n ( 56 ) References Cited ( 54 ) SYSTEM AND PROCESS FOR AUTOMATICALLY ANALYZING CURRENCY \\n OBJECTS U.S. PATENT DOCUMENTS \\n ( 76 ) Inventors : Alon Atsmon , Qiryat Ono ( IL ) ; Dan Atsmon , Rehovot ( IL ) 7,522,768 B2 7,526,117 B2 2004/02 10529 Al 2005/0156942 A1 \\n 2007/0255662 Al \\n 2008/0046410 A1 \\n 2008/0219543 A1 * 4/2009 Bhatti et al . 4/2009 Foth \\n 10/2004 Wu \\n 7/2005 Jones \\n 11/2007 Tumminaro \\n 2/2008 Lieb \\n 9/2008 Csulits ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 0 days . G06K 9/033 \\n 382/135 \\n 2009/0252371 Al ( 21 ) Appl . No .: 13 / 353,791 10/2009 Rao \\n ( Continued ) \\n ( 22 ) Filed : Jan. 19 , 2012 FOREIGN PATENT DOCUMENTS \\n ( 65 ) Prior Publication Data WO \\n US 2012/0185393 A1 WO Jul . 19 , 2012 WO 2004/027695 4/2004 \\n WO / 2006 / 022513 3/2006 \\n ( Continued ) \\n Related U.S. Application Data \\n ( 60 ) Provisional application No. 61 / 438,993 , filed on Feb. 3 , 2011 , provisional application No. 61 / 433,995 , filed on Jan. 19 , 2011 , provisional application No. \\n 61 / 548,267 , filed on Oct. 18 , 2011 . Primary Examiner \\n Assistant Examiner - Jason Borlinghaus \\n Ambreen A. Alladin \\n ( 57 ) ABSTRACT \\n ( 51 ) Int . Cl . G06Q 20/10 ( 2012.01 ) G06Q 20/32 ( 2012.01 ) GO6Q 20/38 ( 2012.01 ) G06Q 20/40 ( 2012.01 ) \\n G06K 9/00 ( 2006.01 ) GO7D 7700 ( 2016.01 ) GO7D 11/30 ( 2019.01 ) ( 52 ) U.S. CI . CPC G06Q 20/10 ( 2013.01 ) ; G06K 9/00 ( 2013.01 ) ; G06Q 20/3276 ( 2013.01 ) ; G06Q 20/389 ( 2013.01 ) ; G06Q 20/40 ( 2013.01 ) ; GO7D 7700 ( 2013.01 ) ; GO7D 11/30 ( 2019.01 ) ( 58 ) Field of Classification Search USPC 705/44 See application file for complete search history . A method , system , and computer program product for ana lyzing images of visual objects , such as currency and / or payment cards , captured on a mobile device . The analysis allows determining the authenticity and / or total amount of value of the currency and / or payment cards . The system may be used to verify the authenticity of hard currency , to count the total amount of the currency captured in one or more images , and to convert the currency using real time mon etary exchange rates . The mobile device may be used to verify the identity of a credit card user by analyzing one or more images of the card holder's face and / or card holder's signature , card holder's name on the card , card number , and / or card security code . \\n 23 Claims , 4 Drawing Sheets \\n Currency 05 \\n Cortina \\n Visus : 3G \\n Analyze captured image \\n Serious 6 \\n Serve ) 206 \\n Server Oraivis \\n Capro \\n axitar ? March found \\n N 216 \\n END Coxnvort to wesot \\n Cur761107 210 \\n Gwerte woon \\n and aos\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 0}), Document(page_content='US 10,504,073 B2 \\n Page 2 \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 2010/0008535 A1 * 1/2010 Abulafia G06K 9/2054 \\n 382/100 2010/0082470 A1 * 4/2010 Walach G06Q 20/0425 705/35 \\n 2010/0113091 A1 * 5/2010 Sharma G06K 9/4642 \\n 455 / 556.1 2010/0331043 A1 * 12/2010 Chapman G01C 21/20 \\n 455 / 556.1 2011/0091092 A1 * 4/2011 Nepomniachtchi .. G06K 9/3275 \\n 382/139 \\n 2011/0099107 Al 4/2011 Saxena 2011/0119141 A1 5/2011 Hoyos \\n 2013/0022264 A1 1/2013 Atsmon et al . \\n FOREIGN PATENT DOCUMENTS \\n WO \\n WO \\n WO \\n WO \\n WO WO / 2008 / 126937 \\n WO / 2008 / 147896 \\n WO 2009/137830 WO / 2011 / 032263 \\n WO / 2011 / 047034 10/2008 \\n 12/2008 \\n 11/2009 \\n 3/2011 \\n 4/2011 \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 1}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 1 of 4 US 10,504,073 B2 \\n 128 \\n 132 \\n Internet \\n www \\n 2003 \\n Currency inventory \\n Note Comments \\n 14.00 USD \\n 100 USD \\n 50 Pilasters 0.10 USD \\n 114.10 USD \\n Figure 1', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 2}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 2 of 4 US 10,504,073 B2 \\n Currency DB \\n Analyze captured \\n Server Analysis \\n Capture \\n Convert to preset currency \\n Generate report \\n Display report \\n Fiqure 2', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 3}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 3 of 4 US 10,504,073 B2 \\n Bank temporary deposit \\n Verify SN \\n Deposit \\n Figure 3', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 4}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 4 of 4 US 10,504,073 B2 \\n Capture 402 \\n Symbol analysis \\n Charge Card \\n Show Charge \\n Figure 4', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 5}), Document(page_content='US 10,504,073 B2 \\n 2 \\n SYSTEM AND PROCESS FOR may further comprise converting the currency in each image AUTOMATICALLY ANALYZING CURRENCY to another monetary currency based on real time world \\n OBJECTS exchange rates . The conversion is preset by the terminal user , the mobile device provider , and / or automatically by the \\n CROSS REFERENCE TO RELATED 5 device based upon location based analysis . \\n APPLICATION The method comprises : capturing a visual object image on a terminal , wherein each image is associated with a particu The present application claims priority benefit under 35 lar object of known authenticity ; conducting a content U.S.C. $ 119 ( e ) to U.S. Provisional Patent Application No. analysis on the captured image ; determining the quantity and 61 / 433,995 filed Jan. 19 , 2011 by Alon Atsmon , entitled 10 authenticity of the captured object based on the content \" System and Process for Automatically Analyzing Currency match , wherein a match exists if the content analysis is Objects ” , No. 61 / 438,993 filed Feb. 3 , 2011 by Alon Ats above a designated threshold for authenticity ; and transmit mon , entitled “ System and Process for Automatically Ana ting an electronic report to the terminal indicating the lyzing Currency Objects ” , and No. 61 / 548,267 filed Oct. 18 , content match . 2011 by Alon Atsmon , entitled “ Automatic Method and 15 Content analysis is conducted using keypoint descriptors System for Visual Analysis of Object Against Preset ” . The as defined herein , and further comprises comparing the present application incorporates the foregoing disclosures image\\'s captured text , visual and symbol data , and option \\n herein by reference . ally other data such as GPS data , the history of the sender and a database of known fake visual objects . Additionally , \\n BACKGROUND 20 the digital images are captured with an electronic commu nications device ( i.e. terminal ) ; to include using terminals 1. Technical Field with a predefined array of cameras to construct a three The present invention relates to systems and processes for dimensional ( 3D ) representation of the object . automatically analyzing currency objects . In a preferred embodiment of the present invention , a user', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content=\"2. Discussion of the Related Art 25 makes a pending deposit to their financial institution by The prior art discloses methods for utilizing mobile transmitting an image of cash or checks captured on their communications devices to process a credit card or debit terminal to their institution account . They subsequently visit card payment . For example , Square , Inc. offers credit card the institution to make the actual deposit , wherein the teller readers that are connected to a mobile device . ( See WIPO will validate the amount and authenticity of the deposit Patent Application Wo / 2011 / 047034 ) . The card reader is 30 before converting the pending deposit to a fully credited configured to read data encoded in a magnetic strip of a deposit credit card and provide a signal that corresponds to the data In another preferred embodiment of the present invention , read to the mobile device , which then decodes the incoming the authenticity of a card , such as a credit or debit card , is signal from the card reader and acts as a point - of - sale device determined by capturing an image of the card on a User's to complete the financial transaction . 35 terminal . An analysis of the card validity is based upon an The prior art also discloses the use of mobile devices to image comprising the name on the card , an embossed credit verify the identity of customers . For example , Siccolla , Inc. card number , expiration date and CVV number and signature offers a mobile device with an identity verification tool on the card . And in addition to capturing an image of the built - in ( See United States Patent Application card , the User's electronic communications device can cap 20110119141 ) . The wireless phone has a specialized built - in 40 ture an image of a card holder's signature executed on the fingerprint sensor , camera ( s ) , and blood sensor to acquire device's screen and a card holder's face and compare them images of biometrics to perform identity verification in order to comparable images stored in the system database . to prevent identity theft and financial fraud during commer Other aspects of the invention may include a system cial transactions . arranged to execute the aforementioned methods and a Both of these products require hardware modification of 45 computer readable program to include a mobile application the mobile device . The prior art also fails to provide a configured to execute the aforementioned methods . These , computer program product and system for use with a mobile additional , and / or other aspects and / or advantages of the device that does not require a hardware modification of the embodiments of the present invention are set forth in the device in order to : 1 ) verify the user of a payment card for detailed description which follows ; possibly inferable from the purpose of preventing identify theft , and 2 ) verify that 50 the detailed description ; and / or learnable by practice of the cash payments are not used with counterfeit currency . Nor embodiments of the present invention . does the prior art provide a mobile device that combines fraud prevention using instantaneous imaging processing of BRIEF DESCRIPTION OF THE DRAWINGS\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content=\"hard currency captured on a device's camera or web - cam , with the ability to count the amount of the currency in an 55 The present invention will now be described in the image , and to convert the amount to another currency using following detailed description of exemplary embodiments of monetary exchange rates in real time . the invention and with reference to the attached drawings , in which dimensions of components and features shown are BRIEF SUMMARY chosen for convenience and clarity of presentation and are 60 not necessarily shown to scale . Generally , only structures , The present invention provides a method , system , and elements or parts that are germane to the discussion are computer program product for analyzing images of an object shown in the figure . ( money , credit cards , etc. ) captured on an electronic com FIG . 1 is a scheme describing the system and process in munications device ( terminal ) , such as a mobile phone accordance with an exemplary embodiment of the invention . camera or laptop web - cam , to quantify their face value , and 65 FIG . 2 is a flowchart of acts performed in capturing and optionally to determine if they are authenticate — not coun matching a visual object , in accordance with an exemplary terfeit or stolen cards . The method of the present invention embodiment of the invention .\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content='15 \\n 30 US 10,504,073 B2 \\n 3 4 \\n FIG . 3 is a flowchart of acts performed in accordance with each descriptor contains an array of 4 histograms around the an exemplary embodiment of the invention to make a band keypoint . This leads to a SIFT feature vector with ( 4x4x deposit . 8 = 128 elements ) . FIG . 4 is a flowchart of acts performed in accordance with The term “ Visual content item ” as used herein in this an exemplary embodiment of the invention to charge a credit 5 application , is defined as an object with visual characteris \\n card . tics such as an image file like BMP , JPG , JPEG , GIF , TIFF , and PNG files ; a screenshot ; a video file like AVI , MPG , DETAILED DESCRIPTION MPEG , MOV , WMV , and FLV files or a one or more frame \\n of a video . Provided herein is a detailed description of this invention . 10 The term “ visual object \" as used herein in this application , It is to be understood , however , that this invention may be is defined as a content that includes visual information such embodied in various forms , and that the suggested ( or as visual content item , images , photos , videos , IR image , proposed ) embodiments are only possible implementations magnified image , an image sequence or TV broadcast . ( or examples for a feasible embodiments , or materializa The term \" currency object ” as used herein in this appli tions ) of this invention . Therefore , specific details disclosed cation , is defined as a physical object having monetary value herein are not to be interpreted as limiting , but rather as a such as paper money , coin , medal , share certificate and basis and / or principle for the claims , and / or as a represen bonds . \\n tative basis for teaching one skilled in the art to employ this The term \" camera ” as used herein in this application is invention in virtually any appropriately detailed system , 20 defined as means of capturing a visual object .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content='structure or manner . The term “ terminal ” as used herein in this application is defined as an apparatus adapted to show visual content such Glossary of Terms as a computer , a laptop computer , mobile phone TV . The term “ visual similarity ” as used herein in this appli To facilitate understanding the present invention , the 25 cation , is defined as the measure of resemblances between following glossary of terms is provided . It is to be noted that two visual objects that can be comprised of : terms used in the specification but not included in this The fit between their color distributions such as the glossary are considered as defined according the normal correlation between their HSV color histograms usage of the computer science art , or alternatively according The fit between their texture to normal dictionary usage . The fit between their shapes The term “ image ” as used herein in this application is The correlation between their edge histograms defined as visual representation that can be presented on two Face similarity dimensional or three dimensional surfaces . Images can be Methods that include local descriptors ( such as keypoint taken in any part of the electromagnetic spectrum such as descriptors ) and such as SIFT , ASIFT , SURF and MSR visible light , infrared , ultraviolet , X - rays , Terahertz , Micro- 35 The term “ Visual analysis ” as used herein in this appli waves , and Radio frequency waves . cation , is defined as the analysis of the characteristics of The term “ photo \" as used herein in this application is visual objects such , as visual similarity , coherence , hierar defined as image in the visible light . chical organization , concept load or density , feature extrac The term \" GPS ” as used herein in this application , is 40 tion and noise removal . defined as a system based on satellites that allows a user with The term “ Text similarity ” as used herein in this appli a receiver to determine precise coordinates for their location cation , is defined as a measure of the pair - wise similarity of on the earth\\'s surface . strings . Text similarity can score the overlaps found between The term “ GPU ” as used herein in this application , is two strings based on text matching . Identical strings will defined as an apparatus adapted to reduce the time it takes 45 have a score of 100 % while “ car ” and “ dogs ” will have close to produce images on the computer screen by incorporating to zero score . “ Nike Air max blue ” and “ Nike Air max red ” its own processor and memory , having more than 16 CPU will have a score which is between the two .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content=\"cores , such as GeForce 8800 . The term “ Regular expression ” as used herein in this The term “ Keypoint ” as used herein in this application , is application , is defined as a string that provides a concise and defined as interest points in an object . For example , in the 50 flexible means for identifying strings of text of interest , such SIFT framework , the image is convolved with Gaussian as particular characters , words , or patterns of characters . filters at different scales , and then the difference of succes The term “ Text analysis ” as used herein in this applica sive Gaussian - blurred images are taken . Keypoints are then tion , is defined as the analysis of the structural characteris taken as maxima / minima of the Difference of Gaussians . tics of text , as text similarity , coherence , hierarchical orga Such keypoint can be calculated for the original image or for 55 nization , concept load or density . Text analysis can use a transformation of the original image such as an affine regular expressions . transform of the original images . The term “ OCR ” as used herein in this application , is The term “ Keypoint descriptor ” as used herein in this defined is the process by which a computer attempts to application , is defined as a descriptor of a keypoint . For match up parts of an electronic image , with characters , such example , in the SIFT framework the feature descriptor is 60 as letters , to produce text . computed as a set of orientation histograms on neighbor The term “ Symbol analysis ” as used herein in this appli hoods . The orientation histograms are relative to the key cation , is defined as analysis of symbolic data such as : OCR , point orientation and the orientation data comes from the hand write recognition , barcode recognition , and QR code Gaussian image closest in scale to the keypoint's scale . Just recognition . like before , the contribution of each pixel is weighted by the 65 The term “ Capturing data analysis ” as used herein in this gradient magnitude , and by a Gaussian with o 1.5 times the application , is defined as the analysis of capturing data such scale of the keypoint . Histograms contain 8 bins each , and as :\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content='5 \\n 10 \\n 15 US 10,504,073 B2 \\n 5 6 \\n X - Y - Z coordinates System for Analyzing Images 3 angles FIG . 1 is a scheme describing the system 100 and process \\n Manufacturer in accordance with an exemplary embodiment of the present Model invention for use in verifying that a form of payment ( i.e. Orientation ( rotation ) top - left currency and payment card ) is valid and to count the amount \\n Software in an automated manner . \\n Date and Time Terminal 101 , such as a mobile phone with camera 102 , YCbCr Positioning centered captures a visual object , of object set 120 comprising paper Compression money bills 122 , 124 and coin 128 . \\n X - Resolution Optionally , object set 120 includes payment card 130 Y - Resolution having visual details 132 such as embossed credit card Resolution Unit number , expiration date and CVV number and card holder Exposure Time name . Optionally , the owner of 132 also signs its name on FNumber terminal 101 or upon deal confirmation . Exposure Program The Capturing can be performed in several ways : 1 ) \\n Exif Version taking a photograph ; 2 ) recording a video ; and 3 ) Continu Date and Time ( original ) ously capturing an image while local or remote processing Date and Time ( digitized ) provides real time feedback such \" currency not fake ” or “ a Components Configuration Y Cb Cr 20 problem was found ” . The continuous capturing can be done Compressed Bits per Pixel while moving the camera such as moving in the directions Exposure Bias shown in 103 . Max Aperture Value The visual object can be captured from a static camera Metering Mode Pattern placed in the marketplace or from a camera held by person Flash fired or not 25 112. Person 112 can be a crowd of people that were Focal Length incentivized to capture the currency object . The visual \\n MakerNote objects 120 comprise recognized world currencies , such as \\n FlashPix Version a Euro Note 122 and U.S. bill 124 , and coins 128. The paper Color Space currency 122 , 124 may also comprise SN 121 , which are PixelXDimension 30 unique identifiers — letters , numbers , threads and symbols Pixel Y Dimension used to authenticate the validity of a currency , and deter', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 8}), Document(page_content=\"File Source counterfeits . Interoperability Index The visual object can be processed locally using the Interoperability Version User's terminal 101 , or it can be sent to a remote server 108 , Derivates of the above such as acceleration in the X - axis 35 as described in step 206 in FIG . 2 , over a network 106 such The term “ Service location ” as used herein in this appli as the internet . cation , is defined as a physical place where objects can be Server 108 or device 101 calculates a currency inventory serviced and / or fixed such as a mobile carrier service center . report 140 that is sent over the internet or created locally . The term “ Location based analysis ” as used herein in this Report 140 shows the currency object identity found , their application , is defined as analysis of local data such as GPS 40 value in a preset conversion currency and the total value location , triangulation data , RFID data , and street address . found in the capturing session . Location data can for example identify the service location A usage case would be that a person takes a photo of 100 or even the specific part of the service location in which the euro note 122 with his mobile device 101. The photo is sent visual object was captured . to a remote server 108 that uses a currency objects database The term “ Content analysis ” as used herein in this appli- 45 to match the objects photographed . Then a currency inven cation , is defined as the combination of text analysis , visual tory report 140 is displayed on device 101 . analysis , symbol analysis , location based analysis , capturing Terminal 101 can also capture a visual object 120 com data analysis and / or analysis of other data such as numerical prising a payment card 130 having visual details 132 , such fields ( price range ) , date fields , logical fields ( Female / male ) , as embossed credit card number , expiration date and CVV arrays and structures , and analysis history . 50 number and card holder name . Optionally , in addition to The term “ Content Match ” as used herein in this appli capturing an image of the card , terminal 101 can also capture cation , is defined as a numerical value that describes the the card holder's signature executed on the terminal 101's results of the content analysis that computes the similarity screen ( graphical interface ) and optionally an image of the between one or more visual objects , or a logical value that card holder's face and submit them to the server to verify the is true in case said similarity is above a certain threshold . 55 card holder is the card owner of record . The term “ marketplace ” as used herein in this application , Mobile Application is defined as a physical place where objects can be bought The present invention further comprises a software appli such as a bank , a change point , a supermarket , a convenience cation loaded onto the User's terminal 101 ( e.g. a mobile store and a grocery store . communications device , such as a smartphone ) configured The term “ Bank ” as used herein in this application , is 60 to communicate with the system server 108 , such as over a defined as a financial institution that accepts deposits . wireless communications network . The application may be The term “ Payment Card ” as used herein in this applica native or web based . The User's device enables the User to tion , is defined as a Card used to make payments such as a instantly transmit an image of the visual object 120 to the debit card , a credit card or a loyalty card . system server 108 , and to receive notifications from the The term “ SN ” as used herein in this application , is 65 system server 108 with the report of the image analysis . The defined as a collection of letters , numbers and symbols terminal 101 of the present invention may further comprise printed on a currency object in order to identify it uniquely . image capture and processing modules that enable the User\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 8}), Document(page_content='US 10,504,073 B2 \\n 7 8 \\n to locally analyze the image and produce a report without In case no match is found in step 208 , a check is done having to electronically communicate with the system server whether another capturing 214 should be performed . The', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"108 . check can be with the User of terminal 101 using his / her Method of Capturing and Matching a Visual Object device's camera , or check against a timer which allows FIG . 2 is a flowchart of acts performed in capturing and 5 taking images for up to a predefined time . In case check matching a visual object , in accordance with an exemplary results are positive , then step 202 is performed again , if not embodiment of the invention . The flowchart describes a then the process ends 216 . process and system 200 to capture and match visual objects . System and Method of Validating Cash Deposits to Banks A Currency object database 201 is loaded , which includes FIG . 3 is a flowchart of acts performed in capturing and photos of a plurality of currency objects from one or more 10 matching a visual object , in accordance with an exemplary sides , such as both sides of hard currency ( i.e. paper cur embodiment of the invention for validating the authenticity rency and coins ) . The image of a visual object 120 is then of cash deposits to banks , either to a bank teller or to an ATM captured in step 202 with the terminal device 101. Option machine . System 300 performs the process described here ally , the object is captured by two or more cameras , thus inafter : constructing a three dimensional ( 3D ) representation of the 15 The Currency objects 120 are authenticated in step 302 object . using methods such as checking their SN comprising : expos Captured object image as mentioned in step 202 is option ing them to an adequate lighting source such as sunlight in ally analyzed locally in step 204 , as further described in step order to enable good capturing of a paper money watermark ; 207 , to get a match using content analysis or to reduce the using an Ultraviolet or Infrared light in order to read UV / IR size of the data to be sent to local or remote servers in step 20 signs ; and using a magnetic sensor in order to conduct a 206 . magnetic reading of the currency object . Optionally the image itself or a processed part of it is sent The total sum of the currency object is deposited in step in step 206 to a remote server 108 or locally processed on a 306 in a bank account temporarily before the currency server at device 101. The server performs server content objects are physically handed to the bank . The bank can analysis 207 to generate a report 140. Such analysis option- 25 credit the account holder in all or a part of the sum ( for ally uses the visual object size estimation of the coin and / or example 99 % as past cases indicate 1 % of the currency bank note based on its 3D representation , and optionally object are fake or will not be physically deposited ) . other data such as GPS data , the history of the sender and a The currency objects are later deposited in step 308 database of known fake visual objects . Object's actual physically in the bank . Optionally the SN’s of the visual volume ( e.g. size , area ) is estimated by performing an 30 object are compared in step 310 to those received in step 306 interline calculation between two registered images and using the SN's in the original visual object so as to verify the using a known distance between camera lens in camera original deposit . Subsequent to that , part or all of the array . The estimated volume is used to aid in the database temporary deposit turns into a permanent deposit 312 . currency search process , and to validate object's authentic FIG . 4 is a flowchart of acts performed in capturing and ity . 35 matching a visual object , in accordance with an exemplary In case predefined criteria are met , such as a match to embodiment of the invention . System 400 performs the predefined database currency object is found 208 , then step process described hereinafter : 210 is performed to convert the currency object to a preset The visual object of payment card 130 is captured in step currency . Preset currency can be set manually by the user , by 402\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"hereinafter : 210 is performed to convert the currency object to a preset The visual object of payment card 130 is captured in step currency . Preset currency can be set manually by the user , by 402 using an electronic communications device 101 with its carrier , or by an operation location based analysis on 40 camera 102 and / or video capacity . Symbol analysis is then capturing location data such as geo - tagging . A report such as performed in step 404 on the visual object to produce a text 140 is then generated 211 and displayed 212 using an such as the visual details of the credit card 132. The text is electronics communication device 101 , such as a smart used to charge the payment card in step 406 with an amount phone . Optionally commercial ads are displayed 212 on 408 that is entered into the electronics communications device 101 , such as ads for a nearby currency exchange 45 device 101. The transaction can further be authenticated place or a bank . using an image of the card owner's signature , his face , or Optionally steps 204 and / or 207 further comprise recog letting him type his PIN number on device 101. Optionally nition of the SN 121 , such as OCR and further measures a charge report 410 is displayed on device 101 . against fake SN’s or fake SN / currency value combinations . Optionally execution is passed to the step 301 further 50 EXEMPLIFICATIONS\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"described in FIG . 3 . In a particular exemplification , report 140 provides the Example 1 — Merchant Receiving Payment in Paper amount of the currency in the original image , the amount of Money of a Foreign Currency the currency after being converted to the designated type , and whether the currency is authenticate . The system can 55 An exemplification of the present invention , especially also sum the amounts of currency in multiple images as that as disclosed in FIG . 1 and FIG . 2 , comprises a merchant selected by the User . The images may be sequential or who receives payment for goods or services that they have non - sequential wherein the User selects which image files to provided to a customer who is paying in paper money and analyze together in one report . See Table 140 in FIG . 1 for coins . an example of a report 212 generated for three images , 60 The merchant can capture an image of the currency on wherein each image is of a different type of currency that are their electronic communications device at the time of pay all converted to the same type of currency ( i.e. US ) . The ment . For example , a store cashier with a computer such as report also provides a “ Comments ” section that details the a laptop , netbook , etc. , or a waitress in a restaurant with a authenticity of the currency in each image . Lastly , the report mobile communications device can capture an image of the will provide a sum of the total amount of all the currencies 65 customer's currency . After the merchant captures the image , successfully analyzed in the different images , wherein the the system will indicate if the image is a match with images sum is in the converted currency . of major world currencies stored in the system database . If\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content='10 US 10,504,073 B2 \\n 9 10 \\n the image is not a match , it is because the image may not be restaurant with a mobile communications device can capture clear enough to make the match , and / or it may indicate the an image of the customer\\'s card , front and back , wherein the currency is a counterfeit . The merchant can elect to re - verify image comprises the card holder\\'s name , the card number , by capturing the image again , and repeating the match . The the signature , and the three digit security code CSC ( also merchant may then instantly convert it to another currency 5 known as a CCID or Credit Card ID or Card Verification if required , and / or sum the total amount of payment by the Value ( CVV or CVV2 ) ) . The merchant can also take an customer . The match and currency conversion analysis is image of the customer\\'s face using his electronic device , accomplished instantaneously through either the software wherein he transmits the images to a local or remote server installed on the merchant\\'s terminal ( i.e. electronic device ) for comparison and analysis to stored images of the card and / or by the merchant transmitting the images via an holder\\'s signature and / or photograph . The merchant will Internet connection to a local or remote system server and then receive a report on his electronic communications then electronically receiving a report of the analysis . The device verifying that the customer is the true owner of the merchant then views the report of the analysis on their card , thus enabling him to process the payment . electronic communications device and processes the cus tomer\\'s payment in accordance with the report . For 15 Computer Program As will be appreciated by one skilled in the art , aspects of example , the merchant may decline the payment and request additional payment if the currency is found to be counterfeit . the present invention may be embodied as a system , method \\n Or the merchant may request additional payment if the or computer program product . Accordingly , aspects of the \\n current exchange rate indicates that the amount of payment present invention may take the form of an entirely hardware \\n is not enough . 20 embodiment , an entirely software embodiment ( including firmware , resident software , micro - code , etc. ) or an embodi Example 2 – Making a Cash Bank Deposit ment combining software and hardware aspects that may all \\n generally be referred to herein as a \" circuit , \" \" module ” or The present invention may also be used in making a cash \" system . ” Furthermore , aspects of the present invention may deposit to a financial institution as illustrated in FIG . 3 , 25 take the form of a computer program product embodied in wherein the bank customer temporarily credits their account one or more computer readable medium ( s ) having computer ( i.e. pending ) by electronically transmitting an image of the readable program code embodied thereon . deposit before visiting the bank and making the actual Any combination of one or more computer readable deposit . The customer would capture an image of the bills medium ( s ) may be utilized . The computer readable medium s / he is depositing using their mobile device camera . The 30 may be a computer readable signal medium or a computer mobile device would have software , or access to an Internet readable storage medium . A computer readable storage connection to the system server for completing the image medium may be , for example , but not limited to , an elec capture , analysis , and reporting of the present invention . tronic , magnetic , optical , electromagnetic , infrared , or semi They would then log into their bank account via the Internet conductor system , apparatus , or device , or any suitable and be authenticated by the bank\\'s system ( 302 ) ; and upload 35 the image of their deposit into their online account . The combination of the foregoing . More specific examples ( a', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 10}), Document(page_content=\"institution will indicate in its electronic records that a non - exhaustive list ) of the computer readable storage \\n temporary deposit 306 ( i.e. pending credit ) has been made at medium would include the following : an electrical connec \\n the time that the image is received . The bank customer will tion having one or more wires , a portable computer diskette , subsequently visit the bank within a set time period as 40 a hard disk , a random access memory ( RAM ) , a read - only determined by bank rules for time limitations to process memory ( ROM ) , an erasable programmable read - only \\n pending deposits . When the customer makes the actual memory ( EPROM or Flash memory ) , an optical fiber , a deposit at the bank , the institution will then indicate that an portable compact disc read - only memory ( CD - ROM ) , an actual deposit 308 was made . Additionally , the bank may optical storage device , a magnetic storage device , or any utilize the software / system of the present invention to deter- 45 suitable combination of the foregoing . In the context of this mine if the actual deposit 308 is counterfeit . If the authen document , a computer readable storage medium may be any ticity of the hard currency is verified by analyzing the tangible medium that can contain , or store a program for use captured image of the currency 310 or by other means by or in connection with an instruction execution system , known in the banking industry ( i.e. teller physically check apparatus , or device . ing ) , then the deposit is designated as permanent in the 50 Program code embodied on a computer readable medium customer's account 312. If the currency is found not to be may be transmitted using any appropriate medium , includ authentic or the customer does not make the actual physical ing but not limited to wireless , wire line , optical fiber cable , deposit at the bank ( 308 ) , then the customer is notified of RF , etc. , or any suitable combination of the foregoing . such , and the pending state of the deposit is dropped from Computer program code for carrying out operations for the customer's account so that no credit is given for the 55 aspects of the present invention may be written in any deposit . combination of one or more programming languages , including an object oriented programming language such as Example 3 — Merchant Processing a Payment Card Java , Smalltalk , C ++ or the like and conventional procedural programming languages , such as the “ C ” programming A merchant receives payment for goods or services that 60 language or similar programming languages . The program they have provided to a customer who is paying using a code may execute entirely on the user's computer , partly on credit or a debit or a loyalty card . The merchant can capture the user's computer , as a stand - alone software package , an image of the card on their electronic communications partly on the user's computer and partly on a remote device at the time of payment and extra the text ( i.e. card computer or entirely on the remote computer or server . In the number ) for electronically submitting a charge to the card 65 latter scenario , the remote computer may be connected to the from the mobile device . For example , a store cashier with a user's computer through any type of network , including a computer such as a laptop , netbook , etc. , or a waitress in a local area network ( LAN ) or a wide area network ( WAN ) , or\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 10}), Document(page_content='10 US 10,504,073 B2 \\n 11 12 \\n the connection may be made to an external computer ( for Reference in the specification to “ some embodiments ” , example , through the Internet using an Internet Service \" an embodiment ” , “ one embodiment ” or “ other embodi \\n Provider ) . ments ” means that a particular feature , structure , or charac Aspects of the present invention are described above with teristic described in connection with the embodiments is reference to flowchart illustrations and / or block diagrams of 5 included in at least some embodiments , but not necessarily methods , apparatus ( systems ) and computer program prod all embodiments , of the inventions . ucts according to embodiments of the invention . It will be It is to be understood that the phraseology and terminol understood that each block of the flowchart illustrations ogy employed herein is not to be construed as limiting and and / or block diagrams , and combinations of blocks in the are for descriptive purpose only . \\n flowchart illustrations and / or block diagrams , can be imple It is to be understood that the details set forth herein do not mented by computer program instructions . These computer construe a limitation to an application of the invention . program instructions may be provided to a processor of a Furthermore , it is to be understood that the invention can general purpose computer , special purpose computer , or be carried out or practiced in various ways and that the other programmable data processing apparatus to produce a 15 invention can be implemented in embodiments other than machine , such that the instructions , which execute via the the ones outlined in the description above . processor of the computer or other programmable data It is to be understood that the terms “ including ” , “ com processing apparatus , create means for implementing the prising ” , “ consisting ” and grammatical variants thereof do functions / acts specified in the flowchart and / or block dia not preclude the addition of one or more components , gram block or blocks . 20 features , steps , or integers or groups thereof and that the These computer program instructions may also be stored terms are to be construed as specifying components , fea in a computer readable medium that can direct a computer , tures , steps or integers . other programmable data processing apparatus , or other If the specification or claims refer to “ an additional ” devices to function in particular manner , such that the element , that does not preclude there being more than one of instructions stored in the computer readable medium pro- 25 the additional element . duce an article of manufacture including instructions which It is to be understood that where the claims or specifica implement the function / act specified in the flowchart and / or tion refer to \" a \" or \" an \" element , such reference is not be block diagram block or blocks . construed that there is only one of that element . The computer program instructions may also be loaded It is to be understood that where the specification states onto a computer , other programmable data processing appa- 30 that a component , feature , structure , or characteristic “ may ” , ratus , or other devices to cause a series of operational steps \" might ” , “ can ” or “ could ” be included , that particular com to be performed on the computer , other programmable apparatus or other devices to produce a computer imple ponent , feature , structure , or characteristic is not required to \\n be included . mented process such that the instructions which execute on the computer or other programmable apparatus provide 35 Where applicable , although state diagrams , flow diagrams \\n processes for implementing the functions / acts specified in or both may be used to describe embodiments , the invention \\n the flowchart and / or block diagram block or blocks . is not limited to those diagrams or to the corresponding', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 11}), Document(page_content='the flowchart and / or block diagram block or blocks . is not limited to those diagrams or to the corresponding \\n The aforementioned flowchart and diagrams illustrate the descriptions . For example , flow need not move through each \\n architecture , functionality , and operation of possible imple illustrated box or state , or in exactly the same order as mentations of systems , methods and computer program 40 illustrated and described . products according to various embodiments of the present Methods of the present invention may be implemented by invention . In this regard , each block in the flowchart or block performing or completing manually , automatically , or a diagrams may represent a module , segment , or portion of combination thereof , selected steps or tasks . code , which comprises one or more executable instructions The descriptions , examples , methods and materials pre for implementing the specified logical function ( s ) . It should 45 sented in the claims and the specification are not to be also be noted that , in some alternative implementations , the construed as limiting but rather as illustrative only . functions noted in the block may occur out of the order noted Meanings of technical and scientific terms used herein are in the figures . For example , two blocks shown in succession to be commonly understood as by one of ordinary skill in the may , in fact , be executed substantially concurrently , or the art to which the invention belongs , unless otherwise defined . blocks may sometimes be executed in the reverse order , 50 The present invention may be implemented in the testing depending upon the functionality involved . It will also be or practice with methods and materials equivalent or similar noted that each block of the block diagrams and / or flowchart to those described herein . illustration , and combinations of blocks in the block dia Any publications , including patents , patent applications grams and / or flowchart illustration , can be implemented by and articles , referenced or mentioned in this specification are special purpose hardware - based systems that perform the 55 herein incorporated in their entirety into the specification , to specified functions or acts , or combinations of special pur the same extent as if each individual publication was spe pose hardware and computer instructions . cifically and individually indicated to be incorporated In the above description , an embodiment is an example or herein . In addition , citation or identification of any reference implementation of the inventions . The various appearances in the description of some embodiments of the invention of “ one embodiment , \" \" an embodiment ” or “ some embodi- 60 shall not be construed as an admission that such reference is ments ” do not necessarily all refer to the same embodiments . available as prior art to the present invention . Although various features of the invention may be While the invention has been described with respect to a described in the context of a single embodiment , the features limited number of embodiments , these should not be con may also be provided separately or in any suitable combi strued as limitations on the scope of the invention , but rather nation . Conversely , although the invention may be described 65 as exemplifications of some of the preferred embodiments . herein in the context of separate embodiments for clarity , the Other possible variations , modifications , and applications invention may also be implemented in a single embodiment . are also within the scope of the invention . Accordingly , the', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 11}), Document(page_content='10 US 10,504,073 B2 \\n 13 14 \\n scope of the invention should not be limited by what has thus 5. The system of claim 1 , wherein said content analysis far been described , but by the appended claims and their comprises performing an authentication of each currency legal equivalents . object of said plurality of types of currency objects . 6. A computer implemented method for utilizing a mobile What claimed is : 5 device to analyze the authenticity and monetary value of 1. A system for conducting a content analysis of an image currency deposited into a bank account , the method per depicting a plurality of currency objects , the system com formed by a server in network communication with the prising a server in network communication with a mobile mobile device , comprising : device , the server comprising : a ) receiving by the server over a network , a single image a hardware processor ; captured by at least one camera of the mobile device , a non - transitory computer readable medium having the single image depicting at least one currency object embodied thereon code instructions that in response to of each of a plurality of types of currency objects execution by a hardware processor of the server , cause denoting respective denominations of major world cur \\n the server to : rencies , and location data indicative of location of the receive over a network , a single image captured by at least 15 mobile device computed by a location based analysis of one camera of the mobile device , the single image output of a location data sensor ; depicting at least one currency object of each of a b ) conducting by the server a content analysis of said plurality of types of currency objects denoting respec single image , wherein said analysis comprises : tive denominations of major world currencies , and identifying each of the plurality of types of currency location data indicative of location of the mobile device 20 objects , computed by a location based analysis of output of a computing a plurality of keypoint descriptors as a set of', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content=\"location data sensor ; orientation histograms on neighboring pixels of said conduct a content analysis of the single image , wherein plurality of types of currency objects ; said content analysis comprises : convoluting the captured image with Gaussian filters at identifying each of the plurality of types of currency 25 different scales to create a plurality of successive objects , Gaussian - blurred images ; computing a plurality of keypoint descriptors as a set of wherein the orientation histograms are relative to the orientation histograms on neighboring pixels of said orientation of the plurality of keypoints , wherein the plurality of types of currency objects ; orientation data for the orientation histograms is convoluting the captured image with Gaussian filters at 30 derived from the Gaussian image closest in scale to different scales to create a plurality of successive the scale of each respective keypoint of the plurality Gaussian - blurred images ; of keypoints ; wherein the orientation histograms are relative to the computing a quantity of each currency object of the orientation of the plurality of keypoints , wherein the plurality of types of currency objects ; orientation data for the orientation histograms is 35 c ) converting each of the plurality of identified types of derived from the Gaussian image closest in scale to currency objects to a common monetary currency in the scale of each respective keypoint of the plurality use at the location of the mobile device ; of keypoints ; d ) calculating a total monetary value of the plurality of computing a quantity of each currency object of the types of currency objects when converted to the com plurality of types of currency objects ; mon monetary currency ; convert each of the plurality of identified types of cur e ) temporarily depositing the total monetary value into a rency objects to a common monetary currency in use at user's bank account according to the common mon the location of the mobile device ; etary currency ; and create a currency inventory report comprising a total f ) authenticating the plurality of currency objects after monetary value of the plurality of types of currency 45 receiving an indication of a physical deposit of the objects when converted to the common monetary cur plurality of currency objects in the user's bank to convert the temporarily deposited monetary value to a transmit from the server over the network to said mobile permanent deposit . device , the currency inventory report for presentation 7. The method of claim 6 , wherein said verifying is on a display of said mobile device . 50 performed by utilizing light to check for watermarks and UV 2. The system of claim 1 , wherein said content analysis or signs . further comprises an analysis of a member selected from a 8. The method of claim 6 , wherein said content analysis group consisting of the image's captured text , visual and further comprises an analysis of a member selected from a symbol data from said captured image , a history of images group consisting of the image's captured text , currency and captured using said mobile device and at least one currency 55 symbol data from said captured image , a history of images object from a database of known fake currency objects . captured using said mobile device and at least one currency 3. The system of claim 1 , wherein said content analysis object from a database of known fake currency objects . further comprises : 9. The method of claim 6 , wherein said content analysis calculating a face value of the plurality of types of further comprises calculating the value of the plurality of currency objects , and 60 types of currency objects in said image by summing the calculating a total value by summing the value of each of value of each of said plurality of types of currency objects said plurality of types of currency objects . in said image . 4. The system of claim 1 , wherein said at least one camera 10. The method of claim 6\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='each of said plurality of types of currency objects said plurality of types of currency objects . in said image . 4. The system of claim 1 , wherein said at least one camera 10. The method of claim 6 , wherein said at least one comprises an array of cameras of said mobile device and camera comprises an array of cameras of said mobile device wherein said capturing is conducted to construct a three 65 and wherein said capturing is conducted to construct a three dimensional ( 3D ) representation of said plurality of types of dimensional ( 3D ) representation of said plurality of types of currency objects . currency objects . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='rency ; and', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='5 \\n 15 \\n 20 US 10,504,073 B2 \\n 15 16 \\n 11. A non - transitory computer readable medium having 17. An internet based system for quantifying and analyz embodied thereon code instructions that , when executed by ing the authenticity of a plurality of types of currency one or more hardware processors of a server in network objects , the system comprising : communication with at least one mobile device , cause the a mobile device comprising : one or more hardware processors of the server to : at least one hardware processor ; a ) receive a single image captured using at least one a non - transitory computer readable medium having camera of the mobile device , the single image com embodied thereon code instructions that in response prising at least one currency object of each of a to execution by the at least one hardware processor plurality of types of currency objects denoting respec of the mobile device , cause the mobile device to : tive denominations of major world currencies , and 10 capture a single image that images at least one location data indicative of location of the mobile device currency object of each of a plurality of types of computed by a location based analysis of output of a \\n location data sensor ; currency objects denoting respective denomina \\n b ) conduct a content analysis of the single image , wherein tions of major world currencies , using a camera or \\n said content analysis comprises : a video recorder of a respective said mobile \\n identifying each of the plurality of types of currency device , and provide location data indicative of \\n objects ; location of the mobile device computed by a \\n computing a plurality of keypoint descriptors as a set of location based analysis of output of a location data orientation histograms on neighboring pixels of said sensor ; plurality of types of currency objects ; a database storing a plurality of reference denominations convoluting the captured image with Gaussian filters at of major world currencies ; \\n different scales to create a plurality of successive a processor ; Gaussian - blurred images ; non - transitory storage coupled to the processor and stor wherein the orientation histograms are relative to the ing code that , when executed by the processor , cause orientation of the plurality of keypoints , wherein the 25 the processor to : orientation data for the orientation histograms is apply a decision function to the plurality of types of derived from the Gaussian image closest in scale to currency objects to yield an analysis of authenticity of the scale of each respective keypoint of the plurality each member of said type of currency object according of keypoints ; to a match with said plurality of denominations of computing a quantity of each currency object of the 30 major world currencies , wherein a match exists when plurality of types of currency objects ; the decision function is above a designated threshold c ) convert each of the plurality of identified types of for authenticity ; currency objects to a common monetary currency in wherein the decision function comprises : use at the location of the mobile device ; identifying each of the plurality of types of currency d ) create a currency inventory report comprising a total 35 objects , monetary value of the plurality of types of currency computing a plurality of keypoint descriptors as a set of objects when converted to the common monetary cur orientation histograms on neighboring pixels of said plurality of types of currency objects ; e ) transmit to said mobile device , the currency inventory convoluting the captured image with Gaussian filters at report for presentation on a display of said mobile 40 different scales to create a plurality of successive', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 13}), Document(page_content=\"device . Gaussian - blurred images ; 12. The non - transitory computer readable medium of wherein the orientation histograms are relative to the claim 11 , wherein said content analysis further comprises an orientation of the plurality of keypoints , wherein the analysis of a member selected from a group consisting of the orientation data for the orientation histograms is image's captured text , visual and symbol data from said 45 derived from the Gaussian image closest in scale to captured image , a history of images captured using said the scale of each respective keypoint of the plurality mobile device and at least one currency object from a of keypoints ; database of known fake currency objects . computing a quantity of each currency object of the 13. The non - transitory computer readable medium of plurality of types of currency objects ; claim 11 , wherein said conducting further comprises : convert each of the plurality of identified types of cur calculating a face value of the plurality of types of rency objects to a common monetary currency in use at currency objects , and the location of the mobile device ; calculating a total value by summing the value of each of create a currency inventory report comprising a total said plurality of types of currency objects . monetary value of the plurality of types of currency 14. The non - transitory computer readable medium of 55 objects when converted to the common monetary cur claim 11 , wherein said at least one camera comprises an array of cameras of said mobile device and wherein said generate a currency inventory report and transmit said capturing is conducted to construct a three dimensional ( 3D ) currency inventory report to said user mobile device for representation of said plurality of types of currency objects . presentation on a display of said mobile device . 15. The non - transitory computer readable medium of 60 18. The system of claim 17 , wherein said content analysis claim 11 , wherein said content analysis comprises perform further includes determining the authenticity of each cur ing an authentication of each currency object of said plu rency object of said plurality of types of currency objects rality of types of currency objects . based on a content match , wherein said match exists when 16. The non - transitory computer readable medium of the content analysis is above a designated threshold for claim 11 , wherein said content analysis further comprises 65 authenticity . comparing differences and similarities between at least one 19. The system of claim 17 wherein said plurality of types reference image object of a reference image and said image . of currency objects comprises a payment card . rency ; and \\n 50 \\n rency ; and\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 13}), Document(page_content='18 US 10,504,073 B2 \\n 17 \\n 20. The system of claim 1 , wherein said plurality of types currency objects include a plurality of types of coins , and said content analysis identifies said plurality of types of coins . 21. The system of claim 1 , wherein computing the plu- 5 rality of keypoints comprises computing the plurality of keypoints as maxima and minima of the differences of the successive Gaussian - blurred images . 22. The system of claim 1 , wherein said currency inven tory report is presented on said display of said mobile device 10 at the same time as said single image . 23. The system of claim 1 , wherein the currency inventory report in the common monetary currency is generated in real time , according to a real time location of the mobile device computed based on said output of the location data sensor 15 and based on the single image captured in real time by the \\n camera of the mobile device .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 14})], [Document(page_content=\"(12) United States Patent \\n Tadayon et al. USOO8873813B2 \\n (10) Patent No.: US 8,873,813 B2 \\n (45) Date of Patent: Oct. 28, 2014 \\n (54) APPLICATION OF Z-WEBS AND Z-FACTORS \\n TO ANALYTICS, SEARCH ENGINE, \\n LEARNING, RECOGNITION, NATURAL \\n LANGUAGE, AND OTHERUTILITIES \\n (71) Applicants: Saied Tadayon, Potomac, MD (US); \\n Bijan Tadayon, Potomac, MD (US) \\n (72) Inventors: Saied Tadayon, Potomac, MD (US); Bijan Tadayon, Potomac, MD (US) \\n (73) Assignee: Z. Advanced Computing, Inc., Potomac, \\n MD (US) \\n (*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 140 days. \\n (21) \\n (22) Appl. No.: 13/781,303 \\n Filed: Feb. 28, 2013 \\n (65) Prior Publication Data \\n US 2014/OO79297 A1 Mar. 20, 2014 \\n Related U.S. Application Data \\n Provisional application No. 61/701,789, filed on Sep. \\n 17, 2012. (60) \\n Int. C. \\n G06K 9/00 \\n G06K9/40 \\n U.S. C. \\n CPC ................................. G06K9/00288 (2013.01); \\n G06K 9/00 (2013.01) \\n USPC .............. 382/118:382/181: 382/263; 706/52 \\n Field of Classification Search \\n CPC ..... G06K9/00; G06K9/00288: G06N 7/005; \\n GO6N 7/OO \\n USPC ......... 382/115, 118,305, 224, 278, 103, 176, \\n 382/190, 195, 209, 218, 219, 282,307, 275, \\n 382/226, 227, 254, 181, 263: 340/5.81, \\n 340/5.83; 707/E17.022, E17.026, E17.023, (51) \\n (2006.01) (2006.01) \\n (52) \\n (58) \\n input module Pre-processor \\n Face recognizer \\n module 707/736, 758,999.107; 706/52; 715/825; \\n 902/3: 348/239,370 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 5,295.228 A 3, 1994 Koda et al. \\n 5,329,611 A 7, 1994 Pechanek et al. \\n (Continued) \\n OTHER PUBLICATIONS \\n Ali Sanayei, titled “Towards a complexity theory: Theoratical foun dations and practical applications'. Submitted to Satellite Meeting Unravelling and Controlling Discrete Dynamical Systems, on Jun. \\n 17, 2011. No page number, volume number, or publisher's location mentioned. (Paper dedicated to Professor Lotfi A. Zadeh, et al., by the \\n author.). \\n (Continued) \\n Primary Examiner — Sheela Chawan (74) Attorney, Agent, or Firm — Saied Tadayon; Bijan Tadayon \\n (57) ABSTRACT \\n Here, we introduce Z-webs, including Z-factors and Z-nodes, for the understanding of relationships between objects, sub jects, abstract ideas, concepts, or the like, including face, car, images, people, emotions, mood, text, natural language, Voice, music, video, locations, formulas, facts, historical data, landmarks, personalities, ownership, family, friends, love, \\n happiness, Social behavior, Voting behavior, and the like, to be used for many applications in our life, including on the search engine, analytics, Big Data processing, natural language pro cessing, economy forecasting, face recognition, dealing with reliability and certainty, medical diagnosis, pattern recogni tion, object recognition, biometrics, security analysis, risk analysis, fraud detection, satellite image analysis, machine generated data analysis, machine learning, training samples, \\n extracting data or patterns (from the video, images, and the like), editing video or images, and the like. Z-factors include reliability factor, confidence factor, expertise factor, bias fac \\n tor, and the like, which is associated with each Z-node in the \\n Z-web. \\n 20 Claims, 81 Drawing Sheets \\n 2-web \\n Output module \\n eigenface \\n module \\n eigenface Component recognition \\n module Edge \\n detector Eigenvector \\n on face components \\n module library \\n wavelet \\n module \\n Anchor points or \\n features \\n module \\n Relationships \\n between \\n features or \\n face components \\n rules or \\n database \\n Z-web \\n Z-factors Training samples \\n input module Face components library wavelet library Input \\n module for \\n libraries \\n Training \\n module for \\n librarias\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 0}), Document(page_content=\"US 8,873,813 B2 \\n Page 2 \\n (56) References Cited 8, 108,207 B1 1/2012 Harvey et al. 8, 108.324 B2 1/2012 Krupka et al. \\n U.S. PATENT DOCUMENTS 8, 116,534 B2 2/2012 Nishiyama et al. 8, 150,109 B2 4/2012 Sung et al. \\n 5,517,596 A 5, 1996 Pechanek et al. 8, 165,354 B1 4/2012 Zhao 6,157,921. A 12/2000 Barnhill 8, 199,203 B2 6/2012 Sugimoto 7,542,947 B2 6/2009 Guyon et al. 8, 199,242 B2 6/2012 Sugihara 7,664,962 B2 * 2/2010 Kuhlman ...................... T13, 186 8,199.979 B2 6/2012 Steinberg et al. 7,689,529 B2 3/2010 Fung et al. 8,204,310 B2 6, 2012 Zou et al. T.697.761 B2 4/2010 Napper 8,208,764 B2 6/2012 Guckenberger 7698.236 B2 4/2010 Coxetal. 8,209,179 B2 6/2012 Aoyama et al. 7,721,336 B1 5/2010 Adjaoute 8,213,737 B2* 7/2012 Steinberg et al. ............. 382,275 7,734.400 B2 6/2010 Gayme et al. 8,224,040 B2 7/2012 Li 7,734.451 B2 6/2010 MacArthur et al. 8.224,042 B2 7/2012 Wang 7,739,337 B1 6, 2010 Jensen 8,233,676 B2 7/2012 Ngan et al. \\n 7,742,103 B1 6/2010 He et al. 8,244,040 B2 8.2012 Imagawa 7,761,742 B2 7/2010 Di Palma et al. 8,249,313 B2 8/2012 Yanagi \\n 7,769,512 B2 8, 2010 Norris et al. 8,254,691 B2 8/2012 Kaneda et al. \\n 7,783,580 B2 8/2010 Huang et al. 8,259,168 B2 9/2012 Wu et al. \\n 7,784,295 B2 8/2010 McCormicket al. 8,260,009 B2 * 9/2012 Du et al. ....................... 382,117 \\n 7,792,746 B2 9, 2010 Del Callar et al. 8,265,399 B2 9/2012 Steinberg et al. \\n 7,792,750 B2 9/2010 Moeller 8,265,474 B2 9/2012 Kanayama \\n 7,797.268 B2 9/2010 Bigus et al. 8,275,175 B2 9/2012 Baltatu et al. 7,801,840 B2 9/2010 Repasi et al. 8,285,006 B2 10/2012 Tang 7,805.396 B2 9/2010 Wagner et al. 8,289.546 B2 10/2012 Hayasaki \\n 7,805,397 B2 9, 2010 Kurian et al. 8,295,558 B2 10/2012 Su et al. \\n 7,805,984 B2 10/2010 McLain et al. 8,300,898 B2 10/2012 Baket al. 7,817,854 B2 10/2010 Taylor 8,300,900 B2 10/2012 Lai et al. 7,832,511 B2 11/2010 Syed et al. 8,306.279 B2 11/2012 Hanna 7,836,496 B2 11/2010 Chesla et al. 8,316,436 B2 11/2012 Shirai et al. 7,840,500 B2 11/2010 Khanbaghi 8,320,682 B2 11/2012 Froeba et al. 7,844,564 B2 11/2010 Donohue et al. 8.325,999 B2 12/2012 Kapoor et al. 7,853,538 B2 12/2010 Hildebrand 8,326,001 B2 12/2012 Free 7.856.356 B2 12/2010 Chung et al. 8.330,831 B2 * 12/2012 Steinberg et al. .......... 348,231.3 \\n 7,857.976 B2 12/2010 Bissler et al. 8.331,632 B1 12/2012 Mohanty et al. 7,864,552 B2 1/2011 Heber et al. 8.332,422 B2 12/2012 Chang et al. 7,869,989 B1 1/2011 Harvey et al. 8,340,366 B2 12/2012 Masuda et al. \\n 7,895,135 B2 2/2011 Norris et al. 8,352,467 B1 1/2013 Guha. \\n 7,921,068 B2 4/2011 Guyon et al. 8,359,611 B2 1/2013 Johnson et al. \\n 7,925,874 B1 4/2011 Zaitsev 8,370,352 B2 2/2013 Lita et al. \\n 7,929,771 B2 4/2011 Koet al. 8,374,405 B2 2/2013 Lee et al. \\n 7,930,265 B2 4/2011 Akelbein et al. 8,379,074 B2 2/2013 Currivan et al. \\n 7,934,499 B2 5/2011 Berthon-Jones 8,379,920 B2 2/2013 Yang et al. 7,936,906 B2 5, 2011 Hua et al. 8,379,940 B2 2/2013 Wechsler et al. \\n 7,941,350 B2 5/2011 Ginsburg et al. 8,386.446 B1 2/2013 Pasupathy et al. 7,966,061 B2 6, 2011 Al-Abed et al. 8,503,800 B2 * 8/2013 Blonk et al. .................. 382,226 \\n 7.974.455 B2 7, 2011 Peters et al. 8,593,542 B2 * 1 1/2013 Steinberg et al. ... 348,239 \\n 7.991,754 B2 8, 2011 Maizel et al. 8,670.597 B2* 3/2014 Petrou et al. ... ... 382,116 \\n 7.999,857 B2 8, 2011 Bunn et al. 8,682,097 B2 * 3/2014 Steinberg et al. ... 382.275 \\n 8,004,544 B1 8/2011 Zhang et al. 2010/0172550 A1* 7/2010 Gilley et al. .................. 382,118 \\n 8,015, 196 B2 9/2011 Taranenko et al. 8,016,319 B2 9, 2011 Winkler et al. OTHER PUBLICATIONS \\n 8,023,974 B1 9, 2011 Diao et al. Ronald Yager, titled “On Z-valuations using Zadeh's Z-numbers'. 8,054,592 B2 11/2011 Rivers, Jr. 8,060,456 B2 11/2011 Gao et al. International Journal of Intelligent Systems, vol. 27. Issue 3, pp. \\n 8,063,889 B2 11/2011 Anderson 259-278, Mar. 2012, Wiley Periodicals, Inc. The online version first \\n 8,077.983 B2 12/2011 Qiu et al. published on Jan 20, 2012. No publisher's location mentioned.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 1}), Document(page_content=\"8,063,889 B2 11/2011 Anderson 259-278, Mar. 2012, Wiley Periodicals, Inc. The online version first \\n 8,077.983 B2 12/2011 Qiu et al. published on Jan 20, 2012. No publisher's location mentioned. \\n 8,081,844 B2 12/2011 Steinberg et al. \\n 8,095,483 B2 1/2012 Weston et al. * cited by examiner\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 1}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 1 of 81 US 8,873,813 B2 \\n FIG 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 2}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 2 of 81 US 8,873,813 B2 \\n f-mark R \\n Approximately 3 O) \\n FIG2(a) \\n e - as o ... 2 Na. v w \\n . \\n : \\n . V M. \\n \"...Y. 1 . . . . . . . . . ... sle 1. ...\\' s - s o * . . . . . . ... 2 NN. v V. \\n : \\n . V M. ... W. W ...SS-...\" \\n A B Y N- 1 FIG2(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 3}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 3 of 81 US 8,873,813 B2 \\n Bandwidth (Ab) \\n o Support \\n FIG 3', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 4}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 4 of 81 US 8,873,813 B2 \\n Probability Membership Test SCOre Models Distribution Function CalculatOr database database database \\n Zn - umber USer interface estimatOr \\n Parameters Integration for prob. Certainty \\n module Distribution E. \\n Functions 33O3S6 \\n database Sup \\n function \\n mOdule Set functions \\n module DOmains \\n database \\n GeOmetrical \\n Shape \\n analyzer 3D rendering \\n images \\n Fig. 4', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 5}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 5 of 81 US 8,873,813 B2 \\n Context \\n determination \\n module large Dissecting & Analyzing \\n p parsing module device \\n Default Context 3 Context 2 Context 1 \\n analyZer analyZer analyzer analyzer \\n Membership \\n values COrrelation \\n module \\n aggregator \\n Fig. 5', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 6}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 6 of 81 US 8,873,813 B2 \\n Natural \\n language \\n processing \\n module Ouestion/ \\n Answering \\n system Search engine \\n Analyzer/ Forecasting Rules engine processor engine \\n -> crisis COnflict Applications E. analyzing and For Analyzer resolving \\n module \\n Fig. 6', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 7}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 7 of 81 US 8,873,813 B2 \\n Intensity (or other image \\n Y parameters) mapping \\n 45 degree X \\n diagonal line \\n Within specific range, for \\n each Section, part, or region of image \\n Fig. 7', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 8}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 8 of 81 US 8,873,813 B2 \\n Dissecting & Task Classification Input Segmentation assignment module device mOdule module \\n Recognizer for type Recognizer for type Recognizer for type \\n 3 module 2 module 1 module \\n analyzer \\n Recognizer for sub-type 3.1 Membership \\n values module COrrelation \\n module \\n Recognizer for aggregator sub-type 3.2 or \\n module \\n Output \\n module \\n Recognizer for \\n Sub-Sub-type \\n 3.2.1 module \\n And type hierarchy Fig. 8 \\n COntinues', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 9}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 9 of 81 US 8,873,813 B2 \\n Input Search engine classifier \\n device \\n Sub Classifier 3 Sub Classifier 2 Sub Classifier 1 \\n Expert module \\n 1 \\n Expert module Z factors \\n 2 module Z-node \\n module \\n Analyzer and \\n Expert module aggregator \\n 3 \\n Output \\n module \\n Sub Expert \\n module 3.1 \\n And expert \\n hierarchy \\n COntinues Fig. 9', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 10}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 10 of 81 US 8,873,813 B2 \\n Cbntext 1 \\n Z-factors parameters? Z-factors for \\n asSOCiated With node 1 \\n each node \\n Z-factorS for \\n node 2 \\n t2to3 (thickness of branch), e.g. for strength of \\n Correspondence or \\n COrrelation, as \\n proportional to thickness Context 2 \\n datos (length of \\n branch), e.g. for \\n CloSeneSS Of \\n COncepts, as \\n inverse of length Z-factorS for \\n nOde 3 \\n One Arrow, indicating one way or asymmetric relationship, e.g. ownership between human and car ( \\n Fig 1 O Z-web with 2 Contexts, or 2 Sub-regions \\n of Z-Web, associated with NOce 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 11}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 11 of 81 US 8,873,813 B2 \\n (2) made processor PD dimensions \\n brackets Classifier for - \\n NP Clusters More age \\n tvOe of head Kid library y face 5-7 years \\n Pre-teen A \\n 8-12 library ge Gender progression module Teenager mOdel \\n 13-16 \\n years old \\n library \\n M Training Emotions \\n Ore age module model brackets \\n Emotions \\n library Training samples \\n Input module \\n Fig.11 Emotions \\n classifier \\n Output \\n module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 12}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 12 of 81 US 8,873,813 B2 \\n Input OCGSSO? Head or face \\n module O library \\n More age brackets Classifier for Classifier for \\n type of head tit Or Kid library Orface rotation 5-7 years \\n Gender Tilt or \\n Pre-teen A model rOtation 8-12 library ge library progression \\n Teenager model \\n 13-16 Analytical \\n years old model library \\n Training Morphing U \\n module model Rotational \\n Operators, to \\n rotate the \\n edges or \\n lines Translational \\n operators, to \\n Training samples move the \\n Input module edges or lines \\n Fig. 12', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 13}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 13 of 81 US 8,873,813 B2 \\n Input Search engine classifier \\n device \\n Wire mesh model Face recognizer training \\n of face library module module \\n Geometrical Analyzer or \\n model of face proCeSSOr \\n library Z factorS \\n module Z-node ContOur model module of face library \\n model of face Crisp rules Output module module library \\n Fuzzy rules \\n engine Fuzzy \\n descriptor for \\n face library, \\n e.g. Small lips Semantic \\n Fuzzy rules \\n library \\n Rules and Fig. 1 3 descriptors \\n Input module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 14}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 14 of 81 US 8,873,813 B2 \\n Front view Storage or Input Pre-processor database module \\n MOdified faces MOdification Translation \\n library module module \\n Eigenface \\n generator Tilt module \\n module \\n Emotions \\n y mOdule \\n Output Training \\n module for module libraries \\n Input \\n module for \\n Training samples libraries \\n Input module \\n Fig. 14', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 15}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 15 of 81 US 8,873,813 B2 \\n Pre-OrOCessor Storage or O database \\n Modified faces \\n Storage Modification fuZZification \\n module module \\n Eicenf FuZZification CenaCe E. averaging Haveraging rules or library \\n module module 2 module 1 \\n Loop Condition cloudifying module \\n Averaging \\n matrix Or \\n filter library Output \\n module Special \\n effect filter \\n module for \\n libraries \\n Training Training samples module for Input module libraries \\n Fig. 15', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 16}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 16 of 81 US 8,873,813 B2 \\n Pre-processor Z-Web \\n Output module \\n Face \\n recognizer \\n input module module \\n Component recognition Eigenvector \\n module Edge On face \\n detector Components \\n mOdule Wavelet \\n module eigenface \\n module \\n eigenface \\n library \\n Anchor \\n points or \\n features \\n module FaCe \\n Components \\n library Wavelet \\n library Input \\n Relationships module for \\n between libraries \\n features Or \\n face Trainind Samoles Training Components n E. module for \\n rules Or O libraries \\n Catabase \\n Z-factors * Fig. 16', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 17}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 17 of 81 US 8,873,813 B2 \\n NOde 6 \\n NOde 8 Jeff's brother's \\n Happiness Birthday party \\n (emotions) \\n No arrow, just indicating a \\n relationship \\n between 2 Z \\n nodes Node 4 NOde 2 y \\n Jeff Smith Jeff's dad \\n TWO arrOWS \\n - - indicate bi \\n z-factors NOde 3 directional \\n for node 3 Toyota sedan relationship, e.g. \\n - - - (Jeff's car) two objects being close to \\n each Other \\n One Arrow, indicating One Way or asymmetric relationship, e.g. \\n Ownership between human and Car \\n Fig. 17\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 18}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 18 of 81 US 8,873,813 B2 \\n (2) face library Classifier for emotions \\n Classifier \\n mitor E. Classifier for for age \\n faces OCUe face \\n Without \\n glasses Classifier for \\n mOdel tilt of the \\n face \\n Geometrical (2) model Analytical model \\n And/or face library \\n Analytical models Computer for glasses generated \\n Simulation geometrical models \\n glasses for glasses \\n Input \\n module for E. face With \\n faces with OCUC acCesory Classifier for \\n glasses \\n Library of glasses \\n Aggregator (styles and models) \\n module \\n face library \\n Fig. 18', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 19}), Document(page_content='U.S. Patent Oct. 28, 2014 \\n A (an ) sky \\n Sky \\n images \\n -O- \\n degree \\n Fig. 19 Continuity \\n analysis \\n module Sheet 19 of 81 US 8,873,813 B2 \\n Tilt angle w.r.t. 90 degree angle \\n -D \\n image input Storage \\n module module \\n Histogram \\n CUVSS \\n analyzer \\n Frequency \\n analyzer \\n Training processor module Color Or \\n pattern \\n recognizer \\n Training samples \\n Input module \\n Tilt \\n COrrection \\n module \\n Output \\n module \\n -CH', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 20}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 20 of 81 US 8,873,813 B2 \\n Image data Video data OCR/ Other data text formats \\n Input module Storage module \\n Context object Context Selected Objects library analyzer analyzer \\n trainer Storage Output \\n module module module Z-factors \\n Trainer samples \\n input module \\n Fig. 20', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 21}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 21 of 81 US 8,873,813 B2 \\n To recognize \\n an object (B), | partially or \\n hidden \\n image input Storage \\n module module \\n Histogram \\n CUWGS \\n Color Or analyzer \\n pattern \\n recognizer Frequency \\n analyzer BAB \\n Region \\n Continuity \\n analysis edge \\n detecting \\n module module \\n Training samples Training processor Input module module \\n Z-Web COntext Expected \\n analyzer objects Obiect library jec recognition \\n Other data module \\n Output Redrawing or \\n mOdule filler line or region \\n or extrapolation \\n module B Storage \\n module \\n Fig. 21', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 22}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 22 of 81 US 8,873,813 B2 \\n Z-Web, including Z-factors Related objects or expected objects \\n White \\n board On \\n Desktop the Wall \\n Computer \\n Inheritance of properties on The child class generally has more \\n Subclass (Or Sub-type Or Sub- Connecting nodes, as shown by \\n Category or specific example) dashed line boxes \\n X-ra Dental y cleaning machine equipment \\n H - - \\n board On \\n Desktop the Wall \\n Computer', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 23}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 23 of 81 US 8,873,813 B2 \\n Z-Web On \\n locations White board On based On - the Wall \\n COOrdinates of \\n - Objects, as proportional to \\n length dato2 Radius Of \\n Search, to find \\n related objects \\n in the COntext Or \\n environment dito2 (length of \\n branch), e.g. for physical -/ Cistance \\n between 2 Relative to \\n objects Z-node, - Center node \\n Node 1, (Node 1) Computer \\n - - \\n Expected or \\n / Node N, OUSG printer \\n -/ Can be expressed by relative average Or \\n median distance \\n distance, perCentage, \\n absolute number, or fuzzy parameter, e.g. \"far\" \\n Search input \\n module \\n Search for objects (in \\n Found Obiect Space Or imade Or Search Criteria Or Ound OOjectS Sp 9 SCope or rules module Storage video frame) module \\n Search module -/ Fig. 23 Related objects \\n Z-web input \\n module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 24}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 24 of 81 US 8,873,813 B2 \\n Rectangle shape in \\n perspective view or \\n COOrdinate System \\n POint at () infinity () \\n ( enlarged \\n PP \\n ( Projections on new \\n COOrdinates \\n Fig. 24', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 25}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 25 of 81 US 8,873,813 B2 \\n Reconstructing the \\n D past events NOce 6 \\n NOde 8 Jeff's brother's \\n Happiness Birthday party \\n (emotions) \\n NOde 4 NOde 1 y Jeff Smith Jeff's dad \\n Node N, 1N Smiling \\n face ReCollection \\n through Z-web, \\n starting from Node \\n N, and coming back \\n to Node 1, original \\n node symbol \\n Fig. 25\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 26}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 26 of 81 US 8,873,813 B2 \\n Arrow indicating the Means that table is Rules & y Conditions\\\\ operator \"OVER\" or over the legs \"ON-TOP-OF\" for \\n positional or \\n location description \\n COndition \\n Means that table is \\n upside down, with \\n legs Over the table \\n Earthquake \\n O War ZO6 \\n COndition \\n Rules storing \\n module or storage \\n Condition input \\n module Context analysis or \\n environmental \\n determination Context \\n Output', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 27}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 27 of 81 US 8,873,813 B2 \\n picture A () () \\n Eiffe John Joe Sun \\n TOWer \\n input \\n preprocessing \\n Image object \\n recognizer \\n mOnument faCe Natural object \\n recognizer recognizer recognizer \\n Eiffe \\n TOWer: \\n recognized \\n Z-Web Relation: Z-Web Z-Web Friend\\'Or \\n \"Family\\' G LOCation: y time: \"day \\n \"Paris\\' Fig 27 time\" Joe: John: Sun: \\n recognized recognized recognized', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 28}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 28 of 81 US 8,873,813 B2 \\n Multiple Input from History or \\n people Trained factual \\n VOting aS Computer- Catabase \\n input Input \\n module module \\n Training Samples Training \\n Input module module \\n Rules \\n builder \\n module Verified input samples \\n module \\n processor \\n Relationship \\n builder \\n module \\n Relationship \\n library \\n Fig. 28', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 29}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 29 Of 81 US 8,873,813 B2 \\n Still image input \\n Voice input module \\n module \\n video input \\n module \\n recognizer \\n Text Music Voice \\n recognizer recognizer recognizer bias factor \\n library \\n Reliability \\n factor \\n library Aggregator \\n analyzer \\n Confidence \\n factor \\n library Z-factors \\n truthfulness \\n factor \\n library \\n expertise \\n factOr \\n Fig. 29 library', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 30}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 30 of 81 US 8,873,813 B2 \\n ? y ? yy \"Jim\" July\" on the VaCation recognized \\n in photo \\n User input Z-web input \\n module \\n Z-factors Trainer trainer \\n Samples input module Z-Web \\n module \\n 2\" or other tag for photo in Comment \\n library module \\n Input from \\n fact fact module \\n library module \\n Conclusion module, e.g.: Output \\n \"Jim, On vacation, in module \\n Southern Hemisphere\" \\n Z-factors Input to Z \\n Web \\n module \\n Fig. 30', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 31}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 31 of 81 US 8,873,813 B2 \\n Library of Library of KnOWn Photo input FaCe faces input faces in the module recognizer SOCial module \\n network \\n y \"Jim \"Jason\" \\n recognized recognized \\n Photo or frame List of \\n matching with people Voice recognition eople, Or peop recognized module \\n tagging, module -record Or \\n Storage \\n PhOtO Or PhOtO Or n & Email \\n frame frame list \\n SelectOr attachment Construction \\n module module Input Email \\n module \\n module \\n Scheduling Email \\n module SeVer \\n Video Conferencing \\n module \\n Calendar \\n mOdule \\n Initiating \\n videO COnf. Fig. 31 module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 32}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 32 of 81 US 8,873,813 B2 \\n Picture Or Video frame \\n O \\n generic \\n Eiffe John Beer Can \\n TOWer \\n input \\n Client: beer Company: \\n module \\n Input from \\n Local beer can image - \"g t : storage or library C OOUGe \\n Normalize size & adjust Find object (a beer can) in the \\n Color - Correction module image-search module \\n Replace module- replacing first beer can image with local beer can \\n Output to picture or frame exchange \\n Picture Or Video frame \\n Fig. 32', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 33}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 33 of 81 US 8,873,813 B2 \\n Relative, \\n Geometrical OCation \\n descriptor: \\n \"U\" shape Fuzzy Mathematical descriptor: \\n Curved shape descriptor, \\n W.r.t. the \\n rest OffaCe \\n Trained data \\n User input Z-web input module Tagged Samples \\n Z-factorS Trainer trainer \\n samples input module Z-Web \\n module \\n Other \\n data- input \\n Rule Rule module \\n library module \\n Context analyzer input \\n fact fact module, e.g. \\n library module \"at a party\" \\n Emotion determination Output \\n module, e.g.: \"Smiling module \\n face\" formulations \\n Z-factorS Input to Z \\n Web \\n module \\n Fig. 33', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 34}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 34 of 81 US 8,873,813 B2 \\n Medical \\n Food allergy Drug Medical diagnosis interactions knowledge library library history library base \\n library \\n DOCtor's notes \\n library \\n User input Z-web input Nutritional module utritiona data library \\n Z-factors Trainer trainer \\n samples input module Z-Web \\n module \\n Image of \\n food-input \\n Rule Rule module \\n library module \\n Z-web FOOC \\n indredient fact fact analyzer E. \\n module library (determinator) \\n GOals Set Food proportion optimizer FOOC library \\n Output module Mobile \\n (including device Food proportion Calculator recommendations or PC, for \\n and Warnings) display \\n Fig. 34\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 35}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 35 of 81 US 8,873,813 B2 \\n Z-web input module \\n Rule database \\n task management \\n module (for agendas) \\n processing module \\n (Controller) \\n resolution module \\n Subgoals \\n Goal analyzing module KnOWledge database \\n Rule execution \\n mOdule', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 36}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 36 of 81 US 8,873,813 B2 \\n Z-web input module \\n Policy (with all rules listed) database \\n (e.g. Rules 1, 2, ..., N, or (R1,R2,..., RN)) \\n (e.g. Containing (if...Then ....) rules) \\n Involving parameters P1, P2, ..... PM \\n List all rules Storage for Rules involving goal \\n Output listed P3 \\n Original goal \\n (P3) value \\n Extract all parameters \\n involved in the \\n LOOp backward, to get all Selected rules above corresponding rules in the chain \\n activated, as we get all the \\n parameters figured out to activate \\n the rules Parameters \\n P1 & P8 \\n Storage for \\n value of \\n parameters Loop, until for one of \\n the Sub-goals, We have \\n a known value, which \\n fires Or activates the Sub-goal: P8 \\n Corresponding rule \\n Fig. 36', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 37}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 37 Of 81 US 8,873,813 B2 \\n Z-web input module \\n Rule database Knowledge database \\n task management \\n module (for agendas) \\n processing module Rule execution \\n (controller) module \\n Pattern \\n matching \\n Module (e.g. for \\n RETE) interpreter module \\n Goal analyzing module \\n exit Fig. 37', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 38}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 38 of 81 US 8,873,813 B2 \\n Fuzzy set \\n Z-web input module library \\n Rule database Knowledge database \\n task management \\n module (for agendas) \\n processing module \\n (controller & execution \\n resolver) \\n aggregator module \\n Fig. 38', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 39}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 39 Of 81 US 8,873,813 B2 \\n /N \\n And the tree \\n Structure \\n COntinues in this \\n nanner \\n C21 C22 \\n C12 \\n C13 C11 \\n Fig. 39', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 40}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 40 of 81 US 8,873,813 B2 \\n Z-web input module Fuzzy Rule database \\n Fuzzy inference \\n Fuzzy engine \\n COntroller \\n DefuZZification module \\n FuZZification (e.g. taking Center of \\n mOdule mass Or average or \\n weighted average for \\n area under the Curve) \\n COnditions actions \\n process \\n User manual input module \\n Fig. 40', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 41}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 41 of 81 US 8,873,813 B2 \\n Another expert \\n System \\n CaSCaded Or in \\n Series \\n Knowledge \\n acquisition module \\n Database interface or library \\n Inference engine Knowledge base library module \\n Meta-knowledge base library \\n Expert system \\n Fig. 41', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 42}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 42 of 81 US 8,873,813 B2 \\n Picture or video frame \\n (distances) 2 people away \\n far away (fuzzy term) \\n input \\n Image library \\n from history, \\n E. Image Object Input from Z pictures in recognizer Web module the photo \\n album, Or \\n Other frames Face recognition module \\n in video \\n Finding distances or positions of faces with respect to each other- for \\n one or more images- values or statistics- position module \\n Input from Z-web module Finding relationships \\n between people Fig. 42', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 43}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 43 of 81 US 8,873,813 B2 \\n Memory unit 1 Memory unit 2 Memory unit 3 \\n $104,322.34 About 100K LOW 6 figures \\n -CHD \\n More crisp More fuzzy \\n Larger Smaller \\n requirement requirement for \\n for storage Storage \\n Slower acCeSS Faster acCeSS \\n Use for Short term Use for long term \\n memory memory \\n -> time \\n In one embodiment, gradually fuzzify & move \\n the data toward memory unit 3, as time passes \\n Fig. 43', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 44}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 44 of 81 US 8,873,813 B2 \\n Vocabulary \\n Or \\n alphabets for pattern N- O \\n recognition \\n Store the encoded \\n patterns \\n Compare the encoded \\n patterns input \\n patterns \\n module Encode the patterns \\n with the alphabets \\n Pattern Fig 44 N- recognizer', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 45}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 45 of 81 US 8,873,813 B2 \\n Picture or Video frame \\n Object recognizer \\n Skeleton operator or thinning filter \\n basic shape \\n Geometrical \\n library Recognizer & \\n matching module \\n Inverted \"Y\" shape alphabets \\n library \\n Stored as \\n GeOmetrical textual descriptor library \\n descriptor (natural language or fuzzy \\n library descriptorS or parameters) \\n Fig. 45', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 46}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 46 of 81 US 8,873,813 B2 \\n /Variations on the right eye \\n O (6. and eyebrow \\n Operator \\n SL7 \\n Shear StreSS Or \\n deformation \\n Elastic model Operator, e.g. where \\n One side of the \\n object moves, but \\n the parallel side \\n stays as-is \\n A0 \\n Eyebrow \\n displacement O O N / \\n value COOrdinates Eve AY y displacement \\n Fig. 46 AX', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 47}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 47 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 48}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 48 of 81 US 8,873,813 B2 \\n - Image of an eye \\n Mimic the \\n shape of an \\n eye (the object) \\n Or, use a more approximated \\n or simpler Use it as a \\n version basis object 17 (in library) \\n Fig. 48', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 49}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 49 of 81 US 8,873,813 B2 \\n Input preprocessing Segmentation Classification \\n device \\n st Or Knowledge base Recognition \\n nory database Module \\n unitS interface & \\n task \\n assignment \\n Video recognizer mOdule face \\n recognizer \\n Recognition Output \\n Natural locations module module \\n and geographical \\n recognizer Image \\n recognition \\n Monuments module \\n and building \\n recognizer Voice \\n recognizer \\n Medical image OCR for text \\n analyzer recognition \\n module \\n Fig. 49', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 50}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 50 Of 81 US 8,873,813 B2 \\n Still image \\n music Voice text \\n Z-web input \\n module \\n table \\n dictionaries \\n encyclopediaS \\n Computer \\n generated \\n data Medical Public forms Images reCOrds \\n Fig. 50', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 51}), Document(page_content='U.S. Patent \\n \"Behind\" \\n operator \\n \"position\" \\n operator \\n Trainer \\n Samples input \\n mOdule \\n Super-template \\n Creating module \\n Rule COnclusion \\n mOdule \\n Fig. 51 Oct. 28, 2014 Sheet 51 of 81 \\n \"more\" \\n operator \"Over\" \\n operator \"ti me\" \\n operator \\n \"less\" rator rul Operator rules Operator and logic \\n library \\n New rules \\n Input Operator mOdule \\n rules \\n applying \\n module trainer \\n module \\n Z-Web \\n Input from \\n analyzer Z-web \\n module \\n Relationship Output \\n establishing module \\n Or extraction \\n module \\n Z-factors \\n Z-web US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 52}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 52 of 81 US 8,873,813 B2 \\n Computer \\n Carea SenSO detector generated \\n data \\n Big Data \\n Trainer trainer \\n Samples input module Z-web \\n mOdule \\n Z-factors \\n Calculator \\n Rule Rule \\n library module \\n Fuzzy an. rules fact fact y engine \\n library module \\n Mobile device or PC, for 1 Math processor function display Or Output library \\n Kernel Library DCT FOUrier \\n FFT, Wavelet, and CustOmized \\n Markov model, Bayesian Other basis basis functions \\n model, and other models functions Or module Pattern Or User input object \\n eigenvectors \\n library Fig. 52', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 53}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 53 Of 81 US 8,873,813 B2 \\n Z-web (for analysis, search, Z-factors (e.g. \\n relationships, and extracted data) reliability factor, bias factor, and \\n truthfulness factor) \\n URL Qstore (storage sh plus \\n Catabase \\n plus \\n (Wsite) \\n Direct 3CCCSS (e.g text, \\n image, and \\n numbers) \\n Internet \\n Search, \\n Query \\n User (U) \\n (User interface, browser \\n Computer or Optional plug-in \\n mobile device) \\n Fig. 53', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 54}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 54 of 81 US 8,873,813 B2 \\n Dissecting & Task Classification Input Segmentation assignment module device module module \\n Category or type 3 Category or type 2 Category or type 1 \\n Transformation Transformation Transformation \\n module module module \\n analyzer \\n SubCategory M \\n 3.1 Membership \\n transformation C lati values \\n mOCdule O63.On \\n module \\n Subcategory \\n 3.2 \\n transformation \\n module aggregator \\n mOdule Sub \\n SubCategory \\n 3.2.1 \\n transformation \\n module \\n And type hierarchy Fig. 54 \\n COntinues', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 55}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 55 Of 81 US 8,873,813 B2 \\n 1N model templates \\n in library-2 \\n methods \\n Na \\n 2 \\n N- Grid model \\n Region model with Kregions & M \\n relationships \\n Fig. 55', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 56}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 56 of 81 US 8,873,813 B2 \\n W \\n B 45 \\n degree () (2 W \\n And similar \\n OCS ......', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 57}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 57 Of 81 US 8,873,813 B2 \\n Or use the \\n tilted One \\n (e.g. at 45 \\n degrees) \\n (e.g. at 45 degrees) \\n Fig. 57', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 58}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 58 Of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 59}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 59 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 60}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 60 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 61}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 61 of 81 US 8,873,813 B2 \\n (1)T W \\n (2)T W \\n (3)T W \\n W(8) \\n W(2) \\n w) \\n data \\n FIG. 62', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 62}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 62 of 81 US 8,873,813 B2 \\n Labels Data (e.g., image) \\n FIG. 63 \\n Degree of \\n Correlation Or Features/ \\n Conformity Classification \\n NetWork/Classifier/Feature Detector \\n Labels Data (e.g., image) \\n FIG. 64', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 63}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 63 of 81 US 8,873,813 B2 \\n w) \\n Labels Data (e.g., image) \\n Model Parameters \\n Parameters Ranges/Constraints || Model Function(s) \\n Sample generator \\n FIG. 65', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 64}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 64 of 81 US 8,873,813 B2 \\n For Model M For Model M2 \\n FIG. 66', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 65}), Document(page_content='US 8,873,813 B2 Sheet 65 of 81 Oct. 28, 2014 U.S. Patent \\n FIG. 67', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 66}), Document(page_content='US 8,873,813 B2 Sheet 66 of 81 Oct. 28, 2014 U.S. Patent \\n ., image pixels \\n FIG. 68', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 67}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 67 of 81 US 8,873,813 B2 \\n Features/Classifications/Recognition \\n Expert Mixer \\n Recognition/Classifier Chooser/Scheduler \\nrt \\n Preprocessing Preprocessing \\n Data (e.g., image) \\n FIG. 69', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 68}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 68 of 81 US 8,873,813 B2 \\n FIG. 70(a) \\n FIG.70(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 69}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 69 of 81 \\nA) E1. xy \\n FIG. 71 (b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 70}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 70 of 81 \\n FIG. 72(a) \\n FIG.72(b) US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 71}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 71 of 81 US 8,873,813 B2 \\n FIG. 73(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 72}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 72 of 81 US 8,873,813 B2 \\n SSSSNSNSSSSSSNSCY SSSSSSSSR3NNSSSC \\n ow \\n a 1 SS (SSNSSS) 1 1 - asSNSSNSSP a \\n Data, e.g., image pixels \\n FIG. 74', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 73}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 73 Of 81 US 8,873,813 B2 \\n Data, e.g., Sparse units approximating low res. thumbnail \\n FIG. 75(a) \\n Thumbnail \\n pixel Data, e.g., sparse V \\n units fed from thumbnail \\n Data, e.g., thumbnail wide pixels on V units \\n FIG. 75(c)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 74}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 74 of 81 US 8,873,813 B2 \\n Data (e.g., image) \\n Object detector/classifier \\n Object/Concept \\n Correlating \\n Objects/Concepts Correlation Weights, \\n Context Clusters \\n A 7 \\n Object detectors/classifiers \\n Additional Objects/Concepts \\n FIG. 76', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 75}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 75 Of 81 US 8,873,813 B2 \\n Pixel Size 41 In-Pixel Size 4N PN NarroWest MYAEEEN IN 24 \\n WindoW Of EEEEE A - Pixel Size \\n FOCUS \\n NarroWer \\n WindoW Of \\n FOCUS \\n FIG. 77(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 76}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 76 of 81 US 8,873,813 B2 \\n L1 Li Li LN \\n E.g.: PerSOn Car \\n Labels Data (e.g., image) \\n Annotation: Jim \\n David \\n Eiffel tower', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 77}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 77 of 81 US 8,873,813 B2 \\n Feature detector/classifier \\n (e.g., with correlation) \\n Data (e.g., image) \\n Jim \\n David \\n Date? Time \\n GPS \\n Annotation and metadata \\n FIG. 79', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 78}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 78 of 81 US 8,873,813 B2 \\n Correlator/Analyzer \\n AL Features of Features of \\n Image 1 Image 2 \\n Without \\n Feature f \\n Database \\n FIG. 80', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 79}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 79 of 81 US 8,873,813 B2 \\n Network Or Sources \\n DOmain/NetWOrk Of Information \\n Traffic Data/Statistics Fetch \\n S Bots or bkgnd processes \\n Content, metadata, tags URL \\n Updates \\n Cache Analytics Bkgnd \\n processes \\n Bkgnd (me) \\n processes Category \\n N \\n N \\n N \\\\ \\n requests and Fetching Content/ \\n Selections Info/URL \\n Ranking Query Content/ \\n Engine Selection Summary/ \\n Ranked URL \\n Result \\n FIG. 81 S', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 80}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 80 of 81 US 8,873,813 B2 \\n CRBM \\n Frame-2 Frame- Frameo \\n FIG. 82(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 81}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 81 of 81 US 8,873,813 B2 \\n Linear COmbination \\n based on mapping \\n (i.e., resolution reduction) \\n Frame att Wo State att \\n FIG. 83(b) \\n Contribution to dynamic \\n mean based on the mapping \\n Frame att Wo State att \\n FIG. 83(c)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 82}), Document(page_content=\"US 8,873,813 B2 \\n 1. \\n APPLICATION OF Z-WEBS AND Z-FACTORS \\n TO ANALYTICS, SEARCH ENGINE, \\n LEARNING, RECOGNITION, NATURAL \\n LANGUAGE, AND OTHERUTILITIES \\n RELATED APPLICATIONS \\n This application claims the benefit of the following appli \\n cation, with the same inventors: The U.S. provisional appli cation No. 61/701,789, filed Sep. 17, 2012, by Tadayonet.al, \\n titled “Method and system for approximate Z-number evalu ation based on categorical sets of probability distributions'. \\n The current application incorporates by reference all of the teachings of the provisional application, including all of its \\n appendices and attachments. It also claims benefits of the earlier (provisional) application. \\n BACKGROUND OF THE INVENTION \\n There are a lot of research going on today, focusing on the search engine, analytics, Big Data processing, natural lan guage processing, economy forecasting, dealing with reli \\n ability and certainty, medical diagnosis, pattern recognition, \\n object recognition, biometrics, security analysis, risk analy \\n sis, fraud detection, satellite image analysis, machine gener ated data, machine learning, training samples, and the like. \\n For example, see the article by Technology Review, pub \\n lished by MIT, “Digging deeper in search web, Jan. 29. \\n 2009, by Kate Greene, or search engine by GOOGLE(R), \\n MICROSOFTR) (BINGO), or YAHOOR, or APPLERSIRI, or WOLFRAMR ALPHA computational knowledge engine, \\n or AMAZON engine, or FACEBOOKR engine, or \\n ORACLER database, orYANDEXOR search engine in Russia, \\n or PICASAR (GOOGLE(R) web albums, or YOUTUBER (GOOGLE(R) engine, or ALIBABA (Chinese supplier con \\n nection), or SPLUNKR (for Big Data), or MICROSTRAT EGYR (for business intelligence), or QUID (or KAGGLE, \\n ZESTFINANCE, APIXIO, DATAMEER, BLUEKAI, GNIP, \\n RETAILNEXT, or RECOMMIND) (for Big Data), or paper by Viola-Jones, Viola et al., at Conference on Computer Vision and Pattern Recognition, 2001, titled “Rapid object \\n detection using a boosted cascade of simple features, from \\n Mitsubishi and Compaq research labs, or paper by Alex Pent \\n land et al., February 2000, at Computer, IEEE, titled “Face \\n recognition for smart environments', or GOOGLE(R) official blog publication, May 16, 2012, titled “Introducing the \\n knowledge graph: things, not strings’, or the article by Tech \\n nology Review, published by MIT, “The future of search”. \\n Jul. 16, 2007, by Kate Greene, or the article by Technology \\n Review, published by MIT, “Microsoft searches for group \\n advantage'. Jan. 30, 2009, by Robert Lemos, or the article by Technology Review, published by MIT, “WOLFRAM \\n ALPHA and GOOGLE face off, May 5, 2009, by David Talbot, or the paper by Devarakonda et al., at International \\n Journal of Software Engineering (IJSE), Vol. 2, Issue 1, 2011, titled “Next generation search engines for information \\n retrieval’, or paper by Nair-Hinton, titled “Implicit mixtures \\n of restricted Boltzmann machines, NIPS, pp. 1145-1152, 2009, or paper by Nair, V. and Hinton, G. E., titled “3-D Object recognition with deep belief nets', published in \\n Advances in Neural Information Processing Systems 22. (Y. \\n Bengio, D. Schuurmans, J. lafferty, C. K. I. Williams, and A. \\n Culotta (Eds.)), pp 1339-1347. \\n One of such research and recent advances is done by Prof Lotfi Zadeh, of UC Berkeley, “the Father of Fuzzy Logic', who recently came up with the concept of Z-numbers, plus \\n related topics and related technologies. In the following sec 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n tion, we discuss the Z-numbers, taught by the U.S. Pat. No. 8.311,973, by Zadeh (issued recently). \\n Z-Numbers: \\n This section about Z-numbers is obtained from the patent by Zadeh, namely, the U.S. Pat. No. 8,311,973, which \\n addresses Z-numbers and its applications, as well as other concepts.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 83}), Document(page_content=\"Z-Numbers: \\n This section about Z-numbers is obtained from the patent by Zadeh, namely, the U.S. Pat. No. 8,311,973, which \\n addresses Z-numbers and its applications, as well as other concepts. \\n A Z-number is an ordered pair of fuzzy numbers, (A,B). For simplicity, in one embodiment, A and B areassumed to be trapezoidal fuzzy numbers. A Z-number is associated with a \\n real-valued uncertain variable, X, with the first component, A, playing the role of a fuzzy restriction, RCX), on the values \\n which X can take, written as X is A, where A is a fuzzy set. What should be noted is that, strictly speaking, the concept of \\n a restriction has greater generality than the concept of a con \\n straint. A probability distribution is a restriction but is not a \\n constraint (see L.A. Zadeh, Calculus of fuZZy restrictions, In: \\n L. A. Zadeh, K. S. Fu, K. Tanaka, and M. Shimura (Eds.), Fuzzy sets and Their Applications to Cognitive and Decision \\n Processes, Academic Press, New York, 1975, pp. 1-39). A restriction may be viewed as a generalized constraint (see L. A. Zadeh, Generalized theory of uncertainty (GTU) princi pal concepts and ideas, Computational Statistics & Data \\n Analysis 51, (2006) 15-46). In this embodiment only, the \\n terms restriction and constraint are used interchangeably. \\n The restriction \\n is referred to as a possibilistic restriction (constraint), with A playing the role of the possibility distribution of X. More specifically, \\n where L is the membership function of A, and u is a \\n generic value of X. L. may be viewed as a constraint which is associated with RCX), meaning that u(u) is the degree to \\n which u satisfies the constraint. \\n When X is a random variable, the probability distribution of X plays the role of a probabilistic restriction on X. A probabilistic restriction is expressed as: \\n where p is the probability density function of X. In this \\n Case, \\n Note. Generally, the term “restriction' applies to X is R. \\n Occasionally, “restriction' applies to R. Context serves to disambiguate the meaning of “restriction.” \\n The ordered triple (XA,B) is referred to as a Z-valuation. A Z-valuation is equivalent to an assignment statement, X is \\n (A,B). X is an uncertain variable if A is not a singleton. In a related way, uncertain computation is a system of computa \\n tion in which the objects of computation are not values of \\n variables but restrictions on values of variables. In this \\n embodiment/section, unless stated to the contrary, X is \\n assumed to be a random variable. For convenience, A is \\n referred to as a value of X, with the understanding that, strictly speaking, A is not a value of X but a restriction on the \\n values which X can take. The second component, B, is referred to as certainty. Certainty concept is related to other \\n concepts, such as Sureness, confidence, reliability, strength of \\n belief, probability, possibility, etc. However, there are some \\n differences between these concepts. \\n In one embodiment, when X is a random variable, certainty may be equated to probability. Informally, B may be inter \\n preted as a response to the question: How Sure are you that X\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 83}), Document(page_content='US 8,873,813 B2 \\n 3 \\n is A2 Typically, A and B are perception-based and are \\n described in a natural language. Example: (about 45 minutes, \\n usually.) A collection of Z-valuations is referred to as Z-in \\n formation. It should be noted that much of everyday reason ing and decision-making is based, ineffect, on Z-information. \\n For purposes of computation, when A and B are described in a natural language, the meaning of A and B is precisiated \\n (graduated) through association with membership functions, \\n LL and LL, respectively, FIG. 1. \\n The membership function of A. L. may be elicited by \\n asking a Succession of questions of the form: To what degree does the number, a, fit your perception of A2 Example: To \\n what degree does 50 minutes fit your perception of about 45 \\n minutes? The same applies to B. The fuzzy set, A, may be interpreted as the possibility distribution of X. The concept of a Z-number may be generalized in various ways. In particular, \\n X may be assumed to take values in R\", in which case A is a Cartesian product of fuzzy numbers. Simple examples of \\n Z-valuations are: \\n (anticipated budget deficit, close to 2 million dollars, very likely) \\n (population of Spain, about 45 million, quite Sure) (degree of Robert\\'s honesty, very high, absolutely) \\n (degree of Robert\\'s honesty, high, not sure) \\n (travel time by car from Berkeley to San Francisco, about 30 minutes, usually) \\n (price of oil in the near future, significantly over 100 dol lars/barrel, very likely) \\n It is important to note that many propositions in a natural language are expressible as Z-valuations. Example: The proposition, p. \\n p: Usually, it takes Robert about an hour to get home from \\n work, is expressible as a Z-valuation: \\n (Robert\\'s travel time from office to home, about one hour, usually) \\n If X is a random variable, then X is A represents a fuzzy event in R, the real line. The probability of this event, p, may \\n be expressed as (see L. A. Zadeh, Probability measures of fuzzy events, Journal of Mathematical Analysis and Applica \\n tions 23 (2), (1968) 421-427.): \\n where p is the underlying (hidden) probability density of \\n X. In effect, the Z-valuation (XA.B) may be viewed as a restriction (generalized constraint) on X defined by: \\n Prob(X is A) is B. \\n What should be underscored is that in a Z-number, (A,B), the underlying probability distribution, p. is not known. \\n What is known is a restriction on p which may be expressed \\n aS \\n Note: In this embodiment only, the term “probability dis \\n tribution\\' is not used in its strict technical sense. \\n In effect, a Z-number may be viewed as a Summary of p. It is important to note that in everyday decision-making, most \\n decisions are based on Summaries of information. Viewing a Z-number as a Summary is consistent with this reality. In 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n applications to decision analysis, a basic problem which \\n arises relates to ranking of Z-numbers. Example: Is (approxi \\n mately 100, likely) greater than (approximately 90, very \\n likely)? Is this a meaningful question? We are going to \\n address these questions below. \\n An immediate consequence of the relation between p and \\n B is the following. If Z=(A,B) then Z=(A\\'.1-B), where A\\' is the complement of A and Z plays the role of the complement \\n of Z. 1-B is the antonym of B (see, e.g., E.Trillas, C. Moraga, \\n S. Guadarrama, S. Cubillo and E. Castifieira, Computing with \\n Antonyms, In: M. Nikravesh, J. Kacprzyk and L. A. Zadeh \\n (Eds.), Forging New Frontiers: Fuzzy Pioneers I, Studies in \\n Fuzziness and Soft Computing Vol 217, Springer-Verlag, \\n Berlin Heidelberg 2007, pp. 133-153.). \\n An important qualitative attribute of a Z-number is infor \\n mativeness. Generally, but not always, a Z-number is infor \\n mative if its value has high specificity, that is, is tightly con \\n strained (see, for example, R. R. Yager. On measures of', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 84}), Document(page_content=\"mativeness. Generally, but not always, a Z-number is infor \\n mative if its value has high specificity, that is, is tightly con \\n strained (see, for example, R. R. Yager. On measures of \\n specificity, In: O. Kaynak, L. A. Zadeh, B. Turksen, I. J. Rudas (Eds.), Computational Intelligence: Soft Computing \\n and FuZZy-Neuro Integration with Applications, Springer \\n Verlag, Berlin, 1998, pp. 94-113.), and its certainty is high. \\n Informativeness is a desideratum when a Z-number is a basis \\n for a decision. It is important to know that if the informative \\n ness of a Z-number is sufficient to serve as a basis for an \\n intelligent decision. \\n The concept of a Z-number is after the concept of a fuzzy \\n granule (see, for example, L.A. Zadeh, Fuzzy sets and infor mation granularity, In: M. Gupta, R. Ragade, R. Yager (Eds.), \\n Advances in Fuzzy Set Theory and Applications, North-Hol \\n land Publishing Co., Amsterdam, 1979, pp. 3-18. Also, see L. A. Zadeh, Possibility theory and soft data analysis, In: L. \\n Cobb, R. M. Thrall (Eds.), Mathematical Frontiers of the Social and Policy Sciences, Westview Press, Boulder, Colo., \\n 1981, pp. 69-129. Also, see L. A. Zadeh, Generalized theory of uncertainty (GTU) principal concepts and ideas, Com \\n putational Statistics & Data Analysis 51, (2006) 15-46.). It \\n should be noted that the concept of a Z-number is much more general than the concept of confidence interval in probability \\n theory. There are some links between the concept of a Z-num ber, the concept of a fuZZy random number and the concept of \\n a fuZZy random variable (see, e.g., J. J. Buckley, J. J. Leonard, \\n Chapter 4: Random fuzzy numbers and vectors, In: Monte Carlo Methods in Fuzzy Optimization, Studies in Fuzziness and Soft Computing 222, Springer-Verlag, Heidelberg, Ger \\n many, 2008. Also, see A. Kaufman, M. M. Gupta, Introduc tion to Fuzzy Arithmetic: Theory and Applications, Van Nos \\n trand Reinhold Company, New York, 1985. Also, see C. V. Negoita, D. A. Ralescu, Applications of Fuzzy Sets to Sys \\n tems Analysis, Wiley, New York, 1975.). A concept which is closely related to the concept of a \\n Z-number is the concept of a Z-number. Basically, a \\n Z'-number, Z is a combination of a fuZZy number, A, and a random number, R, written as an ordered pair Z=(AR). In this pair. A plays the same role as it does in a Z-number, and R is the probability distribution of a random number. Equiva lently, R may be viewed as the underlying probability distri \\n bution of X in the Z-valuation (X, A,B). Alternatively, a Z'-number may be expressed as (Ap) or (Lp), where LL \\n is the membership function of A. A Z-valuation is expressed as (X.A.p.) or, equivalently, as (XLL.p.), where p is the \\n probability distribution (density) of X. A Z-number is asso \\n ciated with what is referred to as a bimodal distribution, that \\n is, a distribution which combines the possibility and probabil \\n ity distributions of X. Informally, these distributions are com \\n patible if the centroids of L and pare coincident, that is,\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 84}), Document(page_content='US 8,873,813 B2 \\n 5 \\n I-4 (a) du R - JAA (u) du \\n The scalar product of L and p, up is the probability \\n measure, P., of A. More concretely, \\n It is this relation that links the concept of a Z-number to that of a Z-number. More concretely, \\n What should be underscored is that in the case of a Z-num \\n ber what is known is not p but a restriction on p. expressed \\n as: LLp is B. By definition, a Z-number carries more \\n information than a Z-number. This is the reason why it is labeled a Z-number. Computation with Z\\'-numbers is a portal to computation with Z-numbers. \\n The concept of a bimodal distribution is of interest in its own right. Let X be a real-valued variable taking values in U. \\n For our purposes, it is convenient to assume that U is a finite set, U-u. . . . , u}. We can associate with X a possibility distribution, L, and a probability distribution, p, expressed as: \\n in which u?u, means that Li, i=1,... n, is the possibility that Xu. Similarly, p,\\\\u, means that p, is the probability that \\n X=u. The possibility distribution, L, may be combined with the probability distribution, p, through what is referred to as \\n confluence. More concretely, \\n AS was noted earlier, the Scalar product, expressed as up, \\n is the probability measure of A. In terms of the bimodal \\n distribution, the Z-valuation and the Z-valuation associated with X may be expressed as: \\n (X, Apx) \\n (X, A,B). Lpx is B, \\n respectively, with the understanding that B is a possibilistic \\n restriction on up. \\n Both Zand Z\\' may be viewed as restrictions on the values which X may take, written as: X is Zand X is Z\", respectively. Viewing Z and Z as restrictions on X adds important con \\n cepts to representation of information and characterization of \\n dependencies. In this connection, what should be noted is that the concept of a fuZZy if-then rule plays a pivotal role in most applications of fuzzy logic. What follows is a very brief \\n discussion of what are referred to as Z-rules—if-then rules in \\n which the antecedents and/or consequents involve Z-numbers \\n or Z-numbers. \\n A basic fuzzy if-then rule may be expressed as: if X is A \\n then Y is B, where A and B are fuzzy numbers. The meaning \\n of such a rule is defined as: \\n if X is Athen Y is B->(X,Y) is AxB \\n where AxB is the Cartesian product of A and B. It is convenient to express a generalization of the basic if-then rule \\n to Z-numbers in terms of Z-valuations. More concretely, \\n if (X, A, B) then (YA,B) 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n Examples \\n if (anticipated budget deficit, about two million dollars, \\n very likely) then (reduction in staff, about ten percent, very likely) \\n if (degree of Robert\\'s honesty, high, not sure) then (offer a \\n position, not, Sure) \\n if (X, Small) then (Y large, usually.) \\n An important question relates to the meaning of Z-rules \\n and Z\\'-rules. The meaning of a Z-rule may be expressed as: \\n if (X, Axspx) then (YApy)- (X,Y) is (AxxApxpy) \\n where AXA is the Cartesian product A and A. Z-rules have the important applications in decision analy \\n sis and modeling of complex systems, especially in the realm \\n of economics (for example, Stock market and specific Stocks) \\n and medicine (e.g. diagnosis and analysis). \\n A problem which plays a key role in many applications of \\n fuZZylogic, especially in the realm of fuZZy control, is that of \\n interpolation. More concretely, the problem of interpolation \\n may be formulated as follows. Consider a collection of fuzzy \\n if-then rules of the form: \\n if X is A, then Y is B. i=1,..., 2 \\n where the A, and B, are fuzzy sets with specified member \\n ship functions. If X is A, where A is not one of the A, then \\n what is the restriction on Y? \\n The problem of interpolation may be generalized in various \\n ways. A generalization to Z-numbers may be described as \\n follows. Consider a collection Z-rules of the form: \\n if X is A, then usually (Y is B.), i=1,..., 2 \\n where the A, and B, are fuzzy sets. Let A be a fuzzy set', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 85}), Document(page_content=\"follows. Consider a collection Z-rules of the form: \\n if X is A, then usually (Y is B.), i=1,..., 2 \\n where the A, and B, are fuzzy sets. Let A be a fuzzy set \\n which is not one of the A. What is the restriction on Y \\n expressed as a Z-number? An answer to this question would \\n add a useful formalism to the analysis of complex systems \\n and decision processes. \\n Representation of Z-numbers can be facilitated through the \\n use of what is called a Z-mouse. Basically, a Z-mouse is a \\n visual means of entry and retrieval of fuZZy data. \\n The cursor of a Z-mouse is a circular fuZZy mark, called an f-mark, with a trapezoidal distribution of light intensity. This distribution is interpreted as a trapezoidal membership func \\n tion of a fuzzy set. The parameters of the trapezoid are con \\n trolled by the user. A fuzzy number such as “approximately 3” \\n is represented as an f-mark on a scale, with 3 being the \\n centroid of the f-mark (FIG. 2a). The size of the f-mark is a measure of the user's uncertainty about the value of the num ber. As was noted already, the Z-mouse interprets an f-markas the membership function of a trapezoidal fuzzy set. This membership function serves as an object of computation. A \\n Z-mouse can be used to draw curves and plot functions. A key idea which underlies the concept of a Z-mouse is that visual interpretation of uncertainty is much more natural than its description in natural language or as a membership func \\n tion of a fuzzy set. This idea is closely related to the remark able human capability to precisiate (graduate) perceptions, \\n that is, to associate perceptions with degrees. As an illustra \\n tion, if I am asked “What is the probability that Obama will be \\n reelected?' I would find it easy to put an f-mark on a scale \\n from 0 to 1. Similarly, I could put an f-mark on a scale from \\n 0 to 1 if I were asked to indicate the degree to which I like my \\n job. It is of interest to note that a Z-mouse could be used as an informative means of polling, making it possible to indicate\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 85}), Document(page_content=\"US 8,873,813 B2 \\n 7 \\n one's strength offeeling about an issue. Conventional polling \\n techniques do not assess strength of feeling. \\n Using a Z-mouse, a Z-number is represented as two \\n f-marks on two different scales (FIG. 2b). The trapezoidal \\n fuzzy sets which are associated with the f-marks serve as objects of computation. \\n Computation with Z-Numbers: \\n What is meant by computation with Z-numbers? Here is a simple example. Suppose that I intend to drive from Berkeley \\n to San Jose via Palo Alto. The perception-based information \\n which I have may be expressed as Z-valuations: (travel time \\n from Berkeley to Palo Alto, about an hour, usually) and (travel \\n time from Palo Alto to San Jose, about twenty-five minutes, usually.) How long will it take me to drive from Berkeley to \\n San Jose? In this case, we are dealing with the sum of two Z-numbers (about an hour, usually) and (about twenty-five minutes, usually.) Another example: What is the square root \\n of (A,B)? Computation with Z-numbers falls within the prov \\n ince of Computing with Words (CW or CWW). Example: \\n What is the square root of a Z-number? Computation with Z-numbers is much simpler than com putation with Z-numbers. Assume that * is a binary operation \\n whose operands are Z-numbers, Z (AR) and Z (A \\n R.) By definition, \\n with the understanding that the meaning of * in R*R is \\n not the same as the meaning of in AA. In this expression, the operands of in AA are fuzzy numbers; the operands \\n of in R*R are probability distributions. \\n Example: Assume that * is sum. In this case, A+A is \\n defined by: \\n Similarly, assuming that R and R are independent, the \\n probability density function of R*R is the convolution, O, of the probability density functions of R and R. Denoting these probability density functions as p and per respec tively, we have: \\n Thus, \\n It should be noted that the assumption that R and R are independent implies worst case analysis. \\n More generally, to compute ZZ what is needed is the \\n extension principle of fuzzy logic (see, e.g., L. A. Zadeh, \\n Probability measures of fuzzy events, Journal of Mathemati \\n cal Analysis and Applications 23 (2), (1968) 421-427.). Basi cally, the extension principle is a rule for evaluating a function \\n when what are known are not the values of arguments but \\n restrictions on the values of arguments. In other words, the \\n rule involves evaluation of the value of a function under less \\n than complete information about the values of arguments. Note. Originally, the term “extension principle” was \\n employed to describe a rule which serves to extend the \\n domain of definition of a function from numbers to fuzzy \\n numbers. In this disclosure, the term “extension principle” \\n has a more general meaning which is stated in terms of restric \\n tions. What should be noted is that, more generally, incom pleteness of information about the values of arguments 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n applies also to incompleteness of information about func \\n tions, in particular, about functions which are described as \\n collections of if-then rules. \\n There are many versions of the extension principle. A basic \\n version was given in the article: (L. A. Zadeh, Fuzzy sets, \\n Information and Control 8, (1965) 338-353.). In this version, the extension principle may be described as: \\n R(X): X is A (constraint on u is a A (it)) \\n R(Y): uy (v) = suppa A (u) (f(A) = R(Y)) \\n Subject to \\n where A is a fuzzy set, L is the membership function of A, \\n L is the membership function of Y, and u and V are generic \\n values of X and Y, respectively. \\n A discrete version of this rule is: \\n R(Y): uy (v) = Sup., H: \\n Subject to \\n V = f(iii) \\n In a more general version, we have \\n R(X): g(X) is A (constraint on u is uta (g (it))) \\n R(Y): uty (V) = Supt A (g(u)) \\n Subject to \\n For a function with two arguments, the extension principle \\n reads: \\n Z=f(X, Y) \\n R(X): g(X) is A (constraint on u is ul A (g (it)))\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 86}), Document(page_content='R(Y): uty (V) = Supt A (g(u)) \\n Subject to \\n For a function with two arguments, the extension principle \\n reads: \\n Z=f(X, Y) \\n R(X): g(X) is A (constraint on u is ul A (g (it))) \\n R(Y): h(Y) is B (constraint on u is up (h(u))) \\n R(Z): uz(w) = Sup, (4 x (g(u)) A ply (h(v))), \\n Wherein: A = min \\n Subject to \\n In application to probabilistic restrictions, the extension \\n principle leads to results which coincide with standard results \\n which relate to functions of probability distributions. Specifi \\n cally, for discrete probability distributions, we have:', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 86}), Document(page_content=\"US 8,873,813 B2 \\n subject to \\n V = f(iii) \\n For functions with two arguments, we have: \\n subject to \\n For the case where the restrictions are Z-numbers, the \\n extension principle reads: \\n Z=f(X, Y) \\n R(X): X is (Aix, px) \\n R(Y): Y is (Ay, py) \\n R(Z): Z is (f(AY, Ay), f(p x, py)) \\n It is this version of the extension principle that is the basis \\n for computation with Z-numbers. Now, one may want to \\n know if f(pp) is compatible with f(AA). \\n Turning to computation with Z-numbers, assume for sim \\n plicity that *=Sum. Assume that Z (AB) and Z (A B). Our problem is to compute the sum Z=X-Y. Assume that \\n the associated Z-valuations are (X, A, B), (Y, A, B) and \\n (Z. A2 B2). \\n The first step involves computation of p2. To begin with, let \\n us assume that p and pare known, and let us proceed as we did in computing the sum of Z'-numbers. Then \\n or more concretely, \\n In the case of Z-numbers what we know are not pand p but restrictions on p and p \\n ?ey (u)py (ii)d it is By \\n R \\n In terms of the membership functions of B and B, these restrictions may be expressed as: 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n He has a produ) R \\n Additional restrictions on p and pare: \\n (compatibility) J. Hay (u)du \\n up A (u)du \\n upy (u)du = R - (compatibility) ? y J. Hay (u)du \\n Applying the extension principle, the membership func \\n tion of p may be expressed as: \\n Subject to \\n J. Hay (u) du \\n up A (u) du \\n upy (u) du = R - ? y J. Hay (u) du \\n In this case, the combined restriction on the arguments is \\n expressed as a conjunction of their restrictions, with m inter \\n preted as min. In effect, application of the extension principle \\n reduces computation of p2 to a problem in functional optimi \\n zation. What is important to note is that the solution is not a \\n value of plbutarestriction on the values of p, consistent with \\n the restrictions on p and p. \\n At this point it is helpful to pause and Summarize where we \\n stand. Proceeding as if we are dealing with Z'-numbers, we \\n arrive at an expression for pas a function of pand p. Using \\n this expression and applying the extension principle we can \\n compute the restriction on pe which is induced by the restric \\n tions on p and p. The allowed values of p consist of those values of p2, which are consistent with the given information, with the understanding that consistency is a matter of degree. \\n The second step involves computation of the probability of \\n the fuzzy event, Z is A2, given pe. As was noted earlier, in fuzzy logic the probability measure of the fuzzy event X is A. \\n where A is a fuzzy set and X is a random variable with probability density p is defined as:\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 87}), Document(page_content='US 8,873,813 B2 \\n 11 \\n 4 (a)p(a)du R \\n Using this expression, the probability measure of AZ may \\n be expressed as: \\n B = A, op.(a)du, 10 R \\n where \\n H.A. (u) = Sup, (pt A (v) A play (u - V)) \\n 15 \\n It should be noted that B2 is a number when pe is a known probability density function. Since what we know about p is its possibility distribution, u(p2), B2 is a fuzzy set with membership function le. Applying the extension principle, we arrive at an expression for ua More specifically, 2O \\n HBZ (w) = supplpz (Pz) \\n subject to 25 \\n where u(p2) is the result of the first step. In principle, this 30 completes computation of the Sum of Z-numbers, Z and Z. \\n In a similar way, we can compute various functions of \\n Z-numbers. The basic idea which underlies these computa tions may be summarized as follows. Suppose that our prob \\n lem is that of computing f(ZZ), where Z and Z are \\n Z-numbers, Z (AB) and Z (AB), respectively, and f(ZZ)-(A2B2). We begin by assuming that the underlying \\n probability distributions p and pare known. This assump tion reduces the computation of f(ZZ) to computation of \\n f(Z.Z), which can be carried out through the use of the version of the extension principle which applies to restric tions which are Z-numbers. At this point, we recognize that \\n what we know are not p and pbut restrictions on p and p. Applying the version of the extension principle which relates \\n to probabilistic restrictions, we are led to f(ZZ). We can compute the restriction, B, of the scalar product of f(AA) and f(pp.). Since A2-f(AA), computation of B2 com pletes the computation of f(ZZ). \\n It is helpful to express the Summary as a version of the \\n extension principle. More concretely, we can write: 35 \\n 40 \\n 45 \\n 50 \\n Z=f(X, Y) \\n X is (Ax, Bx) (restriction on X) \\n Y is (Ay, By) (restriction on Y) 55 \\n Z is (AZ, BZ) (induced restriction on Z) \\n AZ = f(Ax, Ay) (application of \\n extension principle for fuzzy numbers) \\n where p and pare constrained by: 12 \\n -continued \\n ?us, a produ is By R \\n In terms of the membership functions of Band B, these restrictions may be expressed as: \\n He (Ius opx(adu) \\n He (Ius (a) produ) \\n Additional restrictions on p and pare: \\n Consequently, in agreement with earlier results we can \\n write: (compatibility) \\n tlet Ay (u)dit \\n R - - - - - - (compatibility) J. Hay (u)du \\n sup (es (?tay upstadt) up (?tay up adu) \\n subject to \\n up A (u)du R - J. H.A. (u)du \\n What is important to keep in mind is that A and B are, for the most part, perception-based and hence intrinsically \\n imprecise. Imprecision of A and B may be exploited by mak \\n ing simplifying assumptions about A and B assumptions \\n that are aimed at reduction of complexity of computation with \\n Z-numbers and increasing the informativeness of results of computation. Two examples of Such assumptions are \\n sketched in the following. Briefly, a realistic simplifying assumption is that p and p \\n are parametric distributions, in particular, Gaussian distribu tions with parameters m, O, and my O., respectively. \\n Compatibility conditions fix the values of m and m. Con \\n sequently, if b and b are numerical measures of certainty,', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 88}), Document(page_content=\"US 8,873,813 B2 \\n 13 \\n then b and by determine p and p, respectively. Thus, the assumption that we know b and b is equivalent to the assumption that we know p and p. Employing the rules \\n governing computation of functions of Z'-numbers, we can \\n compute B2 as a function of b and by. At this point, we recognize that Band Bare restrictions on bandby, respec tively. Employment of a general version of the extension principle leads to B2 and completes the process of computa \\n tion. This may well be a very effective way of computing with \\n Z-numbers. It should be noted that a Gaussian distribution \\n may be viewed as a very special version of a Z-number. Another effective way of exploiting the imprecision of A and B involves approximation of the trapezoidal membership function of Abyan interval-valued membership function, A. where A is the bandwidth of A (FIG.3). Since A is a crisp set, \\n we can write: \\n where BxB is the product of the fuzzy numbers Band B. Validity of this expression depends on how well an inter Val-Valued membership function approximates to a trapezoi \\n dal membership function. \\n Clearly, the issue of reliability of information is of pivotal importance in planning, decision-making, formulation of \\n algorithms and management of information. There are many important directions which are explored, especially in the \\n realm of calculi of Z-rules and their application to decision analysis and modeling of complex systems. \\n Computation with Z-numbers may be viewed as a gener \\n alization of computation with numbers, intervals, fuzzy num \\n bers and random numbers. More concretely, the levels of generality are: computation with numbers (ground level 1); computation with intervals (level 1); computation with fuzzy \\n numbers (level 2); computation with random numbers (level 2); and computation with Z-numbers (level 3). The higher the level of generality, the greater is the capability to construct \\n realistic models of real-world Systems, especially in the \\n realms of economics, decision analysis, risk assessment, planning, analysis of causality and biomedicine. \\n It should be noted that many numbers, especially in fields \\n Such as economics and decision analysis are in reality Z-num bers, but they are not currently treated as such. Basically, the \\n concept of a Z-number is a step toward formalization of the \\n remarkable human capability to make rational decisions in an environment of imprecision and uncertainty. \\n Now, in the next section, we discuss our inventions and embodiments, extending the concepts above, as well as other applications and examples, incorporating various other tech \\n nologies, including new concepts, methods, systems, devices, processes, and technologies. \\n SUMMARY OF THE INVENTION \\n Here, we introduce Z-webs, including Z-factors and Z-nodes, for the understanding of relationships between \\n objects, Subjects, abstract ideas, concepts, or the like, includ \\n ing face, car, images, people, emotions, mood, text, natural \\n language, Voice, music, video, locations, formulas, facts, his torical data, landmarks, personalities, ownership, family, \\n friends, love, happiness, social behavior, voting behavior, and the like, to be used for many applications in our life, including \\n on the search engine, analytics, Big Data processing, natural language processing, economy forecasting, face recognition, \\n dealing with reliability and certainty, medical diagnosis, pat \\n tern recognition, object recognition, biometrics, security \\n analysis, risk analysis, fraud detection, satellite image analy \\n sis, machine generated data analysis, machine learning, train 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n ing samples, extracting data or patterns (from the video, \\n images, text, or music, and the like), editing video or images, \\n and the like. Z-factors include reliability factor, confidence\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 89}), Document(page_content=\"65 14 \\n ing samples, extracting data or patterns (from the video, \\n images, text, or music, and the like), editing video or images, \\n and the like. Z-factors include reliability factor, confidence \\n factor, expertise factor, bias factor, truth factor, trust factor, validity factor, “trustworthiness of speaker”, “sureness of ”, “statement helpfulness”, “expertise of speaker, speaker. \\n “speaker's truthfulness”, “perception of speaker (or source of \\n information)”, “apparent confidence of speaker”, “broadness \\n of statement, and the like, which is associated with each \\n Z-node in the Z-web. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 shows a membership function and the probability \\n density function of X, as an example. \\n FIGS. 2a and 2b show various examples off-mark. \\n FIG.3 shows the structure of a membership function, as an example. \\n FIG. 4 shows one embodiment for the Z-number estimator \\n or calculator device or system. FIG.5 shows one embodiment for contextanalyzer system. \\n FIG. 6 shows one embodiment for analyzer system, with multiple applications. \\n FIG. 7 shows one embodiment for intensity correction, editing, or mapping. \\n FIG. 8 shows one embodiment for multiple recognizers. \\n FIG.9 shows one embodiment for multiple sub-classifiers and experts. \\n FIG. 10 shows one embodiment for Z-web, its compo nents, and multiple contexts associated with it. \\n FIG. 11 shows one embodiment for classifier for head, \\n face, and emotions. \\n FIG. 12 shows one embodiment for classifier for head or \\n face, with age and rotation parameters. \\n FIG. 13 shows one embodiment for face recognizer. \\n FIG. 14 shows one embodiment for modification module \\n for faces and eigenface generator module. \\n FIG. 15 shows one embodiment for modification module \\n for faces and eigenface generator module. \\n FIG. 16 shows one embodiment for face recognizer. \\n FIG. 17 shows one embodiment for Z-web. \\n FIG. 18 shows one embodiment for classifier for accesso \\n 1S. \\n FIG. 19 shows one embodiment for tilt correction. \\n FIG. 20 shows one embodiment for context analyzer. FIG.21 shows one embodiment for recognizer for partially hidden objects. \\n FIG.22 shows one embodiment for Z-web. \\n FIG. 23 shows one embodiment for Z-web. \\n FIG. 24 shows one embodiment for perspective analysis. \\n FIG. 25 shows one embodiment for Z-web, for recollec \\n tion. \\n FIG. 26 shows one embodiment for Z-web and context analysis. \\n FIG.27 shows one embodiment for feature and data extrac \\n tion. \\n FIG. 28 shows one embodiment for Z-web processing. \\n FIG. 29 shows one embodiment for Z-web and Z-factors. \\n FIG. 30 shows one embodiment for Z-web analysis. \\n FIG. 31 shows one embodiment for face recognition inte grated with email and video conferencing systems. \\n FIG. 32 shows one embodiment for editing image for advertising. \\n FIG. 33 shows one embodiment for Z-web and emotion \\n determination. \\n FIG. 34 shows one embodiment for Z-web and food or \\n health analyzer.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 89}), Document(page_content='US 8,873,813 B2 \\n 15 \\n FIG. 35 shows one embodiment for a backward chaining inference engine. \\n FIG. 36 shows one embodiment for a backward chaining \\n flow chart. \\n FIG. 37 shows one embodiment for a forward chaining \\n inference engine. \\n FIG. 38 shows one embodiment for a fuzzy reasoning \\n inference engine. \\n FIG. 39 shows one embodiment for a decision tree method \\n or system. \\n FIG. 40 shows one embodiment for a fuzzy controller. \\n FIG. 41 shows one embodiment for an expert system. \\n FIG. 42 shows one embodiment for determining relation \\n ship and distances in images. \\n FIG. 43 shows one embodiment for multiple memory unit \\n Storage. \\n FIG. 44 shows one embodiment for pattern recognition. \\n FIG. 45 shows one embodiment for recognition and stor \\n age. \\n FIG. 46 shows one embodiment for elastic model. \\n FIG. 47 shows one embodiment for set of basis functions or \\n filters or eigenvectors. \\n FIG. 48 shows one embodiment for an eye model for basis object. \\n FIG. 49 shows one embodiment for a recognition system. \\n FIG.50 shows one embodiment for a Z-web. \\n FIG. 51 shows one embodiment for a Z-web analysis. \\n FIG. 52 shows one embodiment for a Z-web analysis. \\n FIG. 53 shows one embodiment for a search engine. \\n FIG. 54 shows one embodiment for multiple type transfor \\n mation. \\n FIG. 55 shows one embodiment for 2 face models for \\n analysis or storage. \\n FIG. 56 shows one embodiment for set of basis functions. \\n FIG. 57 shows one embodiment for windows for calcula \\n tion of “integral image, for sum of pixels, for any given \\n initial image, as an intermediate step for our process. \\n FIG. 58 shows one embodiment for an illustration of \\n restricted Boltzmann machine. \\n FIG. 59 shows one embodiment for three-level RBM. \\n FIG. 60 shows one embodiment for stacked RBMs. \\n FIG. 61 shows one embodiment for added weights between \\n visible units in an RBM. \\n FIG. 62 shows one embodiment for a deep auto-encoder. \\n FIG. 63 shows one embodiment for correlation of labels \\n with learned features. \\n FIG. 64 shows one embodiment for degree of correlation or conformity from a network. \\n FIG. 65 shows one embodiment for sample/label generator \\n from model, used for training \\n FIG. 66 shows one embodiment for classifier with multiple \\n label layers for different models. \\n FIG. 67 shows one embodiment for correlation of position \\n with features detected by the network. FIG. 68 shows one embodiment for inter-layer fan-out \\n links. \\n FIG. 69 shows one embodiment for selecting and mixing \\n expert classifiers/feature detectors. \\n FIGS. 70 a-b show one embodiment for non-uniform seg \\n mentation of data. \\n FIGS.71 a-b show one embodiment for non-uniform radial \\n segmentation of data. \\n FIGS. 72 a-b show one embodiment for non-uniform seg \\n mentation in Vertical and horizontal directions. \\n FIGS. 73 a-b show one embodiment for non-uniform trans \\n formed segmentation of data. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n FIG. 74 shows one embodiment for clamping mask data to \\n a network. \\n FIGS. 75 a, b, c show one embodiment for clamping \\n thumbnail size data to network. \\n FIG. 76 shows one embodiment for search for correlating objects and concepts. \\n FIGS. 77 a-b show one embodiment for variable field of \\n focus, with varying resolution. FIG.78 shows one embodiment for learning via partially or \\n mixed labeled training sets. \\n FIG. 79 shows one embodiment for learning correlations \\n between labels for auto-annotation. \\n FIG. 80 shows one embodiment for correlation between \\n blocking and blocked features, using labels. \\n FIG. 81 shows one embodiment for indexing on search system. \\n FIGS. 82 a-b show one embodiment for (a) factored weights in higher order Boltzmann machine, and (b) CRBM \\n for detection and learning from data series. \\n FIGS. 83 a, b, c show one embodiment for (a) variable frame size with CRBM, (b) mapping to a previous frame, and (c) mapping from a previous frame to a dynamic mean. \\n DETAILED DESCRIPTION OF THE \\n EMBODIMENTS', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 90}), Document(page_content='DETAILED DESCRIPTION OF THE \\n EMBODIMENTS \\n Now, we discuss the various embodiments of our current \\n invention: \\n Approximate Z-Number Evaluation: In this section, we present a method for approximate evalu \\n ation of Z-Numbers, using category sets of probability distri butions corresponding to similar certainty measures. All the figures are displayed in Appendix 1, as color images. This is \\n also (partially) the subject of a paper (pages 476-483 of the conf. proceedings) and presentation given at an international Fuzzy confin Baku, Azerbaijan, on Dec. 3-5, 2012 (“The 2\" \\n World Conference on Soft Computing), by the inventors. Appendix 1 is a copy of the paper at the Baku Conf. Appendix \\n 3 is a copy of the VU graph PowerPoint presentation at the \\n Baku Conf. Appendix 2 is a copy of the handwritten notes, in addition to the teachings of Appendices 1 and 3. All the \\n Appendices 1-3 are the teachings of the current inventors, in \\n Support of the current disclosure, and are incorporated herein. \\n A Z-Number is denoted as an ordered pair (A,B), where A and B are fuzzy numbers (typically perception-based and \\n described in natural language), in order to describe the level of certainty or reliability of a fuzzy restriction of a real-valued \\n uncertain variable X in Z-valuation (X, A,B). (See L. A. Zadeh, \\'A note on Z-numbers.” Inform. Sciences, Vol 181, pp. 2923-2932, March 2011.) For example, the proposition “the price of ticket is usually high”, may be expressed as a Z-valu \\n ation (price or ticket, high, usually). In Z-valuation, the cer tainty component B describes the reliability of the possibilis \\n tic restriction, R, for the random variable X, where \\n R(X):X is A (1) \\n with the reliability restriction given by \\n Prob(X is A) is B (2) \\n In another words, the certainty component B, restricts the probability measure of A, denoted by V. \\n v-Prob(X is A) L(x)p(x) dix (3) \\n where L(X) is the membership function of X in fuZZy set A \\n on X domain, and p is the probability distribution of X. Therefore, the certainty component B indirectly restricts the', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 90}), Document(page_content=\"US 8,873,813 B2 \\n 17 \\n possibilities of various (candidate) hidden probability distri \\n butions of X by: (eq. 4 below) \\n where u(v) is the membership function of the probability \\n measure V in fuzzy set B. \\n Here, we show a method to approximate Z-valuation, based on categories (sets) of p's with similar probability \\n measures (or resulting in similar certainty measure), as an approach to reuse predetermined calculations of probability \\n measures. First, we demonstrate an example of Z-valuation without Such approach, and then, we present an approximate approach to Z-valuation via categorical sets of probability \\n distributions. \\n A. Z-Valuation: Basics \\n The Z-valuation uses the mapping of the test scores given \\n by (4) to each of hidden probability distribution candidates of \\n X (See L. A. Zadeh, 'A note on Z-numbers.” Inform. Sci \\n ences, vol 181, pp. 2923-2932, March 2011. See also R. Yager. “On Z-valuations using Zadeh's Z-numbers.” Int. J. \\n Intell. Syst., Vol. 27, Issue 3, pp. 259-278, March 2012.), collectively referred to as 10 \\n 15 \\n 25 \\n Prob. Distrib. Candidates={p,3, (5) \\n where inumerates different candidates. Fig. 1 of Appendix 1 conceptually illustrates the mapping, where each p, is first mapped to a probability measure of A. V., and then mapped to \\n a test score determined by B, where \\n and 35 \\n ts, le(v). (7) \\n Note that the dot symbol in (Lp) in (6) is used as short hand for the probability measure. Fig. 1 of Appendix 1 shows \\n the test score mapping to hidden probability distribution can \\n didates p, in X, for Z-valuation (XA,B). Via the extension principle, the application of the restric tion (test scores) on p(x) (i.e., probability distribution can didates in X domain) to other entities is illustrated. For example, the restriction on p(x) can be extended to the possibilistic restriction on the corresponding probability dis \\n tributions, p(y), in Y domain, where \\n In such a case, the restrictions can further be extended to the probability measures, w, of a fuzzy set A, in Y domain, based on p(y). The aggregation of the best test scores for w, would determine the certainty component B in Z-valuation \\n (YAB), based on the original Z-valuation (X.A.B.), as indicated in Fig. 2 of Appendix 1, which illustrates the exten sion of test scores to Y domain. Fig. 2 of Appendix 1 is a test score mapping from X domain to Y domain and aggregation \\n of test scores on probability measures, w, for Z-valuation \\n (YAB). For simplicity, as shown in Fig. 2 of Appendix 1, three \\n probability distribution candidates in X domain, p, p, and ps, are assigned test scorests and ts, via certainty restric tion on probability measures V, and V (with p, and ps. having the same probability measure V for A). By applying \\n f(X) to each probability distribution candidate in X domain, 65 we can obtain a corresponding probability distribution in Y \\n domain, denoted as p, which can be used to compute the 40 \\n 45 \\n 50 \\n 55 \\n 60 18 \\n corresponding probability measure of A (assume given), denoted as w, . In this example, p, and pa (mapped from p, and p,) result in the same probability measure wa (or aggregated w bin), while pis (mapped from ps) maps into w. In this simple example, the aggregation of the best test \\n scores for p, denoted as ts(p), in W domain (e.g., in eachw bin) would result in the following membership function for \\n B: \\n In other words, in this scenario, \\n Subject to \\n Will Api. \\n In case of single variable dependency Y=f(X), the prob \\n ability measure w can be evaluated by unpacking the prob \\n ability distribution in Y as illustrated by (9) and transforming \\n the integration over X domain as shown in (10), without explicitly evaluating p, \\n (9) wi = {t Ay py. \\n ?us (y) pity) dy \\n = u(y) X. y i Psi(xi). f(x) dy\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 91}), Document(page_content='the integration over X domain as shown in (10), without explicitly evaluating p, \\n (9) wi = {t Ay py. \\n ?us (y) pity) dy \\n = u(y) X. y i Psi(xi). f(x) dy \\n where denotes the consecutive monotonic ranges of f(X) in X domain, and X, is the solution for f(y), if any, within the monotonic range j, for a given y. This takes into account that the probability (p, dy) for an event within the infinitesimal interval of y, y+dy in Y domain, is the Summation of the infinitesimal probabilities from various infinitesimal inter vals x+dx (if applicable) in X domain, where for each j: \\n dy f(x) dy, \\n Therefore, with repacking the integration (9) in X domain \\n over the consecutive monotonic ranges of f(X), we obtain: \\n wikila (f(x))p(x) dix \\n Furthermore, if f(X) is monotonic (i.e., f(y) has only one \\n solution in X, if any) AND LL is obtained from LL via the extension principle by applying f(X) to A, then w, is guar \\n anteed to be equal to V, for all candidate probability distribu tions p, because LL(y) L(x) for Wy=f(x) in such a case. This also means that in such a case, B becomes equal to B. and no additional computation would be necessary. \\n B. Z-Valuation: Example \\n To illustrate an example of Z-valuation, assume the follow ing is given: (10) \\n X(AB), \\n Y f(X)=(X+2), and \\n Ay. \\n The goal is to determine the certainty value B for the \\n proposition that (Y is A), i.e., the Z-valuation (Y. A, B). For purpose of this example, assume Figs. 3, 4, and 5 of \\n Appendix 1 depict the membership functions for A, B, and \\n A respectively. The function f(X) is also depicted in Fig. 6 of Appendix 1. Fig. 3 of Appendix 1 is the membership function', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 91}), Document(page_content='US 8,873,813 B2 \\n 19 \\n of A, e.g., \"X is around Zero”. Fig. 4 of Appendix 1 is the membership function of B, e.g., “Likely. Fig. 5 of Appen \\n dix 1 is the membership function of A. e.g., “Y is about \\n nine\\'. Fig. 6 of Appendix 1 is a diagram depicting f(X). \\n In this example, the set of candidate probability distribu \\n tion for X was constructed using Normal distributions with \\n mean (m) ranging from -2 to 2 and standard deviation (O.) \\n ranging from O\\' (close to Dirac delta function) to 1.2. Figs. 7 \\n and 8 of Appendix 1 depict the probability measure of A, \\n denoted as V, based on (3) and each of these probability \\n distribution candidates represented by a point on (m, O.) \\n plane. These also illustrate the contour maps of constant probability measures. Figs. 9 and 10 of Appendix 1 depict the \\n test scores (denoted as ts) for each probability distribution candidate, based on the application of certainty component \\n B to each probability measure, V, via (4). Given that B \\n imposes a test score on each V, the probability distribution \\n candidates that form a contour (on (m, O.) plane) for constant \\n V, also form a contour for the corresponding test score. How \\n ever, given that a range of V values may result in the same test \\n score (e.g., for v less than 0.5 or above 0.75, in this example), some test score contours on (m, O.) plane collapse to flat ranges (e.g., for test scores 0 and 1, in this example), as depicted on Figs. 9 and 10 of Appendix 1. \\n By applying (10), we can then determine the probability \\n measure of A (in Y domain), denoted as w, based on the probability distribution candidates in X domain (i.e., bypass ing the direct calculation of the corresponding probability \\n distributions in Y domain). The probability measure w is depicted in Figs. 11 and 12 of Appendix 1 for each probability \\n distribution candidate in (m, O,) plane. Given that each probability distribution candidate is asso \\n ciated with a possibility restriction test score (as shown for example in Fig. 10 of Appendix 1). Such test score can be \\n applied and correlated with the probability measure w (shown for example in Fig. 12 of Appendix 1). A given w (or a w bin) \\n may be associated with multiple test scores as indicated by \\n contours of constant w or regions of very close or similar win Fig. 12 of Appendix 1. \\n Therefore, to assign a final test score to a given w (or w bin) \\n based on (8), we can determine the maximum test score for all w’s associated with the given wbin. The result of an intermediate step for determining the \\n maximum test score for correlated w\\'s (i.e., falling in the same w bin) is illustrated in Fig. 13 of Appendix 1, on the (m, O.) plane (for illustrative comparison with Fig. 11 of Appen \\n dix 1). The resulting maximum test score associated with a given \\n w bin defines the membership function of w (or a value of w representing the w bin) in B, as depicted for this example in \\n Fig. 14 of Appendix 1. As shown in Figs. 11 and 13 of \\n Appendix 1, where w is high, the maximum associated test score is low, resulting in B which represents “significantly \\n less than 25% for this example. Fig. 7 of Appendix 1 is the probability measure of A. V., per each (Normal) probability \\n distribution candidate represented by (m, O.). Fig. 8 of \\n Appendix 1 is the contours of the probability measure of A, \\n V, per each (Normal) probability distribution candidate rep \\n resented by (m, O.). Fig. 9 of Appendix 1 is the test score based on certainty measure B for each (Normal) probability distribution candidate represented by (m, O.). Fig. 10 of Appendix 1 is the test score based oncertainty measure B for each (Normal) probability distribution candidate represented by (m, O.). Fig. 11 of Appendix 1 is the probability measure \\n of A, w, per each probability distribution (Normal) candidate represented by (m, O). 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n Fig. 12 of Appendix 1 is the contours of the probability', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 92}), Document(page_content=\"15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n Fig. 12 of Appendix 1 is the contours of the probability \\n measure of A, w, per each probability distribution (Normal) candidate represented by (m, O.). Fig. 13 of Appendix 1 is \\n the maximum test score for a w-bin associated with each \\n probability distribution (Normal) candidate represented by \\n (m O). Fig. 14 of Appendix 1 is the maximum test scores \\n for w-bins defining the membership function of win fuzzy set \\n B, e.g., “significantly less than 25%'. \\n II. Z-Valuation Using Granular Category Sets \\n A. Predetermined Category Sets: Test Scores, Probability \\n Measures, and Probability Distributions \\n The probability measure of A denoted as V, may be pre determined and reused, given that the integration in (3) may \\n be normalized based on the general shape of the membership \\n function of A and the class/parameters of probability distri \\n bution candidates. In normalized form, for example, a cat egory of normalized membership function may be defined as symmetric trapezoid with its Support at interval -1.1 with a \\n single parameter, B, indicating the ratio of its core to its \\n Support (as shown in Fig. 15 of Appendix 1). Examples of \\n classes of probability distribution are Normal distribution and Poisson distribution, with their corresponding parameters \\n normalized with respect to normalized A. For example, for \\n Normal distribution, the parameters (m, O, ) may be normal ized with respect to halfwidth of the support having the origin \\n of the normalized coordinate translated to cross Zero at the \\n center of the Support. \\n Furthermore, we may reduce the level and complexity of computation in approximating the Z-valuation by using a \\n granular approach. For example, for a category of normalized \\n A (e.g., symmetric trapezoid with B of about 0.5, as shown in Fig. 15 of Appendix 1), we may predetermine relations/map \\n ping (or a set of inference rules) between (fuzzy or crisp) \\n subset of probability distribution candidates (of a given class \\n such as Normal or Poisson distribution) and (fuzzy or crisp) Subsets of probability measures, vs (as for example shown in Fig. 16 of Appendix 1). \\n Let V, denote a category/set of probability measures of A. (e.g., probability measure “High”), where numerates Such categories in V domain. Each V, corresponds to a range or (fuzzy or crisp) subset of probability distribution candidates, \\n denoted by C, whose p, members are defined via the following membership function: (eq. 11, below) \\n Therefore according to (11), we may predetermine C, via a similar method of applying test scores to the probability dis \\n tribution candidates, p, (as for example shown in Fig. 9 of Appendix 1), by replacing B, with V. For example, the cat egories of probability measure V, and V., (shown in Figs. 17 and 18 of Appendix 1, respectively), correspond to \\n the (category) fuzzy sets of probability distribution candi \\n dates, denotes as C, and C, (with labels used in place of j), with a membership function depicted in Figs. 19 and 20 of Appendix 1, respectively. \\n Furthermore, the certainty levels (test scores) may also be made into granular (fuzzy or crisp) sets TS, e.g., in order to reduce the complexity of calculation during the aggregation \\n process of Z-valuation. Index k numerates these test score category sets. Fig. 16 of Appendix 1 may also serve as an \\n example of such categorization (with test score replacing V). In one approach, the certainty component B is granularly \\n decomposed or mapped (or approximately expressed) via\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 92}), Document(page_content=\"US 8,873,813 B2 \\n 21 \\n pairs of probability measure and test score category sets, i.e., (VTS)'s, as for example demonstrated in Fig. 21 of Appen dix 1. In one approach, each relation pair may be further associated with a weight that indicates the degree of map ping of B among the pairs (e.g., when TS is a predefined \\n set). For example: \\n weight = SE (hy, (v) a urs (upy (v)). \\n In one scenario, the decomposition of B may be expressed as series of tuples in the form (VTS, weight) or simply as a matrix with weight, as its elements. Given the correspon dence between C, and V, the granular test score sets TS's are also associated with granular probability distribution candi date sets, C.’s (with the same weight). In another approach, a non-categorical test score (e.g., a \\n fuzzy or crisp set)TS, is determined for each V, (and C), e.g., by using extension principle, based on mapping via B. \\n |lts (ts) supy-roll (li,(v), (12) \\n subject to: ts-up(v). \\n Fig. 15 of Appendix 1 is a membership function parameter \\n B (ratio of core to Support), which adjusts the symmetric \\n trapezoid shape from triangular with (B=0) to crisp with \\n (B=1). Fig. 16 of Appendix 1 shows examples of various granular (fuzzy) sets of probability measures. Fig. 17 of \\n Appendix 1 is membership function of V in V, Fig. 18 of Appendix 1 is membership function of v in V. Fig. 19 of Appendix 1 is membership function of p, in C (with p, represented by its parameters (m, O)). Fig. 20 of Appendix 1 is membership function of p, in C, (withp, represented by its parameters (m, O)). Fig. 21 of Appendix 1 is an example of granularizing/mapping of B via (VTS) pairs. B. Computation and Aggregation Via Normalized Catego \\n 1S \\n One advantage of reusing the predetermined normalized \\n categories is the reduction in number of calculations, such as the integration or Summation in determining probability mea \\n sures per individual probability distribution candidates in X domain or their corresponding probability distributions in Y \\n domain, per (4) and (8). In addition, instead of propagating \\n the test scores via an individual probability distribution can \\n didate, the extension of the test scores may be done at a more granular level of the probability distribution candidate sub \\n sets, C, which are typically far fewer in number than the individual probability distribution candidates. However, the \\n aggregation of test scores for Z-valuation, e.g., for (YAB), \\n will involve additional overlap determination involving vari \\n ous normalized category sets, as described below. The normalization of symmetrical trapezoid membership \\n function A, e.g., “Y is about nine as shown in Fig. 5 of Appendix 1, involves shifting the origin by-9 and Scaling the \\n width by 0.5 (in Y domain) in order to match the position and width of the support to the normalized template depicted in \\n Fig.15 of Appendix 1 (with B=0 determined as the ratio of the \\n core to Support). Note that such normalization (translation and Scaling) also impacts the location and Scaling of associ \\n ated p’s (e.g., mean and standard deviation) in order to pre serve the probability measure of A per (8). Note that the predetermined categorical subset of probabil \\n ity distributions in Y domain, denoted as C, that is associ ated with V may be distinct from the corresponding one in X domain, denoted as C. e.g., due to parameters such as f (or the class of the membership. Such as trapezoid or ramp). For 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 22 \\n example, Fig. 22 of Appendix 1 illustrates the membership \\n function of C. for normalized A (3-0), for comparison with C. depicted in Fig. 20 of Appendix 1, for the same values of normalized probability distribution parameters. Fig. 22 of Appendix 1 is membership function of p, in C, (with p, represented by its parameters (m. O)). i) Mapping in X Domain\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 93}), Document(page_content='In one approach to estimate (10), we may determine (or approximate) u(f(x)) in X domain as for example depicted in Fig. 23 of Appendix 1, labeled u(x). Then, we may proceed with mapping and normalization of the membership \\n function to one or more normalized categories of membership functions (e.g., a symmetric trapezoid shape with (3-0)). Fig. \\n 23 of Appendix 1 is membership function u(x). In such an approach, the normalization effects on AX and A are combined into a transformation operation, T. (e.g., translation \\n and Scaling) used to also transform the normalized probabil \\n ity distribution parameters (e.g., mean and standard devia \\n tion). Thus, T also transforms the predetermined subsets of \\n probability distribution candidates, C., to C.?, e.g., via the extension principle, as follows: \\n \\'...) = (13) Hey (Pk) spect (P.) \\n Subject to \\n p = T(p xi), \\n where px.\" represents the transformed probability distri \\n bution candidate (in X domain) from px. Since in our example, LL (depicted in Fig. 3 of Appendix \\n 1) is already in a normalized form, we focus on the transfor mation due normalization of u(x). Note that in Fig. 11 of Appendix 1, the outline of probability measure w for (O-0+) \\n is the same as the membership function u(x) prior to the normalization, as depicted in Fig. 23 of Appendix 1. To nor \\n malize LL(x), the membership function must be scaled by factor of about 3, denoted by s, and translated by the amount of-3 (or -1 before scaling), denoted by t. The ordered trans lation and Scaling operations, denoted by T, and T respec \\n tively, define the transformation operation which also trans forms a probability distribution (13) by scaling and translating its parameters, for example: \\n with \\n T. px; T (mx.Ox)–(Smx, SOx), \\n Once normalized, u(x) is associated with a predeter mined subset(s) of normalized probability distributions, C’s (e.g., as shown in Figs. 22.24 and 25 of Appendix 1 for j as “High.” “Med,” and “Med-Low” (or “ML\\'), respec tively). To associate C, with the test score value(s) (e.g., TSy) assigned to C (shown for example in Fig. 20 of Appendix 1 with n as “High”), the relative position and scal ing of C, and Cx are adjusted by transforming Cy to Cx, per (13), to determine the intersection between Cx.\" and C. for example by: \\n (15) \\n where I, describes a grade for overlap between Cx, and C. Fig. 26 of Appendix 1 schematically illustrates the (fuzzy) intersection of Cx, and C with n being \"High”', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 93}), Document(page_content=\"US 8,873,813 B2 \\n 23 \\n and being “ML, based on the predetermined category sets C., and C from Figs. 20 and 25 of Appendix 1, respec tively. Fig. 24 of Appendix 1 is membership function C. Fig. 25 of Appendix 1 is membership function C. Fig. 26 of Appendix 1 is illustrating the fuzzy intersection of C, and C., where C., is transformed from C, via scaling and translation. For the predetermined category sets C, and C. Cz and Cyr, are used from Figs. 25 and 20 of Appendix \\n 1. \\n For example, as shown in Fig. 26 of Appendix 1, C., Overlaps C (to a degree), while it may not intersect C, (which is depicted in Fig. 24 of Appendix 1). If I, exceeds an (optional) overlap threshold value, then we may apply the category test score TS associated with C. to C. Note that the association with TS was determined based on B, e.g., through mapping of Lla to the relation pairs (V. TS). This means that the category set of probability measures V, associated with C may get associated with category test score TSr. as well. In general, Va., and V may be sets of probability measures belonging to the same family of sets (i.e., without X or Y dependencies). The steps from B to approximating B is conceptually summarized as: \\n inap Bx - (Vx, TS xk) \\n By - Cx C. - (Vy, TSX.) By. lin Ayl, Ayx - Cy. \\n The determination of the test scores for V may be imple mented via a set of fuzzy rules linking C., and C. For example, the antecedent of each rule is triggered if the corre sponding L, is above an overlap threshold, and the conse quent of the rule assigns TS's (or an aggregate of TS's based on weight, for a given n) to a variable SC. A simpler test score assignment rule may use a non-categorical test \\n score TSr., which is determined for each Vy, e.g., via (12), based on the mapping through B: \\n Rule, if (I) then (SCy is TSx.) (16) \\n However, in correlation/aggregation of assigned (fuZZy) \\n test scores to variable SC, we must consider the maximiza tion of test score required by (8). For example, in aggregating \\n the rules for SC, we may use C-cuts to determine an aggre gated (fuzzy) result, denoted as AGSC as follows: (Eq. 17 below) \\n AGSCy = MAX(Correl(I, TSy)) \\n where Correl(ITS) modifies the membership function of TS by correlating it with the factor I, e.g., via scaling or truncation. Membership function of B is then approxi \\n mated by a series of fuzzy relations (V. AGSC). For a given w (probability measure of A), L(w) may be \\n approximated as a fuZZy number (or a defuZZified value), by further aggregation using fuzzy relations (V. AGSC), e.g.: (Eq. 18 below) \\n Hey (w, ts) = supply (w) a u AGSC (ts). f \\n ii) Overlap Approximation \\n An approach to approximate or render the overlap (15) \\n between the category sets, such as C. may use C-cuts to 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 24 \\n present each crisp C-cuts of predetermined category set as a \\n set of points in (m,O) space. These sets of points may be \\n modeled efficiently, e.g., based on graphical models, opti \\n mized for fast transformation and intersection operations. For example, the models that use peripheral description for the \\n C-cuts allow robust and efficient determination of intersec \\n tion and avoid the need to transform all the points within the set individually, in order to reduce the computation involved \\n in (13). iii) Estimation Using Contour Approach \\n In addition to predetermining C. based on V for a normalized set A, we can predetermine various C-cuts of \\n probability measures (e.g., depicted as contours of constant V \\n in Figs. 7 and 8 of Appendix 1) or various C-cuts of associated \\n test scores (e.g., depicted as contours of constant test scores, \\n ts, in Figs. 9 and 10 of Appendix 1) for a set of predefined \\n (e.g., most frequently used) B components. These C-cuts \\n that represent sets of probability distribution candidates in \\n (m.O) space (already associated with specific test scores) may\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 94}), Document(page_content='(e.g., most frequently used) B components. These C-cuts \\n that represent sets of probability distribution candidates in \\n (m.O) space (already associated with specific test scores) may \\n be transformed per (13) and intersected with C in extend ing their test scores to V. In essence, this is similar to the previous analysis except Va., and TSr., become singleton, and C, becomes a crisp set, while C, and V are predeter mined (crisp or fuZZy) set. \\n Another approach uses (e.g., piecewise) representation of \\n B (not predefined) where based on inspection or description, \\n key values of V associated with key values of test scores may \\n readily be ascertained (e.g., based on C-cuts), resulting in a \\n set of (vts,) pairs. Then, the predetermine C-cuts of prob \\n ability measures (e.g., depicted as contours of constant V in \\n Figs. 7 and 8 of Appendix 1) are used to interpolate the \\n contours of constant ts, sin (m.O) space, based on the corre \\n sponding V, values. Again, these crisp contours of constant \\n (crisp) ts,’s, may be transformed and intersected with C to extend the test scores to V, for estimating B. For quick estimation of B in an alternate approach, the \\n predetermined C-cuts (i.e., ws) of probability measures for \\n normalized A may be used (similar to those shown in Figs. 7 \\n and 8 of Appendix 1 based on A), in essence, turning V to a singleton and C to a crisp set (contour) for carrying out the intersect determination. The estimates for u(w) may be determined via interpolation between the aggregated test \\n score results obtained those w values associated with the \\n C-CutS. \\n In one embodiment, for Z-number analysis, for probability \\n distributions analysis, the predetermined categories of hidden \\n probability distribution candidates and normalized Fuzzy \\n membership functions facilitate the pre-calculation of prob \\n ability measures and their associated reliability measures in Z \\n evaluation or as Z-factors, for fast determination of the reli \\n ability levels of new propositions or conclusions. This \\n approach opens the door to the extension of the reliability \\n measures (e.g., via extension principle) to new propositions, \\n based on graphical analysis of contours (C-cuts) of similar \\n probability measures in the domain of parameters represent \\n ing the probability distribution candidates. Basically, we will \\n use the transformation and mapping of categorical set of the \\n probability distribution candidates (represented as regions or \\n C-cut contours) for extension of the reliability measures. This way, as we pre-calculate and store the shapes and results in \\n our library or database for future use (as templates), the new \\n analysis on any new data can be much faster, because we can readily match it with one of the templates, whose results are \\n already calculated and stored, for immediate use.', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 94}), Document(page_content='US 8,873,813 B2 \\n 25 \\n Now, let\\'s look at Appendix 2. In one embodiment, refer ring to the top Fig. and derivation on page 1 of Appendix 2, we \\n have different values of V, based on various C-cuts (with (ts-C)). Then, we match against category (singleton) V. (see \\n the bottom Fig. on page 1 of Appendix 2). Then, on Fig. and 5 \\n derivation on page 2 of our Appendix 2, we get a series of the curves. We use the predetermined contours C of probabil ity measures V. Note that (V, p, LL\"-e). Note that p,’s define the contour(s) for V., (or regions of p,’s) defining region(s) for V., (such as 0 or 1), to interpolate and determine 10 contours (or regions) of constant denoted by C. These C.\\'s are associated with test scores set by a, i.e. (ts-C) for C ona Then, on Fig. and derivation on page 3 of our Appendix 2. \\n we transform or do other manipulations, according to exten- 15 \\n sion rules (e.g. on normalized) for L: \\n C.\" -T(C., m) \\n While maintaining the test score for C.,\" (as C.). Based on categories of w (similar to V., except for w). 20 probability measure of AinY-domain, where we are single tons (predefined), have corresponding contours (or regions) \\n C (see the figure on the bottom of page 3 of our Appendix 2). Then, we find the intercepts between Co., and Cifany, i.e. \\n I Crai f 25 Then, on Fig. and derivation on page 4 of our Appendix 2. \\n based on the intercepts, we find the best test score for a given C extended from C.\", e.g.: \\n tiss Supw.C.\\' 30 \\n where I exists. (i.e., the best test score from intercept points to a given \\n C.) Now, we associate ts, to w, to construct (Lloy (w)), and interpolate for other w (see the figure on the bottom of page 4 35 \\n of our Appendix 2). Since ts,’s source is C, ts\\'s appear as C-cuts in L, as well. Then, on derivation on page 5 of our Appendix 2, we have: \\n Where the scenario involves e.g. Z f(x,y), instead of y=f(x) \\n (where the solution may be worked out in the X-domain), we 40 can still use contours (or regions) of specific test scores (e.g. \\n based on C-cuts), and contours determined by interpolation of predefined or predetermined probability measure contours or regions. The manipulation, e.g. (pp.Op.), can be imple \\n mented based on contours or regions of constant test scores 45 \\n (for X or Y), instead of individual p, and p, to reduce the number of combinations and calculation. The test scores can \\n be extracted from X, Y domains to Z domain (in this example) and maximized based on the intercept points in p domain with predetermined contours of probability measures of(nor- 50 \\n malized) A2, to again calculate L2. \\n FIG. 4 is a system for Z-number estimation and calcula \\n tion, with all related modules and components shown in the Figure, with a processor or computing unit in the middle for \\n controlling all the operations and commands (Z-number esti- 55 \\n mator). Thus, in Summary, the above section provides the methods for approximation or calculation or manipulation of Z-num \\n bers, and related concepts. Now, we explain other compo \\n nents of our inventions, below. 60 \\n Thumbnail Transformation \\n In one embodiment, the input data (e.g., image) is prepro \\n cessed. For example, the image is transformed into a smaller \\n thumbnail that preserve the high level nature of the image content, while not necessarily preserving its unique charac- 65 \\n teristics. This may be achieved, for example, by down Sam pling or aggregation of neighboring pixels. Other methods 26 \\n may include reduction of the variable space by consolidating \\n the colors into intensity (e.g., gray Scale) and/or reducing the \\n number of bits representing color or intensity. Such a trans \\n formation is denoted as thumbnail. \\n A thumbnail includes less resolution and data, and hence, it \\n contains less overall detailed features. The purpose is to sim plify the task of dealing with many pixels while still manag \\n ing to detect the high level features associated with the images', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 95}), Document(page_content='contains less overall detailed features. The purpose is to sim plify the task of dealing with many pixels while still manag \\n ing to detect the high level features associated with the images \\n (or other type of data). For example, using a thumbnail, a recognition module quickly identifies the presence of a head \\n or face (while not intended to necessarily determine the iden tity of the person or object). \\n One embodiment uses a preliminary search to detect main features in a thumbnail data/image for fast computation. In \\n one embodiment, the limitation may be on the number of pixels on the visual layer (via preprocessing). In one embodi \\n ment, the limitation is imposed on the detection/classifier network (e.g., on hidden layers) itself. For example, the main \\n features are learned and isolated (e.g., by units or neurons of higher hidden layers) or learned by targeted attempt (e.g., by \\n keeping all other weights and letting the weight on certain \\n units change when learning a certain feature.) \\n Feature Detection and Learning In one embodiment, for example where labeled training samples may be difficult to prepare or scarce, the training is \\n done with unlabeled samples to learn the features from the sample details. For example, a restricted Boltzmann machine \\n (RBM) may be used to successively learn the features one layer at a time. \\n A Boltzmann machine refers to a type of stochastic recur rent neural network, where the probability of the state is based on an energy function defined based on the weights/biases \\n associated with the units and the state of Such units. In a \\n Boltzmann machine, some units are denoted visible where the state may be set/clamped or observed and others may be \\n hidden (e.g., those used for determining features). In the \\n Restricted Boltzmann machine (RBM), the weights between hidden units within the same layer are eliminated to simplify the learning process. The learning process tends modifies the \\n weights and biases so that the energy state associated with the samples learned are lowered and the probability of such states \\n is increased. In one embodiment, the state of hidden layers are presented by a stochastic binary variable (e.g., in 0, 1 range) \\n based on a sigmoid Such as logistic function. In one embodi ment, the energy function is given as \\n i \\n where V, and h, denote the state of the i\\' visible unit and the j\" hidden unit (as for example depicted in FIG. 58), respec \\n tively, and b, and c, are bias or threshold associated to such units, respectively. W, is an undirected weight or connection strength linking Such units. Per Boltzmann machine, the \\n probability of the state C. (for a given set of H and V states of the units) depends on the weights (including bias values) and \\n the state of H and V:', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 95}), Document(page_content=\"US 8,873,813 B2 \\n 27 \\n where E, is the energy associated with state a; T denotes the “Temperature' of the system; the denominator denotes \\n the “partition function'. Z; and B denotes any state of the system. Since the energy of a state is proportional to negative \\n log probability of the state, the probability that a binary sto \\n chastic unit is at state 1 (or ON) in such RBM becomes the following logistic function: \\n 1 \\n -AEi 1 + eT Pi is ON \\n where T controls relative width of the above logistic func tion, and AE, (for example for a hidden unit) is given by: \\n Note that in an embodiment with T is set to zero, the \\n stochastic nature of the binary units becomes deterministic, i.e., taking the value sigmoid function (Zero or one), as in Hopfield Network. \\n In one embodiment, the training attempts to reduce the \\n Kullback-Leibler divergence, G, between the distributions of \\n V states based on the training sets and based on thermal equilibrium of the Boltzmann machine, by modifying \\n weights and biases, e.g., via a gradient decent over G with respect to a given weight or bias. The aim of training is to determine weights/biases such that the training samples have high probability. In maximizing the average probability of a \\n state V. P(V), with respect to weights, we have \\n 0 will l. = (vihi) - Kvihi), \\n where the average over the data means average over the training data (i.e., when V units sample from the training sets \\n and are clamped to a training sample while hidden units are \\n updated repeatedly to reach equilibrium distribution), and the \\n average over model means the average from Boltzmann \\n machine sampling from its equilibrium distribution (at a givenT). In one embodiment, learning algorithm uses a small \\n learning rate with the above to perform gradient decent. Simi larly, the following can be used in learning bias c, \\n 0ci . F (hi) (hi), it \\n In one embodiment, where the weights are absent between the hidden units, the updating of the hidden states, H, is done in parallel as the hidden units are conditionally independent \\n for a given set of visible states, V. In one embodiment, Sam pling from model involves one or more iterations alternating between updating (in parallel) hidden and visible layers based \\n on each other. In one embodiment, Sampling for the model is \\n Substituted with sampling from reconstruction, which updates the hidden units (for example, in parallel) using the \\n visible units clamped to a training set, then updates the visible \\n units (e.g., in parallel) to get a reconstruction from the fea tures in the hidden layers, followed by updating the hidden \\n units based on the reconstruction. This approach approxi 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 28 \\n mates the gradient decent of contrastive divergence in an \\n efficient and fast manner. In RBM learning, contrastive diver \\n gence can be used instead of maximum likelihood learning \\n which is expensive. In one embodiment, T is lowered from a higher initial value to make low cost (energy) states more \\n probable than high cost states, while the higher initial value of T allows for reaching and sampling equilibrium states \\n quicker. In one embodiment, the stochastic nature of binary \\n units allows escaping from local minima. In one embodiment, \\n during the reconstruction, a Subset of visible units are clamped to input data to reconstruct other visible units from the features including those affected or derived (e.g., Stochas tically) from the input data. The training in Such a conditional \\n Boltzmann machine tends to maximize the log probability of \\n the observed visual units (now taken as output in reconstruc tion), given the input data. \\n In one embodiment, other non-binary discrete stochastic \\n units may be used. In one embodiment, continuous value units may be used. In one embodiment, mean filed units are used having their state (in the range of 0, 1) determined by the total input (e.g., a logistic function) and a noise (e.g., as a\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 96}), Document(page_content='Gaussian). In one embodiment, other stochastic functions/ distributions (e.g., binomial and Poisson) are used for the \\n units. In one embodiment, where continuous data (including \\n semi-continuous data with many levels as opposed to few \\n discrete levels) is used for state of the visible units, the sam pling from a probability distribution (e.g., Gaussian with a \\n given variance, with the mean determined by the other signal and weights) keeps the stochastic nature, while making the \\n signal in visible unit continuous (as opposed to discrete). The hidden layers may stay binary (stochastic). In one embodi \\n ment, stochastic visible units use continuous signal (e.g., in 0, 1 range) based on other signals and weights and a prob \\n ability distribution (e.g., logistic function) for sampling or updating its signal. \\n In one embodiment, following the training of one RBM, \\n another hidden layer is added on top which employs the lower RBMs hidden layer as input to determine higher level fea \\n tures, and the training is done one layer at the time. For example, FIG.59 illustrates 3 level RBM with 3 hidden layers H\\'\\', H\\'), and H\\'. In one embodiment, in training the weights (w) for additional hidden layer (H), the weights \\n for the trained lower layers are fixed. The fixed weights are used to pass data from bottom up to higher layer and to \\n reconstruct from top down based on higher order features. In one embodiment, as for example depicted in FIG. 60, RBMs are stack on top of each other and training is done one layer at \\n the time from bottom up. In one embodiment, the visible units have continuous value state (e.g., logistic units). In one embodiment, in training a higher level RBM (e.g., RBM), signals in its corresponding visible units (e.g., V) are set to \\n the probability values associated with the corresponding hid den units (e.g., H\\') of the previous RBM, while the hidden units (H) themselves are binary stochastic units. In one embodiment, the top hidden layer (e.g., H) has continuous stochastic value, e.g., based on Gaussian probability distribu \\n tion (e.g., with unit variance) having a mean based on the weights (e.g., w\") and signals from its corresponding visible units, V (e.g., logistic units). In one embodiment, the top \\n hidden layer includes a relatively low number of units (e.g., \\n for representing the high level features as low dimensional \\n codes). In one embodiment, hidden units use continuous vari ables for to represent their features/dimensions, e.g., to facili \\n tate classification based on high level features from the top hidden level (e.g., via training one or more correlation layers, \\n or other methods such as SVM). In one embodiment, layer by layer training creates proper features detection in the hidden', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 96}), Document(page_content=\"US 8,873,813 B2 \\n 29 \\n layers to enhance the back-propagation in discrimination. \\n This allows for fine tuning by local search, e.g., via contras tive wake-sleep approach for better generation. In one \\n embodiment, few labeled samples are used to fine tune the \\n classification boundaries after the features have already been \\n determined primarily based on the unlabeled data features. In one embodiment, weights (y) are introduced in the visible layer while training the weights (w) between the visible layer and the hidden layer (e.g., as depicted in FIG. \\n 61). In one embodiment, this approach is also used for higher level RBMs by introducing weights between hidden units of the lower RBM while training the weights for the higher \\n RBM. In this sense, the RBM becomes a semi-restricted \\n Boltzmann machine. In one embodiment, a gradient decent approach for modifying the weights follows the following \\n update contrastive divergence method: \\n Awe ( vh) o_( vh) 1) \\n where superscript 0 indicates the correlation after the ini tial update of hidden layer after clamping the training sample \\n to the visual units, and SuperScript 1 indicates the correlation \\n after the hidden layer is updated next time by the reconstruc \\n tion at the visual layer. In one embodiment, to get to the \\n reconstruction in the visible layer, the visible units are updated one or more times (e.g., iteratively in parallel) based \\n on the current weights, the updated hidden units, and the state \\n of the visible units (from the initial or prioriteration). In one embodiment, the update activity involves stochastic sampling \\n from the probability distribution (e.g., logistic function). \\n Note that e and e' correspond to the learning rate. In one \\n embodiment, the hidden units are updated multiple times \\n before the correlations are used to determine changes in weight. In one embodiment, visible units with continuous value state (e.g., mean field units) are updated in parallel \\n based on the total input to the unit (e.g., based on a logistic \\n function). In one embodiment, intra-layerweights are introduced dur ing the training of a higher hidden layer in order to establish tighter relationships among inter-layer units (e.g., neighbor \\n ing visible units corresponding to neighboring pixels in an \\n image/data). This enforces constraint during generation. In an \\n embodiment, this facilitates the generation of the parts of a larger recognized object that would not fit each other due to \\n loose relationships between corresponding Sub-features. In \\n one embodiment, more features (e.g., redundant) are used to tighten the relationships. In one embodiment, the interrela \\n tions between the features (e.g., constraints or rules) are used to limit the choices (i.e., placement of parts), and the place \\n ment of one feature helps determine the placement of the other features based on the interrelationship between those \\n features. \\n In one embodiment, as for example depicted in FIG. 62, an autoencoder, e.g., a deep autoencoder, is provided by stacking \\n further hidden layers, in reverse order with respect to the lower layer, having the same size and the same corresponding interlayer weights as their corresponding lower layers. While the lower half layers (including the coding layer H) act as a \\n decoder, the added top layers act as encoder to produce simi lar data in V (output) based on the features learned/captured \\n at the coding layer. The added weights in FIG. 62 are depicted with superscript T to indicate that these weights (initially) are represented by the transpose matrix representing the corre \\n sponding weights in the lower layers. In one embodiment, the \\n weights of the autoencoder is fine tuned, e.g., by using a back \\n propagation method based on gradient decent. Since the ini 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 30 \\n tial weights of autoencoder were determined by a greedy \\n pre-training of lower RBMs, the back propagation will be\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 97}), Document(page_content='15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 30 \\n tial weights of autoencoder were determined by a greedy \\n pre-training of lower RBMs, the back propagation will be \\n efficient. In one embodiment, during the back propagation \\n fine tuning, the stochastic binary units are assumed to be \\n deterministic continuous value units adopting the probability \\n value as their state value, to carry out the back propagation. In \\n one embodiment, the objective function (error function) to optimize in back propagation, is the cross entropy error, E. \\n between the data (e.g., image pixel intensity in V layer) and \\n the reconstruction (e.g., the corresponding pixel intensities in V\" output), for a given sample: \\n E = -X (v; logy, + (1 - vi) log(1 - v)) \\n where V, and v, are the state of the i\\' units (or intensity of the image at given pixel corresponding to unit i) associated \\n with V and V\\', respectively. In one embodiment, for the same number of parameters, deep autoencoders tend to produce \\n less generalization errors compared to shallow ones. \\n In one embodiment, the dimensionality of the data is reduced via the coding presentation at the coding layer (e.g., H\\') having few units compared to the number of units in V. \\n In one embodiment, a noise signal is introduced in the top hidden layer units (e.g., H) during training (but the same for the corresponding training data sample used in V layer) to \\n adjust the weights resulting in more bimodal probabilities in \\n order to make the system more resilient against the noise in \\n the data. \\n In one embodiment, the features of the training samples are learned, e.g., via an unsupervised learning algorithm (e.g., by \\n greedy learning by RBMs). Then, the features are correlated \\n or associated with labels from a Subset of training sample, as for example depicted in FIG. 63. Labels are clamped to a set \\n of units (in L. layer) during the training, while data (e.g., \\n image pixels) are clamped to the Vunits. An RBM is added on \\n top to learn the correlation or association between the data features and the labels. During the training, Llayer and one or more hidden layers (e.g., H’) provide data to Clayer (which \\n may bean RBM, as well). Labels may be binary, multi-valued discrete, or continuous. Similarly the weights (e.g., WP) and \\n biases related to the added layer are learned by feeding labels and corresponding Data at L and V layers, respectively. \\n Once the association between the labels and Data is \\n learned, in one embodiment, data is input to V layer, and its corresponding label is ascertained at L layer, by having the \\n units in Clayer drive the units in Llayer. In one embodiment, data samples corresponding to a label may be constructed by \\n clamping unit(s) in L. layer to derive units in C Layer, and \\n followed by a top-down reconstruction in V layer. In one \\n embodiment, a Subset of units in V layer are clamped to input (e.g., to input a partial image or a portion of image) and the \\n state of one or more labels are set in L. layer by clamping to \\n environment. Then, the other unclamped V units are used to \\n determine the state of the other V units (given the clamped visible and label units), deterministically or stochastically \\n (e.g., through iteration). In one embodiment, a larger image may be recovered from partial data (e.g., partial image) \\n through reconstruction. Reliability Measure \\n In one embodiment, the strength of the correlation between data and label or conformity of data to the system (e.g., a \\n trained system) may be determined based on the energy of \\n states given the clamped data (and label). In one embodiment, \\n the strength of correlation or conformity is based on relative', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 97}), Document(page_content='US 8,873,813 B2 \\n 31 \\n probability of various states. For example, the energy differ \\n ence of two states in Boltzmann machine (in equilibrium) is proportional to the log of the ratio of their probabilities. In one \\n embodiment, the relative strength of the correlation or con formity is based on the relative probability of two states. In \\n one embodiment, a baseline for the probability of training samples is established during and/or after training. In one \\n embodiment, the strength of correlation or conformity indi \\n cates how well the state(s) representing the data (and label) fit into the energy landscape of the system. In one embodiment, \\n as depicted in FIG. 64, the strength of correlation or confor mity of a dataset (including any associated label) is used to \\n determine Z-factor associated with the associated features \\n and/or classification of the data from the network. \\n In one embodiment, the quality of the search is evaluated based one or more approaches including for example, the \\n probability, e.g., the total energy of RBM, or the difference between the regenerated data/image and the input, the fre quency the recognized labels change while anchoring the \\n visible units/neurons to the input/image. Learning Based on Models \\n In one embodiment, the learning is achieved through simu lation using a data (and label) sample generation based on one \\n or more models. In one embodiment, a network trained based \\n on model(s) is used to recognize and classify actual data \\n which may not have been seen before. In one embodiment, the system is trained to infer the potential model(s) itself by recognizing the (e.g., observed) data conforming to a particu \\n lar model and its associated labels/parameters. \\n In one embodiment, as for example depicted in FIG. 65, a sample generator is used to provide data (e.g., images) for \\n training. A rendering unit renders the data according to one or \\n more models (e.g., functional, tabular, and/or heuristic) and the corresponding model parameters governing the instantia \\n tion of the model by the rendering unit. In one embodiment, at least a Subset of model parameters are generated stochasti cally (or via a deterministic sequential algorithm) by a ran domizer unit, which for example, uses applicable probability \\n model(s) and/or model rules to generate the subset of model parameters within given ranges or constraints. In one embodi \\n ment, the training of the network (e.g., a deep belief network based on Boltzmann machines) is done repeatedly generating training data samples via the sample generator to feed to the \\n V layer of a network being trained. In one embodiment, the training is done one hidden layer at the time (e.g., until H). \\n In one embodiment, the training of hidden layers is done unsupervised (i.e., without Supplying labeled training \\n samples). In one embodiment, an autoencoder is setup (e.g., \\n as shown in FIG. 65) and fine tuned using back propagation. \\n In one embodiment, a correlation or associative layer is added \\n to learn the correlation between the features of the data and \\n the labels (L), where the labels are supplied by the sample \\n generator (along with the rendered data). In one embodiment, for example as depicted in FIG. 66, multiple L, layers (e.g., \\n in parallel) are used to represent various classes of (e.g., \\n independent) models. In one embodiment, the relevant \\n weights between C layer and an L. layer are fixed for one \\n class of model(s) while training another class of model(s) through the same C layer. In one embodiment, the cross \\n correlation between two models is determined, via cross cor relation (e.g., through layer C) between the labels associates \\n with both models. For example, by a subset of labels from L layer is clamped and sampled generated from top-down \\n reconstruction from layer C to layer LM2 are used to deter \\n mine Such cross correlation. In one embodiment, states on layer Care stochastically run to derive the reconstruction in', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content=\"reconstruction from layer C to layer LM2 are used to deter \\n mine Such cross correlation. In one embodiment, states on layer Care stochastically run to derive the reconstruction in \\n both L and La layers for determining a correlation 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 32 \\n between the reconstructions samples. In one embodiment, the units in layer Care derived (e.g., through inference) from V \\n layer (by inputting data), and labels are reconstructed in lay \\n ers L and L. In one embodiment, the levels of conformity \\n or correlation of data supplied to V units (or a subset of V \\n units) with models(s) are obtained for each model based on relative probabilities and energy of States. In comparing on \\n model to another, the weights associated with one model are not used in determining energy or probability associated with \\n the other model (for Such comparison). \\n In one embodiment, noise is incorporated into the render \\n ing in order to make the network more resilient to noise. In \\n one embodiment, a stochastic noise (e.g., Gaussian) is applied to the rendering, e.g., in illumination, intensity, tex \\n ture, color, contrast, Saturation, edges, Scale, angles, perspec \\n tive, projection, skew, rotation, or twist, across or for por \\n tion(s) of the image. In one embodiment, noise is added to a hidden layer in a reproducible manner, i.e., for a given data sample (or for a given model parameters), in order to adjust \\n the weight to result in a more modal range of activities to \\n increase tolerance for noise. \\n In one embodiment, elastic distortions (as well as affine transformations) are used to expand the size and variety of the training set, e.g., when the training set is produced from a \\n model (such as a rendered data/image) or when the data/ image is provided separately as part of a training set. In one \\n embodiment, Such a distortion is parameterized and rendered by the rendering unit. One embodiment used both affine (e.g., \\n translation, Scaling, reflection, rotation, homothety, shear mapping, and Squeeze mapping) and distorting type transfor \\n mations. In one embodiment, various transformations are rendered to generate training dataset to let the system learn \\n features that are transformation invariant. In one embodi \\n ment, a shape model is generated with various parameters, \\n Such as various textures, colors, sizes and orientations, to let \\n the system learn the invariant features such as the relative positions of the sub features of the modeled shape. In one embodiment, orthogonal matrixes, for example, are used to \\n perform rotation and reflection transformation for rendering the image or on the provided data sample. \\n In one embodiment, the features of a high level model (with parameters) are learned by a system (such as RBM) through training (e.g., unsupervised). For example, in one embodi \\n ment, a 3D model generates various 2D images at different poses (including position, orientation, and Scale) and expres \\n sions/emotions (or illumination), and the system would learn correlation between the images and their features (derived from the model). Then, the model parameters (and their prob abilities) may be obtained for an image. In one embodiment, various samples are generated/ren \\n dered from a 3D model, by varying relative location and angle of the viewer and the model object (e.g., polar coordinates (r. \\n 0, (p)). These variation span various poses (based on 0 and (p) and Scaling (based on r), using other perspective parameters \\n (e.g., derived from camera/viewer's view span). In one embodiment, a 3D model rendering mapped to 2D \\n images is based on the normal vectors at a given point of the \\n 3D model, illumination parameters (e.g., location of light(s) \\n and intensity), and reflectivity and texture model of the sur \\n face. In one embodiment, the location/presence of rigid points from the model improves the accuracy. In one embodi\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content='and intensity), and reflectivity and texture model of the sur \\n face. In one embodiment, the location/presence of rigid points from the model improves the accuracy. In one embodi \\n ment, PIE (pose, illumination, expression) variations are used to generate training data/images (e.g., by rendering in 2D). \\n In one embodiment, multiple models can be learned in combination. E.g., the model for generating of texture of \\n surfaces or colors can be learned in conjunction with a 3D \\n model of head or body. In rendering a 3D model, the texture', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content='US 8,873,813 B2 \\n 33 \\n model may be incorporated to provide textures and colors for \\n the rendered images used for training. The correlation \\n between the model parameters and the rendered images is \\n learned via training. In one embodiment, noise is added to prevent over fitting and regularize the weights to better gen \\n eralize when used with out of sample data/images. \\n In one embodiment, getting a low level of conformity of a data/image (for example based in a conformity measure Such as energy error or probabilities) with a trained system (e.g., \\n based on a model) causes the data to be marked/tagged or included in a set of data to be recognized/classified by other expert Systems/networks. \\n In one embodiment, the model comprises of rules govern ing the parameters, structure, and relationships between vari \\n ous components and Sub-components of the model. In one \\n embodiment, the rules engine is iteratively executed to gen erate sample data for training, by using a rules engine. \\n In one embodiment, the model includes a databases of background and foreground objects (with parameters) or \\n images. In one embodiment, various data samples are created with various background and foreground models to train the system recognize high level features of foreground and back \\n ground (e.g., wide uniform horizontal bands or regions of \\n color/intensity). In one embodiment, generic labels are used \\n to train the correlation between the labels and the features of \\n the background or foreground Scenes. \\n Correlating of Features and Locations of Interest within the Data (e.g., Image) \\n In one embodiment, a location within the image is specified by a continuous value (e.g., in range of 0, 1 to indicate/ identify the location or pixel along a direction (e.g., X or y \\n direction) in the data/image) or a multi-discrete value (e.g., indicating?identifying a range of locations or pixels along a \\n direction in the date/image). In one embodiment, as for example depicted in FIG. 67, a position L in the data (e.g., a \\n pixel map), is represented by its (x, y) coordinate. In one \\n embodiment, X or y may be fuzzy numbers (e.g., with mem bership functions such as triangular, trapezoidal, rectangular, \\n or singular). In one embodiment, the state of a unit (e.g., neurons) is represented by fuZZy values. In one embodiment, \\n information Such as coordinates, width, height, orientation, type of shape, are presented by units in a parameter layer P. In \\n one embodiment, Mlayer(s) are used to provide?approximate \\n the membership function value of a parameter, Such as coor \\n dinate of allocation. The units in M represent the values (or range of values) that a parameter may take. In one embodi ment, a unit in Mlayer corresponds to a pixel (or a range of \\n pixels) along a direction (e.g., X axis) within the image. In one \\n embodiment, one or more units (e.g., continuous valued) in M layer are set to represent the membership function over the pixels (or range of pixels), for example in X axis, correspond ing to the corresponding fuZZy parameter in Player that, for \\n example, represents the X coordinate of L. In one embodi \\n ment, units in M layer are used to train association of for example, a location on the image and the features of the image. In one embodiment, weighted link are made from Por \\n Munits to a correlation layer C for training the association. In \\n one embodiment, weighted links from Mlayer are made to hidden layers to associate parameters to features of the image. \\n In one embodiment, Mlayer(s) includes a unit for every pixel (or a range of pixels) on the image, e.g., full coverage to \\n specify any shape (or blob) in Mlayer for association with the image. \\n In one embodiment, where inter-layer links between units \\n are not fully connected, the connection from Mlayers to units in lower hidden layer(s) are Substantially arranged to spatially \\n resemble or correspond to Munits corresponding pixels (or 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99}), Document(page_content=\"resemble or correspond to Munits corresponding pixels (or 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 34 \\n range of pixels) in the image viewed via V layer. In Such a \\n case, the links from V layer to higher hidden layers are also \\n limited in number of connectivity, and for example, the few \\n links follow a fan out pattern from a 2D layout of V layer to \\n next hidden layer. \\n In one embodiment, blobs (of fuzzy blobs) are provided on Mlayer for association with the image during training. FuZZy \\n blob, for example, may have fractional membership function \\n value at the blob’s edge. In an embodiment, the membership function value in range of 0, 1 is represented by a logistic \\n function in a unit. \\n In one embodiment, the location, area, or focus of interest is provided on M layer with the corresponding training \\n sample in Vlayer, to train the correlation. In one embodiment, the representation of the focus of interest may be a (fuzzy or crisp) border or a region specified parametrically or per pixel. \\n In one embodiment, with a training sample having multiple \\n focuses of interest, the training may be performed by Submit \\n ting the same data (image) with individual focus of interests \\n during the training. In one embodiment, the stochastic nature \\n of C layer will cause reconstruction of focus of interest in M or Players, given an input image (or a portion of image) in V layer. For example, in training face recognition, images \\n including one or more faces are supplied to Vlayer while their \\n corresponding focuses of interest (e.g., the location/size of \\n the face) are supplied to Mor Players, to train the correlation. \\n In one embodiment, the various focuses of interest are itera tively constructed in M or Player by clamping data (e.g., an \\n image) in V to, for example, derive stochastically the corre \\n sponding focuses of interest from C layer. In one embodi ment, the reconstructed parameters are output in Mor Players based on their corresponding probability. \\n In one embodiment, the correlation of image/data to its locations of interest is performed during training by imple \\n menting a representation of such locations on a layer of units laid out to correspond to the image/data (e.g., by linking Such \\n units to a hidden layer above Vlayer). In one embodiment, the position parameters (e.g., location, width/height, type, orien \\n tation) and the coverage parameters (border type, fill type, \\n fuZZy/crisp) are used to render representation of the loca \\n tion(s) of interest on the representation units, e.g., by using a \\n value in range of 0, 1). In one embodiment, the fuZZy type rendering helps avoid making false correlations with other \\n irrelevant features in the image/data, by representing the fea \\n tures of the location of interest as coarse. Fill type rendering \\n identifies a blob where the location of interest is in the image, \\n so that if the features of the interest are in the middle of the \\n location, the training would catch the correlation. Limiting Number of Weights Based on 2D Fan Out Layout \\n In one embodiment, as for example depicted in FIG. 68, the \\n extent of the inter-layer connections are limited for the lower layers (e.g., H' and/or H). In one embodiment, the number \\n of inter-layer connections between the lower layers is sub stantially less than that of fully connected ones. For example, \\n if the (average) number of fan out links per unit, f, is signifi \\n cantly smaller than the number of units in the higher layer, the number of inter-layer connections (or weights) are signifi \\n cantly reduced compared to the fully connected scheme. This \\n scheme helps reduce the complexity of the structure, reduces the over fitting, and enhances generalization. Conversely, the number of fan out links (top-down, e.g., from H' to Vunits) are also limiting a until in the higher layer to relatively few\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99}), Document(page_content=\"units at the lower unit. Therefore, in one embodiment, for example, the number of fan out links from a unit in H' to V \\n units may be about 3 to 10 pixel wide. \\n In one embodiment, there are multiple type of units in a hidden layer (e.g., H'), with each type corresponding to\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99})], [Document(page_content=\"(12) United States Patent \\n Ryabov et al. US009095285B2 \\n (10) Patent No.: US 9,095,285 B2 \\n (45) Date of Patent: Aug. 4, 2015 \\n (54) PORTABLE BIOMETRIC IDENTIFICATION \\n DEVICE USINGADORSAL HAND VEN \\n PATTERN \\n (71) Applicants:Yaroslav Ryabov, Rockville, MD (US); \\n Denis Broydo, Rockville, MD (US) \\n (72) Inventors: Yaroslav Ryabov, Rockville, MD (US); Denis Broydo, Rockville, MD (US) \\n (*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 320 days. \\n (21) Appl. No.: 13/860,669 \\n (22) Filed: Apr. 11, 2013 \\n (65) Prior Publication Data \\n US 2014/0307074 A1 Oct. 16, 2014 \\n (51) Int. Cl. \\n A6B 5/17 (2006.01) \\n H04N 5/33 (2006.01) \\n A61B5/OO (2006.01) \\n GO6K 9/OO (2006.01) \\n (52) U.S. Cl. CPC A61B5/117 (2013.01); H04N 5/33 (2013.01); \\n A61B5/0077 (2013.01); A61 B 5/489 (2013.01); A61 B 5/6825 (2013.01); A61 B 5/6898 \\n (2013.01); G06K 9/00 (2013.01) \\n age : : \\n Acquisition & \\n s&as x \\n 8 \\n Registration \\n to the \\n datatase (58) Field of Classification Search \\n CPC ............. A61B5/117; H04N 5/33; G06K9/00 \\n USPC ........ 348/77, 61,78, 180, 189; 382/117, 115, \\n 382/116, 124, 118; 726/2 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 8,811,681 B2 * 8/2014 Watanabe ..................... 382,115 \\n 2012fO281890 A1* 11/2012 Kamakura et al. ............ 382,126 \\n * cited by examiner \\n Primary Examiner — Jefferey Harold \\n Assistant Examiner — Jean W. Desir \\n (74) Attorney, Agent, or Firm — Nadya Reingand \\n (57) ABSTRACT \\n A portable device for personal identification using a dorsal hand vein-pattern in preferable configuration is disclosed. \\n The mobile device utilizes an on-board camera operating in both visible and near infrared range, a memory unit, a pro cessor and Speeded-Up Robust Features algorithm for image acquisition, processing and comparison against the existing \\n database of hand vein-pattern images. The matching criterion \\n between the images to be used for the person's authentication. Device can optionally use wireless connection for image transferring and processing. \\n 20 Claims, 7 Drawing Sheets \\n image \\n identification \\n Analysis\", metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 0}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 1 of 7 US 9,095.285 B2 \\n PRIOR ART \\n Figure 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 1}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 2 of 7 US 9,095.285 B2 \\n Figure 2', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 2}), Document(page_content='U.S. Patent \\n image Acquisition \\n Image Enhancing \\n Image \\n Processing \\n Registration \\n to the \\n database \\n 7s 9-10 - 1 - 12 - is \\n 88: 88888 g33 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: g::::::: Aug. 4, 2015 \\n 8x88.88% ... \\n ... \\n s \\n I \\n Figure 3 Sheet 3 of 7 \\n Image \\n identification US 9,095.285 B2 \\n image \\n Analysis', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 3}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 4 of 7 US 9,095.285 B2 \\n Figure 4', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 4}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 5 Of 7 US 9,095.285 B2 \\n irrors Loop for each interest point of database image \\n Set up current values of smallest distance, so, and next smallest distance, nsD, to some global MaxValue. \\n ... Loop for each interest point of input image \\n Calculate distance D for 64 dimensional descriptors of the pair of current point of interest for database and input image \\n Yes \\n ns as D. s. :D and mark the current pair of points \\n of interest as possible match \\n No \\n Yes \\n SD ::D \\n and mark the current pair of points of interest as possible matching pair \\n ir Loop for each interest point of input image \\n No s) - Treshold A \\n and \\n siris) K Threshold B \\n Yes \\n Add the current possible matching pair to the list of matching pairs \\n im. Loop for each interest point of database image \\n Figure 5', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 5}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 6 of 7 US 9,095.285 B2 \\n Figure 6', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 6}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 7 Of 7 US 9,095.285 B2 \\n s region of 4 different matched images for the same subject \\n O 20 40 60 80 100 120 \\n sunbject # \\n Figure 7', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 7}), Document(page_content='US 9,095,285 B2 \\n 1. \\n PORTABLE BOMETRIC IDENTIFICATION \\n DEVICE USINGADORSAL HAND VEN \\n PATTERN \\n FIELD OF INVENTION \\n The present invention relates broadly to device for biomet \\n ric person identification based on the platform of a Portable \\n Assistance Device (PAD) such a smartphone, cell phone or \\n tablet. \\n More precisely, the invention is based on the analysis and comparison of unique vein-patterns of human hands. The \\n technology can be applied to various human populations, \\n regardless of race, skin color or age (i.e. newborns, toddlers, teenagers, adults, and elderly people). \\n BACKGROUND OF THE INVENTION \\n The progress of the information age brings unprecedented \\n changes to the human Society. The requirements related the personal information are being growing dramatically day by \\n clay. As the result, the personal identification, protection and security become extremely important in the modern age. \\n There are various methods of personal and/or biometric identification have been developed, including fingerprinting, face-Voice-recognition, vein pattern (iris, palm), etc. \\n There are multiple problems associated with the existing \\n methods, such as requirement for in-contact authentication procedure, time-consuming or obstructive procedure, unreli \\n able measurements and low recognition rate. For example, the accuracy of a face-recognition-based per \\n sonal identification is relatively low as the technology has to overcome the problems of lighting, pose, orientation and gesture. Fingerprint identification is widely used for personal \\n identification. However, it is difficult to acquire fingerprint \\n features (i.e. minutiae), for Some people such as elderly \\n people, manual laborers, etc. Moreover, in-contact identifi cation devices may invoke hygiene concern and reluctance to \\n use by a general public. As a result, other biometric charac teristics are receiving increasing attention. \\n Recently, a growing trend towards relieving the users from \\n a contact device has emerged and the idea of peg-free, or \\n further contact-free, hand biometrics have been is proposed. The hand vein recognition technology has been also proposed \\n for image biometrical verification, see, for example U.S. Pat. \\n No. 4,699,149 by Rice, US20120281890 by Kamakura and U.S. Pat. No. 5,787,185 by Clayden. \\n Compared to other biometric authentication techniques, the vein recognition has many advantages, such as unique \\n ness, life-long, time invariant consistency of the vein-pattern \\n for each human body, as well as a non-contact, fast, unobtru sive vein-pattern image acquisition procedure. \\n FIG. 1 shows the typical example of matched portions of the two Superimposed vein-patterns of a person, as shown in \\n U.S. Pat. No. 5,787,185 by Clayden. The advancement in biometric image matching technology \\n has promoted the development of various biometric identifi cation systems. \\n Vein biometric systems are also capable to record the infra red absorption patterns to produce unique and private identi \\n fication templates for users, see, for example U.S. Pat. No. 8.229,178 by Zhang. The matching and comparison of images is part of many modern computer-vision applications. Image registration, \\n camera calibration, object recognition, and image retrieval, to \\n mention a few, see for example U.S. Pat. No. 8,165,401 by Funayama. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n The contemporary compact devices such as a Smartphone are capable of performing the task offinding correspondences \\n between two images. Moreover, the Smartphone can be modi fied to process the image at the infrared part of the spectrum \\n with minimal or no modification. \\n The disclosed invention provides a novel, compact, fast, \\n portable and mobile authentication and identification device of a person biased on person’s hand vein-pattern. \\n The preferred embodiment of the invention uses an image \\n of the vein-pattern of a dorsal (back) side of a human palm for', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 8}), Document(page_content='The preferred embodiment of the invention uses an image \\n of the vein-pattern of a dorsal (back) side of a human palm for \\n biometric authentication of a person. There is no limitation, \\n however, to apply the disclosed method to the front side of human palm, as well as to other regions of skin of human body, as long as blood vessels are located relatively close to \\n the skin Surface. \\n The Surface of the person’s hand dorsal side is less Suscep \\n tible to accidental damage, thus making disclose device pref \\n erable for the situations of field deployment. Moreover, it is much more convenient to acquire the images of the hand \\n dorsal side of small children and newborns. \\n Furthermore, for small children it might be preferable to use the images of their feet particularly in infrared spectrum. \\n The disclosed device and technology is relies on the dis covering intrinsically specific points of interest and match \\n them using geometric affine, projective, or other types of \\n geometric transformations. \\n The disclosed device and technology is relies on the image processing algorithm similar to Speeded-Up Robust Features \\n (SURF) algorithm reported by Bay et al. in \"Surf Speeded \\n up robust features, ECCV (European Conference on Com \\n puter Vision), 2006, pp. 404-417. Together with this algorithm the disclosed device employs \\n geometric affine and projective transformations which are insensitive to rotations, Scaling, tilt, image plane, etc., making \\n the disclosed our technology applicable to biometric identi \\n fication of humans of all ages: from newborns to adults. The term “vein-pattern’, as used herein is defined as the image having a pattern of veins, capillaries and other blood \\n vessels that are unique for each individual. \\n The term “PAD, as used herein is defined as any mobile \\n Portable Assistance Device, such as a cell phone, a Smart phone, a tablet computer, a personal computer, etc. \\n The term “identification\\', as used herein is defined as a procedure of providing and proving an identity of the indi vidual by searching against a database of previously acquired \\n information. \\n The term \"NIR\\', as used herein is defined as electromag \\n netic radiation within the 750 mm-2500 nm range of the spectrum. \\n The term “point of interest of an image, as used herein is \\n defined as a pixel of the image with a specific coordinates on the image. The local image features around of the point of interest are stable under local and global image perturbations, \\n Such as deformations as those arising from perspective trans \\n formations (e.g. affine transformations, Scale changes, rota tions and/or translations) as well as illumination/brightness \\n variations, such that the locations of the points of interest can be reliably computed with a high degree of reproducibility. \\n The term “descriptor of a point of interest, as used here is \\n defined as a set of numeric values, usually represented by a \\n 64-dimentional vector, which contains information about the \\n local environment of the point of interest. In the general embodiment of the invention, the image acquisition is performed using a PAD. In such embodiment \\n the PAD-connected camera is used to obtain the images of \\n blood veins. The camera can be either a PAD built-in camera \\n or connected to the PAD by a wire or wirelessly.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 8}), Document(page_content='US 9,095,285 B2 \\n 3 \\n In other embodiment of the invention the disclosed per Sonal authentication device includes a Switching capabilities \\n between visible and near infrared spectral ranges, namely, \\n acquiring the vein-patter image from a person under the vis \\n ible and the near infrared spectrum and Subsequent extracting \\n points of interest, their locations and descriptors, from the acquired images. \\n In another embodiment of invention a lighting feature is provided by the device to improve the quality of the image \\n both in the visible and the near infrared spectral regions. \\n The disclosed device for non-contact personauthentication \\n and identification do not require a direct contact between the \\n disclosed biometric device and a human Subject, making it \\n indispensible tool for sterile hospital environment when the \\n alleviation of possible contamination is important. \\n Being portable, the disclosed invention can provide an \\n immediate personnel identification or authorization at virtu \\n ally any location. The invention might also be important for \\n the personal working in remote areas or in the areas with underdeveloped/damaged infrastructure. \\n The disclosed invention can also find a particular applica \\n tion in instances where the restricted authorized access is \\n required. Such as admission to secure sites, operation of sen sitive machinery or credit/cash dispensing. \\n It is also possible to use the disclosed invention for statis tical data analysis and medical diagnostics. \\n Moreover, the disclosed invention can be used for indica \\n tion of subcutaneous bleeding, new born birth trauma, arthri tis, symptoms of a high blood pressure and atherosclerosis, \\n etc. \\n Further features and aspects of the present invention will become apparent from the following description of preferred \\n and optional embodiments with reference to the attached drawings. \\n SUMMARY OF THE INVENTION \\n A portable identification device using an individual hand vein-patternis disclosed. The device is based on a Smartphone \\n with a built-in camera to acquire a near-infrared image of a skin area with the vein-pattern of the individual. A Smartphone performs a processing of the acquired image \\n to extract a vein-pattern image and applies a Speeded-Up \\n Robust Features (SURF) method (with adjustable hessian \\n thresholds to deduct of points of interest from the image and create a 64-dimensional descriptor vector for the pixels \\n neighborhood for each of the point of interest. In order to facilitate the vein-pattern extraction, a spatial low- and high frequency filtering is applied to the (optionally gray-scale) \\n vein-pattern image, along with a contrast enhancement. A Smartphone compares the descriptors of the acquired image \\n and an image stored in a database and further uses a minimum (Euclidian) distance criterion between descriptor to establish pairs of matching points of interest for two images, resulting \\n in an individual identification based on a threshold value. \\n The invention uses an adjustable threshold value of mini \\n mum distance between descriptor vectors. It also uses an adjustable number of matching points of interest that deter \\n mine the possible identification. The device can include addi tional processing to improve an accuracy of the identification \\n by calculating a transformation matrix using the coordinates \\n of the matched pairs and calculating transformed coordinates \\n of the points of interest of the input image. The transformed \\n coordinates can be compared with initial coordinates for improved individual identification using a cumulative thresh \\n old (a mean square distance) for distance between the initial 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n and the transformed coordinates. The transformation matrix \\n can belong to affine or projective transformation class. \\n The device can use a Smartphone memory, a 3G or 4G \\n technology, Bluetooth, or Wi-Fito access the database. More', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 9}), Document(page_content='can belong to affine or projective transformation class. \\n The device can use a Smartphone memory, a 3G or 4G \\n technology, Bluetooth, or Wi-Fito access the database. More \\n over, the camera of the device can have auto-focus feature and \\n being switched to operate in visible or NIR spectrum regions. \\n A lighting feature is provided by the device to improve the \\n quality of the image both in the visible and the NIR light. The \\n Smartphone can employ a wireless communication with a \\n remote server for storage, image processing and exchanging \\n the identification information. Apart of the personal identifi \\n cation, disclosed device is capable of collecting the personal \\n biometric information, accessing the individually specific \\n information for the identified person and/or granting an indi \\n vidual access to the specific information or site. The device is also capable of performing tasks of personnel/population Sur \\n Veying and management. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1: Prior Art. The matched portions of the two super imposed patterns. \\n FIG. 2: The general layout of the proposed invention. \\n FIG. 3: The block-diagram of the image matching algo \\n rithm disclosed in the preferred embodiment invention. \\n FIG. 4: Applying a high-pass, low-pass filters and contrast \\n enhancement to the image. Original (4A) and processed (4B) \\n images of the human hand veins pattern are shown. FIG. 5: Detailed diagram of the image descriptor matching \\n part of the algorithm in the preferred embodiment of the \\n invention. \\n FIG. 6: Illustration of the matching procedure of the algo \\n rithm for two different images that taken for the same person \\n at different time: FIG. 6A illustrates a good match between the points of interest while FIG. 6B illustrates a bad match. FIG. 7: Algorithm implementation example. A matching \\n criterion values for the comparison between an acquired \\n image and multiple images from a database is shown. The \\n ellipse marks the region where all 4 images are matched for \\n the same individual. \\n DETAILED DESCRIPTION OF THE PREFERRED \\n EMBODIMENT \\n The disclosed invention is based on a device and method \\n for biometric human identification based on the platform of a \\n PAD. The general layout of the disclosed invention is show in \\n FIG 2. \\n In the preferred embodiment of the invention, the image acquisition is performed using a Smartphone. In such embodi \\n ment a Smartphone-built-in (standard) camera is used to \\n obtain the blood vein-pattern. \\n In preferable embodiment of the invention the distance \\n from the camera and the object (i.e. a skin area) is within 0.1-1 \\n meter range. \\n In another embodiment of the invention, the camera is specifically modified to adjust the camera sensitivity for dif ferent wavelength regions, i.e. spectral region or spectrum. \\n In the preferred embodiment of the invention, the Near InfraRed (NIR) region of electromagnetic spectrum, Such as \\n a wavelength region between 750 nm and 2500 nm, for example, is used by the camera. This embodiment exploits the \\n fact that oxygenated blood in vein vessels absorbs the NIR part of the spectrum more efficiently than the visible spec \\n trum, while the outer skin layers are Sufficiently transparent \\n for the NIR radiation.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 9}), Document(page_content='US 9,095,285 B2 \\n 5 \\n In yet another embodiment of the invention, a specific \\n camera modification is implemented that enables a sequential \\n operation within two spectral regions, for example, between \\n visible and NIR regions, preferably having an engaging/dis \\n engaging mechanism (e.g. a button). \\n In yet another embodiment of the invention, the built-in \\n NIR illumination device is used in combination with the PDA \\n if necessary to improve the image acquisition by the camera at \\n low-light conditions, preferably having an engaging/disen \\n gaging mechanism (e.g. a button). \\n The disclosed invention is based on the device and method \\n to compare and match the unique vein patterns of human \\n hands. The method includes an algorithm capable of process ing and comparing the captured image against the set of \\n images stored in the database. \\n The database can be located either within the PAD memory \\n unit or remotely. For the latter case, the PAD is capable of a \\n bi-directional remote access, including a real-time access, to \\n the database using any wireless protocol available (e.g. Wi-Fi \\n or Bluetooth). \\n In the preferred embodiment, of the invention all the ele ments of the image processing and algorithm operation are \\n realized on the platform of onboard PAD processing unit, \\n such as a smartphone CPU. It is also possible, however, to implement a remote processing (e.g. remote server) for the algorithm operation, either entirely or partially. \\n In the preferred embodiment of the invention, the algo rithm is capable of saving captured images in the database and processing any stored images within the database, e.g. per \\n forming a search and statistical data analysis upon the stored \\n images within the database. FIG. 3 shows the block-diagram of the image matching \\n algorithm disclosed in the preferred embodiment of the invention. The numbering in FIG. 3 corresponds to the soft \\n ware routine steps that are used in the preferred embodiment \\n of the disclosed biometric identification device. \\n The realization of the identification routine in the preferred \\n embodiment of the invention is described in details below and \\n comprises the following steps: Image Acquisition: \\n Step-1: An input image acquisition of human dorsal hand \\n with PAD camera, selecting the area of interest from the raw image. Image Enhancing: \\n Step-2: Enhancing the input image, using: i) Conversion the input image to a gray-scale image (i.e. intensity image); ii) \\n Enhancing contrast of the gray-scale image by mapping the existing intensity range of the gray-scale image to the entire possible intensity range and leaving out an adjustable per \\n centage of pixels (e.g. one percent of pixels) having lowest \\n and highest intensities. Step-3: Further enhancing the contrast of the input image by application of a high-frequency and a low-frequency \\n image filtering with respective adjustable parameters. \\n Obtaining the image of a hand vein pattern image from the \\n enhanced image. Image Processing: \\n Step-4: Application of the SURF algorithm to the (hand) \\n vein pattern to obtain locations of points of interest of the vein pattern; the SURF algorithm using a set of adjustable param \\n eters, including hessian threshold values, hessian balance value, and number hessian octaves. Preparing a unique descriptor for each of the points of interest. Each descriptor \\n includes a 64-dimentional vector which, in turn, contains the information about local pixel neighborhood of Such respec \\n tive point of interest. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n Registration to Database: \\n Step-5: Storing to data base the original image, its \\n enhanced image, locations of the points of interest, descrip \\n tors, personal information about the individual being imaged, \\n a geo-location information (e.g. GPS coordinates), time, etc. \\n Step-6: Choosing a database image from the multiple \\n images in the database; the database image including its', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 10}), Document(page_content='a geo-location information (e.g. GPS coordinates), time, etc. \\n Step-6: Choosing a database image from the multiple \\n images in the database; the database image including its \\n enhanced image, locations of points of interest, the descrip \\n tors, a personal information about the individual being \\n imaged a geo-location information (e.g. GPS coordinates), \\n time, etc. \\n In the preferred embodiment of the invention the linear \\n search over the database of previously collected images is \\n employed. Alternative search techniques can be used, such as \\n a decision-tree clustering at the particular point of interest. \\n The binary or, more general K-tree-structures at the space of \\n the chosen descriptor can be used to provide a non-linear \\n search time reduction. \\n Image Identification: Step-7: Comparing the set of descriptors (Step-4) with the \\n descriptors retrieved from the database image (Step-7). Cal \\n culation the Euclidian distance between descriptors in a 64-dimensional space of the descriptor vectors. Recording \\n the measured distance for the respective pair of descriptors. Step-8: Performing an initial search of the matching \\n descriptors among the stored database images from the data \\n base. FIG. 5 illustrates the implementation of a search for the matching pairs of the points of interest between the input image and a database image. The descriptor of the each point \\n of interest from a tested input image is compared with all the descriptors from the database image. \\n For each pair of these descriptors the Euclidian distance \\n between the 64-dimentional descriptor vectors is calculated. \\n Among all the calculated distances the minimal distance and \\n the distance which is Smallest among all other calculated \\n distances (i.e. next Smallest distance) are retained. The point of interest of the database image is selected as a matching point of interest for the point of interest of the tested image if: \\n i) the corresponding minimal distance of this pairis Smaller \\n than a certain adjustable threshold value, see Threshold \\n A in FIG. 5, and \\n ii) the ratio between such minimal distance and the next Smallest distance is Smaller than an another adjustable \\n threshold value, see Threshold B in FIG. 5. Simultaneous application of these two conditions provides \\n that 1) the selected pair of the points of interest matches the points of interest of the input and the database images with a \\n similar local environment, and 2) that these local environ ments are substantially different from the local neighborhood \\n of the other points of interest. Step-9. The set of pairs of the points of interest obtained by \\n aforementioned way for the input image and the database \\n image is called a threshold-matched set of the points of inter est. If, for the particular database image, the number of pairs \\n of matched points of interest in the threshold-matched set is longer than a certain adjustable threshold number, then this database image is selected for the initial set of matching images. \\n Step-10: Deduction of the transformation matrixes which represents the transformation between the set of points of interest obtained as described in Step-4 for the input image to \\n the paired points of interest selected for the threshold \\n matched sets, as described in Step 8, corresponding to data base images in the initial matching set of the database images \\n obtained in Step 9.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 10}), Document(page_content='US 9,095,285 B2 \\n 7 \\n The choice of the type of the transformation matrix can be made upon the assumption about the type of possible geo \\n metrical transformation, Such as affine, projective, etc. These \\n transformations are insensitive to rotations, Scaling, tilt, of image plane, etc. In the preferred embodiment of the inven \\n tion, the transformation matrix is derived from two column \\n matrixes of coordinates of the points of interest that are rel \\n evant to these images. Each of these column matrixes have X \\n and Y coordinates of the point of interest as rows. In the preferred embodiment of the invention, the Singular \\n Value Decomposition method is used to evaluate a Moore \\n Penrose pseudo inverse matrix relevant to the column matrix of the points of interests coordinates in the input image. \\n Accordingly, the transformation matrix is calculated as the matrix product of original column matrix for database image \\n and Moore-Penrose pseudo inverse of column matrix for tested image. \\n Preferred embodiment of the invention implements the \\n matrix of affine transformation which relates the points of \\n interest coordinates for both aforementioned images. How ever, it is also possible for one skilled in the art to evaluate a more general projective transformation matrix or even more \\n complex transformations by similar means. \\n Step-11: Deduction of the locations of the points of interest \\n in the database image by applying the transformation matrix \\n (obtained in Step-10) to the set of points of interest corre \\n sponded to the input image. The transformation matrix is applied to the original column matrix containing the coordi \\n nates of the input image points of interest to obtain a set of \\n transformed coordinates for the points of interest in the input image. \\n Step-12: Comparison the transformed coordinates of the points of interest in the database image (obtained in Step-11) \\n with the locations of the points of interest obtained by the SURF method for this image. Calculation of squares of \\n Euclidian distances between the points of interest coordinates \\n in the database image and transformed coordinates of the input image. This step is used to assess how well the deduced \\n transformation matrix matches the paired points of interest, in other words, the input coordinates with applied transforma \\n tion matrix are compared to the actual points of interest loca \\n tions at the database image. Step-13: The personal identification is performed based on \\n a value of the cumulative matching criterion chi2. If the value \\n of chi2 is below of a cumulative threshold value, the two \\n images considered to be matched. Applying a cumulative \\n predefined criterion chi2 to characterize and score the match ing degree between the current and the database images (i.e. the vein patterns). \\n In the preferred embodiment of the invention, the adjust \\n able cumulative threshold value for Chi2 for a pair of two images is evaluated as a mean square distance between the \\n coordinates of the points of interest of database image and transformed coordinates of input image. \\n The tested image with the smallest Chi2 value is selected as a Best Matching Image for the input image. Image Analysis: \\n Step-14: Deriving probabilities (i.e. rates) of a true and a \\n false positive matching from the obtained values of Chi2 for \\n the total number of compared and matched images. \\n The algorithm disclosed in Steps 1-14 indicate that the \\n false positive rate of the matching is less than 0.001%, which is in a good agreement with the results have been reported for \\n alternative well-known biometric authentication methods. \\n FIG. 4 shows the example of a high- and low-frequency pass filtering application to the image, in combination with \\n the contrast enhancement, used to obtain the of human hand 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 11}), Document(page_content='the contrast enhancement, used to obtain the of human hand 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n veins patterns (Steps-2, 3 of the aforementioned routine) as a preprocessing required for the matching analysis. The origi \\n nal image (FIG. 4A) and the processed (FIG. 4B) image are \\n shown. \\n FIG. 5 shows the image descriptor matching part of the \\n algorithm (see FIG. 3, Steps-7-8) more in detail, including \\n comparing the sets of points of interest for the database image and the input images, and selecting the pairs of possible \\n matching points of interest. \\n While in the preferred embodiment of the invention the gray-scale (intensity) images of the human hand vein-pattern \\n is used, other embodiments could use foil color images for the \\n same purposes. \\n It is obvious to the skilled in the art, that most of the image processing procedures are dependent on a number of adjust \\n able parameters, such as settings of averaging windows for \\n high-pass and low-pass filters, percentage of pixels with out \\n lying intensity values removed during image contrast \\n enhancement, settings of the SURF procedure, as well as \\n multiple threshold values relevant to image comparison pro \\n cedures. \\n In the preferred embodiment of the device, a generic pro \\n cedure, which is using a set of test images to simultaneously \\n optimize the values of these adjustable parameters, is imple \\n mented. This procedure establishes a target image matching \\n matrix which contains a Zero values for the elements repre senting non matching images and some values for the pairs of \\n matching images. The procedure starts from a pre-defined set \\n of adjustable parameters and performs all routine shown in \\n the FIG. 3 for any set of images from the database. \\n Moreover, the procedure establishes penalty values for \\n each occurrence of incorrect match or absence of a correct \\n match, further collecting these penalty values in an aggre \\n gated target function value. The procedure employs non \\n gradient Nedler-Mead simplex search in the space of adjust \\n able parameters to minimize the target function value and to establish a set mutually optimized parameters for our image processing and image comparison procedures. \\n FIG. 6 illustrated the actual performance of the matching procedure of the algorithm, (see the Step-12). The points of \\n interest for two different images are taken for the same person \\n at different times. FIG. 6A illustrates a good match with, a \\n chi2 cumulative criterion having value of 30, while FIG. 6B \\n illustrates a bad match with a cumulative criterion chi2 hav \\n ing value in the range of 2000-10000. This demonstrates a good efficiency of the matching procedure. \\n FIG. 7 shows a comparison between a single image against \\n multiple database images. The ellipse marks the region where \\n all four images of the same Subjects are matched which dem onstrates high sensitivity of image matching procedure. \\n Although several exemplary embodiments have been \\n herein shown and described, those of skill in the art will recognize that many modifications and variations are possible \\n without departing from the spirit and scope of the invention, \\n and it is intended to measure the invention only by the appended claims. \\n We claim: \\n 1. A portable device for identification of an individual based on a vein-pattern comprising: a Smartphone; \\n a camera built in the Smartphone, the camera acquiring an \\n input image of a skin area with the vein-pattern of the \\n individual, the input image is acquired in the near infra \\n red (NIR) spectrum of light radiation;', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 11}), Document(page_content='US 9,095,285 B2 \\n a built-in smartphone central processor unit (CPU) per forms processing of the acquired input image to extract an input vein-pattern image from the obtained image of \\n the skin area; the CPU performs deduction of points of interest from the Vein-pattern image and calculation of their coordinates; the CPU also creates a 64-dimensional descriptor vector for each of the point of interest, the 64-dimensional descriptor Vector including information about features of a local image around the each respective point of \\n interest; the CPU compares the obtained image descriptor vector with each descriptor vector from a database of descriptor vectors; the database is stored in the smartphone; \\n the CPU uses a criterion of minimum Euclidian distance \\n between descriptor vectors and a criterion of sufficient separation between minimum Euclidian distances to establish pairs of matching points of interest for two images: the input image and each image stored in the \\n database, and assuring the individual identification if the number of pairs of matched points of interest is greater than a predefined \\n threshold. \\n 2. The device of claim 1, further comprising additional processing in the CPU to improve an accuracy of the identi fication, the additional processing comprising: calculating coordinates of the matched pairs of points of interest of input and database image: calculating a transformation matrix using the coordinates of the matched pairs; the transformation matrix reflecting a geometrical trans \\n formation of the coordinates of the input image into coordinates of a selected database image; the selected database image is one that was selected after the vectors comparison and for which the number of pairs of the matched points of interest is greater than the predefined \\n threshold; calculating transformed coordinates of the points of inter est of the input image: \\n comparing the transformed coordinates with initial coor \\n dinates; and determining an improved individual identification if a dis \\n tance between the initial and the transformed coordi \\n nates is less than a cumulative threshold. \\n 3. The device of claim 2, wherein the cumulative threshold \\n is determined as a mean square distance between the initial \\n and the transformed coordinates. \\n 4. The device of claim 2, wherein the transformation matrix is an affine, projective transformation. \\n 5. The device of claim 2, wherein the transformation matrix is a general class of image transformations. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 10 \\n 6. The device of claim 1, wherein the coordinates of the points of interest and 64-dimentional descriptor vectors are calculated using a Speeded-Up Robust Features (SURF) \\n method. \\n 7. The device of claim 1, wherein the database is transmit \\n ted to the smartphone via a 3G or 4G technology, Bluetooth, \\n or Wi-Fi, and the database is stored in the smartphone \\n memory. \\n 8. The device of claim 1, further comprising a switch, performing switching of the camera operation between a \\n visible and the NIR spectrum. \\n 9. The device of claim 8, wherein the camera has an auto \\n focus feature operating in both the visible and the NIR spec \\n trum. \\n 10. The device of claim 9, further comprising a lighting feature is provided by the device to improve the quality of the image both in the visible and the NIR light spectrum. 11. The device of claim 1, further comprising a wireless communication of the Smartphone with a remote server; the Smartphone having an option to choose the server for the image processing, a storage of the database and a command to \\n send results of the individual identification back to the Smart phone for display. \\n 12. The device of claim 1, wherein the scale vein-pattern image is a gray scale image extracted based on a spatial low frequency and a high frequency filtering and image contrast \\n enhancement. \\n 13. The device of claim 1, being used for collecting per \\n sonal biometric information.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 12}), Document(page_content='enhancement. \\n 13. The device of claim 1, being used for collecting per \\n sonal biometric information. \\n 14. The device of claim 1, being used for accessing indi vidual specific information for the identified individual. 15. The device of claim 1, being used for allowing an \\n access to the information or site. \\n 16. The device of claim 1, being used for performing tasks related with personnel and population surveying and manage \\n ment. \\n 17. The device of claim 1, wherein the number of matching pairs of points of interest signifying possible identification of an input image is adjustable. \\n 18. The device of claim 1, wherein the threshold value of \\n minimum Euclidian distance between 64-dimentional \\n descriptor vectors of two points of interest is adjustable. \\n 19. The device of claim 1, wherein the threshold value of a ratio between the minimum Euclidian distances for two pairs of matching points of interest is adjustable. \\n 20. The device of claim 1, wherein the coordinates of the points of interest and 64-dimentional descriptor vectors are calculated using a Speeded-Up Robust Features (SURF) method, and a value of a hessian threshold is adjustable. \\n ck ck ck ck ck', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 12})], [Document(page_content=\"US010922957B2 \\n ( 12 ) United States Patent ( 10 ) Patent No .: US 10,922,957 B2 \\n ( 45 ) Date of Patent : Feb. 16 , 2021 Rhoads et al . \\n ( 56 ) References Cited ( 54 ) METHODS AND SYSTEMS FOR CONTENT \\n PROCESSING U.S. PATENT DOCUMENTS \\n ( 71 ) Applicant : Digimarc Corporation , Beaverton , OR ( US ) 5,905,248 A 5,932,863 A 5/1999 Russell \\n 8/1999 Rathus \\n ( Continued ) ( 72 ) Inventors : Geoffrey B. Rhoads , West Linn , OR ( US ) ; Tony F. Rodriguez , Portland , OR \\n ( US ) ; William Y. Conwell , Portland , OR ( US ) FOREIGN PATENT DOCUMENTS \\n EP \\n WO 1069529 \\n WO2007001774 1/2001 \\n 1/2007 ( 73 ) Assignee : Digimare Corporation , Beaverton , OR \\n ( US ) OTHER PUBLICATIONS \\n ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 0 days . Alleysson et al , Linear demosaicing inspired by the human visual system , IEEE Trans . on Image Processing , vol . 14 , No. 4 , Apr. 2005 . \\n ( Continued ) \\n ( 21 ) Appl . No .: 15 / 889,013 Primary Examiner — Hadi Akhavannik ( 74 ) Attorney , Agent , or Firm — Digimarc Corporation ( 22 ) Filed : Feb. 5 , 2018 \\n ( 65 ) Prior Publication Data \\n US 2018/0233028 A1 Aug. 16 , 2018 \\n Related U.S. Application Data \\n ( 60 ) Division of application No. 14 / 456,784 , filed on Aug. 11 , 2014 , now Pat . No. 9,886,845 , which is a division \\n ( Continued ) ( 57 ) ABSTRACT \\n Mobile phones and other portable devices are equipped with a variety of technologies by which existing functionality can be improved , and new functionality can be provided . Some aspects relate to visual search capabilities , and determining appropriate actions responsive to different image inputs . Others relate to processing of image data . Still others concern metadata generation , processing , and representa tion . Yet others concern user interface improvements . Other aspects relate to imaging architectures , in which a mobile phone's image sensor is one in a chain of stages that successively act on packetized instructions / data , to capture and later process imagery . Still other aspects relate to distribution of processing tasks between the mobile device and remote resources ( “ the cloud ” ) . Elemental image pro cessing ( e.g. , simple filtering and edge detection ) can be performed on the mobile phone , while other operations can be referred out to remote service providers . The remote service providers can be selected using techniques such as reverse auctions , through which they compete for processing tasks . A great number of other features and arrangements are also detailed . ( 51 ) Int . Ci . \\n G06K 9/00 ( 2006.01 ) G08C 17/02 ( 2006.01 ) \\n ( Continued ) ( 52 ) U.S. Ci . \\n CPC G08C 17/02 ( 2013.01 ) ; G06K 900 ( 2013.01 ) ; H04L 67/34 ( 2013.01 ) ; G06F 3/0482 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search None See application file for complete search history . 47 Claims , 60 Drawing Sheets \\n IMAGE SENSOR3 \\n AMPLIFY ANALO 3GIJALS \\n CONVERT TO DIGITAL NATIVE IMAGE DATA \\n BACR INTERPOLATION FOURIER TRANSFORM ETC EIGENSACE CALCULATON \\n Eliss in \\n MATCH 54RCODE TEMPLATE ? WATCH FACE TEMPLATE : Yes \\n WELLIN TRANSFOAM SEND DIGITAL NATI / E IMAGE LATA ANDOR ORIENWOMANDATA ? O BARCODE SERVICE \\n MATCH TEXT TERPLATS ? SEND DIGITAL NATIVE MAGELATA ANDORRA DOMAIN DATAT OCR SERVICE \\n SEWO DIGITA NATIVE TRADE DATA ANDA \\n IOMAIN DATATOVA WATCH WN ORIENTATION TEMPLATE Yes \\n MATCH PACS TEMPLATE ? Yes SEND DIGITAL NATIVE \\n ! MAGE DATA AND / OR FM \\n DOMAIN DATA ANDO ? C ! GENFACE DATAT SACIAL PECOGNITION SERVICE\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 0}), Document(page_content='US 10,922,957 B2 Page 2 \\n 7,460,160 B2 7,502,759 B2 * 12/2008 Hershey et al . 3/2009 Hannigan G06Q 30/02 123/201 Related U.S. Application Data \\n of application No. 13 / 011,618 , filed on Jan. 21 , 2011 , now Pat . No. 8,805,110 , which is a continuation of application No. PCT / US2009 / 054358 , filed on Aug. 19 , 2009 , which is a continuation - in - part of applica tion No. 12 / 271,692 , filed on Nov. 14 , 2008 , now Pat . No. 8,520,979 , and a continuation - in - part of applica tion No. 12 / 484,115 , filed on Jun . 12 , 2009 , now Pat . No. 8,385,971 , and a continuation - in - part of applica tion No. 12 / 498,709 , filed on Jul . 7 , 2009 , now abandoned . \\n ( 60 ) Provisional application No. 61 / 090,083 , filed on Aug. 19 , 2008 , provisional application No. 61 / 096,703 , filed on Sep. 12 , 2008 , provisional application No. 61 / 100,643 , filed on Sep. 26 , 2008 , provisional application No. 61 / 103,907 , filed on Oct. 8 , 2008 , provisional application No. 61 / 110,490 , filed on Oct. 31 , 2008 , provisional application No. 61 / 169,266 , filed on Apr. 14 , 2009 , provisional application No. 61 / 174,822 , filed on May 1 , 2009 , provisional application No. 61 / 176,739 , filed on May 8 , 2009 , provisional application No. 61 / 226,195 , filed on Jul . 16 , 2009 , provisional application No. 61 / 234,542 , filed on Aug. 17 , 2009 . 7,565,139 B2 7,751,805 B2 7,837,094 B2 8,230,337 B2 8,385,971 B2 8,755,837 B2 8,886,206 B2 9,497,341 B2 2001/0001854 A1 2001/0055391 A1 \\n 2002/0032027 Al 2002/0062346 Al 2002/0072982 A1 2002/0075298 Al 2002/0102966 A1 \\n 2002/0152388 A1 \\n 2002/0178410 A1 \\n 2003/0083098 Al \\n 2004/0199387 Al \\n 2004/0263663 A1 \\n 2005/0144455 Al \\n 2005/0185060 A1 \\n 2005/0227674 Al \\n 2005/0264658 Al 2005/0276486 Al \\n 2006/0012677 Al \\n 2006/0026140 A1 \\n 2006/0031684 Al \\n 2006/0056707 A1 \\n 2006/0097062 A1 * 7/2009 Neven \\n 7/2010 Neven 11/2010 Rhoads 7/2012 Rhoads \\n 2/2013 Rhoads et al . \\n 6/2014 Rhoads et al . 11/2014 Lord \\n 11/2016 Rhoads \\n 5/2001 Schena 12/2001 Jacobs \\n 3/2002 Kirani \\n 5/2002 Chen \\n 6/2002 Barton \\n 6/2002 Schena \\n 8/2002 Lev 10/2002 Linnartz \\n 11/2002 Haitsma \\n 5/2003 Yamazaki 10/2004 Wang 12/2004 Lee et al . \\n 6/2005 Haitsma \\n 8/2005 Neven 10/2005 Kopra 12/2005 Ray et al . 12/2005 Withers et al . \\n 1/2006 Neven 2/2006 King 2/2006 Sharma \\n 3/2006 Suomela 5/2006 Cheong G06K 19/06037 \\n 235/494 ( 51 ) Int . Ci . \\n H04L 29/08 ( 2006.01 ) \\n G06T 1/20 ( 2006.01 ) HO4N 13/00 ( 2018.01 ) \\n G06F 3/0482 ( 2013.01 ) \\n H046 88/02 ( 2009.01 ) \\n ( 52 ) U.S. CI . CPC G06T 1/20 ( 2013.01 ) ; G08C 2201/93 ( 2013.01 ) ; HO4N 2013/0074 ( 2013.01 ) ; H04W 88/02 ( 2013.01 ) 2006/0143684 A1 \\n 2006/0204118 A1 \\n 2006/0240862 Al \\n 2007/0174180 A1 2007/0175998 A1 2007/0228175 A1 \\n 2007/0286524 A1 \\n 2008/0080396 Al \\n 2008/0086311 A1 \\n 2008/0267504 Al 2008/0300011 A1 \\n 2009/0237546 A1 \\n 2009/0290807 A1 \\n 2010/0110222 A1 \\n 2011/0231469 Al 6/2006 Morris 9/2006 Fehmi et al . 10/2006 Neven \\n 7/2007 Shin \\n 8/2007 Lev 10/2007 Kotlarsky 12/2007 Song 4/2008 Meijer et al . 4/2008 Conwell et al . \\n 10/2008 Schloter 12/2008 Rhoads 9/2009 Bloebaum \\n 11/2009 Marchesotti \\n 5/2010 Smith et al . \\n 9/2011 Wolman ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n OTHER PUBLICATIONS 6,002,946 A 6,081,827 A 6,121,530 A 6,199,048 B1 6,249,226 B1 6,389,055 B1 6,411,725 B1 6,491,217 B2 6,549,933 B1 6,675,165 B1 6,694,042 B2 6,766,363 B1 6,788,293 B1 6,941,275 B1 6,947,571 B1 6,961,083 B2 6,993,573 B2 7,003,731 B1 7,016,532 B2 7,022,075 B2 7,058,223 B2 7,065,559 B1 7,133,069 B2 7,174,293 B2 7,190,844 B2 * 12/1999 Reber 6/2000 Reber \\n 9/2000 Sonoda \\n 3/2001 Hudetz \\n 6/2001 Harrison 5/2002 August 6/2002 Rhoads 12/2002 Catan \\n 4/2003 Barrett \\n 1/2004 Rothschild \\n 2/2004 Seder \\n 7/2004 Rothschild \\n 9/2004 Silverbrook \\n 9/2005 Swierczek 9/2005 Rhoads \\n 11/2005 Obrador \\n 1/2006 Hunter 2/2006 Rhoads 3/2006 Boncyk 4/2006 Grunwald 6/2006 Cox \\n 6/2006 Weiss', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 1}), Document(page_content='2/2004 Seder \\n 7/2004 Rothschild \\n 9/2004 Silverbrook \\n 9/2005 Swierczek 9/2005 Rhoads \\n 11/2005 Obrador \\n 1/2006 Hunter 2/2006 Rhoads 3/2006 Boncyk 4/2006 Grunwald 6/2006 Cox \\n 6/2006 Weiss \\n 11/2006 Wallach et al . 2/2007 Kenyon 3/2007 Kobayashi Barbaro , et al , A 100x 100 pixel silicon retina for gradient extraction with steering filter capabilities and temporal output coding , IEEE J. \\n of Solid - State Circuits , vol . 37 , No. 2 , 2002 . Dudek , et al , A general - purpose processor - per - pixel analog SIMD vision chip , IEEE Trans . on Circuits and Systems , vol . 52 , No. 1 , 2005 . Dudek , et al , General - purpose 128x 128 SIMD processor array with integrated image sensor , Electronics Letters 42.12 , pp . 678-679 . 2006 . Flores - Cuautle , System for face detection on a mobile phone using Java technology , Masters Thesis , U. of Oslo , Nov. 2007 . Grablink Quickpack CFA brochure , Feb. 2007 . Johansson , et al . A multi - resolution 100 GOPS 4 Gpixels - sec programmable CMOS image sensor for machine vision , IEEE Workshop on CCDs and Adv . Image Sensors , 2003 . Kamal et al , Online machine vision inspection system for detecting coating defects in metal lids , IMECS , Mar. 2008 . Lin , et al , a CMOS image sensor for multi - level focal plane image decomposition , IEEE Trans . on Circuits and Systems , vol . 55 , No. \\n 9 , Oct. 2008 . Mosqueron et al , Smart camera based on embedded HW - SW coprocessor , EURASIP Journal on Embedded Systems , Jan. 2008 . Ni , et al , A 256x256 pixel smart CMOS image sensor for line - based stereo vision applications , IEEE J. of Solid - State Circuits , vol . 35 , No. 7 , Jul . 2000 . HO4N 1/407 \\n 358 / 3.26 \\n 7,251,475 B2 7,283,983 B2 7,432,940 B2 * 7/2007 Kawamoto 10/2007 Dooley 10/2008 Brook G11B 27/11 \\n 345/629', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 1}), Document(page_content=\"US 10,922,957 B2 \\n Page 3 \\n ( 56 ) References Cited \\n OTHER PUBLICATIONS \\n Nishikawa , et al , A high - speed CMOS image sensor with on - chip parallel image compression circuits , IEEE Custom Integrated Cir cuits Conf . , 2007 . Seitz , et al , CCD and APS - CMOS technology for smart pixels and image sensors , Proc . of SPIE vol . 5251 , 2004 . Simoni et al , A digital camera for machine vision , 20th Int’l IEEE Conf . on Industrial Electronics , Control and Instrumentation , 1994 . Xu et al , Design of a DSP - based CMOS imaging system for embedded computer vision , IEEE Conf . on Cybernetics and Intel ligent Systems , Sep. 2008 . Prosecution excerpt from corresponding Korean application 10-2011 7006167 , namely English translation of Notice of Preliminary Rejection dated Sep. 23 , 2015 . Prosecution excerpts from corresponding Canadian application 2,734,613 , namely Examiner's reports dated Apr. 25 , 2016 , Dec. 29 , 2016 and Dec. 6 , 2017 . Prosecution excerpts from Corresponding Chinese application 200980141567.8 , namely English translations of Examination reports dated Dec. 6 , 2012 , Oct. 15 , 2013 , May 5 , 2014 and Nov. 2 , 2014 . Machine translation of JP3927802 , published as JP2003-189325 . Prosecution excerpts from U.S. Appl . No. 13 / 774,056 ( now U.S. Pat . No. 8,755,837 ) . Prosecution excerpts from U.S. Appl . No. 12 / 484,115 ( now U.S. Pat . No. 8,385,971 ) . Further prosecution excerpts from corresponding Canadian appli cation 2,734,613 , namely Applicant's response dated Jun . 6 , 2018 ( to Examiner report dated Dec. 6 , 2017 ) ; Examiner report dated Oct. 25 , 2018 ; Applicant's response dated Apr. 24 , 2019 ; and Notice of \\n Allowance dated Jan. 22 , 2020 . LeCun et al , Handwritten Digit Recognition with a Back Propagation Network , Advances in Neural Information Processing Systems , pp . 396-404 , 1990 . Balan , Tactics - Based Remote Execution for Mobile Computing , MobiSys 2003 . Chun , Augmented Smartphone Applications Through Clone Cloud Execution , Proc . of the 8th Workshop on Hot Topics in Operating Systems , May 18 , 2009 . Flinn et al , Balancing Performance , Energy , and Quality in Perva sive Computing , Proc . of the 22nd International Conference on Distributed Computing Systems ( ICDCS ) , Jul . 2002 . Flinn , et al , Self - Tuned Remote Execution for Pervasive Comput ing , Proc . of the 8th Workshop on Hot Topics in Operating Systems ( HotOS ) , May 2001 . Satyanarayanan , The Case for VM - based Cloudlets in Mobile Computing , IEEE Pervasive Computing , vol . 8 , No. 4 , pp . 14-23 , \\n Nov. 2009 . Reichle , et al , A Comprehensive Context Modeling Framework for Pervasive Computing Systems , Distributed Applications and Interop erable Systems , Springer Berlin Heidelberg , 2008 . Geihs , et al , Modeling of Context - Aware Self - Adaptive Applica tions in Ubiquitous and Service - Oriented Environments , Software Engineering for Self - Adaptive Systems , Springer Berlin Heidelberg , \\n Jun . 19 , 2009 , pp . 146-163 . Kirsch - Pinheiro et al , Context - Aware Service Selection Using Graph Matching , 2nd Non Functional Properties and Service Level Agree ments in Service Oriented Computing Workshop ( NFPSLA SOC'08 ) , CEUR Workshop Proceedings , vol . 411. 2008 . Zachariadis , et al , Building Adaptable Mobile Middleware Services Using Logical Mobility Techniques , in Contributions to Ubiquitous Computing , Springer Berlin Heidelberg , 2007 , pp . 3-26 . Zachariadis , et al , Adaptable Mobile Applications : Exploiting Logi cal Mobility in Mobile Computing , in Mobile Agents for Telecom munication Applications , Springer Berlin Heidelberg , 2003 , pp . \\n 170-179 . Kemp , et al , eyeDentify : Multimedia cyber foraging from a smart phone , 11th IEEE International Symposium on Multimedia , Dec. 14 , 2009 , pp . 392-399 . JP2004179783 , 2004 , with machine translation . Prosecution excerpts from U.S. Appl . No. 12 / 835,527 , which matured into U.S. Pat . No. 8,886,206 . \\n * cited by examiner\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 2}), Document(page_content=\"U.S. Patent \\n 7 \\n ! 1 Feb.16 , 2021 \\n Show Times ? ? \\n Jane's Review Pretty Good 11 \\n My Car Sheet 1 of 60 \\n B08 '06 Vista ? \\n 0 0 0 0 0 0 \\n O US 10,922,957 B2 \\n FIG . 1\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 3}), Document(page_content='U.S. Patent \\n SEN SENSOR LENS CELL NETWORK TO SERVICES \\n CPU \\n LOW LEVEL PIXEL PROC . COMM CHANNEL INTERFACE Feb. 16 , 2021 \\n Tamil WIFI TO INTERNET Sheet 2 of 60 \\n TRAFFIC MONITORING & BILLING SYSTEM \\n FIG.1A US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 4}), Document(page_content='U.S. Patent \\n FIG . 2 \\n GET PRICES RETRIEVE MANUAL \\nGESTURE WATERMARK TEXT FACIAL RECOGNITION Feb. 16 , 2021 \\nNAVIGATION \\n OBJECT RECOGNITION \\n NEARBY ? BARCODE READING POST TO FACEBOOK \\n LIST FOR SALE ON CRAIGSLIST Sheet 3 of 60 \\n # $ % ^ \\n TRANSLATE TO ENGLISH \\n DISCOVER HISTORY COMMAND ENTRY HUMAN WEB ( DEGREES OF SEPARATION ) ACCESS AUTHENTICATION US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 5}), Document(page_content='NOKIA , APPLE , HTC , SAMSUNG ... \\n EN SENSOR LENS U.S. Patent \\nLEVEL CPU WIRELESS INTERFACE NETWORK INFRA STRUCTURE \\n Z \\n PROC . BROADCOM QUALCOMM ATHEROS VERIZON AT & T \\n ZEISS OMNI VISION MICRON SAMSUNG AVAGO TOSHIBA INTEL NVIDIA AMD MIPS CROWN CASTLE CISCO \\n AMERICAN TOWER ALCATEL - LUCENT \\n SBA COMM AT & T Feb. 16 , 2021 \\n T - MOBILE CLEAR Sheet 4 of 60 \\n DATA PROVIDERS SERVICE PROVIDERS PAYMENT PROCESSORS FIRST DATA \\n AMAZON GOOGLE \\n GOOGLE NIELSEN THOMSON THE WEB PAYPAL CITIBANK \\n FIG . 3 IBM RAUNHOFER STARTUPS US 10,922,957 B2 \\n O.', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 6}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 5 of 60 US 10,922,957 B2 \\n KEYVECTOR DATA ( PIXELS & DERIVATIVES ) \\n FIG . 4 ** tt ta \\n VISUAL \\n TASK \\n ? VISUAL \\n TASK VISUAL \\n TASK \\n C \\n RESULTA RESULTB RESULTC \\n KEYVECTOR \\n FIG . 4A \\n ? ADDRESSING , BA \\n ? \\n PIXEL PACKET', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 7}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 6 of 60 US 10,922,957 B2 \\n 1 \\n FIG . 4B \\n HEADER DATA KEYVECTOR DATA \\n TASKA TASKB TASK C TASKD \\n VISUAL TASK COMMONALITY PIE CHARTS \\n RE - SAMPLING \\n PDF417 BARCODE READING \\n FIG . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 8}), Document(page_content=\"U.S. Patent Feb. 16 , 2021 Sheet 7 of 60 US 10,922,957 B2 \\n RESIDENT CALL - ON VISUAL PROCESSING SERVICES \\n ON ON OFF ON OFF \\n T \\n } \\n 3 \\n COMMON \\n SERVICES \\n SORTER \\n LEVEL \\n PIXEL \\n PRO \\n CESSING \\n LIBRARY \\n FLOW GATE \\n CONFIGURATION , \\n SOFTWARE PROGRAMMING EXTERNAL \\n PIXEL \\n SEG SENSOR HARDWARE KEYVECTOR \\n DATA \\n ( PROC'D PIXELS & \\n OTHER ) ROUTING \\nFIG . 6 INTERNAL\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 9}), Document(page_content='DEVICE UI SENSOR \\nREGISTRY & CONFIGURATION U.S. Patent \\nQUERY ROUTER AND RESPONSE MANAGER \\nSERVICE RESPONSE MANAGER PIXEL PACKAGING & ROUTING Feb. 16 , 2021 \\n COMM CHANNEL ( ) Sheet 8 of 60 \\n LOCAL BUS - US 10,922,957 B2 \\n FIG . 7', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 10}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 9 of 60 US 10,922,957 B2 \\n ONISSpodd OBUWINO IOWM ARBITRARILY COMPLEX RESULTS \\n FIG . 8', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 11}), Document(page_content='$$ CUSTOMER BILLING U.S. Patent \\n BILL DATA S. DATA \\nCARRIER SWITCHBOARD \\n DEVICES PIXEL PACKET PROCESSING & ROUTING Feb. 16 , 2021 \\n DATA $ ---- >> \\n CARRIER SWITCHBOARD DATA DATA --- \\n $ Sheet 10 of 60 US 10,922,957 B2 \\n FIG.9', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 12}), Document(page_content='U.S. Patent \\n SUITED , E.G. , FOR TEMPLATE MATCHING SUITED , E.G. , FOR DATA ASSOCIATION \\n CELL PHONE I \\n GPU ( S ) PROCESSOR Feb. 16 , 2021 \\n PROCESSOR , REF , DATA \\n 1 \\n LENS , SENSOR , LOW LEVEL PIXEL PROCESSING PACKAGER / ROUTER DECIDE WHAT IS DONE LOCALLY , REMOTELY PROCESSOR \\n ROUTING , BILLING , TRAFFIC MONITORING \\n COM PROCESSOR Sheet 11 of 60 \\n REF , DATA \\n CPU ( S ) PROCESSOR US 10,922,957 B2 \\n FIG . 10', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 13}), Document(page_content='AS CLOSE TO THE RAW SENSOR DATA AS POSSIBLE AS LOW IN THE COMMUNICATIONS STACK AS POSSIBLE U.S. Patent \\n LOCAL CLOUD \\n MOBILE CAMERA VISUAL KEYVECTOR PROCESSING AND PACKAGING KEYVECTOR REQUEST AND RESPONSE ROUTER Feb. 16 , 2021 \\n ROUTER \\n VISUAL TASK LISTING WITH CONFIGURATION AND SERVICES ADDRESSING LOCAL DEVICE SERVICES PROCESSING REMOTE SERVICE PROVIDERS AND RICH CONTENT AGGREGATORS Sheet 12 of 60 \\n MOBILE THE TWO OVALS ARE SOFTWARE COMPONENTS OF A REAL - TIME LOCAL / REMOTE OBJECT RECOGNITION MOBILE DEVICE NETWORK . THE REMAINING HIGH LEVEL FLOW PROCESSES USE NATIVE CAPABILITIES A “ OF BOTH A MOBILE DEVICE , AS WELL AS \" THE CLOUD . \" US 10,922,957 B2 \\n FIG . 10A', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 14}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 13 of 60 US 10,922,957 B2 \\n MY PROFILE , INCLUDING WHO I AM , AND \\n WHAT DO I GENERALLYLKENWANT \\n MY CONTEXT , INCLUDING WHERE I AM , AND \\n WHAT ARE MY CURRENT DESIRES \\n MY VISUAL QUERY PIXELS THEMSELVES \\n PACKETS RESULTS \\n VISUAL QUERY PACKET \\n PACKETS RESULTS PACKETS RESULTS \\n CONFIGURABLE ARRAY OF \\n SERVICE PROVIDERS OFFERED AT AUCTION TO AN \\n ARRAY OF SERVICE PROVIDERS \\n FIG . 11', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 15}), Document(page_content='USER DEVICE U.S. Patent \\nRESULTS VISUAL QUERY PACKET Feb. 16 , 2021 \\n VISUAL QUERY QUICK , HIGH VALUE RESPONSE \\n ACCOUNTED AND PACKAGED RESPONSE ACCOUNTED AND PACKAGED RESPONSE \\n QUERY ROUTER AND RESPONSE MANAGER Sheet 14 of 60 \\n BIDDER A \\n FIXED SERVICE CHOICE QUERY PUT OUT TO BID BIDDERB \\n SERVICE A BIDDER C \\n SERVICE B BID FILTER AND BROADCAST AGENT www.ws BIDDERD WINNING BID \\n SERVICE C - BID ---- US 10,922,957 B2 \\n BIDDER F \\n FIG . 12 BIDDERG', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 16}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 15 of 60 US 10,922,957 B2 \\n 12 \\n SENSOR \\n 17 PROCESSING FOR OBJECT \\n IDENTIFICATION \\n PROCESSING FOR \\n HUMAN VISUAL \\n SYSTEM ( e.g. , JPEG compression ) ADDRESSING \\n 13 \\n FIG . 13 \\n 16 \\n ********* \\n 56 \\n STAGE 1 STAGE 2 \\n INSTRUCTIONS INSTRUCTIONS INSTRUCTIONS STAGE Y \\n INSTRUCTIONS \\n 58a 58b \\n STAGE Z \\n INSTRUCTIONS \\n FIG . 17', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 17}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 16 of 60 US 10,922,957 B2 \\n CELL PHONE \\n 1 \\n PROCESSING FOR OBJECT \\n IDENTIFICATION \\n FIG . 14 14 \\n ADDRESSING \\n 15 \\n APPLICATION \\n IN CELL \\n PHONE PROCESSING \\n EXTERNAL FROM \\n CELL PHONE APPLICATION \\n PHONE \\n APPLICATION \\n EXTERNAL FROM \\n CELL PHONE \\n EXTERNAL FROM \\n CELL PHONE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 18}), Document(page_content='A r \\n 40 U.S. Patent \\n 27 : Feb. 16 , 2021 \\n HEINZ TOMATO ETCHUP 46 \\n 43 Sheet 17 of 60 \\nMARMITE 19.9 \\n Nora US 10,922,957 B2 \\n TROVA \\n FIG . 15 48 42 45', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 19}), Document(page_content='16 \\n 53 \\n 3 U.S. Patent \\n 1 for \\n $ \\n 2 \\n W PIPE MANAGER 52 \\n & \\n 7 I 5 Feb. 16 , 2021 \\n 8 33 216.239.32.10 \\n 5 33 \\n FREECE \\n 9 38a ????? \\n 60 STAGE 2a \\n 32 380 \\n SETUP PACKET CAMERA PACKET STAGE 1 PACKET STAGE 3 PACKET STAGE 4 Sheet 18 of 60 \\n 380 \\n STAGE 2 \\n 31 31 \\n 38b \\n 31 \\n 36 12.232.235.27 \\n 5 $ \\n CONTROL PROCESSOR 35 FIG . 16 US 10,922,957 B2 \\n SYNCHRON IZATION', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 20}), Document(page_content='U.S. Patent \\nHARDWARE 73 Feb. 16 , 2021 \\nSOFTWARE \\n 32 \\n CAMERA ori & mag 25 bits ) ORIENTATION AND GRADIENT MAGNITUDE COMPUTATION FIFOA ( 32x2048 ) 1 FIFOB ( 32x1024 ) FIFOC ( 32x512 ) DESCRIPTOR COMPUTATION ( NIOS II FPGA ) \\n 72 Sheet 19 of 60 \\n 74 \\n GAUSSIAN CASCADE AND DIFFERENCE OF GAUSSIANS FIFO octv0 ( 42x127 ) FIFO_octvo ( 40x256 ) FIFO octvo ( 38x512 ) KEYPOINT DETECTION WITH STABILITY CHECKS 1 FIG . 18 US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 21}), Document(page_content=\"MEMORY \\nPROCESSING STAGE U.S. Patent \\n < 38 > parameters < / 38 > \\nPROCESSING u \\n 38c < 58e ' > Instructions ... < / 58e ' > \\n < 587 > instructions < 58g > Instructions < / 58g > Feb. 16 , 2021 \\n *** \\n 57 . Sheet 20 of 60 \\n STAGE 3 INSTRUCTIONS STAGE 4 INSTRUCTIONS , ... ; EDGE FILTER DATA , APPEND RESULTS TO PACKET BODY , CHECK RESULTS V. THRESHOLD . IF GREATER , REPLACE HEADER FIELDS 588-589 WITH INSTRUCTIONS FROM MEMORY LOCATION F000 \\n 58c 580 58e 58f 689 US 10,922,957 B2 \\n 56 \\n FIG . 19\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 22}), Document(page_content='U.S. Patent \\nCAMERA DRIVER \\n MIC . DRIVER CM VISION Feb. 16 , 2021 \\n MOBILE ROBOT ( CLIENT ) FIXED SERVER \\n Z ? PROTOCOL \\n GPS DRIVER OTHER SERVICES \\n OTHER FIXED ROBOTIC RESOURCES Sheet 21 of 60 \\n SENSOR DRIVER FIG . 19A Prior Art Robotic Architecture US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 23}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 22 of 60 US 10,922,957 B2 \\n RESPONSE \\n ROUTING \\n CONSTRAINTS NEEDED \\n OPERATION DETAILS High level operations USER \\n PREFERENCES HARDWARE \\n What special purpose \\n hardware is local ? \\n What is current hardware utilization ? CPU pipeline length Pipeline stall risks POWER operations \\n Common operations Operations that are preconditions to \\n others Power consumption CONNECTIVITY \\n GEOGRAPHICAL \\n CONSIDERATIONS RULES , \\n HEURISTICS \\n INFO RE REMOTE \\n PROVIDER ( S ) , E.G. , \\n Readiness Speed \\n Cost \\n importance to user \\n OPERATIONS FOR \\n LOCAL PROCESSING OPERATIONS FOR \\n REMOTE PROCESSING \\n FIG . 19B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 24}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 23 of 60 US 10,922,957 B2 \\n 81 pana \\n 80 \\n 12 \\n CAMERA \\n SENSOR \\n 82 83 86 \\n 85 ? MICRO \\n MIRROR \\n PROJECTOR \\n I \\n w \\n FIG . 20', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 25}), Document(page_content='MICROPHONE CAMERA & 3 FIG . 20A 3 IC HW 3 3 3 U { Digital Watermarking 3 DSP 3 3 $ \\n 1D / 2D Bar Code Reader 3 OCR 3 ? 3 3 Fingerprinting 3 CPU Reference Platform uteisks Gunesedo 3 suopeoiddy 3 Image / Facial Recognition ? OpenCL ? 3 3 \\n Emotion Classifier 3 3 GPU Image Classification $ 3 OpenGL 3 3 ? Facial Recogn . Image Recogn . Fingerprint SOINS 0010 US 10,922,957 B2 Sheet 24 of 60 Feb. 16 , 2021 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 26}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 25 of 60 US 10,922,957 B2 \\n FIG . 21 \\n 3 : \\n FIG . 22', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 27}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 26 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE \\n 1 \\n 1 \\n 1 SUBMIT TO GOOGLE ; RECEIVE AUXILIARY INFORMATION PRE - PROCESS IMAGE , \\n e.g. , IMAGE ENHANCEMENT , \\n OBJECT SEGMENTATIONI \\n EXTRACTION , etc. \\n SUBMIT TO SERVICE \\n FEATURE EXTRACTION \\n FIG . 24 \\n SEARCH FOR IMAGES WITH \\n SIMILAR METRICS CAPTURE OR RECEIVE IMAGE \\n HARVEST AND RANK FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMILAR METRICS \\n SUBMIT TO SERVICE USE SIMLAR IMAGE DATA IN \\n LEU OF USER IMAGE DATA \\n FIG , 25 FIG . 23', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 28}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 27 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEVE IMAGE CAPTURE OR RECEIVE IMAGE \\n FEATURE EXTRACTION FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS SEARCH FOR IMAGES WITH \\n SIMLAR METRICS \\n SUBMIT PURAL SETS OF \\n IMAGE DATA TO SERVICE \\n PROVIDER COMPOSITE OR MERGE \\n SEVERAL SETS OF IMAGE DATA \\n SUBMIT TO SERVICE PROVIDER \\n SERVICE PROVIDER , OR USER \\n DEVICE , DETERMINES \\n ENHANCED RESPONSE BASED \\n ON PLURAL SETS OF DATA FIG . 27 \\n CAPTURE OR RECEVE USER IMAGE FIG . 26 I \\n FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS \\n USE INFO FROM SIMILAR \\n IMAGE ( S ) TO ENHANCE USER \\n FIG , 28', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 29}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 28 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS INPUT GEOLOCATION INFO TO \\n DB ; GET TEXTUAL \\n DESCRIPTORS \\n HARVEST METADATA SEARCH IMAGE DATABASE \\n USING TEXTUAL DESCRIPTORS ; IDENTIFY OTHER IMAGE ( S ) OF SUBJECT SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n SUBMT TO SERVICE \\n SUBMIT TO SERVICE \\n FIG . 30 \\n FIG . 28A \\n CAPTURE OR RECEIVE MAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH IMAGE DATABASE \\n USING GEOLOCATION DATA ; \\n IDENTIFY OTHER IMAGE ( S ) OF \\n SUBJECT \\n SUBMIT OTHER IMAGE ( S ) TO \\n SERVICE \\n FIG . 31', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 30}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 29 of 60 US 10,922,957 B2 \\n FIG . 29 FIG . 35 \\n FIG . 36 FIG . 37 \\nA \\n FIG . 38', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 31}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 30 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH FOR IMAGES WITH \\n SIMILAR IMAGE METRICS SEARCH FOR IMAGES WITH \\n SIMILAR IMAGE METRICS \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES HARVEST METADATA \\n FIG . 32 SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n FIG . 33 \\n SEARCH FOR IMAGES WITH \\n MATCHING GEOLOCATION \\n SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES \\n FIG . 34', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 32}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 31 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n URBAN / RURAL \\n ANALYSIS FAMILYFRIENDI \\n STRANGER ANALYSIS CONSUMER \\n PRODUCTIOTHER \\n ANALYSIS \\n RESPONSE \\n ENGINE # RESPONSE \\n ENGINE # 2 RESPONSE \\n ENGINE # 3 \\n FIG . 49 CONSOLDATOR \\n CELL PHONE USER INTERFACE \\n IMAGE \\n 2 PAGE 1 3 \\n IMAGE \\n 1 ROUTER \\n PAGE 2 PAGE 3 \\n RESPONSE \\n ENGINE A RESPONSE \\n ENGINE B \\n PAGE 4 \\n FIG . 37A FIG . 37B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 33}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 32 of 60 US 10,922,957 B2 \\n USER - IMAGE ( algorithmic , crowd - sourced , etc. ) \\n USER IMAGE - RELATED FACTORS \\n ( First Tier ) e.g. , data about one or more of : Color histogram , Pattern , Shape , Texture , Facial recognition , Eigenvalue , Object recognition , Orientation , Text , Numbers , \\n OCR , Barcode , Watermark , Emotion , Location , Transform data , etc 1 \\n SEARCH ENGINE DATABASE OF PUBLC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n OTHER \\n IMAGES / CONTENT \\n ( Second Tier ) OTHER IMAGE - RELATED \\n FACTORS AND / OR \\n OTHER METADATA \\n ( Second Tier ) \\n SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n un www . 13 \\n FINAL \\n IMAGES / CONTENT FINAL OTHER \\n INFORMATION \\n USER FIG . 39', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 34}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 33 of 60 US 10,922,957 B2 \\n USER - IMAGE \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n USER IMAGE - RELATED FACTORS \\n ( First Tier ) \\n SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n OTHER \\n IMAGES / CONTENT \\n ( Second Tier ) OTHER IMAGE - RELATED \\n FACTORS AND / OR \\n OTHER METADATA \\n ( Second Tier ) \\n SEARCH ENGINE GOOGLE TEXT , \\n WEB DATABASE \\n 1 SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n FINAL \\n IMAGES / CONTENT \\n ( Nth Tier ) INFORMATION \\n ( Nth Tier ) \\n USER FIG . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 35}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 34 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n Culmage eigenvalues ; \\n Image classifier concludes \" drill \" ... \\n SEARCH ENGINE DATABASE OF PUBLIC \\n IMAGES , \\n IMAGE - RELATED FACTORS , \\n AND OTHER METADATA \\n Similar looking Text metadata for similar - looking pix \\n PROCESSOR \\n Ranked metadata , incl . \" Drill , \" \\n \" Black & Decker and \" DR250B \" \\n GOOGLE \\n SEARCH DATABASE OF \\n PUBLIC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA ONE OR MORE \\n IMAGE - BASED \\n ROUTING \\n SERVICES ( may employ user , preference data ) , \\n E.G. , SNAPNOW Pix with similar metadata BROWSER \\n Cached \\n FIG . 41 pages \\n USER INTERFACE OOOO \\n Options presented may include \" Similar looks , \" \" Similar descriptors , \" \" Web \\n results , \" \" Buy , \" \" Sell , \" \" Manual , \" \" More , \" \" SnapNow , \" etc.', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 36}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 35 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n 2.Geolocation coordinates , Data re straight edge / arced edge density ... \\n SEARCH ENGINE DATABASE OF PUBLIC \\n IMAGES , \\n IMAGE - RELATED FACTORS , \\n AND OTHER METADATA Pix that are similarly \\n located \\' , with similar edge / arc density Text metadata for pix that are similarly - located , with similar edgelarc density \\n PROCESSOR \\n Ranked metadata , incl . \" Eiffel Tower , \" \\n and \" Paris \" \\n FIG . 42 GOOGLE \\n SEARCH \\n ENGINE DATABASE OF \\n PUBLC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA ONE OR MORE \\n IMAGE - BASED \\n ROUTING \\n SERVICES ( may employ user , preference data ) , \\n E.G. , SNAPNOW Pix with similar metadata BROWSER \\n Cached \\n web \\n pages Ul options presented may include * Similar looks , \" \" Similar descriptors , \" \" Similar location , \" \" Web results , \" \" \" SnapNow , \" Map , Directions , \\n History , Search nearby , etc. USER INTERFACE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 37}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 36 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" \\n ( algorithmic , crowd - sourced , etc. ) \\n ... Geolocation coordinates , Eigenvectors .... \\n DATABASE OF \\n PUBLIC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA SEARCH ENGINE MLS DATABASE OF \\n IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA \\n Neighborhood \\n scenes , text \\n metadata from similarly - located images Address , zip code , school district , square footage , price , bedrooms / \\n baths , acreage , time on market , data / imagery re similarly - located houses , similarly - priced houses , similarly - featured houses , etc \\n GOOGLE , \\n GOOGLE EARTH \\n USER INTERFACE \\n FIG . 43', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 38}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 37 of 60 US 10,922,957 B2 \\n 0 K 999 00 O XO O \\n ???? ?? ?? wuuwl ! OUW nor * An Anarnom vu ???????? 5 \\n * \\n FIG . 44 \\n 114 \\n 1201 \\n 120a 116a \\n 122a 1165 \\n 122b 118 \\n 1160 ) a ) OOOO 2000 202 124 \\n 8 O 112 R \\n 8 \\n 8 \\n db o og oo \\n o oo oo 126a 126b', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 39}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 38 of 60 US 10,922,957 B2 \\n o \\n 130 132 \\n 128 \\n FIG . 45A \\n??????? ?? ?? \\n 142 \\n FIG . 458', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 40}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 39 of 60 US 10,922,957 B2 \\n GET GEOCOORDINATES FROM IMAGE \\n SEARCH FLICKR , ETC , FOR SIMILARLY LOCATED IMAGES ( AND / OR SIMILAR IN OTHER WAYS ) ; \" SET 1 \" IMAGES \\n HARVEST , CLEAN - UP AND CLUSTER \\n METADATA FROM SET 1 IMAGES Rockefeller Center ( 21 ) New York ( 18 ) \\n NYC ( 10 ) Manhattan ( 8 ) Empire State Building ( 4 ) \\n Midtown ( 4 ) Top of the rock ( 4 ) Nueva York ( 3 ) \\n USA ( 3 ) \\n 30 Rock ( 2 ) August ( 2 ) \\n Atlas ( 2 ) Christmas ( 2 ) Cityscape ( 2 ) \\n Fifth Avenue ( 2 ) Me ( 2 ) \\n Prometheus ( 2 ) Skating rink ( 2 ) Skyline ( 2 ) Skyscraper ( 2 ) \\n Clouds ( 1 ) General Motors Building ( 1 ) Gertrude ( 1 ) FROM METADATA , DETERMINE \\n UKELHOOD THAT EACH MAGE IS \\n PLACE - CENTRIC ( CLASS 1 ) \\n TI \\n FROM METADATA AND FACE - FINDING , \\n DETERMINE LIKELIHOOD THAT EACH \\n IMAGE IS PERSON - CENTRIC ( CLASS 2 ) \\n Lights ( 1 ) \\n Statue ( 1 ) A FROM METADATA , AND ABOVE \\n SCORES , DETERMINE LIKELIHOOD \\n THAT EACH IMAGE IS THING - CENTRIC \\n ( CLASS 3 ) \\n DETERMINE WHICH MAGE METRICS \\n RELIABLY GROUP LIKE - CLASSED \\n IMAGES TOGETHER , AND DISTINGUISH \\n DIFFERENTLY - CLASSED IMAGES \\n FIG . 46A', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 41}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 40 of 60 US 10,922,957 B2 \\n APPLY TESTS TO INPUT IMAGE : \\n DETERMINE FROM METRICS ITS \\n SCORE IN DIFFERENT CLASSES \\n PERFORM SIMILARITY TESTING \\n BETWEEN INPUT IMAGE AND \\n EACH IMAGE IN SET 1 \\n WEIGHT METADATA FROM EACH \\n IMAGE IN ACCORDANCE WITH \\n SIMILARITY SCORE : COMBINE 19 Rockefeller Center \\n 12 Prometheus 5 Skating rink \\n SEARCH IMAGE DATABASE FOR \\n ADD\\'L SIMILARLY - LOCATED \\n IMAGES , THIS TIME ALSO USING \\n INFERRED POSSIBLE METADATA \\n AS SEARCH CRITERIA : \" SET 2 \" ? C New York ( 17 ) Rockefeller Center ( 12 ) Manhattan ( 6 ) Prometheus ( 5 ) Fountain ( 4 ) Christmas ( 3 ) Paul Manship ( 3 ) Sculpture ( 3 ) Skating Rink ( 3 ) Jerry ( 1 ) \\n National register of historical places ( 2 ) \\n Nikon ( 2 ) RCA Building ( 2 ) Greek mythology ( 1 ) PERFORM SIMILARITY TESTING \\n BETWEEN INPUT IMAGE AND \\n EACH IMAGE IN SET 2 \\n IMAGE IN ACCORDANCE WITH \\n SIMILARITY SCORE ; COMBINE \\n 5 Prometheus \\n FURTHER WEIGHT METADATA IN \\n ACCORDANCE WITH \\n UNUSUALNESS 3 Paul Manship \\n 3 Rockefeller Center 3 Sculpture \\n 2 National register of historical places 2 RCA Building 1 Greek mythology FIG . 46B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 42}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 41 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n GEO INFO \\n DATABASE PROFILE DATA \\n PLACE - TERM \\n GLOSSARY GOOGLE , ETC \\n PICASA \\n DATA IMAGE \\n ANALYSIS \\n PROCESSING \\n ENGINE FLICKR \\n PERSON - NAME \\n GLOSSARY WORD \\n FREQUENCY \\n DATABASE \\n FACEBOOK \\n DATA \\n GOOGLE \\n FIG . 47 \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 43}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 42 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n PERSON / PLACE / \\n THING ANALYSIS \\n FAMILY / FRIENDI \\n STRANGER ANALYSIS CHILD / ADULT \\n ANALYSIS \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE \\n FIG . 48A \\n USER SNAPS IMAGE \\n FAMILY / FRIEND ! \\n STRANGER ANALYSIS PERSON PLACE \\n THING ANALYSIS CHILD / ADULT \\n ANALYSIS \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE \\n FIG . 48B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 44}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 43 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n FACIAL \\n RECOGNITION TRANSFORM WAVELET \\n TRANSFORM TRANSFORM \\n OCR EDGE \\n DETECTION EIGENVALUE \\n ANALYSIS SPECTRUM \\n ANALYSIS \\n PATTERN \\n EXTRACTION FOURIER - MELLIN \\n TRANSFORM TEXTURE COLOR \\n CLASSFER ANALYSIS \\n GIST \\n PROCESSING EMOTION \\n CLASSIFIER BARCODE \\n DECODER AGE \\n DETECTION DATABASES , EXTERNAL RESOURCES \\n WATERMARK | ORIENTATION \\n DECODER DETECTION GENDER \\n DETECTION PROCESSOR \\n OBJECT \\n SEGMENTATION METADATA \\n ANALYSIS GEOLOCATION \\n PROCESSING \\n SERVICE \\n PROVIDER 2 SERVICE \\n PROVIDERN PROVIDER 1 \\n CELL PHONE USER INTERFACE \\n FIG . 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 45}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 44 of 60 US 10,922,957 B2 \\n INPUT IMAGE , OR \\n INFORMATION ABOUT \\n THE MAGE \\n THING PLACE PERSON \\nSTRANGE FRIEND --FAMILY --- --- RECREATION -HOME \\n OTHER 000 : BOAT -TRUCK mommm CAR SHIRT -OUTERWEAR ( a ) Post annotated photo to user\\'s Facebook page ( b ) Start a text message to depicted person \\n 2 \\n ? \\n $ \\n ( a ) Show estimations of who this may be ( b ) Send \" Friend \" invite on MySpace ( c ) Who are the degrees of separation between us \\n ( a ) Buy online \\n ( b ) Show nutrition information ( c ) Identify local stores that sell \\n FIG , 51', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 46}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 45 of 60 US 10,922,957 B2 \\n COMPUTER A \\n CONTENT SET A ROUTER \\n ROUTERA COMPUTER B \\n RESPONSE \\n ENGINE 1A RESPONSE \\n ENGINE 2A CONTENT SET B \\n COMPUTER C RESPONSE \\n CONTENT SET C \\n ENGINE C \\n CONTENT SET D \\n COMPUTERE ROUTER D \\n CONTENT SETE \\n 150 \\n RESPONSE \\n ENGINE \\n FIG . 52', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 47}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 46 of 60 US 10,922,957 B2 \\n FIG . 53 \\n BABYCAM FEATURE \\n VECTORS ( URL , etc. ) \\n 1E 5G 18D 23 8F BABYCAM WWW.SMITH.HOME . \\n COM / BAGYCAM.HTM \\n 52 88 26 A279 TRAFFIC MAP WWW.TRAFFIC.COM \\n SEATTLE \\n TRAFFIC MAP.HTML Ý \\n o \\n FIG . 55 \\n SIGN GLOSSARY FIG . 54', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 48}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 47 of 60 US 10,922,957 B2 \\n IMAGE SENSORS \\n AMPLIFY ANALOG SIGNALS \\n CONVERT TO DIGITAL NATIVE IMAGE DATA \\n BAYER INTERPOLATION \\n WHITE BALANCE CORRECTION \\n I T \\n GAMMA CORRECTION \\n T \\n EDGE ENHANCEMENT \\n JPEG COMPRESSION \\n STORE IN BUFFER MEMORY \\n DISPLAY CAPTURED IMAGE ON SCREEN \\n ON USER COMMAND , STORE IN CARD AND / OR \\n TRANSMIT \\n FIG . 56 \\n ( Prior Art )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 49}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 48 of 60 US 10,922,957 B2 \\n IMAGE SENSORS \\n AMPLIFY ANALOG \\n SIGNALS FIG . 57 \\n CONVERT TO DIGITAL \\n NATIVE IMAGE DATA \\n BAYER \\n INTERPOLATION FOURIER \\n TRANSFORM EIGENFACE \\n CALCULATION ETC \\n In and \\n Etc ( as in Fig . 56 ) \\n MATCH \\n BARCODE \\n TEMPLATE ? MATCH FACE \\n TEMPLATE ? Yes \\n TRANSFORM SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR \\n FOURIER DOMAIN DATA \\n TO BARCODE SERVICE \\nHOATA ANDIOREM MATCH TEXT \\n TEMPLATE ? SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR F - M \\n DOMAIN DATA TO OCR Yes SERVICE \\n SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR E - M \\n DOMAIN DATA TO WM \\n SERVICE MATCH WM \\n ORIENTATION \\n Yes \\n SEND DIGITAL NATIVE MATCH FACE \\n TEMPLATE ? Yes \\n DOMAIN DATA AND / OR \\n EIGENFACE DATA TO', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 50}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 49 of 60 US 10,922,957 B2 \\n CURRENT ROTATION \\n FROM APPARENT 12 degrees right \\n HORIZON : \\n FIG . 58 \\n 17 degrees left SINCE \\n START \\n # 1733 WU *** 31 U 11:11 HD SCALE \\n CHANGE SINCE 11 1 30 % closer -- > \\n IMAGE CAPTURE \\n PROCESSING \\n ANALYSIS , MEMORY \\n FIG . 59 DEPENDENT ON \\n IMAGE DATA', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 51}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 50 of 60 US 10,922,957 B2 \\nE WELCOME ” To Sauerlana \\n FIG . 60 \\n TI \\n FIG , 65 FIG . 66 \\nWELCOS WELCOME ro LAS VEGAS LAS VEGAS romantic LAS VEGAS \\n 1 \\n FIG . 62 FIG . 63 FIG . 64', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 52}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 51 of 60 US 10,922,957 B2 \\n FOURIER TRANSFORM \\n DATA \\n BARCODE \\n FOURIER - MELLIN \\n TRANSFORM DATA \\n OCR INFORMATION \\n WATERMARK \\n INFORMATION \\n DCT DATA \\n MOTION INFORMATION \\n METADATA \\n EIGENFACE DATA \\n FACE INFORMATION \\n SOBEL FILTER DATA \\n TRANSFORM DATA \\n COLOR HISTOGRAM \\n DATA \\n FIG . 61', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 53}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 52 of 60 US 10,922,957 B2 \\n . \\n 1.5 \\n ? XGES \\n GMC \\n ? ? ? ? ” ? \\n ? , ** * \\n € ? ? ? ? \\n 22 \\n · ### ? ?? ? , ###### . ### * ? ? # ? ? # , #### \\n $ 3 9 \\n 2 ## 2 # V > \\n 2 + \\n 2 . ? ? ? ? ? 4 · \\n 1 ? ? ### ?? \\n 3 * O ### - # ?? ?? rt ? ? # ?? ? ? : 4 # #### ?? ? ?? , \\n ??? ? ? ?? 4 : #### ? ? ?? ! \\n # ?? FE ? , ? ? ? ?? ? ? , # ? , \\n ? ? * ### ## : * 4 \\n y . 3 ? 13 ? * y \\n ? Ir X 1', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 54}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 53 of 60 US 10,922,957 B2 \\n R G B \\n FIG . 69 \\n ? \\n FIG . 70 FIG.71', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 55}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 54 of 60 US 10,922,957 B2 \\n Spatial Frequency ( Cycles / Degree ) \\n 1 10 0.1 100 \\n 1,000 \\n Invisible \\n Threshold Contrast Sensitivity \\n ( 1 / Threshold Contrast ) Visible \\n 1 \\n 0.1 FIG . 72 Spatial Frequency ( Cycles / MM On Retina ) \\n 103 , \\n FIG . 73', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 56}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 55 of 60 US 10,922,957 B2 \\n THE WALL STREET JOURNAL : \\n * .. * ce V. \\n . 2 \\n th + -1 *** \\n 2 X \\n N \" \\n KXX \\n 7 7 S. 1 \\n 1 \\n TX \\n HY 19 ? \\n 2 2 . 2 \\n 2 \\n 7 \\n 11 , \\n *** \\n * I \\n davca \\n ty \\n WY \\n 14 \\n N \\n . \\n FIG . 74 FIG . 75 \\n Watermark Origin Captured Image \\n Frame ? HE WALL STREET JOURNAL Offset of \\n { Decoded \\n Watermark \\n FIG . 76', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 57}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 56 of 60 US 10,922,957 B2 \\n START START \\n ROUGH \\n OUTLINES TRUCK \\n LARGE \\n DETAILS PERSON \\n INTERMEDIATE \\n DETAILS OCR FONT TEXT \\n FACIAL \\n EIGENVALUES \\n R , G , B Data Alpha Channel Data \\n FIG . 77A FIG . 77B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 58}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 57 of 60 US 10,922,957 B2 \\n THERMOSTAT \\n Indicators 526 \\n 522 transceiver \\n LCD Display 514 \\n 520 Temperature \\n Sensor \\n Buttons \\n 518 MEMORY : \\n Op . sys . Processor 524 516 FIG . 78 ( Prior \\n Art ) -512 \\n 518 \\n 522 75.5 OG \\n 520 \\n FIG . 79 ( Prior \\n Art )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 59}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 58 of 60 US 10,922,957 B2 \\n THERMOSTAT \\n 526 \\n transceiver \\n Indicators 514 \\n 522 h Temperature \\n Sensor } 1 T. 528 Location \\n ( GPS ) \\n 596 MEMORY : Op . sys . Ap SW \\n Data Processor \\n 532 34 \\n FIG . 80 \\n 536 \\n 530 \\n 530 552 \\n CELL PHONE mwy ramena SERVER FIG . 82 \\n olarak 1 555 554 \\n ROUTER NAMESPACE \\n DATABASE \\n 556a 556b -530 \\n ? ?? ? \\n SERVER mwingine hanno THERMOSTAT \\n ( OR OTHER DEVICE )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 60}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 59 of 60 US 10,922,957 B2 \\n CELL PHONE V \\n RF transceiver \\n Microphone \\n Network adapter \\n Camera \\n 544 Location \\n ( e.g. , GPS ) \\n Processor 596 \\n 542 \\n Display MEMORY : \\n Op . sys . \\n Touchscreen SW Modules \\n Etc. 546 \\n Physical UI \\n -540 \\n FIG . 81 \\n ALARM CLOCK 580 \\n 588 \\n Display Physical UI Bluetooth 582 \\n 584 \\n Processor Memory \\n 588 590 \\n 1 Location 592 598 FIG . 85', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 61}), Document(page_content=\"U.S. Patent Feb. 16 , 2021 Sheet 60 of 60 US 10,922,957 B2 \\n SETPOINT TEMP : 72.0 \\n CURRENITEMP : 70.7 SETPPOINT TEMP : 72.0 \\n CURRENTIEMP : 70.7 \\n 564 \\n 2 \\n ? \\n 562 562 JG MG 8 566 \\n 560 5602 \\n FIG . 83 FIG . 84 \\n 595 598 \\n PRESS DIGIT KEYS TO \\n SET ALARM TIME TO : NEARBY DEVICES : \\n 05 : 3 0 1. THERMOSTAT ( 4 ' away ) \\n ( Outside Conf Rm 1500 ) \\n 2. LaserJet Printer ( 15 * away ) \\n ( Cubicle 1250 ) a.m. \\n THEN PRESS ' OK \\n BUTTON TO SET ALARM PRESS DIGIT , THEN ' OK ' BUTTON TO SELECT \\n FIG . 86 FIG . 87\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 62}), Document(page_content='15 \\n 20 \\n 30 US 10,922,957 B2 \\n 1 2 \\n METHODS AND SYSTEMS FOR CONTENT In early roll - out , the class of recognizable objects will be \\n PROCESSING limited but useful . Object identification events will primarily fetch and associate public domain information and social RELATED APPLICATION DATA web connections to the baubles . Applications employing 5 barcodes , digital watermarks , facial recognition , OCR , etc. , This application is a division of application Ser . No. can help support initial deployment of the technology . 14 / 456,784 , filed Aug. 11 , 2014 ( now U.S. Pat . No. 9,886 , Later , the arrangement is expected to evolve into an 845 ) , which is a division of application Ser . No. 13 / 011,618 , auction market , in which paying enterprises want to place filed Jan. 21 , 2011 ( now U.S. Pat . No. 8,805,110 ) , which is their own baubles ( or associated information ) onto highly a continuation of co - pending PCT application PCT / US09 / 10 targeted demographic user screens . User profiles , in con 54358 , filed Aug. 19 , 2009 ( published as WO2010022185 ) . junction with the input visual stimuli ( aided , in some cases Application PCT / US09 / 54358 claims priority benefit to by GPS / magnetometer data ) , is fed into a Google - esque each of the following provisional applications : mix - master in the cloud , matching buyers of mobile device 61 / 090,083 , filed 19 Aug. 2008 ; screen real estate to users requesting the baubles . 61 / 096,703 , filed 12 Sep. 2008 ; Eventually , such functionality may become so ubiquitous 61 / 100,643 , filed 26 Sep. 2008 ; as to enter into the common lexicon , as in “ I\\'ll try to get a 61 / 103,907 , filed 8 Oct. 2008 ; Bauble on that ” or “ See what happens if you Viewgle that 61 / 110,490 , filed 31 Oct. 2008 ; scene . \" 61 / 169,266 , filed 14 Apr. 2009 ; 61 / 174,822 , filed 1 May 2009 ; BACKGROUND 61 / 176,739 , filed 8 May 2009 ; 61 / 226,195 , filed 16 Jul . 2009 ; and Digimarc\\'s U.S. Pat . No. 6,947,571 shows a system in 61 / 234,542 , filed 17 Aug. 2009 . which a cell phone camera captures content ( e.g. , image Application PCT / US09 / 54358 also claims priority to , and data ) , and processes same to derive an identifier related to may be regarded as a continuation - in - part of , each of the 25 the imagery . This derived identifier is submitted to a data following applications : structure ( e.g. , a database ) , which indicates corresponding Ser . No. 12 / 271,692 , filed 14 Nov. 2008 ( now U.S. Pat . data or actions . The cell phone then displays responsive No. 8,520,979 ) ; information , or takes responsive action . Such sequence of Ser . No. 12 / 484,115 , filed 12 Jun . 2009 ( now U.S. Pat . No. operations is sometimes referred to as \" visual search . ” 8,385,971 ) ; and Related technologies are shown in patent publications Ser . No. 12 / 498,709 , filed 7 Jul . 2009 ( published as 20080300011 ( Digimarc ) , U.S. Pat . No. 7,283,983 and 20100261465 ) WO07 / 130688 ( Evolution Robotics ) , 20070175998 and Priority is claimed to each of the foregoing applications . The 20020102966 ( DSPV ) , 20060012677 , 20060240862 and foregoing applications are incorporated herein by reference , 20050185060 ( Google ) , 20060056707 and 20050227674', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 63}), Document(page_content=\"in their entireties . 35 ( Nokia ) , 20060026140 ( ExBiblio ) , U.S. Pat . No. 6,491,217 , 20020152388 , 20020178410 and 20050144455 ( Philips ) , INTRODUCTION 20020072982 and 20040199387 ( Shazam ) , 20030083098 ( Canon ) , 20010055391 ( Qualcomm ) , 20010001854 ( Air Certain aspects of the technology detailed herein are Clic ) , U.S. Pat . No. 7,251,475 ( Sony ) , U.S. Pat . No. 7,174 , introduced in FIG . 1. A user's mobile phone captures 40 293 ( Iceberg ) , U.S. Pat . No. 7,065,559 ( Organnon Wireless ) , imagery ( either in response to user command , or autono- U.S. Pat . No. 7,016,532 ( Evryx Technologies ) , U.S. Pat . mously ) , and objects within the scene are recognized . Infor- Nos . 6,993,573 and 6,199,048 ( Neomedia ) , U.S. Pat . No. mation associated with each object is identified , and made 6,941,275 ( Tune Hunter ) , U.S. Pat . No. 6,788,293 ( Silver available to the user through a scene - registered interactive brook Research ) , U.S. Pat . Nos . 6,766,363 and 6,675,165 visual “ bauble ” that is graphically overlaid on the imagery . 45 ( BarPoint ) , U.S. Pat . No. 6,389,055 ( Alcatel - Lucent ) , U.S. The bauble may itself present information , or may simply be Pat . No. 6,121,530 ( Sonoda ) , and U.S. Pat . No. 6,002,946 an indicia that the user can tap at the indicated location to ( Reber / Motorola ) . obtain a lengthier listing of related information , or launch a Aspects of the presently - detailed technology concern related function / application . improvements to such technologies — moving towards the In the illustrated scene , the camera has recognized the 50 goal of intuitive computing : devices that can see and / or hear , face in the foreground as “ Bob ” and annotated the image and infer the user's desire in that sensed context . accordingly . A billboard promoting the Godzilla movie has been recognized , and a bauble saying “ Show Times ” has BRIEF DESCRIPTION OF THE DRAWINGS been blitted onto the display — inviting the user to tap for screening information . FIG . 1 is a diagram showing an exemplary embodiment The phone has recognized the user's car from the scene , incorporating certain aspects of the technology detailed and has also identified by make and year another vehicle herein . in the picture . Both are noted by overlaid text . A restaurant FIG . 1A is a high level view of an embodiment incorpo has also been identified , and an initial review from a rating aspects of the present technology . collection of reviews ( “ Jane's review : Pretty Good ! ” ) is 60 FIG . 2 shows some of the applications that a user may shown . Tapping brings up more reviews . request a camera - equipped cell phone to perform . In one particular arrangement , this scenario is imple- FIG . 3 identifies some of the commercial entities in an mented as a cloud - side service assisted by local device embodiment incorporating aspects of the present technol object recognition core services . Users may leave notes on ogy . both fixed and mobile objects . Tapped baubles can trigger 65 FIGS . 4 , 4A and 4B conceptually illustrate how pixel data , other applications . Social networks can keep track of objec- and derivatives , are applied in different tasks , and packaged tion relationships — forming a virtual “ web of objects . ” into packet form . 55\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 63}), Document(page_content='10 US 10,922,957 B2 \\n 3 4 \\n FIG . 5 shows how different tasks may have certain image FIG . 29 is an arty shot of the Eiffel Tower , captured by a processing operations in common . cell phone user . FIG . 6 is a diagram illustrating how common image FIG . 35 is another image captured by a cell phone user . processing operations can be identified , and used to config- FIG . 36 is an image of an underside of a telephone , ure cell phone processing hardware to perform these opera- 5 discovered using methods according to aspects of the pres tions . ent technology FIG . 7 is a diagram showing how a cell phone can send FIG . 37 shows part of the physical user interface of one certain pixel - related data across an internal bus for local style of cell phone . processing , and send other pixel - related data across a com FIGS . 37A and 37B illustrate different linking topologies . munications channel for processing in the cloud . FIG . 38 is an image captured by a cell phone user , FIG . 8 shows how the cloud processing in FIG . 7 allows tremendously more “ intelligence ” to be applied to a task depicting an Appalachian Trail trail marker . FIGS . 39-43 detail methods incorporating aspects of the desired by a user . FIG . 9 details how keyvector data is distributed to dif present technology . \\n ferent external service providers , who perform services in 15 FIG . 44 shows the user interface of one style of cell \\n exchange for compensation , which is handled in consoli phone . \\n dated fashion for the user . FIGS . 45A and 45B illustrate how different dimensions of \\n FIG . 10 shows an embodiment incorporating aspects of commonality may be explored through use of a user inter \\n the present technology , noting how cell phone - based pro face control of a cell phone . cessing is suited for simple object identification tasks — such 20 FIGS . 46A and 46B detail a particular method incorpo as template matching , whereas cloud - based processing is rating aspects of the present technology , by which keywords suited for complex tasks — such as data association . such as Prometheus and Paul Manship are automatically FIG . 10A shows an embodiment incorporating aspects of determined from a cell phone image . the present technology , noting that the user experience is FIG . 47 shows some of the different data sources that may optimized by performing visual keyvector processing as 25 be consulted in processing imagery according to aspects of close to a sensor as possible , and administering traffic to the the present technology . cloud as low in a communications stack as possible . FIGS . 48A , 48B and 49 show different processing meth FIG . 11 illustrates that tasks referred for external process- ods according to aspects of the present technology . ing can be routed to a first group of service providers who FIG . 50 identifies some of the different processing that routinely perform certain tasks for the cell phone , or can be 30 may be performed on image data , in accordance with aspects routed to a second group of service providers who compete of the present technology on a dynamic basis for processing tasks from the cell phone . FIG . 51 shows an illustrative tree structure that can be FIG . 12 further expands on concepts of FIG . 11 , e.g. , employed in accordance with certain aspects of the present showing how a bid filter and broadcast agent software technology module may oversee a reverse auction process . FIG . 52 shows a network of wearable computers ( e.g. , cell FIG . 13 is a high level block diagram of a processing phones ) that can cooperate with each other , e.g. , in a arrangement incorporating aspects of the present technol- peer - to - peer network .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 64}), Document(page_content='ogy . FIGS . 53-55 detail how a glossary of signs can be FIG . 14 is a high level block diagram of another process- identified by a cell phone , and used to trigger different ing arrangement incorporating aspects of the present tech- 40 actions . nology FIG . 56 illustrates aspects of prior art digital camera FIG . 15 shows an illustrative range of image types that technology may be captured by a cell phone camera . FIG . 57 details an embodiment incorporating aspects of FIG . 16 shows a particular hardware implementation the present technology . incorporating aspects of the present technology . FIG . 58 shows how a cell phone can be used to sense and FIG . 17 illustrates aspects of a packet used in an exem- display affine parameters . plary embodiment . FIG . 59 illustrates certain state machine aspects of the FIG . 18 is a block diagram illustrating an implementation present technology of the SIFT technique . FIG . 60 illustrates how even “ still ” imagery can include FIG . 19 is a block diagram illustrating , e.g. , how packet 50 temporal , or motion , aspects . header data can be changed during processing , through use FIG . 61 shows some metadata that may be involved in an of a memory implementation incorporating aspects of the present tech \\n FIG . 19A shows a prior art architecture from the robotic nology . Player Project . FIG . 62 shows an image that may be captured by a cell FIG . 19B shows how various factors can influence how 55 phone camera user . different operations may be handled . FIGS . 63-66 detail how the image of FIG . 62 can be FIG . 20 shows an arrangement by which a cell phone processed to convey semantic metadata . camera and a cell phone projector share a lens . FIG . 67 shows another image that may be captured by a FIG . 20A shows a reference platform architecture that can cell phone camera user . be used in embodiments of the present technology . FIGS . 68 and 69 detail how the image of FIG . 67 can be FIG . 21 shows an image of a desktop telephone captured processed to convey semantic metadata . by a cell phone camera . FIG . 70 shows an image that may be captured by a cell FIG . 22 shows a collection of similar images found in a phone camera user . repository of public images , by reference to characteristics FIG . 71 details how the image of FIG . 70 can be pro discerned from the image of FIG . 21 . 65 cessed to convey semantic metadata . FIGS . 23-28A , and 30-34 are flow diagrams detailing FIG . 72 is a chart showing aspects of the human visual methods incorporating aspects of the present technology . system . 35 \\n 45 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 64}), Document(page_content='5 \\n 10 \\n every second . US 10,922,957 B2 \\n 5 6 \\n FIG . 73 shows different low , mid and high frequency mobile device users and meet most qualitative “ human real components of an image . time interactivity \" requirements , generally with feedback in \\n FIG . 74 shows a newspaper page . much less than one second . Implementation desirably pro FIG . 75 shows the layout of the FIG . 74 page , as set by vides certain basic features on the mobile device , including layout software . a rather intimate relationship between the image sensor\\'s FIG . 76 details how user interaction with imagery cap- output pixels and the native communications channel avail tured from printed text may be enhanced . able to the mobile device . Certain levels of basic \" content FIGS . 77A and 77B illustrate how semantic conveyance filtering and classification ” of the pixel data on the local of metadata can have a progressive aspect , akin to device , followed by attaching routing instructions to the JPEG2000 and the like . pixel data as specified by the user\\'s intentions and subscrip FIG . 78 is a block diagram of a prior art thermostat . tions , leads to an interactive session between a mobile FIG . 79 is an exterior view of the thermostat of FIG . 78 . device and one or more “ cloud based ” pixel processing FIG . 80 is a block diagram of a thermostat employing services . The key word “ session ” further indicates fast certain aspects of the present technology ( “ ThingPipe ” ) . FIG . 81 is a block diagram of a cell phone embodying 15 responses transmitted back to the mobile device , where for certain aspects of the present technology . some services marketed as “ real time ” or “ interactive , \" a \\n FIG . 82 is a block diagram by which certain operations of session essentially represents a duplex , generally packet \\n the thermostat of FIG . 80 are explained . based , communication , where several outgoing “ pixel pack \\n FIG . 83 shows a cell phone display depicting an image ets ” and several incoming response packets ( which may be captured from a thermostat , onto which is overlaid certain 20 pixel packets updated with the processed data ) may occur \\n touch - screen targets that the user can touch to increment or decrement the thermostat temperature . Business factors and good old competition are at the heart FIG . 84 is similar to FIG . 83 , but shows a graphical user of the distributed network . Users can subscribe to or other interface for use on a phone without a touch - screen . wise tap into any external services they choose . The local FIG . 85 is a block diagram of an alarm clock employing 25 device itself and / or the carrier service provider to that device aspects of the present technology . can be configured as the user chooses , routing filtered and FIG . 86 shows a screen of an alarm clock user interface pertinent pixel data to specified object interaction services . that may be presented on a cell phone , in accordance with Billing mechanisms for such services can directly plug into one aspect of the technology . existing cell and / or mobile device billing networks , wherein FIG . 87 shows a screen of a user interface , detailing 30 users get billed and service providers get paid . nearby devices that may be controlled through use of the cell But let\\'s back up a bit . The addition of camera systems to phone . mobile devices has ignited an explosion of applications . The primordial application certainly must be folks simply snap', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 65}), Document(page_content='DETAILED DESCRIPTION ping quick visual aspects of their environment and sharing 35 such pictures with friends and family . The present specification details a diversity of technolo- The fanning out of applications from that starting point gies , assembled over an extended period of time , to serve a arguably hinges on a set of core plumbing features inherent variety of different objectives . Yet they relate together in in mobile cameras . In short ( and non - exhaustive of course ) , various ways , and so are presented collectively in this single such features include : a ) higher quality pixel capture and \\n document . 40 low level processing ; b ) better local device CPU and GPU This varied , interrelated subject matter does not lend itself resources for on - device pixel processing with subsequent to a straightforward presentation . Thus , the reader\\'s indul- user feedback ; c ) structured connectivity into “ the cloud ; \" gence is solicited as this narrative occasionally proceeds in and importantly , d ) a maturing traffic monitoring and billing nonlinear fashion among the assorted topics and technolo- infrastructure . FIG . 1A is but one graphic perspective on gies . 45 some of these plumbing features of what might be called a Each portion of this specification details technology that visually intelligent network . ( Conventional details of a cell desirably incorporates technical features detailed in other phone , such as the microphone , A / D converter , modulation portions . Thus , it is difficult to identify “ a beginning ” from and demodulation systems , IF stages , cellular transceiver , which this disclosure should logically begin . That said , we etc. , are not shown for clarity of illustration . ) simply dive in . It is all well and good to get better CPUs and GPUs , and Mobile Device Object Recognition and Interaction Using more memory , on mobile devices . However , cost , weight Distributed Network Services and power considerations seem to favor getting “ the cloud ” There is presently a huge disconnect between the unfath- to do as much of the “ intelligence ” heavy lifting as possible . omable volume of information that is contained in high Relatedly , it seems that there should be a common quality image data streaming from a mobile device camera 55 denominator set of \" device - side ” operations performed on ( e.g. , in a cell phone ) , and the ability of that mobile device visual data that will serve all cloud processes , including to process this data to whatever end . “ Off device ” processing certain formatting , elemental graphic processing , and other of visual data can help handle this fire hose of data , rote operations . Similarly , it seems there should be a stan especially when a multitude of visual processing tasks may dardized basic header and addressing scheme for the result be desired . These issues become even more critical once 60 ing communication traffic ( typically packetized ) back and “ real time object recognition and interaction ” is contem- forth with the cloud . plated , where a user of the mobile device expects virtually This conceptualization is akin to the human visual system . instantaneous results and augmented reality graphic feed- The eye performs baseline operations , such as chromaticity back on the mobile device screen , as that user points the groupings , and it optimizes necessary information for trans camera at a scene or object . 65 mission along the optic nerve to the brain . The brain does the In accordance with one aspect of the present technology , real cognitive work . And there\\'s feedback the other way a distributed network of pixel processing engines serve such too with the brain sending information controlling muscle 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 65}), Document(page_content='10 US 10,922,957 B2 \\n 7 8 \\n movement where to point eyes , scanning lines of a book , FIGS . 4A and 4B also introduce a central player in this controlling the iris ( lighting ) , etc. disclosure : the packaged and address - labeled pixel packet , FIG . 2 depicts a non - exhaustive but illustrative list of into a body of which keyvector data is inserted . The keyvec visual processing applications for mobile devices . Again , it tor data may be a single patch , or a collection of patches , or is hard not to see analogies between this list and the 5 a time - series of patches / collections . A pixel packet may be \\n fundamentals of how the human visual system and the less than a kilobyte , or its size can be much much larger . It \\n human brain operate . It is a well studied academic area that may convey information about an isolated patch of pixels \\n deals with how “ optimized ” the human visual system is excerpted from a larger image , or it may convey a massive \\n relative to any given object recognition task , where a general Photosynth of Notre Dame cathedral . \\n consensus is that the eye - retina - optic nerve - cortex system is ( As presently conceived , a pixel packet is an application', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='pretty darn wonderful in how efficiently it serves a vast array layer construct . When actually pushed around a network , however , it may be broken into smaller portions as trans of cognitive demands . This aspect of the technology relates port layer constraints in a network may require . ) to how similarly efficient and broadly enabling elements can FIG . 5 is a segue diagramstill at an abstract level , but be built into mobile devices , mobile device connections and 15 pointing toward the concrete . A list of user - defined applica network services , all with the goal of serving the applica tions , such as illustrated in FIG . 2 , will map to a state - of tions depicted in FIG . 2 , as well as those new applications the - art inventory of pixel processing methods and which may show up as the technology dance continues . approaches which can accomplish each and every applica Perhaps the central difference between the human analogy tion . These pixel processing methods break down into and mobile device networks must surely revolve around the 20 common and not - so - common component sub - tasks . Object basic concept of “ the marketplace , ” where buyers buy better recognition textbooks are filled with a wide variety of and better things so long as businesses know how to profit approaches and terminologies which bring a sense of order accordingly . Any technology which aims to serve the appli- into what at first glance might appear to be a bewildering cations listed in FIG . 2 must necessarily assume that hun- array of “ unique requirements ” relative to the applications dreds if not thousands of business entities will be developing 25 shown in FIG . 2. ( In addition , multiple computer vision and the nitty gritty details of specific commercial offerings , with image processing libraries , such as OpenCV and CMVI the expectation of one way or another profiting from those sion discussed below , have been created that identify and offerings . Yes , a few behemoths will dominate main lines of render functional operations , which can be considered cash flows in the overall mobile industry , but an equal \" atomic ” functions within object recognition paradigms . ) certainty will be that niche players will be continually 30 But FIG . 5 attempts to show that there are indeed a set of developing niche applications and services . Thus , this dis- common steps and processes shared between visual process closure describes how a marketplace for visual processing ing applications . The differently shaded pie slices attempt to services can develop , whereby business interests across the illustrate that certain pixel operations are of a specific class spectrum have something to gain . FIG . 3 attempts a crude and may simply have differences in low level variables or categorization of some of the business interests applicable to 35 optimizations . The size of the overall pie ( thought of in a the global business ecosystem operative in the era of this logarithmic sense , where a pie twice the size of another may filing represent 10 times more Flops , for example ) , and the per FIG . 4 sprints toward the abstract in the introduction of centage size of the slice , represent degrees of commonality . the technology aspect now being considered . Here we find FIG . 6 takes a major step toward the concrete , sacrificing a highly abstracted bit of information derived from some 40 simplicity in the process . Here we see a top portion labeled batch of photons that impinged on some form of electronic “ Resident Call - Up Visual Processing Services , \" which rep image sensor , with a universe of waiting consumers of that resents all of the possible list of applications from FIG . 2 that lowly bit . FIG . 4A then quickly introduces the intuitively a given mobile device may be aware of , or downright well - known concept that singular bits of visual information enabled to perform . The idea is that not all of these appli aren\\'t worth much outside of', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content=\"mobile device may be aware of , or downright well - known concept that singular bits of visual information enabled to perform . The idea is that not all of these appli aren't worth much outside of their role in both spatial and 45 cations have to be active all of the time , and hence some temporal groupings . This core concept is well exploited in sub - set of services is actually “ turned on ” at any given modern video compression standards such as MPEG ? and moment . The turned on applications , as a one - time configu\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='H.264 . ration activity , negotiate to identify their common compo The “ visual ” character of the bits may be pretty far nent tasks , labeled the “ Common Processes Sorter ” —first removed from the visual domain by certain of the processing 50 generating an overall common list of pixel processing rou ( consider , e.g. , the vector strings representing eigenface tines available for on - device processing , chosen from a data ) . Thus , we sometimes use the term “ keyvector data ” ( or library of these elemental image processing routines ( e.g. , “ keyvector strings ” ) to refer collectively to raw sensor / FFT , filtering , edge detection , resampling , color histogram stimulus data ( e.g. , pixel data ) , and / or to processed infor- ming , log - polar transform , etc. ) . Generation of correspond mation and associated derivatives . A keyvector may take the 55 ing Flow Gate Configuration / Software Programming infor form of a container in which such information is conveyed mation follows , which literally loads library elements into ( e.g. , a data structure such as a packet ) . A tag or other data properly ordered places in a field programmable gate array can be included to identify the type of information ( e.g. , set - up , or otherwise configures a suitable processor to per JPEG image data , eigenface data ) , or the data type may be form the required component tasks . otherwise evident from the data or from context . One or 60 FIG . 6 also includes depictions of the image sensor , more instructions , or operations , may be associated with followed by a universal pixel segmenter . This pixel seg keyvector data either expressly detailed in the keyvector , menter breaks down the massive stream of imagery from the or implied . An operation may be implied in default fashion , sensor into manageable spatial and / or temporal blobs ( e.g. , for keyvector data of certain types ( e.g. , for JPEG data it akin to MPEG macroblocks , wavelet transform blocks , may be “ store the image ; \" for eigenface data is may be 65 64x64 pixel blocks , etc. ) . After the torrent of pixels has been \" match this eigenface template \" ) . Or an implied operation broken down into chewable chunks , they are fed into the may be dependent on context . newly programmed gate array ( or other hardware ) , which', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='US 10,922,957 B2 \\n 9 10', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='performs the elemental image processing tasks associated template - like approach to object recognition , where for with the selected applications . ( Such arrangements are fur- contained applications involving such objects , local process ther detailed below , in an exemplary system employing ing services can largely get the job done . But even in the \" pixel packets . \" ) Various output products are sent to a barcode example , flexibility in the growth and evolution of routing engine , which refers the elementally - processed data 5 overt visual coding targets begs for an architecture which ( e.g. , keyvector data ) to other resources ( internal and / or doesn\\'t force “ code upgrades ” to a gazillion devices every external ) for further processing . This further processing time there is some advance in the overt symbology art . typically is more complex that that already performed . At the other end of the spectrum , arbitrarily complex tasks Examples include making associations , deriving inferences , can be imagined , e.g. , referring to a network of supercom pattern and template matching , etc. This further processing 10 puters the task of predicting the apocryphal typhoon result can be highly application - specific . ing from the fluttering of a butterfly\\'s wings halfway around ( Consider a promotional game from Pepsi , inviting the the world if the application requires it . Oz beckons . public to participate in a treasure hunt in a state park . Based FIG . 8 attempts to illustrate this radical extra dimension on internet - distributed clues , people try to find a hidden ality of pixel processing in the cloud as opposed to the local six - pack of soda to earn a $ 500 prize . Participants must 15 device . This virtually goes without saying ( or without a download a special application from the Pepsi - dot - com web picture ) , but FIG . 8 is also a segue figure to FIG . 9 , where site ( or the Apple AppStore ) , which serves to distribute the Dorothy gets back to Kansas and is happy about it . clues ( which may also be published to Twitter ) . The down- FIG . 9 is all about cash , cash flow , and happy humans loaded application also has a prize verification component , using cameras on their mobile devices and getting highly which processes image data captured by the users \\' cell 20 meaningful results back from their visual queries , all the phones to identify a special pattern with which the hidden while paying one monthly bill . It turns out the Google six - pack is uniquely marked . SIFT object recognition is used “ AdWords ” auction genie is out of the bottle . Behind the ( discussed below ) , with the SIFT feature descriptors for the scenes of the moment - by - moment visual scans from a special package conveyed with the downloaded application . mobile user of their immediate visual environment are When an image match is found , the cell phone immediately 25 hundreds and thousands of micro - decisions , pixel routings , reports same wirelessly to Pepsi . The winner is the user results comparisons and micro - auctioned channels back to whose cell phone first reports detection of the specially- the mobile device user for the hard good they are “ truly ” marked six - pack . In the FIG . 6 arrangement , some of the looking for , whether they know it or not . This last point is component tasks in the SIFT pattern matching operation are deliberately cheeky , in that searching of any kind is inher performed by the elemental image processing in the config- 30 ently open ended and magical at some level , and part of the ured hardware ; others are referred for more specialized fun of searching in the first place is that surprisingly new processing either internal or external . ) associations are part of the results . The search user knows FIG . 7 up - levels the picture to a generic distributed pixel after the fact what they were truly looking for . The system , services network view , where local device pixel services and', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='user knows FIG . 7 up - levels the picture to a generic distributed pixel after the fact what they were truly looking for . The system , services network view , where local device pixel services and represented in FIG . 9 as the carrier - based financial tracking “ cloud based ” pixel services have a kind of symmetry in 35 server , now sees the addition of our networked pixel services how they operate . The router in FIG . 7 takes care of how any module and its role in facilitating pertinent results being sent given packaged pixel packet gets sent to the appropriate back to a user , all the while monitoring the uses of the pixel processing location , whether local or remote ( with the services in order to populate the monthly bill and send the style of fill pattern denoting different component processing proceeds to the proper entities . functions ; only a few of the processing functions required by 40 ( As detailed further elsewhere , the money flow may not the enabled visual processing services are depicted ) . Some exclusively be to remote service providers . Other money of the data shipped to cloud - based pixel services may have flows can arise , such as to users or other parties , e.g. , to been first processed by local device pixel services . The induce or reward certain actions . ) circles indicate that the routing functionality may have FIG . 10 focuses on functional division of processing components in the cloud - nodes that serve to distribute 45 illustrating how tasks in the nature of template matching can tasks to active service providers , and collect results for be performed on the cell phone itself , whereas more sophis transmission back to the device . In some implementations ticated tasks in the nature of data association ) desirably are these functions may be performed at the edge of the wireless referred to the cloud for processing . network , e.g. , by modules at wireless service towers , so as Elements of the foregoing are distilled in FIG . 10A , to ensure the fastest action . Results collected from the active 50 showing an implementation of aspects of the technology as external service providers , and the active local processing a physical matter of ( usually ) software components . The two stages , are fed back to Pixel Service Manager software , ovals in the figure highlight the symmetric pair of software', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content=\"which then interacts with the device user interface . components which are involved in setting up a “ human FIG . 8 is an expanded view of the lower right portion of real - time ” visual recognition session between a mobile FIG . 7 and represents the moment where Dorothy's shoes 55 device and the generic cloud or service providers , data turn red and why distributed pixel services provided by the associations and visual query results . The oval on the left cloud — as opposed to the local device — will probably trump refers to “ keyvectors ” and more specifically “ visual keyvec all but the most mundane object recognition tasks . tors . ” As noted , this term can encompass everything from Object recognition in its richer form is based on visual simple JPEG compressed blocks all the way through log association rather than strict template matching rules . If we 60 polar transformed facial feature vectors and anything in all were taught that the capital letter “ A ” will always be between and beyond . The point of a keyvector is that the strictly following some pre - historic form never to change , a essential raw information of some given visual recognition universal template image if you will , then pretty clean and task has been optimally pre - processed and packaged ( pos locally prescriptive methods can be placed into a mobile sibly compressed ) . The oval on the left assembles these imaging device in order to get it to reliably read a capital A 65 packets , and typically inserts some addressing information any time that ordained form “ A ” is presented to the camera . by which they will be routed . ( Final addressing may not be 2D and even three 3D barcodes in many ways follow this possible , as the packet may ultimately be routed to remote\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='US 10,922,957 B2 \\n 11 12', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content='service providers — the details of which may not yet be on domestic data processing . Others may want to stick to known . ) Desirably , this processing is performed as close to service providers that meet \" green , ” “ ethical , ” or other the raw sensor data as possible , such as by processing standards of corporate practice . Others may prefer richer circuitry integrated on the same substrate as the image data output . Weightings of different criteria can be applied sensor , which is responsive to software instructions stored in 5 by the query router and response manager in making the memory or provided from another stage in packet form . decision . The oval on the right administers the remote processing of In some circumstances , one input to the query router and keyvector data , e.g. , attending to arranging appropriate response manager may be the user\\'s location , so that a services , directing traffic flow , etc. Desirably , this software different service provider may be selected when the user is process is implemented as low down on a communications 10 at home in Oregon , than when she is vacationing in Mexico . stack as possible , generally on a “ cloud side ” device , access In other instances , the required turnaround time is specified , point , or cell tower . ( When real - time visual keyvector pack- which may disqualify some vendors , and make others more ets stream over a communications channel , the lower down competitive . In some instances the query router and in the communications stack they are identified and routed , response manager need not decide at all , e.g. , if cached the smoother the “ human real - time ” look and feel a given 15 results identifying a service provider selected in a previous visual recognition task will be . ) Remaining high level pro- auction are still available and not beyond a “ freshness ” cessing needed to support this arrangement is included in threshold . FIG . 10A for context , and can generally be performed Pricing offered by the vendors may change with process through native mobile and remote hardware capabilities . ing load , bandwidth , time of day , and other considerations . FIGS . 11 and 12 illustrate the concept that some providers 20 In some embodiments the providers may be informed of of some cloud - based pixel processing services may be offers submitted by competitors ( using known trust arrange established in advance , in a pseudo - static fashion , whereas ments assuring data integrity ) , and given the opportunity to other providers may periodically vie for the privilege of make their offers more enticing . Such a bidding war may processing a user\\'s keyvector data , through participation in continue until no bidder is willing to change the offered a reverse auction . In many implementations , these latter 25 terms . The query router and response manager ( or in some providers compete each time a packet is available for implementations , the user ) then makes a selection . processing . For expository convenience and visual clarity , FIG . 12 Consider a user who snaps a cell phone picture of an shows a software module labeled “ Bid Filter and Broadcast unfamiliar car , wanting to learn the make and model . Vari- Agent . ” In most implementations this forms part of the ous service providers may compete for this business . A 30 query router and response manager module . The bid filter startup vendor may offer to perform recognition for free to module decides which vendors from a universe of possible build its brand or collect data . Imagery submitted to this vendors should be given a chance to bid on a processing service returns information simply indicating the car\\'s make task . ( The user\\'s preference data , or historical experience , and model . Consumer Reports may offer an alternative may indicate that certain service providers be disqualified . ) service which provides make and model data , but also 35 The broadcast agent module then communicates with the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content=\"may offer an alternative may indicate that certain service providers be disqualified . ) service which provides make and model data , but also 35 The broadcast agent module then communicates with the provides technical specifications for the car . However , they selected bidders to inform them of a user task for processing , may charge 2 cents for the service ( or the cost may be and provides information needed for them to make a bid . bandwidth based , e.g. , 1 cent per megapixel ) . Edmunds , or Desirably , the bid filter and broadcast agent do at least JD Powers , may offer still another service , which provides some their work in advance of data being available for data like Consumer Reports , but pays the user for the 40 processing . That is , as soon as a prediction can be made as privilege of providing data . In exchange , the vendor is given to an operation that the user may likely soon request , these the right to have one of its partners send a text message to modules start working to identify a provider to perform a the user promoting goods or services . The payment may take service expected to be required . A few hundred milliseconds the form of a credit on the user's monthly cell phone later the user keyvector data may actually be available for voice / data service billing . 45 processing ( if the prediction turns out to be accurate ) . Using criteria specified by the user , stored preferences , Sometimes , as with Google's present AdWords system , context , and other rules / heuristics , a query router and the service providers are not consulted at each user trans response manager in the cell phone , in the cloud , distrib- action . Instead , each provides bidding parameters , which are uted , etc. ) determines whether the packet of data needing stored and consulted whenever a transaction is considered , processing should be handled by one of the service providers 50 to determine which service provider wins . These stored in the stable of static standbys , or whether it should be parameters may be updated occasionally . In some imple offered to providers on an auction basis in which case it mentations the service provider pushes updated parameters\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content=\"arbitrates the outcome of the auction . to the bid filter and broadcast agent whenever available . The static standby service providers may be identified ( The bid filter and broadcast agent may serve a large when the phone is initially programmed , and only reconfig- 55 population of users , such as all Verizon subscribers in area ured when the phone is reprogrammed ( For example , Veri- code 503 , or all subscribers to an ISP in a community , or all zon may specify that all FFT operations on its phones be users at the domain well - dot - com , etc .; or more localized routed to a server that it provides for this purpose . ) Or , the agents may be employed , such as one for each cell phone user may be able to periodically identify preferred providers tower . ) for certain tasks , as through a configuration menu , or specify 60 If there is a lull in traffic , a service provider may discount that certain tasks should be referred for auction . Some its services for the next minute . The service provider may applications may emerge where static service providers are thus transmit ( or post ) a message stating that it will perform favored ; the task may be so mundane , or one provider's eigenvector extraction on an image file of up to 10 mega services may be so un - paralleled , that competition for the bytes for 2 cents until 1244754176 Coordinated Universal provision of services isn't warranted . 65 Time in the Unix epoch , after which time the price will In the case of services referred to auction , some users may return to 3 cents . The bid filter and broadcast agent updates exalt price above all other considerations . Others may insist a table with stored bidding parameters accordingly .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content='US 10,922,957 B2 \\n 13 14 \\n ( The reader is presumed to be familiar with the reverse share some of their media consumption data with Nielsen auction arrangements used by Google to place sponsored ( such as by serving as an anonymous member for a city\\'s advertising on web search results page . An illustrative audience measurement panel ) , and provided on a fee basis to description is provided in Levy , \" Secret of Googlenomics : others . Nielsen may offer , for example , 100 units of credit Data - Fueled Recipe Brews Profitability , ” Wired Magazine , 5 micropayments or other value to participating consumers May 22 , 2009. ) each month , or may provide credit each time the user In other implementations , the broadcast agent polls the submits information to Nielsen . bidders — communicating relevant parameters , and soliciting In another example , a consumer may be rewarded for bid responses whenever a transaction is offered for process- accepting commercials , or commercial impressions , from a ing . 10 company . If a consumer goes into the Pepsi Center in Once a prevailing bidder is decided , and data is available Denver , she may receive a reward for each Pepsi - branded for processing , the broadcast agent transmits the keyvector experience she encounters . The amount of micropayment data ( and other parameters as may be appropriate to a may scale with the amount of time that she interacts with the particular task ) to the winning bidder . The bidder then different Pepsi - branded objects ( including audio and imag performs the requested operation , and returns the processed 15 ery ) in the venue . data to the query router and response manager . This module Not just large brand owners can provide credits to indi logs the processed data , and attends to any necessary viduals . Credits can be routed to friends and social / business accounting ( e.g. , crediting the service provider with the acquaintances . To illustrate , a user of Facebook may share appropriate fee ) . The response data is then forwarded back credit ( redeemable for goods / services , or exhangable for to the user device . 20 cash ) from his Facebook page enticing others to visit , or In a variant arrangement , one or more of the competing linger . In some cases , the credit can be made available only service providers actually performs some or all of the to people who navigate to the Facebook page in a certain requested processing , but “ teases ” the user ( or the query manner — such as by linking to the page from the user\\'s router and response manager ) by presenting only partial business card , or from another launch page . results . With a taste of what\\'s available , the user ( or the 25 As another example , consider a Facebook user who has query router and response manager ) may be induced to make earned , or paid for , or otherwise received credit that can be a different choice than relevant criteria / heuristics would applied to certain services such as for downloading songs', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content=\"otherwise indicate . from iTunes , or for music recognition services , or for The function calls sent to external service providers , of identifying clothes that go with particular shoes ( for which course , do not have to provide the ultimate result sought by 30 an image has been submitted ) , etc. These services may be a consumer ( e.g. , identifying a car , or translating a menu associated with the particular Facebook page , so that friends listing from French to English ) . They can be component can invoke the services from that page — essentially spend operations , such as calculating an FFT , or performing a SIFT ing the host's credit ( again , with suitable authorization or procedure or a log - polar transform , or computing a histo- invitation by that hosting user ) . Likewise , friends may gram or eigenvectors , or identifying edges , etc. 35 submit images to a facial recognition service accessible In time , it is expected that a rich ecosystem of expert through an application associated with the user's Facebook processors will emergeserving myriad processing page . Images submitted in such fashion are analyzed for requests from cell phones and other thin client devices . faces of the host’s friends , and identification information is More on Monetary Flow returned to the submitter , e.g. , through a user interface Additional business models can be enabled , involving the 40 presented on the originating Facebook page . Again , the host subsidization of consumed remote services by the service may be assessed a fee for each such operation , but may allow\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content=\"providers themselves in exchange for user information ( e.g. , authorized friends to avail themselves of such service at no for audience measurement ) , or in exchange for action taken cost . by the user , such as completing a survey , visiting specific Credits , and payments , can also be routed to charities . A sites , locations in store , etc. 45 viewer exiting a theatre after a particularly poignant movie Services may be subsidized by third parties as well , such about poverty in Bangladesh may capture an image of an a coffee shop that derives value by providing a differenti- associated movie poster , which serves as a portal for dona ating service to its customers in the form of free / discounted tions for a charity that serves the poor in Bangladesh . Upon usage of remote services while they are seated in the shop . recognizing the movie poster , the cell phone can present a In one arrangement an economy is enabled wherein a 50 graphical / touch user interface through which the user spins currency of remote processing credits is created and dials to specify an amount of a charitable donation , which at exchanged between users and remote service providers . This the conclusion of the transaction is transferred from a may be entirely transparent to the user and managed as part financial account associated with the user , to one associated of a service plan , e.g. , with the user's cell phone or data with the charity . service provider . Or it can be exposed as a very explicit 55 More on a Particular Hardware Arrangement aspect of certain embodiments of the present technology . As noted above and in the cited patent documents , there Service providers and others may award credits to users for is a need for generic object recognition by a mobile device . taking actions or being part of a frequent - user program to Some approaches to specialized object recognition have build allegiance with specific providers . emerged , and these have given rise to specific data process As with other currencies , users may choose to explicitly 60 ing approaches . However , no architecture has been proposed donate , save , exchange or generally barter credits as needed . that goes beyond specialized object recognition toward Considering these points in further detail , a service may generic object recognition . pay a user for opting - in to an audience measurement panel . Visually , a generic object recognition arrangement E.g. , The Nielsen Company may provide services to the requires access to good raw visual data preferably free of public — such as identification of television programming 65 device quirks , scene quirks , user quirks , etc. Developers of from audio or video samples submitted by consumers . These systems built around object identification will best prosper services may be provided free to consumers who agree to and serve their users by concentrating on the object identi\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content='5 \\n as are US 10,922,957 B2 \\n 15 16 \\n fication task at hand , and not the myriad existing roadblocks , includes processing such as JPEG compression . Another , 14 , resource sinks , and third party dependencies that currently is tailored for object recognition . As discussed , some of this', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='must be confronted . processing may be performed by the mobile device , while As noted , virtually all object identification techniques can other processing may be referred to the cloud 16 . make use of or even rely upon a pipe to “ the cloud . ” FIG . 14 takes an application - centric view of the object “ Cloud ” can include anything external to the cell phone . recognition processing path . Some applications reside An example is a nearby cell phone , or plural phones on a wholly in the cell phone . Other applications reside wholly distributed network . Unused processing power on such other outside the cell phone e.g. , simply taking keyvector data as phone devices can be made available for hire ( or for free ) to stimulus . More common are hybrids , such as where some call upon as needed . The cell phones of the implementations 10 processing is done in the cell phone , other processing is done detailed herein can scavenge processing power from such externally , and the application software orchestrating the other cell phones . process resides in the cell phone . Such a cloud may be ad hoc , e.g. , other cell phones within To illustrate further discussion , FIG . 15 shows a range 40 Bluetooth range of the user\\'s phone . The ad hoc network can of some of the different types of images 41-46 that may be be extended by having such other phones also extend the 15 captured by a particular user\\'s cell phone . A few brief ( and local cloud to further phones that they can reach by Blu- incomplete ) comments about some of the processing that etooth , but the user cannot . may be applied to each image are provided in the following The “ cloud ” can also comprise other computational plat- paragraphs . forms , such as set - top boxes ; processors in automobiles , Image 41 depicts a thermostat . A steganographic digital thermostats , HVAC systems , wireless routers , local cell 20 watermark 47 is textured or printed on the thermostat’s case . phone towers and other wireless network edges ( including ( The watermark is shown as visible in FIG . 15 , but is the processing hardware for their software - defined radio typically imperceptible to the viewer ) . The watermark con equipment ) , etc. Such processors can be used in conjunction veys information intended for the cell phone , allowing it to with more traditional cloud computing resources present a graphic user interface by which the user can offered by Google , Amazon , etc. 25 interact with the thermostat . A bar code or other data carrier ( In view of concerns of certain users about privacy , the can alternatively be used . Such technology is further phone desirably has a user - configurable option indicating detailed below . whether the phone can refer data to cloud resources for Image 42 depicts an item including a barcode 48. This processing . In one arrangement , this option has a default barcode conveys Universal Product Code ( UPC ) data . Other value of “ No , ” limiting functionality and impairing battery 30 barcodes may convey other information . The barcode pay life , but also limiting privacy concerns . In another arrange- load is not primarily intended for reading by a user cell ment , this option has a default value of “ Yes . \" ) phone ( in contrast to watermark 47 ) , but it nonetheless may Desirably , image - responsive techniques should produce a be used by the cell phone to help determine an appropriate short term \" result or answer , \" which generally requires some response for the user . level of interactivity with a user - hopefully measured in 35 Image 43 shows a product that may be identified without fractions of a second for truly interactive applications , or a reference to any express machine readable information ( such few seconds or fractions of a minute for nearer - term \" I\\'m as a bar code or watermark ) . A segmentation algorithm may patient to wait ” applications . be applied to edge - detected image data to distinguish the As for the objects in', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='for nearer - term \" I\\'m as a bar code or watermark ) . A segmentation algorithm may patient to wait ” applications . be applied to edge - detected image data to distinguish the As for the objects in question , they can break down into apparent image subject from the apparent background . The various categories , including ( 1 ) generic passive ( clues to 40 image subject may be identified through its shape , color and basic searches ) , ( 2 ) geographic passive ( at least you know texture . Image fingerprinting may be used to identify refer where you are , and may hook into geographic - specific ence images having similar labels , and metadata associated resources ) , ( 3 ) “ cloud supported ” passive , as with “ identi- with those other images may be harvested . SIFT techniques fied / enumerated objects ” and their associated sites , and ( 4 ) ( discussed below ) may be employed for such pattern - based active controllable , a la ThingPipe ( a reference to technol- 45 recognition tasks . Specular reflections in low texture regions ogy detailed below , such as WiFi - equipped thermostats and may tend to indicate the image subject is made of glass . parking meters ) . Optical character recognition can be applied for further An object recognition platform should not , it seems , be information ( reading the visible text ) . All of these clues can conceived in the classic \" local device and local resources be employed to identify the depicted item , and help deter only ” software mentality . However , it may be conceived as 50 mine an appropriate response for the user . a local device optimization problem . That is , the software on Additionally ( or alternatively ) , similar - image search sys the local device , and its processing hardware , should be tems , such as Google Similar Images , and Microsoft Live designed in contemplation of their interaction with off- Search , can be employed to find similar images , and their device software and hardware . Ditto the balance and inter- metadata can then be harvested . ( As of this writing , these play of both control functionality , pixel crunching function- 55 services do not directly support upload of a user picture to ality , and application software / GUI provided on the device , find similar web pictures . However , the user can post the versus off the device . ( In many implementations , certain image to Flickr ( using Flickr’s cell phone upload function databases useful for object identification / recognition will ality ) , and it will soon be found and processed by Google reside remote from the device . ) and Microsoft . ) In a particularly preferred arrangement , such a processing 60 Image 44 is a snapshot of friends . Facial detection and platform employs image processing near the sensor - opti- recognition may be employed ( i.e. , to indicate that there are mally on the same chip , with at least some processing tasks faces in the image , and to identify particular faces and desirably performed by dedicated , special purpose hard- annotate the image with metadata accordingly , e.g. , by reference to user - associated data maintained by Apple\\'s Consider FIG . 13 , which shows an architecture of a cell 65 iPhoto service , Google\\'s Picasa service , Facebook , etc. ) phone 10 in which an image sensor 12 feeds two processing Some facial recognition applications can be trained for paths . One , 13 , is tailored for the human visual system , and non - human faces , e.g. , cats , dogs animated characters ware .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='10 US 10,922,957 B2 \\n 17 18 \\n including avatars , etc. Geolocation and date / time informa- distributed . ) The packet data further specifies operations to tion from the cell phone may also provide useful informa- be performed by an ensuing chain of processing stages 38 . \\n tion . In one particular implementation , setup module 34 dic The persons wearing sunglasses pose a challenge for tates on a frame by frame basis — the parameters that are to some facial recognition algorithms . Identification of those 5 be employed by camera 32 in gathering an exposure . Setup \\n individuals may be aided by their association with persons module 34 also specifies the type of data the camera is to \\n whose identities can more easily be determined ( e.g. , by output . These instructional parameters are conveyed in a first \\n conventional facial recognition ) . That is , by identifying field 55 of a header portion 56 of a data packet 57 corre \\n other group pictures in iPhoto / Picasa / Facebook / etc . that sponding to that frame ( FIG . 17 ) . \\n include one or more of the latter individuals , the other For example , for each frame , the setup module 34 may \\n individuals depicted in such photographs may also be pres issue a packet 57 whose first field 55 instructs the camera \\n ent in the subject image . These candidate persons form a about , e.g. , the length of the exposure , the aperture size , the lens focus , the depth of field , etc. Module 34 may further much smaller universe of possibilities than is normally author the field 55 to specify that the sensor is to sum sensor provided by unbounded iPhoto / Picasa / Facebook / etc data . 15 charges to reduce resolution ( e.g. , producing a frame of The facial vectors discernable from the sunglass - wearing 640x480 data from a sensor capable of 1280x960 ) , output faces in the subject image can then be compared against this data only from red - filtered sensor cells , output data only smaller universe of possibilities in order to determine a best from a horizontal line of cells across the middle of the match . If , in the usual case of recognizing a face , a score of sensor , output data only from a 128x128 patch of cells in the 90 is required to be considered a match ( out of an arbitrary 20 center of the pixel array , etc. The camera instruction field 55 top match score of 100 ) , in searching such a group - con- may further specify the exact time that the camera is to strained set of images a score of 70 or 80 might suffice . capture data so as to allow , e.g. , desired synchronization ( Where , as in image 44 , there are two persons depicted with ambient lighting ( as detailed later ) . without sunglasses , the occurrence of both of these indi- Each packet 56 issued by setup module 34 may include viduals in a photo with one or more other individuals may 25 different camera parameters in the first header field 55. Thus , increase its relevance to such an analysis — implemented , a first packet may cause camera 32 to capture a full frame e.g. , by increasing a weighting factor in a matching algo- image with an exposure time of 1 millisecond . A next packet', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content='rithm . ) may cause the camera to capture a full frame image with an Image 45 shows part of the statue of Prometheus in exposure time of 10 milliseconds , and a third may dictate an Rockefeller Center , NY . Its identification can follow teach- 30 exposure time of 100 milliseconds . ( Such frames may later ings detailed elsewhere in this specification . be processed in combination to yield a high dynamic range Image 46 is a landscape , depicting the Maroon Bells image . ) A fourth packet may instruct the camera to down mountain range in Colorado . This image subject may be sample data from the image sensor , and combine signals recognized by reference to geolocation data from the cell from differently color - filtered sensor cells , so as to output a phone , in conjunction with geographic information services 35 4x3 array of grayscale luminance values . A fifth packet may', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content=\"such as GeoNames or Yahoo ! ' s GeoPlanet . instruct the camera to output data only from an 8x8 patch of ( It should be understood that techniques noted above in pixels at the center of the frame . A sixth packet may instruct connection with processing of one of the images 41-46 in the camera to output only five lines of image data , from the FIG . 15 can likewise be applied to others of the images . top , bottom , middle , and mid - upper and mid - lower rows of Moreover , it should be understood that while in some 40 the sensor . A seventh packet may instruct the camera to respects the depicted images are ordered according to ease output only data from blue - filtered sensor cells . An eighth of identifying the subject and formulating a response , in packet may instruct the camera to disregard any auto - focus other respects they are not . For example , although landscape instructions but instead capture a full frame at infinity focus . image 46 is depicted to the far right , its geolocation data is And so on . strongly correlated with the metadata “ Maroon Bells . ” Thus , 45 Each such packet 57 is provided from setup module 34 this particular image presents an easier case than that across a bus or other data channel 60 to a camera controller presented by many other images . ) In one embodiment , such module associated with the camera . ( The details of a digital processing of imagery occurs automatically without camera including an array of photosensor cells , associated express user instruction each time . Subject to network analog - digital converters and control circuitry , etc. , are well connectivity and power constraints , information can be 50 known to artisans and so are not belabored . ) Camera 32 gleaned continuously from such processing , and may be captures digital image data in accordance with instructions used in processing subsequently - captured images . For in the header field 55 of the packet and stuffs the resulting example , an earlier image in a sequence that includes image data into a body 59 of the packet . It also deletes the photograph 44 may show members of the depicted group camera instructions 55 from the packet header ( or otherwise without sunglasses — simplifying identification of the per- 55 marks header field 55 in a manner permitting it to be sons later depicted with sunglasses . disregarded by subsequent processing stages ) . FIG . 16 , Etc. , Implementation When the packet 57 was authored by setup module 34 it FIG . 16 gets into the nitty gritty of a particular imple- also included a series of further header fields 58 , each mentation incorporating certain of the features earlier dis- specifying how a corresponding , successive post - sensor cussed . ( The other discussed features can be implemented 60 stage 38 should process the captured data . As shown in FIG . by the artisan within this architecture , based on the provided 16 , there are several such post - sensor processing stages 38 . disclosure . ) In this data driven arrangement 30 , operation of Camera 32 outputs the image - stuffed packet produced by a cell phone camera 32 is dynamically controlled in accor- the camera ( a pixel packet ) onto a bus or other data channel dance with packet data sent by a setup module 34 , which in 61 , which conveys it to a first processing stage 38 . turn is controlled by a control processor module 36. ( Control 65 Stage 38 examines the header of the packet . Since the processor module 36 may be the cell phone's primary camera deleted the instruction field 55 that conveyed camera processor , or an auxiliary processor , or this function may be instructions ( or marked it to be disregarded ) , the first header\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content='US 10,922,957 B2 \\n 19 20', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content=\"field encountered by a control portion of stage 38 is field on which it acted . The instructions are ordered in the header 58a . This field details parameters of an operation to be in the sequence of processing stages , so this removal allows applied by stage 38 to data in the body of the packet . each stage to look to the first instructions remaining in the For example , field 58a may specify parameters of an edge header for direction . Other arrangements , of course , can detection algorithm to be applied by stage 38 to the packet's 5 alternatively be employed . ( For example , a module may image data ( or simply that such an algorithm should be insert new information into the header at the front , tail , or applied ) . It may further specify that stage 38 is to substitute elsewhere in the sequence_based on processing results . the resulting edge - detected set of data for the original image This amended header then controls packet flow and there data in the body of the packet . ( Substituting of data , rather fore processing . ) In addition to outputting data for the next than appending , may be indicated by the value of a single bit 10 stage , each stage 38 may further have an output 31 providing flag in the packet header . ) Stage 38 performs the requested data back to the control processor module 36. For example , operation ( which may involve configuring programmable processing undertaken by one of the local stages 38 may hardware in certain implementations ) . First stage 38 then indicate that the exposure or focus of the camera should be deletes instructions 58a from the packet header 56 ( or marks adjusted to optimize suitability of an upcoming frame of them to be disregarded ) and outputs the processed pixel 15 captured data for a particular type of processing ( e.g. , object packet for action by a next processing stage . identification ) . This focus / exposure information can be used A control portion of a next processing stage ( which here as predictive setup data for the camera the next time a frame comprises stages 38a and 38b , discussed later ) examines the of the same or similar type is captured . The control processor header of the packet . Since field 58a was deleted ( or marked module 36 can set up a frame request using a filtered or to be disregarded ) , the first field encountered is field 586. In 20 time - series prediction sequence of focus information from this particular packet , field 58b may instruct the second stage previous frames , or a sub - set of those frames . not to perform any processing on the data in the body of the Error and status reporting functions may also be accom packet , but instead simply delete field 58b from the packet plished using outputs 31. Each stage may also have one or header and pass the pixel packet to the next stage . more other outputs 33 for providing data to other processes A next field of the packet header may instruct the third 25 or modules — locally within the cell phone , or remote ( “ in stage 38c to perform 2D FFT operations on the image data the cloud ” ) . Data ( in packet form , or in other format ) may be found in the packet body , based on 16x16 blocks . It may directed to such outputs in accordance with instructions in further direct the stage to hand - off the processed FFT data to packet 57 , or otherwise . a wireless interface , for internet transmission to address For example , a processing module 38 may make a data 216.239.32.10 , accompanied by specified data ( detailing , 30 flow selection based on some result of processing it per e.g. , the task to be performed on the received FFT data by forms . E.g. , if an edge detection stage discerns a sharp the computer at that address , such as texture classification ) . contrast image , then an outgoing packet may be routed to an It may further direct the stage to hand off a single 16x16 external service provider for FFT processing . That provider block of FFT data , corresponding to the center of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content=\"packet may be routed to an It may further direct the stage to hand off a single 16x16 external service provider for FFT processing . That provider block of FFT data , corresponding to the center of the may return the resultant FFT data to other stages . However , captured image , to the same or a different wireless interface 35 if the image has poor edges ( such as being out of focus ) , then for transmission to address 12.232.235.27 — again accom- the system may not want FFT- and following processing to panied by corresponding instructions about its use ( e.g. , be performed on the data . Thus , the processing stages can search an archive of stored FFT data for a match , and return cause branches in the data flow , dependent on parameters of information if a match is found ; also , store this 16x16 block the processing ( such as discerned image characteristics ) . in the archive with an associated identifier ) . Finally , the 40 Instructions specifying such conditional branching can be header authored by setup module 34 may instruct stage 38c included in the header of packet 57 , or they can be provided to replace the body of the packet with the single 16x16 block otherwise . FIG . 19 shows one arrangement . Instructions 58d of FFT data dispatched to the wireless interface . As before , originally in packet 57 specify a condition , and specify a the stage also edits the packet header to delete ( or mark ) the location in a memory 79 from which replacement subse instructions to which it responded , so that a header instruc- 45 quent instructions ( 58e - 58g ' ) can be read , and substituted tion field for the next processing stage is the first to be into the packet header , if the condition is met . If the encountered . condition is not met , execution proceeds in accordance with In other arrangements , the addresses of the remote com- header instructions already in the packet . puters are not hard - coded . For example , the packet may In other arrangements , other variations can be employed . include a pointer to a database record or memory location ( in 50 For example , all of the possible conditional instructions can the phone or in the cloud ) , which contains the destination be provided in the packet . In another arrangement , a packet address . Or , stage 38c may be directed to hand - off the architecture is still used , but one or more of the header fields processed pixel packet to the Query Router and Response do not include explicit instructions . Rather , they simply Manager ( e.g. , FIG . 7 ) . This module examines the pixel point to memory locations from which corresponding packet to determine what type of processing is next required , 55 instructions ( or data ) are retrieved , e.g. , by the correspond and it routes it to an appropriate provider ( which may be in ing processing stage 38 . the cell phone if resources permit , or in the cloud among Memory 79 ( which can include a cloud component ) can the stable of static providers , or to a provider identified also facilitate adaptation of processing flow even if condi through an auction ) . The provider returns the requested tional branching is not employed . For example , a processing output data ( e.g. , texture classification information , and 60 stage may yield output data that determines parameters of a information about any matching FFT in the archive ) , and filter or other algorithm to be applied by a later stage ( e.g. , processing continues per the next item of instruction in the a convolution kernel , a time delay , a pixel mask , etc ) . Such pixel packet header . parameters may be identified by the former processing stage The data flow continues through as many functions as a in memory ( e.g. , determined / calculated , and stored ) , and particular operation may require . 65 recalled for use by the later stage . In FIG . 19 , for example , In the particular arrangement illustrated , each processing processing stage 38 produces parameters that are\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content='may require . 65 recalled for use by the later stage . In FIG . 19 , for example , In the particular arrangement illustrated , each processing processing stage 38 produces parameters that are stored in stage 38 strips - out , from the packet header , the instructions memory 79. A subsequent processing stage 38c later', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content='on . \\n 15 US 10,922,957 B2 \\n 21 22 \\n retrieves these parameters , and uses them in execution of its In some embodiments , the respective purpose processors assigned operation . ( The information in memory can be may be chained in a fixed order . The edge detection pro labeled to identify the module / provider from which they cessor may be first , the FFT processor may be third , and so originated , or to which they are destined < if known > , or other addressing arrangements can be used . ) Thus , the 5 Alternatively , the processing modules may be intercon \\n processing flow can adapt to circumstances and parameters nected by one or more busses ( and / or a crossbar arrange \\n that were not known at the time control processor module 36 ment or other interconnection architecture ) that permit any \\n originally directed setup module 34 to author packet 57 . stage to receive data from any stage , and to output data to \\n In one particular embodiment , each of the processing any stage . Another interconnect method is a network on a \\n stages 38 comprises hardware circuitry dedicated to a par 10 chip ( effectively a packet - based LAN ; similar to crossbar in \\n ticular task . The first stage 38 may be a dedicated edge adaptability , but programmable by network protocols ) . Such arrangements can also support having one or more stages detection processor . The third stage 38c may be a dedicated iteratively process data — taking output as input , to perform FFT processor . Other stages may be dedicated to other further processing . processes . These may include DCT , wavelet , Haar , Hough , One iterative processing arrangement is shown by stages and Fourier - Mellin transform processors , filters of different 38a / 38b in FIG . 16. Output from stage 38a can be taken as sorts ( e.g. , Wiener , low pass , bandpass , highpass ) , and stages input to stage 38b . Stage 38b can be instructed to do no for performing all or part of operations such as facial processing on the data , but simply apply it again back to the recognition , optical character recognition , computation of input of stage 38a . This can loop as many times as desired . eigenvalues , extraction of shape , color and texture feature 20 When iterative processing by stage 38a is completed , its data , barcode decoding , watermark decoding , object seg- output can be passed to a next stage 38c in the chain . mentation , pattern recognition , age and gender detection , In addition to simply serving as a pass - through stage , emotion classification , orientation determination , compres- stage 38b can perform its own type of processing on the data sion , decompression , log - polar mapping , convolution , inter- processed by stage 38a . Its output can be applied to the input polation , decimation / down - sampling / anti - aliasing ; correla- 25 of stage 38a . Stage 38a can be instructed to apply , again , its tion , performing square - root and squaring operations , array process to the data produced by stage 38b , or to pass it \\n multiplication , perspective transformation , butterfly opera through . Any serial combination of stage 38a / 38b process \\n tions ( combining results of smaller DFTs into a larger DFT , ing can thus be achieved . \\n or decomposing a larger DCT into subtransforms ) , etc. The roles of stages 38a and 38b in the foregoing can also \\n These hardware processors can be field - configurable , 30 be reversed . \\n instead of dedicated . Thus , each of the processing blocks in In this fashion , stages 38a and 38b can be operated to ( 1 )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content=\"instead of dedicated . Thus , each of the processing blocks in In this fashion , stages 38a and 38b can be operated to ( 1 ) \\n FIG . 16 may be dynamically reconfigurable , as circum apply a stage 38a process one or more times to data ; ( 2 ) apply a stage 38b process one or more times to data ; ( 3 ) stances warrant . At one instant a block may be configured as apply any combination and order of 38a and 38b processes an FFT processing module . The next instant it may be 35 to data ; or ( 4 ) simply pass the input data to the next stage , configured as a filter stage , etc. One moment the hardware without processing . processing chain may be configured as a barcode reader ; the The camera stage can be incorporated into an iterative next it may be configured as a facial recognition system , etc. processing loop . For example , to gain focus - lock , a packet Such hardware reconfiguration information can be down may be passed from the camera to a processing module that loaded from the cloud , or from services such as the Apple 40 assesses focus . ( Examples may include an FFT stage AppStore . And the information needn't be statically resident looking for high frequency image components ; an edge on the phone once downloaded it can be summoned from detector stage_looking for strong edges , etc. Sample edge the cloud / AppStore whenever needed . detection algorithms include Canny , Sobel , and differential . Given increasing broadband availability and speed , the Edge detection is also useful for object tracking . ) An output hardware reconfiguration data can be downloaded to the cell 45 from such a processing module can loop back to the cam phone each time it is turned on , or otherwise initialized era's controller module and vary a focus signal . The camera whenever a particular function is initialized . Gone would be captures a subsequent frame with the varied focus signal , the dilemma of dozens of different versions of an application and the resulting image is again provided to the processing \\n being deployed in the market at any given time depending module that assesses focus . This loop continues until the on when different users last downloaded updates , and the 50 processing module reports focus within a threshold range is conundrums that companies confront in supporting disparate achieved . ( The packet header , or a parameter in memory , can \\n versions of products in the field . Each time a device or specify an iteration limit , e.g. , specifying that the iterating\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content='versions of products in the field . Each time a device or specify an iteration limit , e.g. , specifying that the iterating \\n application is initialized , the latest version of all or selected should terminate and output an error signal if no focus meeting the specified requirement is met within ten itera functionalities is downloaded to the phone . And this works 55 tions . ) not just for full system functionality , but also components , While the discussion has focused on serial data process such as hardware drivers , software for hardware layers , etc. ing , image or other data may be processed in two or more At each initialization , hardware is configured anew — with parallel paths . For example , the output of stage 38d may be the latest version of applicable instructions . ( For code used applied to two subsequent stages , each of which starts a during initializing , it can be downloaded for use at the next 60 respective branch of a fork in the processing . Those two initialization . ) Some updated code may be downloaded and chains can be processed independently thereafter , or data dynamically loaded only when particular applications resulting from such processing can be combined or used in require it , such as to configure the hardware of FIG . 6 for conjunction — in a subsequent stage . ( Each of those process specialized functions . The instructions can also be tailored to ing chains , in turn , can be forked , etc. ) the particular platforms , e.g. , the iPhone device may employ 65 As noted , a fork commonly will appear much earlier in the different accelerometers than the Android device , and appli- chain . That is , in most implementations , a parallel process cation instructions may be varied accordingly . ing chain will be employed to produce imagery for human or', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content=\"source . 10 \\n 15 US 10,922,957 B2 \\n 23 24 \\n as opposed to machine — consumption . Thus , a parallel convey firmware to be loaded into a module having a CPU process may fork immediately following the camera sensor core , or into an application- or cloud - based layer ; likewise 12 , as shown by juncture 17 in FIG . 13. The processing for with software instructions . the human visual system 13 includes operations such as The module configuration instructions may be received noise reduction , white balance , and compression . Processing 5 over a wireless or other external network ; it needn't always for object identification 14 , in contrast , may include the be resident on the local system . If the user requests an operations detailed in this specification . operation for which local instructions are not available , the When an architecture involves forked or other parallel system can request the configuration data from a remote processes , the different modules may finish their processing at different times . They may output data as they finish Instead of conveying the configuration data / instructions asynchronously , as the pipeline or other interconnection themselves , the packet may simply convey an index number , network permits . When the pipeline / network is free , a next pointer , or other address information . This information can module can transfer its completed results . Flow control may be used by the processing module to access a corresponding involve some arbitration , such as giving one path or data a memory store from which the needed data / instructions can higher priority . Packets may convey priority data - deter- be retrieved . Like a cache , if the local memory store is not mining their precedence in case arbitration is needed . For found to contain the needed data / instructions , they can then example , many image processing operations / modules make be requested from another source ( e.g. , across an external \\n use of Fourier domain data , such as produced by an FFT network ) . module . The output from an FFT module may thus be given 20 Such arrangements bring the dynamic of routability down a high priority , and precedence over others in arbitrating data to the hardware layer configuring the module as data \\n traffic , so that the Fourier data that may be needed by other arrives at it . modules be made available with a minimum of delay . Parallelism is widely employed in graphics processing In other implementations , some or all of the processing units ( GPUs ) . Many computer systems employ GPUs as stages are not dedicated purpose processors , but rather are 25 auxiliary processors to handle operations such as graphics general purpose microprocessors programmed by software . rendering . Cell phones increasingly include GPU chips to \\n In still other implementations , the processors are hardware allow the phones to serve as gaming platforms ; these can be \\n reconfigurable . For example , some or all may be field employed to advantage in certain implementations of the programmable gate arrays , such as Xilinx Virtex series present technology . ( By way of example and not limitation ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content=\"devices . Alternatively they may be digital signal processing 30 a GPU can be used to perform bilinear and bicubic inter polation , projective transformations , filtering , etc. ) cores , such as Texas Instruments TMS320 series devices . In accordance with another aspect of the present technol Other implementations can include PicoChip devices , ogy , a GPU is used to correct for lens aberrations and other such as the PC302 and PC312 multicore DSPs . Their optical distortion . programming model allows each core to be coded indepen Cell phone cameras often display optical non - linearieties , dently ( e.g. , in C ) , and then to communicate with others over such as barrel distortion , focus anomalies at the perimeter , an internal interconnect mesh . The associated tools particu etc. This is particularly a problem when decoding digital larly lend themselves to use of such processors in cellular watermark information from captured imagery . With a GPU , equipment . the image can be treated as a texture map , and applied to a Still other implementations may employ configurable 40 correction surface . logic on an ASIC . For example , a processor can include a Typically , texture mapping is used to put a picture of region of configuration logic - mixed with dedicated logic . bricks or a stone wall onto a surface , e.g. , of a dungeon . This allows configurable logic in a pipeline , with dedicated Texture memory data is referenced , and mapped onto a plane pipeline or bus interface circuitry . or polygon as it is drawn . In the present context it is the An implementation can also include one or more modules 45 image that is applied to a surface . The surface is shaped so with a small CPU and RAM , with programmable code space that the image is drawn with an arbitrary , correcting trans for firmware , and workspace for processing essentially a form . dedicated core . Such a module can perform fairly extensive Steganographic calibration signals in a digitally water computations configurable as needed by the process that is marked image can be used to discern the distortion by which using the hardware at the time . 50 an image has been transformed . ( See , e.g. , Digimarc's U.S. All such devices can be deployed in a bus , crossbar or Pat . No. 6,590,996 . ) Each patch of a watermarked image can other interconnection architecture that again permits any be characterized by affine transformation parameters , such stage to receive data from , and output data to , any stage . ( A as translation and scale . An error function for each location FFT or other transform processor implemented in this fash- in the captured frame can thereby be derived . From this error ion may be reconfigured dynamically to process blocks of 55 information , a corresponding surface can be devised 16x16 , 64x64 , 4096x4096 , 1x64 , 32x128 , etc. ) which — when the distorted image is projected onto it by the In certain implementations , some processing modules are GPU , the surface causes the image to appear in its counter replicated permitting parallel execution on parallel hard- distorted , original form . ware . For example , several FFTs may be processing simul- A lens can be characterized in this fashion with a refer taneously 60 ence watermark image . Once the associated correction sur In a variant arrangement , a packet conveys instructions face has been devised , it can be re - used with other imagery that serve to reconfigure hardware of one or more of the captured through that optical system ( since the associated processing modules . As a packet enters a module , the header distortion is fixed ) . Other imagery can be projected onto this causes the module to reconfigure the hardware before the correction surface by the GPU to correct the lens distortion . image - related data is accepted for processing . The architec- 65 ( Different focal depths , and apertures , may require charac ture is thus configured on the fly by packets ( which may terization of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content='- related data is accepted for processing . The architec- 65 ( Different focal depths , and apertures , may require charac ture is thus configured on the fly by packets ( which may terization of different correction functions , since the optical convey image related data , or not ) . The packets can similarly path through the lens may be different . ) 35', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content='5 US 10,922,957 B2 \\n 25 26', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='When a new image is captured , it can be initially recti- ( e.g. , by magnetometer ) , the camera can check that it is linearized , to rid it of keystone / trapezoidal perspective capturing an intended target . The camera set - up module may effect . Once rectilinearized ( e.g. , re - squared relative to the request images of not just certain exposure parameters , but camera lens ) , the local distortions can be corrected by also of certain subjects , or locations . When a camera is in the mapping the rectilinearized image onto the correction sur- correct position to capture a specific subject ( which may face , using the GPU . have been previously user - specified , or identified by a Thus , the correction model is in essence a polygon computer process ) , one or more frames of image data surface , where the tilts and elevations correspond to focus automatically can be captured . ( In some arrangements , the irregularities . Each region of the image has a local transform orientation of the camera is controlled by stepper motors or matrix allowing for correction of that piece of the image . 10 other electromechanical arrangements , so that the camera The same arrangement can likewise be used to correct can autonomously set the azimuth and elevation to capture distortion of a lens in an image projection system . Before image data from a particular direction , to capture a desired projection , the image is mapped like a texture — onto a subject . Electronic or fluid steering of the lens direction can correction surface synthesized to counteract lens distortion . also be utilized . ) When the thus - processed image is projected through the 15 As noted , the camera setup module may instruct the lens , the lens distortion counteracts the correction surface camera to capture a sequence of frames . In addition to distortion earlier applied , causing a corrected image to be benefits such as synthesizing high dynamic range imagery , projected from the system . such frames can also be aligned and combined to obtain Reference was made to the depth of field as one of the super - resolution images . ( As is known in the art , super parameters that can be employed by camera 32 in gathering 20 resolution can be achieved by diverse methods . For exposures . Although a lens can precisely focus at only one example , the frequency content of the images can be ana distance , the decrease in sharpness is gradual on either side lyzed , related to each other by linear transform , affine of the focused distance . ( The depth of field depends on the transformed to correct alignment , then overlaid and com point spread function of the optics — including the lens focal bined . In addition to other applications , this can be used in length and aperture . ) As long as the captured pixels yield 25 decoding digital watermark data from imagery . If the subject information useful for the intended operation , they need not is too far from the camera to obtain satisfactory image be in perfect focus . resolution normally , it may be doubled by such super Sometime focusing algorithms hunt for , but fail to achieve resolution techniques to obtain the higher resolution needed focus wasting cycles and battery life . Better , in some for successful watermark decoding . ) instances , is to simply grab frames at a series of different 30 In the exemplary embodiment , each processing stage focus settings . A search tree of focus depths , or depths of substituted the results of its processing for the input data field , may be used . This is particularly useful where an contained in the packet when received . In other arrange image may include multiple subjects of potential interest- ments , the processed data can be added the packet body , each at a different plane . The system may capture a frame while maintaining the data originally present . In such case focused at 6 inches and another at 24 inches . The different 35 the packet grows during processing as more', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='The system may capture a frame while maintaining the data originally present . In such case focused at 6 inches and another at 24 inches . The different 35 the packet grows during processing as more information is frames may reveal that there are two objects of interest added . While this may be disadvantageous in some contexts , within the field of view- one better captured in one frame , it can also provide advantages . For example , it may obviate the other better captured in the other . Or the 24 inch - focused the need to fork a processing chain into two packets or two frame may be found to have no useful data , but the 6 threads . Sometimes both the original and the processed data inch - focused frame may include enough discriminatory fre- 40 are useful to a subsequent stage . For example , an FFT stage quency content to see that there are two or more subject may add frequency domain information to a packet contain image planes . Based on the frequency content , one or more ing original pixel domain imagery . Both of these may be frames with other focus settings may then be captured . Or a used by a subsequent stage , e.g. , in performing sub - pixel region in the 24 inch - focused frame may have one set of alignment for super - resolution processing . Likewise , a focus Fourier attributes , and the same region in the 6 inch - focused 45 metric may be extracted from imagery and used— with frame may have a different set of Fourier attributes , and from accompanying image data — by a subsequent stage . the difference between the two frames a next trial focus It will be recognized that the detailed arrangements can be setting may be identified ( e.g. , at 10 inches ) , and a further used to control the camera to generate different types of frame at that focus setting may be captured . Feedback is image data on a per - frame basis , and to control subsequent applied — not necessarily to obtain perfect focus lock , but in 50 stages of the system to process each such frame differently . accordance with search criteria to make decisions about Thus , the system may capture a first frame under conditions further captures that may reveal additional useful detail . The selected to optimize green watermark detection , capture a search may fork and branch , depending on the number of second frame under conditions selected to optimize barcode subjects discerned , and associated Fourier , etc. , information , reading , capture a third frame under conditions selected to until satisfactory information about all subjects has been 55 optimize facial recognition , etc. Subsequent stages may be fathered . directed to process each of these frames differently , in order A related approach is to capture and buffer plural frames to best extract the data sought . All of the frames may be as a camera lens system is undergoing adjustment to an processed to sense illumination variations . Every other intended focus setting . Analysis of the frame finally captured frame may be processed to assess focus , e.g. , by computing at the intended focus may suggest that intermediate focus 60 16x16 pixel FFTs at nine different locations within the frames would reveal useful information , e.g. , about subjects image frame . ( Or there may be a fork that allows all frames not earlier apparent or significant . One or more of the frames to be assessed for focus , and the focus branch may be earlier captured and buffered can then be recalled and disabled when not needed , or reconfigured to serve another processed to provide information whose significance was purpose . ) Etc. , etc. not earlier recognized . In some implementations , frame capture can be tuned to Camera control can also be responsive to spatial coordi- capture the steganographic calibration signals present in a nate information . By using geolocation data , and orientation digital watermark signal , without regard to successful 65', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='US 10,922,957 B2 \\n 27 28', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='decoding of the watermark payload data itself . For example , purchase , a posting on Facebook , a face- or object - recogni captured image data can be at a lower resolution sufficient tion operation , etc. Once such an indexed transaction to discern the calibration signals , but insufficient to discern arrangement is initially configured , it can be easily invoked the payload . Or the camera can expose the image without simply by sending a packet to the cloud containing the regard to human perception , e.g. , overexposing so image 5 image - related data , and an identifier indicating the desired highlights are washed - out , or underexposed so other parts of operation . ) the image are indistinguishable . Yet such an exposure may At the Apple service , for example , a server may examine be adequate to capture the watermark orientation signal . the incoming packet , look - up the user\\'s iPhoto account , ( Feedback can of course be employed to capture one or more access facial recognition data for the user\\'s friends from that subsequent image frames — redressing one or more short- 10 account , compute facial recognition features from image comings of a previous image frame . ) data conveyed with the packet , determine a best match , and Some digital watermarks are embedded in specific color return result information ( e.g. , a name of a depicted indi channels ( e.g. , blue ) , rather than across colors as modulation vidual ) back to the originating device . of image luminance ( see , e.g. , commonly - owned patent At the IP address for the Google service , a server may application Ser . No. 12 / 337,029 to Reed , published as 15 undertake similar operations , but would refer to the user\\'s US20100150434 ) . In capturing a frame including such a Picasa account . Ditto for Facebook . watermark , exposure can be selected to yield maximum Identifying a face from among faces for dozens or hun dynamic range in the blue channel ( e.g. , 0-255 in an 8 - bit dreds of known friends is easier than identifying faces of sensor ) , without regard to exposure of other colors in the strangers . Other vendors may offer services of the latter sort . image . One frame may be captured to maximize dynamic 20 For example , L - 1 Identity Solutions , Inc. maintains data range of one color , such as blue , and a later frame may be bases of images from government - issued credentials — such captured to maximize dynamic range of another color chan- as drivers \\' licenses . With appropriate permissions , it may nel , such as yellow ( i.e. , along the red - green axis ) . These offer facial recognition services drawing from such data frames may then be aligned , and the blue - yellow difference bases . determined . The frames may have wholly different exposure 25 Other processing operations can similarly be operated times , depending on lighting , subject , etc. remotely . One is a barcode processor , which would take Desirably , the system has an operational mode in which it processed image data sent from the mobile phone , apply a captures and processes imagery even when the user is not decoding algorithm particular to the type of barcode present . intending to \" snap \" a picture . If the user pushes a shutter A service may support one , a few , or dozens of different button , the otherwise - scheduled image capture / processing 30 types of barcode . The decoded data may be returned to the operations may be suspended , and a consumer photo taking phone , or the service provider can access further data mode can take precedence . In this mode , capture parameters indexed by the decoded data , such as product information , and processes designed to enhance human visual system instructions , purchase options , etc. , and return such further aspects of the image can be employed instead . data to the phone . ( Or both can be provided . ) ( It will be recognized that the particular embodiment 35 Another service is digital watermark reading . Another is shown', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='can be employed instead . data to the phone . ( Or both can be provided . ) ( It will be recognized that the particular embodiment 35 Another service is digital watermark reading . Another is shown in FIG . 16 generates packets before any image data optical character recognition ( OCR ) . An OCR service pro is collected . In contrast , FIG . 10A and associated discussion vider may further offer translation services , e.g. , converting do not refer to packets existing before the camera . Either processed image data into ASCII symbols , and then submit arrangement can be used in either embodiment . That is , in ting the ASCII words to a translation engine to render them FIG . 10A , packets may be established prior to the capture of 40 in a different language . Other services are sampled in FIG . image data by the camera , in which case the visual keyvector 2. ( Practicality prevents enumeration of the myriad other processing and packaging module serves to insert the pixel services , and component operations , that may also be pro data or more typically , sub - sets or super - sets of the pixel vided . ) data - into earlier - formed packets . Similarly , in FIG . 16 , the The output from the remote service provider is commonly packets need not be created until after the camera has 45 returned to the cell phone . In many instances the remote captured image data . ) service provider will return processed image data . In some As noted earlier , one or more of the processing stages can it may return ASCII or other such data . Sometimes , be remote from the cell phone . One or more pixel packets however , the remote service provider may produce other can be routed to the cloud ( or through the cloud ) for forms of output , including audio ( e.g. , MP3 ) and / or video processing . The results can be returned to the cell phone , or 50 ( e.g. , MPEG4 and Adobe Flash ) . forwarded to another cloud processing stage ( or both ) . Once Video returned to the cell phone from the remote provider back at the cell phone , one or more further local operations may be presented on the cell phone display . In some may be performed . Data may then be sent back out the implementations such video presents a user interface screen , cloud , etc. Processing can thus alternate between the cell inviting the user to touch or gesture within the displayed phone and the cloud . Eventually , result data is usually 55 presentation to select information or an operation , or issue presented to the user back at the cell phone . an instruction . Software in the cell phone can receive such Applicants expect that different vendors will offer com- user input and undertake responsive operations , or present peting cloud services for specialized processing tasks . For responsive information . example , Apple , Google and Facebook , may each offer In still other arrangements , the data provided back to the cloud - based facial recognition services . A user device would 60 cell phone from the remote service provider can include transmit a packet of processed data for processing . The JavaScript or other such instructions . When run by the cell header of the packet can indicate the user , the requested phone , the JavaScript provides a response associated with service , and_optionally - micropayment instructions . the processed data referred out to the remote provider . ( Again , the header could convey an index or other identifier Remote processing services can be provided under a by which a desired transaction is looked - up in a cloud 65 variety of different financial models . An Apple iPhone database , or which serves to arrange an operation , or a service plan may be bundled with a variety of remote sequence of processes for some transaction such as a services at no additional cost , e.g. , iPhoto - based facial cases', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='US 10,922,957 B2 \\n 29 30 \\n recognition . Other services may bill on a per - use , monthly One function performed by pipe manager 51 is to nego subscription , or other usage plans . tiate for needed communication resources . The cell phone Some services will doubtless be highly branded , and can employ a variety of communication networks and com marketed . Others may compete on quality ; others on price . mercial data carriers , e.g. , cellular data , WiFi , Bluetooth , As noted , stored data may indicate preferred providers for 5 etc. any or all of which may be utilized . Each may have its different services . These may be explicitly identified ( e.g. , own protocol stack . In one respect the pipe manager 51 send all FFT operations to the Fraunhofer Institute service ) , interacts with respective interfaces for these data channels or they can be specified by other attributes . For example , a determining the availability of bandwidth for different data cell phone user may direct that all remote service requests payloads . are to be routed to providers that are ranked as fastest in a 10 For example , the pipe manager may alert the cellular data periodically updated survey of providers ( e.g. , by Consum carrier local interface and network that there will be a ers Union ) . The cell phone can periodically check the published results for this information , or it can be checked payload ready for transmission starting in about 450 milli \\n dynamically when a service is requested . Another user may seconds . It may further specify the size of the payload ( e.g. , specify that service requests are to be routed to service 15 two megabits ) , its character ( e.g. , block data ) , and a needed \\n providers that have highest customer satisfaction scores quality of service ( e.g. , data throughput rate ) . It may also \\n again by reference to an online rating resource . Still another specify a priority level for the transmission , so that the \\n user may specify that requests should be routed to the interface and network can service such transmission ahead providers having highest customer satisfaction scores — but of lower - priority data exchanges , in the event of a conflict . only if the service is provided for free ; else route to the 20 The pipe manager knows the expected size of the payload lowest cost provider . Combinations of these arrangements , due to information provided by the control processor module and others , are of course possible . The user may , in a 36. ( In the illustrated embodiment , the control processor particular case , specify a particular service provider- module specifies the particular processing that will yield the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"trumping any selection that would be made by the stored payload , and so it can estimate the size of the resulting data ) . profile data . 25 The control processor module can also predict the character In still other arrangements the user's request for service of the data , e.g. , whether it will be available as a fixed block can be externally posted , and several service providers may or intermittently in bursts , the rate at which it will be express interest in performing the requested operation . Or provided for transmission , etc. The control processor mod the request can be sent to several specific service providers ule 36 can also predict the time at which the data will be for proposals ( e.g. , to Amazon , Google and Microsoft ) . The 30 ready for transmission . The priority information , too , is different providers ' responses ( pricing , other terms , etc. ) known by the control processor module . In some instances may be presented to the user , who selects between them , or the control processor module autonomously sets the priority a selection may be made automatically - based on previ- level . In other instances the priority level is dictated by the ously stored rules . In some cases , one or more competing user , or by the particular application being serviced . service providers can be provided user data with which they 35 For example , the user may expressly signal — through the start performing , or wholly perform , the subject operation cell phone's graphical user interface , or a particular appli before a service provider selection is finally made giving cation may regularly require , that an image - based action is such providers a chance to speed their response times , and to be processed immediately . This may be the case , for encounter additional real - world data . ( See , also , the earlier example , where further action from the user is expected discussion of remote service providers , including auction- 40 based on the results of the image processing . In other cases based services , e.g. , in connection with FIGS . 7-12 . ) the user may expressly signal , or a particular application As elsewhere indicated , certain external service requests may normally permit , that an image - based action can be may pass through a common hub ( module ) , which is respon- performed whenever convenient ( e.g. , when needed sible for distributing the requests to appropriate service resources have low or nil utilization ) . This may be the case , providers . Reciprocally , results from certain external service 45 for example , if a user is posting a snapshot to a social requests may similarly be routed through a common hub . networking site such as Facebook , and would like the image For example , payloads decoded by different service provid- annotated with names of depicted individuals through ers from different digital watermarks ( or payloads decoded facial recognition processing . Intermediate prioritization from different barcodes , or fingerprints computed from ( expressed by the user , or by the application ) can also be different content objects ) may be referred to a common hub , 50 employed , e.g. , process within a minute , ten minutes , an which may compile statistics and aggregate information hour , a day , etc. ( akin to Nielsen's monitoring services — surveying con- In the illustrated arrangement , the control processor mod sumer encounters with different data ) . Besides the decoded ule 36 informs the pipe manager of the expected data size , watermark data ( barcode data , fingerprint data ) , the hub may character , timing , and priority , so that the pipe manager can also ( or alternatively ) be provided with a quality or confi- 55 use same in negotiating for the desired service . ( In other dence metric associated with each decoding / computing embodiments , less or more information can be provided . ) operation . This may help reveal packaging issues , print If the carrier and interface can meet\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"associated with each decoding / computing embodiments , less or more information can be provided . ) operation . This may help reveal packaging issues , print If the carrier and interface can meet the pipe manager's issues , media corruption issues , etc. , that need consideration . request , further data exchange may ensue to prepare for the Pipe Manager data transmission and ready the remote system for the In the FIG . 16 implementation , communications to and 60 expected operation . For example , the pipe manager may from the cloud are facilitated by a pipe manager 51. This establish a secure socket connection with a particular com module ( which may be realized as the cell phone - side puter in the cloud that is to receive that particular data portion of the query router and response manager of FIG . 7 ) payload , and identify the user . If the cloud computer is to performs a variety of functions relating to communicating perform a facial recognition operation , it may prepare for the across a data pipe 52. ( It will be recognized that pipe 52 is 65 operation by retrieving from Apple / Google / Facebook the a data construct that may comprise a variety of communi- facial recognition features , and associated names , for friends cation channels . ) of the specified user .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"US 10,922,957 B2 \\n 31 32 \\n Thus , in addition to preparing a channel for the external These modes may be selected by the user in advance of communication , the pipe manager enables pre - warming of operating a shutter control , or after . In other arrangements , the remote computer , to ready it for the expected service plural shutter controls ( physical or GUI ) are provided for the request . ( The service may request may not follow . ) In some user - respectively invoking different of the available opera instances the user may operate the shutter button , and the 5 tions . ( In still other embodiments , the device infers what cell phone may not know what operation will follow . Will operation ( s ) is / are possibly desired , rather than having the the user request a facial recognition operation ? A barcode user expressly indicate same . ) decoding operation ? Posting of the image to Flickr or If the user at the business gathering takes a group shot Facebook ? In some cases the pipe manager or control depicting twelve individuals , and requests the names on an processor module — may pre - warm several processes . Or it 10 immediate basis , the pipeline manager 51 may report back may predict , based on past experience , what operation will to the control processor module ( or to application software ) be undertaken , and warm appropriate resources . ( E.g. , if the that the requested service cannot be provided . Due to a user performed facial recognition operations following the bottleneck or other constraint , the manager 51 may report last three shutter operations , there's a good chance the user will request facial recognition again . ) The cell phone may 15 that identification of only three of the depicted faces can be \\n actually start performing component operations for various accommodated within service quality parameters considered \\n of the possible functions before any has been selected to constitute an “ immediate ” basis . Another three faces may \\n particularly those operations whose results may be useful to be recognized within two seconds , and recognition of the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content='particularly those operations whose results may be useful to be recognized within two seconds , and recognition of the \\n several of the functions . Pre - warming can also include full set of faces may be expected in five seconds . ( This may resources within the cell phone : configuring processors , 20 be due to a constraint by the remote service provider , rather loading caches , etc. than the carrier , per se . ) The situation just reviewed contemplates that desired The control processor module 36 ( or application software ) resources are ready to handle the expected traffic . In another may respond to this report in accordance with an algorithm , situation the pipe manager may report that the carrier is or by reference to a rule set stored in a local or remote data unavailable ( e.g. , due to the user being in a region of 25 structure . The algorithm or rule set may conclude that for impaired radio service ) . This information is reported to facial recognition operations , delayed service should be control processor module 36 , which may change the sched accepted on whatever terms are available , and the user ule of image processing , buffer results , or take other respon should be alerted ( through the device GUI ) that there will be sive action . a delay of about N seconds before full results are available . If other , conflicting , data transfers are underway , the 30 Optionally , the reported cause of the expected delay may carrier or interface may respond to the pipe manager that the also be exposed to the user . Other service exceptions may be requested transmission cannot be accommodated , e.g. , at the requested time or with the requested quality of service . In handled differently — in some cases with the operation aborted or rescheduled or routed to a less - preferred provider , this case the pipe manager may report same to the control and / or with the user not alerted . processor module 36. The control processor module may 35 abort the process that was to result in the two megabit data In addition to considering the ability of the local device \\n service requirement and reschedule it for later . Alternatively , interface to the network , and the ability of the network / \\n the control processor module may decide that the two carrier , to handle the forecast data traffic ( within specified megabit payload may be generated as originally scheduled , parameters ) , the pipeline manager may also query resources and the results may be locally buffered for transmission 40 out in the cloud — to ensure that they are able to perform when the carrier and interface are able to do so . Or other whatever services are requested ( within specified param', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content=\"action may be taken . eters ) . These cloud resources can include , e.g. , data net Consider a business gathering in which a participant works and remote computers . If any responds in the nega gathers a group for a photo before dinner . The user may want tive , or with a service level qualification , this too can be all faces in the photo to be recognized immediately , so that 45 reported back to the control processor module 36 , so that they can be quickly reviewed to avoid the embarrassment of appropriate action can be taken . not recalling a colleague's name . Even before the user In response to any communication from the pipe manager operates the cell phone's user - shutter button the control 51 indicating possible trouble servicing the expected data processor module causes the system to process frames of flow , the control process 36 may issue corresponding image data , and is identifying apparent faces in the field of 50 instructions to the pipe manager and / or other modules , as view ( e.g. , oval shapes , with two seeming eyes in expected necessary . positions ) . These may be highlighted by rectangles on the In addition to the just - detailed tasks of negotiating in cell phone's viewfinder ( screen ) display . advance for needed services , and setting up appropriate data While current cameras have picture - taking modes based connections , the pipe manager can also act as a flow control on lens / exposure profiles ( e.g. , close - up , night - time , beach , 55 manager orchestrating the transfer of data from the differ landscape , snow scenes , etc ) , imaging devices may addi- ent modules out of the cell phone , resolving conflicts , and tionally ( or alternatively ) have different image - processing reporting errors back to the control processor module 36 . modes . One mode may be selected by the user to obtain While the foregoing discussion has focused on outbound names of people depicted in a photo ( e.g. , through facial data traffic , there is a similar flow inbound , back to the cell recognition ) . Another mode may be selected to perform 60 phone . The pipe manager ( and control processor module ) optical character recognition of text found in an image can help administer this traffic as well providing services frame . Another may trigger operations relating to purchasing complementary to those discussed in connection with out a depicted item . Ditto for selling a depicted item . Ditto for bound traffic . obtaining information about a depicted object , scene or In some embodiments , there may be a pipe manager person ( e.g. , from Wikipedia , a social network , a manufac- 65 counterpart module 53 out in the cloud cooperating with turer's web site ) , etc. Ditto for establishing a ThinkPipe pipe manager 51 in the cell phone in performance of the session with the item , or a related system . Etc. detailed functionality .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content='US 10,922,957 B2 \\n 33 34 \\n Software Embodiment of Control Processor & Pipe Man- specific — based on context or user desires ( in a photo taking \\n ager mode , face detection may be presumed . ) Research in the area of autonomous robotics shares some In a particular arrangement , keyvectors from each sensor similar challenges with the scenarios described herein , spe- are created and packaged by device driver software pro cifically that of enabling a system of sensors to communicate 5 cesses that abstract the hardware specific embodiments of \\n data to local and remote processes , resulting in action to be the sensor and provide a fully formed keyvector adhering to \\n taken locally . In the case of a robotics it involves moving a a selected protocol . \\n robot out of harm\\'s way ; in the present case it is most The device driver software can then place the formed \\n commonly focused on providing a desired experience based keyvector on an output queue unique to that sensor , or in a \\n on image , sound , etc. encountered . 10 common message queue shared by all the sensors . Regard \\n As opposed to performing simple operations such as less of approach , local processes can consume the keyvec tors and perform the needed operations before placing the obstacle avoidance , aspects of the present technology desire resultant keyvectors back on the queue . Those keyvectors to provide higher levels of semantics , and hence richer that are to be processed by remote services are then placed experiences , based on sensory input . A user pointing a 15 in packets and transmitted directly to a remote processes for camera at a poster does not want to know the distance to the additional processing or to a remote service that distributes wall ; the user is much more inclined to want to know the the keyvectors — similar to a router . It should be clear to the about the content on the poster , if it concerns a movie , where reader , that commands to initialize or setup any of the it is playing , reviews , what their friends think , etc. sensors or processes in the system can be distributed in a Despite such differences , architectural approaches from 20 similar fashion from a Control Process ( e.g. , box 36 in FIG . robotic toolkits can be adapted for use in the present context . 16. ) One such robotic toolkit is such as the Player Project — a set Branch Prediction ; Commercial Incentives of free software tools for robot and sensor applications , The technology of branch prediction arose to meet the available as open source from sourceforge - dot - net . needs of increasingly complex processor hardware ; it allows An illustration of the Player Project architecture is shown 25 processors with lengthy pipelines to fetch data and instruc in FIG . 19A . The mobile robot ( which typically has a tions ( and in some cases , execute the instructions ) , without relatively low performance processor ) communicates with a waiting for conditional branches to be resolved . fixed server ( with a relatively higher performance processor ) A similar science can be applied in the present context using a wireless protocol . Various sensor peripherals are predicting what action a human user will take . For example , coupled to the mobile robot ( client ) processor through 30 as discussed above , the just - detailed system may \" pre respective drivers , and an API . Likewise , services may be warm ” certain processors , or communication channels , in invoked by the server processor from software libraries , anticipation that certain data or processing operations will be through another API . ( The CMU CMVision library is shown forthcoming', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 79}), Document(page_content=\"in FIG . 19A . ) When a user removes an iPhone from her purse ( exposing ( In addition to the basic tools for interfacing robotic 35 the sensor to increased light ) and lifts it to eye level ( as equipment to sensors and service libraries , the Player Project sensed by accelerometers ) , what is she about to do ? Refer includes “ Stage ” software that simulates a population of ence can be made to past behavior to make a prediction . mobile robots moving in a 2D environment , with various Particularly relevant may include what the user did with the sensors and processing — including visual blob detection . phone camera the last time it was used ; what the user did “ Gazebo ” extends the Stage model to 3D . ) 40 with the phone camera at about the same time yesterday ( and By such system architecture , new sensors can quickly be at the same time a week ago ) ; what the user last did at about utilized by provision of driver software that interfaces with the same location , etc. Corresponding actions can be taken the robot API . Similarly , new services can be readily in anticipation . plugged in through the server API . The two Player Project If her latitude / longitude correspond to a location within a APIs provide standardized abstractions so that the drivers 45 video rental store , that helps . Expect to maybe perform and services do not need to concern themselves with the image recognition on artwork from a DVD box . To speed particular configuration of the robot or server ( and vice- possible recognition , perhaps SIFT or other feature recog \\n versa ) . nition reference data should be downloaded for candidate ( FIG . 20A , discussed below , also provides a layer of DVDs and stored in a cell phone cache . Recent releases are abstraction between the sensors , the locally - available opera- 50 good prospects ( except those rated G , or rated high for tions , and the externally - available operations . ) violence — stored profile data indicates the user just doesn't Certain embodiments of the present technology can be have a history of watching those ) . So are movies that she's implemented using a local process & remote process para- watched in the past ( as indicated by historical rental digm akin to that of the Player Project , connected by a records also available to the phone ) . packet network and inter - process & intra - process commu- 55 If the user's position corresponds to a downtown street , nication constructs familiar to artisans ( e.g. , named pipes , and magnetometer and other position data indicates she is sockets , etc. ) . Above the communication minutiae is a looking north , inclined up from the horizontal , what's likely protocol by which different processes may communicate ; to be of interest ? Even without image data , a quick reference this may take the form of a message passing paradigm and to online resources such as Google Streetview can suggest message queue , or more of a network centric approach 60 she's looking at business signage along 5th Avenue . Maybe where collisions of keyvectors are addressed after the fact feature recognition reference data for this geography should ( re - transmission , drop if timely in nature , etc. ) . be downloaded into the cache for rapid matching against In such embodiments , data from sensors on the mobile to - be - acquired image data . device ( e.g. , microphone , camera ) can be packaged in To speed performance , the cache should be loaded in a keyvector form , with associated instructions . The 65 rational fashion so that the most likely object is considered instruction ( s ) associated with data may not be express ; they first . Google Streetview for that location includes metadata can be implicit ( such as Bayer conversion ) or session indicating 5th Avenue has signs for a Starbucks , a Nordstrom\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 79}), Document(page_content='presented US 10,922,957 B2 \\n 35 36', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='store , and a Thai restaurant . Stored profile data for the user sponding Amazon link atop the list ( as may occur with a reveals she visits Starbucks daily ( she has their branded regular Google search ) , the cell phone recognizes that the loyalty card ) ; she is a frequent clothes shopper ( albeit with user is located in an independent bookstore . Context - based a Macy\\'s , rather than a Nordstrom\\'s charge card ) ; and she\\'s rules consequently dictate that it present a non - commercial never eaten at a Thai restaurant . Perhaps the cache should be 5 link first . Top ranked of this type is a Wall Street Journal loaded so as to most quickly identify the Starbucks sign , review of the book , which goes to the top of the followed by Nordstrom , followed by the Thai restaurant . list of links Decorum , however , only goes so far . The cell Low resolution imagery captured for presentation on the phone passes the book title or ISBN ( or the image itself ) to viewfinder fails to trigger the camera\\'s feature highlighting Google AdSense or AdWords , which identifies sponsored probable faces ( e.g. , for exposure optimization purposes ) . 10 links to be associated with that object . ( Google may inde That helps . There\\'s no need to pre - warm the complex pendently perform its own image analysis on any provided processing associated with facial recognition . imagery . In some cases it may pay for such cell phone She touches the virtual shutter button , capturing a frame submitted imagery — since Google has a knack for exploiting of high resolution imagery , and image analysis gets under- data from diverse sources . ) Per Google , Barnes and Noble way trying to recognize what\\'s in the field of view , so that 15 has the top sponsored position , followed by alldiscount the camera application can overlay graphical links related to books - dot - net . The cell phone application may present these objects in the captured frame . ( Or this may happen without sponsored links in a graphically distinct manner to indicate user action — the camera may be watching proactively . ) their origin ( e.g. , in a different part of the display , or In one particular arrangement , visual “ baubles ” ( FIG . O ) presented in a different color ) , or it may insert them alter are overlaid on the captured imagery . Tapping on any of the 20 nately with non - commercial search results , i.e. , at positions baubles pulls up a screen of information , such as a ranked two and four . The AdSense revenue collected by Google can list of links Unlike Google web search which ranks search again be shared with the user , or with the user\\'s carrier . results in an order based on aggregate user data , the camera In some embodiments , the cell phone ( or Google ) again application attempts a ranking customized to the user\\'s pings the servers of companies for whom links will be profile . If a Starbucks sign or logo is found in the frame , the 25 presented helping them track their physical world - based Starbucks link gets top position for this user . online visibility . The pings can include the location of the If signs for Starbucks , Nordstrom , and the Thai restaurant user , and an identification of the object that prompted the are all found , links would normally be presented in that ping . When alldiscountbooks - dot - net receives the ping , it order ( per the user\\'s preferences inferred from profile data ) . may check inventory and find it has a significant overstock However , the cell phone application may have a capitalistic 30 of Snowball . As in the example earlier given , it may offer an bent and be willing to promote a link by a position or two extra payment for some extra promotion ( e.g. , including ( although perhaps not to the top position ) if circumstances “ We have 732 copies cheap ! \" in the presented link ) . warrant . In the present case , the cell phone routinely IP In addition to offering an incentive for a more prominent packets to the web servers at', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='have 732 copies cheap ! \" in the presented link ) . warrant . In the present case , the cell phone routinely IP In addition to offering an incentive for a more prominent packets to the web servers at addresses associated with each search listing ( e.g. , higher in the list , or augmented with of the links , alerting them that an iPhone user had recog- 35 additional information ) , a company may also offer addi nized their corporate signage from a particular latitude / tional bandwidth to serve information to a customer . For longitude . ( Other user data may also be provided , if privacy example , a user may capture video imagery from an elec considerations and user permissions allow . ) The Thai res- tronic billboard , and want to download a copy to show to taurant server responds back in an instant offering to the friends . The user\\'s cell phone identifies the content as a next two customers 25 % off any one item ( the restaurant\\'s 40 popular clip of user generated content ( e.g. , by reference to point of sale system indicates only four tables are occupied an encoded watermark ) , and finds that the clip is available and no order is pending ; the cook is idle ) . The restaurant from several sites — the most popular of which is YouTube , server offers three cents if the phone will present the followed by MySpace . To induce the user to link to discount offer to the user in its presentation of search results , MySpace , MySpace may offer to upgrade the user\\'s baseline or five cents if it will also promote the link to second place 45 wireless service from 3 megabits per second to 10 megabits in the ranked list , or ten cents if it will do that and be the only per second , so the video will download in a third of the time . discount offer presented in the results list . ( Starbucks also This upgraded service can be only for the video download , responded with an incentive , but not as attractive ) . The cell or it can be longer . The link presented on the screen of the phone quickly accepts the restaurant\\'s offer , and payments user\\'s cell phone can be amended to highlight the availabil are quickly made either to the user ( e.g. , defraying the 50 ity of the faster service . ( Again , MySpace may make an monthly phone bill ) or more likely to the phone carrier ( e.g. , associated payment . ) AT & T ) . Links are presented to Starbucks , the Thai restau- Sometimes alleviating a bandwidth bottleneck requires rant , and Nordstrom , in that order , with the restaurant\\'s link opening a bandwidth throttle on a cell phone end of the noting the discount for the next two customers . wireless link Or the bandwidth service change must be Google\\'s AdWord technology has already been noted . It 55 requested , or authorized , by the cell phone . In such case decides , based on factors including anauction determined MySpace can tell the cell phone application to take needed payment , which ads to present as Sponsored Links adjacent steps for higher bandwidth service , and MySpace will rebate the results of a Google web search . Google has adapted this to the user ( or to the carrier , for benefit of the user\\'s account ) technology to present ads on third party web sites and blogs , the extra associated costs . based on the particular contents of those sites , terming the 60 In some arrangements , the quality of service ( e.g. , band service AdSense . width ) is managed by pipe manager 51. Instructions from In accordance with another aspect of the present technol- MySpace may request that the pipe manager start requesting ogy , the AdWord / AdSense technology is extended to visual augmented service quality , and setting up the expected high image search on cell phones . bandwidth session , even before the user selects the MySpace Consider a user located in a small bookstore who snaps a 65 link . picture of the Warren Buffet biography Snowball . The book In some scenarios , vendors may negotiate preferential is quickly recognized , but rather', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='located in a small bookstore who snaps a 65 link . picture of the Warren Buffet biography Snowball . The book In some scenarios , vendors may negotiate preferential is quickly recognized , but rather than presenting a corre- bandwidth for its content . MySpace may make a deal with', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='US 10,922,957 B2 \\n 37 38', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"AT & T , for example , that all MySpace content delivered to when the user approaches or enters the museum . ( Of equal AT & T phone subscribers be delivered at 10 megabits per importance is the role that CTR plays in monetizing the second even though most subscribers normally only experience and environment . ) receive 3 megabits per second service . The higher quality Consider a school group that enters a sculpture museum service may be highlighted to the user in the presented links . 5 having a garden with a collection of Rodin works . The From the foregoing it will be recognized that , in certain museum may provide content related to Rodin and his works embodiments , the information presented by a mobile phone on servers or infrastructure ( e.g. , router caches ) that serve in response to visual stimuli is a function both of ( 1 ) the the garden . Moreover , because the visitors comprise a pre user's preferences , and ( 2 ) third party competition for that established social group , the museum may expect some user's attention , probably based on the user's demographic 10 social connectivity . So the museum may enable sharing profile . Demographically identical users , but with different capabilities ( e.g. , ad hoc networking ) that might not other tastes in food , will likely be presented with different baubles , wise be used . If one student queries the museum's online or associated information , when viewing a street crowded content to learn more about a particular Rodin sculpture , the with restaurants . Users with identical tastes in food and other system may accompany delivery of the solicited information preference information — but differing in a demographic 15 with a prompt inviting the student to share this information factor ( e.g. , age , gender ) may likewise be presented with with others in the group . The museum server can suggest different baubles / information , because vendors , etc. , are particular “ friends ” of the student with whom such infor willing to pay differently for different demographic eyeballs . mation might be shared if such information is publicly Modeling of User Behavior . accessible from Facebook or other social networking data Aided by knowledge of a particular physical environment , 20 source . In addition to names of friends , such a social a specific place and time , and behavior profiles of expected networking data source can also provide device identifiers , users , simulation models of human computer interaction IP addresses , profile information , etc. , for the student's with the physical world can be based on tools and techniques friends which may be leveraged to assist the dissemination from fields as disperse as robotics , and audience measure- of educational material to others in the group . These other ment . An example of this might be the number of expected 25 students may find this particular information relevant , since mobile devices in a museum at a particular time ; the it was of interest to another in their group even if the particular sensors that such devices are likely to be using ; original student's name is not identified . If the original and what stimuli are expected to be captured by those student is identified with the conveyed information , then this sensors ( e.g. , where are they pointing the camera , what is the may heighten the information's interest to others in the microphone hearing , etc. ) . Additional information can 30 group . include assumptions about social relationships between ( Detection of a socially - linked group may be inferred users : Are they likely to share common interests ? Are they from review of the museum's network traffic . For example , within common social circles that are likely to share content , if a device sends packets of data to an and the to share experiences , or desire creating location - based expe- museum's network handles both ends of the communica riences such as wiki - maps ( c.f. , Barricelli , “ Map -\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"of data to an and the to share experiences , or desire creating location - based expe- museum's network handles both ends of the communica riences such as wiki - maps ( c.f. , Barricelli , “ Map - Based 35 tion_dispatch and delivery , then there's an association Wilds as Contextual and Cultural Mediators , ” MobileHCI , between two devices in the museum . If the devices are not 2009 ) ? ones that have historical patterns of network usage , e.g. , In addition , modeling can be based on generalized heu- employees , then the system can conclude that two visitors to ristics derived from observations at past events ( e.g. , how the museum are socially connected . If a web of such many people used their cell phone cameras to capture 40 communications is detected involving several unfamiliar imagery from the Portland Trailblazers ' scoreboard during a devices , then a social group of visitors can be discerned . The basketball game , etc. ) , to more evolved predictive models size of the group can be gauged by the number of different that are based on innate human behavior ( e.g. , people are participants in such network traffic . Demographic informa more likely to capture imagery from a scoreboard during tion about the group can be inferred from external addresses overtime than during a game's half - time ) . 45 with which data is exchanged ; middle schoolers may have a Such models can inform many aspects of the experience high incidence of MySpace traffic ; college students may for the users , in addition to the business entities involved in communicate with external addresses at a university provisioning and measuring the experience . domain ; senior citizens may demonstrate a different traffic These latter entities may consist of the traditional value profile . All such information can be employed in automati chain participants involved in event production , and the 50 cally adapting the information and services provided to the arrangements involved in measuring interaction and mon- visitors as well as providing useful information to the etizing it . Event planners , producers , artists on the creation museum's administration and marketing departments . )\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content='side and the associated rights societies ( ASCAP , Directors Consider other situations . One is halftime at the U.S. Guild of America , etc. ) on the royalties side . From a football Superbowl , featuring a headline performer ( e.g. , measurement perspective , both sampling - based techniques 55 Bruce Springsteen , or Prince ) . The show may cause hun from opt - in users and devices , and census - driven techniques dreds of fans to capture pictures or audio - video of the event . can be utilized . Metrics for more static environments may Another context with predictable public behavior is the end consist of Revenue Per Unit ( RPU ) created by digital traffic of an NBA championship basketball game . Fans may want created on the digital service provider network ( how much to memorialize the final buzzer excitement : the scoreboard , bandwidth is being consumed ) to more evolved models of 60 streamers and confetti dropping from the ceiling , etc. In such Click Through Rates ( CTR ) for particular sensor stimuli . cases , actions that can be taken to prepare , or optimize , For example , the Mona - Lisa painting in the Louvre is delivery of content or experience should be taken . Examples likely to have a much higher CTR than other paintings in the include rights clearance for associated content , rendering museum , informing matters such as priority for content virtual worlds and other synthesized content , throttling provisioning , e.g. , content related to the Mona Lisa should 65 down routine time - insensitive network traffic , queuing com be cached and be as close to the edge of the cloud as mercial resources that may be invoked as people purchase possible , if not pre - loaded onto the mobile device itself souvenir books / music from Amazon ( caching pages , authen', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"10 \\n 15 US 10,922,957 B2 \\n 39 40 \\n ticating users to financial sites ) , propagating links for post- This may be accomplished on the device as well , through game interviews ( some prebuilt / edited and ready to go ) , the use of content rules , such as the Movielabs Content caching the Twitter feeds of the star players , buffering video Recognition Rules related to conflicting media content ( c.f. , from city center showing the hometown crowds watching on www.movielabs - dot - com / CRR ) , parental controls provided a Jumbotron display - erupting with joy at the buzzer , etc .; 5 by carriers to the device , or by adhering to DMCA Auto anything relating to the experience or follow - on actions matic Take Down Notices . should prepped / cached in advance , where possible . Under various rights management paradigms , licenses \\n Stimuli for sensors ( audio , visual , tactile , odor , etc. ) that play a key role in determining how content can be con are most likely to instigate user action and attention are sumed , shared , modified etc. A result of extracting semantic much more valuable from a commercial standpoint than meaning from stimulus presented to the user ( and the user's stimuli less likely to instigate such action ( similar to the mobile device ) , and / or the location in which stimulus is economic principles on which Google's Adwords ad - serving presented , can be issuance of a license to desired content or system is based ) . Such factors and metrics directly influence experiences ( games , etc. ) by third parties . To illustrate , advertising models through auction models well understood consider a user at a rock concert in an arena . The user may by those in the art . be granted a temporary license to peruse and listen to all Multiple delivery mechanisms exist for advertising deliv- music tracks by the performing artist ( and / or others ) on ery by third parties , leveraging known protocols such as iTunes — beyond the 30 second preview rights normally VAST . VAST ( Digital Video Ad Serving Template ) is a granted to the public . However , such license may only standard issued by the Interactive Advertising Bureau that 20 persist during the concert , or only from when the doors open establishes reference communication protocols between until the headline act begins its performance , or only while scriptable video rendering systems and ad servers , as well as the user is in the arena , etc. Thereafter , such license ends . associated XML schemas . As an example , VAST helps Similarly , passengers disembarking from an international standardize the service of video ads to independent web sites flight may be granted location - based or time - limited licenses ( replacing old - style banner ads ) , commonly based on a bit of 25 to translation services or navigation services ( e.g. , an aug Javascript included in the web page code code that also mented reality system overlaying directions for baggage aids in tracking traffic and managing cookies . VAST can also claim , bathrooms , etc. , on camera - captured scenes ) for their insert promotional messages in the pre - roll and post - roll mobile devices , while they transit through customs , are in viewing of other video content delivered by the web site . the airport , for 90 minutes after their arrival , etc. The web site owner doesn't concern itself with selling or 30 Such arrangements can serve as metaphors for experi running the advertisements , yet at the end of the month the ence , and as filtering mechanisms . One embodiment in web site owner receives payment based on audience view which sharing of experiences are triggered by sensor stimuli ership / impressions . In similar fashion , physical stimuli pre sented to users in the real world , sensed by mobile technol is through broadcast social networks ( e.g. , Twitter ) and ogy , can be the basis for payments to the parties involved . 35 syndication protocols ( e.g. , RSS web feeds / channels ) . Other\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 82}), Document(page_content=\"Dynamic environments in which stimulus presented to users , entities or devices can subscribe to such broadcasts / \\n users and their mobile devices can be controlled ( such as feeds as the basis for subsequent communication ( social , \\n video displays , as contrasted with static posters ) provide information retrieval , etc. ) , as logging of activities ( e.g. , a new opportunities for measurement and utilization of met- person's daily journal ) , or measurement ( audience , etc. ) . \\n rics such as CTR . 40 Traffic associated with such networks / feeds can also be Background music , content on digital displays , illumina- measured by devices at a particular location allowing users tion , etc. , can be modified to maximize CTR and shape to traverse in time to understand who was communicating traffic . For example , illumination on particular signage can what at a particular point in time . This enables searching for be increased , or flash , as a targeted individual passes by . and mining additional information , e.g. , was my friend here Similarly when a flight from Japan lands at the airport , 45 last week ? Was someone from my peer group here ? What digital signage , music , etc. can all be modified overtly content was consumed ? Such traffic also enables real - time ( change in the advertising to the interests of the expected monitoring of how users share experiences . Monitoring audience ) or covertly ( changing the linked experience to “ tweets ” about a performer's song selection during a concert take the user to the Japanese language website ) , to maximize may cause the performer to alter the songs to be played for the CTR . 50 the remainder of a concert . The same is true for brand Mechanisms may be introduced as well to contend with management . For example , if users share their opinions rogue or un - approved sensor stimuli . Within the confined about a car during a car show , live keyword filtering on the spaces of a university of business park , stimuli ( posters , traffic can allow the brand owner to re - position certain music , digital signage , etc. ) that don't adhere to the inten- products for maximum effect ( e.g. , the new model of Cor tions or policies of the property owner- or the entity respon- 55 vette should spend more time on the spinning platform , etc. ) . sible for a domain may need to be managed . This can be More on Optimization accomplished through the use of simple blocking mecha- Predicting the user's action or intent is one form of nisms that are geography - specific ( not dissimilar to region optimization . Another form involves configuring the pro coding on DVD's ) , indicating that all attempts within spe- cessing so as to improve performance . cific GPS coordinates to route a keyvector to a specific place 60 To illustrate one particular arrangement , consider again in the cloud must be mediated by a routing service or the Common Services Sorter of FIG . 6. What keyvector gateway managed by the domain owner . operations should be performed locally , or remotely , or as a Other options include filtering the resultant experience . Is hybrid of some sort ? In what order should keyvector opera it age appropriate ? Does it run counter to pre - existing tions be performed ? Etc. The mix of expected operations , advertising or branding arrangements , such as a Coca Cola 65 and their scheduling , should be arranged in an appropriate advertisement being delivered to a user inside the Pepsi fashion for the processing architecture being used , the center during a Denver Nuggets game . circumstances , and the context .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 82}), Document(page_content='10 \\n 15 US 10,922,957 B2 \\n 41 42 \\n One step in the process is to determine which operations service providers for the duration of the concert . In this case , need to occur . This determination can be based on express services normally routed for external execution should be requests from the user , historical patterns of usage , context performed locally . \\n and status , etc. Yet another factor is the particular hardware with which Many operations are high level functions , which involve 5 the cell phone is equipped . If a dedicated FFT processor is a number of component operations performed in a par- available in the phone , then performing intensive FFT ticular order . For example , optical character recognition may operations locally makes sense . If only a feeble general require edge detection , followed by region - of - interest seg- purpose CPU is available , then an intensive FFT operation mentation , followed by template pattern matching . Facial is probably best referred out for external execution . recognition may involve skintone detection , Hough trans- A related factor is current hardware utilization . Even if a forms ( to identify oval - shaped areas ) , identification of fea- cell phone is equipped with hardware that is well configured ture locations ( pupils , corners of mouth , nose ) , eigenface for a certain task , it may be so busy and backlogged that the calculation , and template matching . system may refer a next task of this sort to an external The system can identify the component operations that resource for completion . may need to be performed , and the order in which their Another factor may be the length of the local processing respective results are required . Rules and heuristics can be chain , and the risk of a stall . Pipelined processing architec applied to help determine whether these operations should tures may become stalled for intervals as they wait for data be performed locally or remotely . needed to complete an operation . Such a stall can cause all For example , at one extreme , the rules may specify that 20 other subsequent operations to be similarly delayed . The risk simple operations , such as color histograms and threshold- of a possible stall can be assessed ( e.g. , by historical ing , should generally be performed locally . At the other patterns , or knowledge that completion of an operation extreme , complex operations may usually default to outside requires further data whose timely availability is not providers . assured — such as a result from another external process ) Scheduling can be determined based on which operations 25 and , if the risk is great enough , the operation may be referred are preconditions to other operations . This can also influence for external processing to avoid stalling the local processing chain . whether an operation is performed locally or remotely ( local performance may provide quicker results — allowing subse Yet another factor is connectivity status . Is a reliable , high \\n quent operations to be started with less delay ) . The rules may speed network connection established ? Or are packets seek to identify the operation whose output ( s ) is used by the 30 dropped , or network speed slow ( or wholly unavailable ) ? greatest number of subsequent operations , and perform this Geographical considerations of different sorts can also be', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content='operation first ( its respective precedent ( s ) permitting ) . factors . One is network proximity to the service provider . Another is whether the cell phone has unlimited access to the Operations that are preconditions to successively fewer network ( as in a home region ) , or a pay - per - use arrangement other operations are performed successively later . The 35 ( as when roaming in another country ) . operations , and their sequence , may be conceived as a tree Information about the remote service provider ( s ) can also structure — with the most globally important performed first , be factored . Is the service provider offering immediate and operations of lesser relevance to other operations per turnaround , or are requested operations placed in a long formed later . queue , behind other users awaiting service ? Once the pro Such determinations , however , may also be tempered ( or 40 vider is ready to process the task , what speed of execution dominated ) by other factors . One is power . If the cell phone is expected ? Costs may also be key factors , together with battery is low , or if an operation will involve a significant other attributes of importance to the user ( e.g. , whether the drain on a low capacity battery , this can tip the balance in service provider meets “ green ” standards of environmental favor of having the operation performed remotely . responsibility ) . A great many other factors can also be Another factor is response time . In some instances , the 45 considered , as may be appropriate in particular contexts . limited processing capability of the cell phone may mean Sources for such data can include the various elements that processing locally is slower than processing remotely shown in the illustrative block diagrams , as well as external ( e.g. , where a more robust , parallel , architecture might be available to perform the operation ) . In other instances , the A conceptual illustration of the foregoing is provided in delays of establishing communication with a remote server , 50 FIG . 19B . and establishing a session , may make local performance of Based on the various factors , a determination can be made an operation quicker . Depending on user demand , and needs as to whether an operation should be performed locally , or of other operation ( s ) , the speed with which results are remotely . ( The same factors may be assessed in determining returned may be important , or not . the order in which operations should be performed . ) Still another factor is user preferences . As noted else- 55 In some embodiments , the different factors can be quan where , the user may set parameters influencing where , and tified by scores , which can be combined in polynomial when , operations are performed . For example , a user may fashion to yield an overall score , indicating how an opera specify that an operation may be referred to remote process- tion should be handled . Such an overall score serves as a ing by a domestic service provider , but if none is available , metric indicating the relative suitability of the operation for then the operation should be performed locally . 60 remote or external processing . ( A similar scoring approach Routing constraints are another factor . Sometimes the cell can be employed to choose between different service pro phone will be in a WiFi or other service area ( e.g. , in a viders . ) concert arena ) in which the local network provider places Depending on changing circumstances , a given operation limits or conditions on remote service requests that may be may be performed locally at one instant , and performed accessed through that network . In a concert where photog- 65 remotely at a later instant ( or vice versa ) . Or , the same raphy is forbidden , for example , the local network may be operation may be performed on two sets of keyvector data configured to block access to external image processing at the same time one locally , and one remotely .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content='example , the local network may be operation may be performed on two sets of keyvector data configured to block access to external image processing at the same time one locally , and one remotely . resources .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content=\"5 \\n 20 US 10,922,957 B2 \\n 43 44 \\n While described in the context of determining whether an opportunity cost , given the current state of the device , operation should be performed locally or remotely , the same e.g. , what other processes should take priority such factors can influence other matters as well . For example , as a voice call , GPS navigation , etc. they can also be used in deciding what information is user preferences , i.e. , I want a “ green ” provider , or open conveyed by keyvectors . source provider Consider a circumstance in which the cell phone is to legal uncertainties ( e.g. , certain providers may be at perform OCR on captured imagery . With one set of factors , greater risk of patent infringement charges , e.g. , due unprocessed pixel data from a captured image may be sent to their use of an allegedly patented method ) to a remote service provider to make this determination . Domain owner influence : Under a different set of factors , the cell phone may perform 10 privacy concerns in specific physical arenas such as no initial processing , such as edge detection , and then package face recognition at schools the edge - detected data in keyvector form , and route to an pre - determined content based rules prohibiting specific external provider to complete the OCR operation . Under still operations against specific stimuli another set of factors , the cell phone may perform all of the voiceprint matching against broadcast songs highlight component OCR operations up until the last ( template 15 ing the use other singers ( Milli - Vanilli's Grammy matching ) , and send out data only for this last operation . award was revoked when officials discovered that the ( Under yet another set of factors , the OCR operation may be actual vocals on the subject recording were per completed wholly by the cell phone , or different components formed by other singers ) of operation can be performed alternately by the cell phone All of the above influence scheduling and ability to and remote service provider ( s ) , etc. ) perform out of order execution of keyvectors based on Reference was made to routing constraints as one possible the optimal path to the desired outcome factor . This is a particular example of a more general uncertainty in a long chain of operations , making \\n factor - external business rules . Consider the earlier prediction of need for subsequent keyvector opera example of a user who is attending an event at the Pepsi tions difficult ( akin to the deep pipeline in processors Center in Denver . The Pepsi Center may provide wireless 25 & branch prediction ) _difficulties might be due to communication services to patrons , through its own WiFi or weak metrics on keyvectors other network . Naturally , the Pepsi Center is reluctant for its past behavior . network resources to be used for the benefit of competitors , location ( GPS indicates that the device is quick such as Coca Cola . The host network may thus influence motion ) & pattern of GPS movements cloud services that can be utilized by its patrons ( e.g. , by 30 is there a pattern of exposure to stimuli , such as a making some inaccessible , or by giving lower priority to user walking through an airport terminal being\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 84}), Document(page_content=\"data traffic of certain types , or with certain destinations ) . The repeatedly exposed to CNN that is being pre domain owner may exe control over what operations a sented at each gate mobile device is capable of performing . This control can proximity sensors indicating the device was placed influence the local / remote decision , as well as the type of 35 in a pocket , etc. data conveyed in keyvector packets . other approaches such as Least Recently Used Another example is a gym , which may want to impede ( LRU ) can be used to track how infrequent the usage of cell phone cameras , e.g. , by interfering with access desired keyvector operation resulted or contrib to remote service providers for imagery , as well as photo uted to the desired effect ( recognition of a song , sharing sites such as Flickr and Picasa . Still another example 40 etc. ) is a school which , for privacy reasons , may want to discour- Further regarding pipelined or other time - consuming age facial recognition of its students and staff . In such case , operations , a particular embodiment may undertake some access to facial recognition service providers can be suitability testing before engaging a processing resource for blocked , or granted only on a moderated case - by - case basis . what may be more than a threshold number of clock cycles . Venues may find it difficult to stop individuals from using 45 A simple suitability test is to make sure the image data is cell phone cameras- or using them for particular purposes , potentially useful for the intended purpose , as contrasted but they can take various actions to impede such use ( e.g. , with data that can be quickly disqualified from analysis . For by denying services that would promote or facilitate such example , whether it is all black ( e.g. , a frame captured in the use ) . user's pocket ) . Adequate focus can also be checked quickly The following outline identifies other factors that may be 50 before committing to an extended operation . relevant in determining which operations are performed ( The artisan will recognize that certain of the aspects of where , and in what sequence : this technology discussed above have antecedents visible in 1. Scheduling optimization of keyvector processing units hindsight . For example , considerable work has been put into \\n based on numerous factors : instruction optimization for pipelined processors . Also , Operation mix , which operations consist of similar atomic 55 some devices have allowed user configuration of power instructions ( MicroOps , Pentium II etc. ) settings , e.g. , user - selectable deactivation of a power - hungry Stall states , which operations will generate stalls due to : GPU in certain Apple notebooks to extend battery life . ) waiting for external keyvector processing The above - discussed determination of an appropriate poor connectivity instruction mix ( e.g. , by the Common Services Sorter of user input 60 FIG . 6 ) particularly considered certain issues arising in change in user focus pipelined architectures . Different principles can apply in Cost of operation based on : embodiments in which one or more GPUs is available . published cost These devices typically have hundreds or thousands of expected cost based on state of auction scalar processors that are adapted for parallel execution , so state of battery and power mode 65 that costs of execution ( time , stall risk , etc. ) are small . power profile of the operation ( is it expensive ? ) Branch prediction can be handled by not predicting : instead , past history of power consumption the GPU processes for all of the potential outcomes of a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 84}), Document(page_content='US 10,922,957 B2 \\n 45 46', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content=\"branch in parallel , and the system uses whatever output packets . This intensity data can be applied to an output 33 of corresponds to the actual branch condition when it becomes that stage . With the image data , each packet can convey a known . timestamp indicating the particular time ( absolute , or based To illustrate , consider facial recognition . A GPU- on a local clock ) at which the image data was captured . This equipped cell phone may invoke instructions — when its 5 time data , too , can be provided on output 33 . camera is activated in a user photo - shoot mode — that con- A synchronization processor 35 coupled to such an output figure 20 clusters of scalar processors in the GPU . ( Such a 33 can examine the variation in frame - to - frame intensity ( or cluster is sometimes termed a “ stream processor . ” ) In par- color ) , as a function of timestamp data , to discern its ticular , each cluster is configured to perform a Hough periodicity . Moreover , this module can predict the next time transform on a small tile from a captured image frame 10 instant at which the intensity ( or color ) will have a maxima , looking for one or more oval shapes that may be candidate minima , or other particular state . A phase - locked loop may faces . The GPU thus processes the entire frame in parallel , control an oscillator that is synced to mirror the periodicity by 20 concurrent Hough transforms . ( Many of the stream of an aspect of the illumination . More typically , a digital processors probably found nothing , but the process speed filter computes a time interval that is used to set or compare wasn't impaired . ) 15 against timers optionally with software interrupts . A digi When these GPU Hough transform operations complete , tal phased - locked loop or delay - locked loop can also be the GPU may be reconfigured into a lesser number of stream used . ( A Kalman filter is commonly used for this type of processors one dedicated to analyzing each candidate oval phase locking . ) shape , to determine positions of eye pupils , nose location , Control processor module 36 can poll the synchronization and distance across the mouth . For any oval that yielded 20 module 35 to determine when a lighting condition is useful candidate facial information , associated parameters expected to have a desired state . With this information , would be packaged in keyvector form , and transmitted to a control processor module 36 can direct setup module 34 to cloud service that checks the keyvectors of analyzed facial capture a frame of data under favorable lighting conditions parameters against known templates , e.g. , of the user's for a particular purpose . For example , if the camera is Facebook friends . ( Or , such checking could also be per- 25 imaging an object suspected of having a digital watermark formed by the GPU , or by another processor in the cell encoded in a green color channel , processor 36 may direct phone . ) camera 32 to capture a frame of imagery at an instant that ( It is interesting to note that this facial recognition — like green illumination is expected to be at a maximum , and others detailed in this specification — distills the volume of direct processing stages 38 to process that frame for detec data , e.g. , from millions of pixels ( bytes ) in the originally 30 tion of such a watermark . captured image , to a keyvector that may comprise a few tens , The camera phone may be equipped with plural LED light hundreds , or thousands of bytes . This smaller parcel of sources that are usually operated in tandem to produce a information , with its denser information content , is more flash of white light illumination on a subject . Operated quickly routed for processing sometimes externally . Com- individually or in different combinations , however , they can munication of the distilled keyvector information takes place 35 cast different colors of light on the subject . The phone over a channel with a corresponding\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content='combinations , however , they can munication of the distilled keyvector information takes place 35 cast different colors of light on the subject . The phone over a channel with a corresponding bandwidth capability- processor may control the component LED sources indi keeping costs reasonable and implementation practical . ) vidually , to capture frames with non - white illumination . If Contrast the just - described GPU implementation of face capturing an image that is to be read to decode a green detection to such an operation as it might be implemented on channel watermark , only green illumination may be applied a scalar processor . Performing Hough - transform - based oval 40 when the frame is captured . Or a camera may capture plural detection across the entire image frame is prohibitive in successive frames — with different LEDs illuminating the terms of processing time much of the effort would be for subject . One frame may be captured at a 1 / 250th second naught , and would delay other tasks assigned to the proces- exposure with a corresponding period of red - only illumina sor . Instead , such an implementation would typically have tion ; a subsequent frame may be captured at a Viooth second the processor examine pixels as they come from the cam- 45 exposure with a corresponding period of green - only illumi era looking for those having color within an expected nation , etc. These frames may be analyzed separately , or \" skintone ” range . Only if a region of skintone pixels is may be combined , e.g. , for analysis in the aggregate . Or a identified would a Hough transform then be attempted on single frame of imagery may be captured over an interval of that excerpt of the image data . In similar fashion , attempting 1 / 100th of a second , with the green LED activated for that to extract facial parameters from detected ovals would be 50 entire interval , and the red LED activated for 1 / 250th of a done in a laborious serial fashion - often yielding no useful second during that 1 / 100th second interval . The instantaneous result . ambient illumination can be sensed ( or predicted , as above ) , Ambient Light and the component LED colored light sources can be Many artificial light sources do not provide a consistent operated in a responsive manner ( e.g. , to counteract orange illumination . Most exhibit a temporal variation in intensity 55 ness of tungsten illumination by adding blue illumination ( luminance ) and / or color . These variations commonly track from a blue LED ) . the AC power frequency ( 50/60 or 100/120 Hz ) , but some- Other Notes ; Projectors times do not . For example , fluorescent tubes can give off While a packet - based , data driven architecture is shown in infrared illumination that varies at a -40 KHz rate . The FIG . 16 , a variety of other implementations are of course emitted spectra depend on the particular lighting technology . 60 possible . Such alternative architectures are straightforward Organic LEDs for domestic and industrial lighting some- to the artisan , based on the details given . times can use distinct color mixtures ( e.g. , blue and amber ) The artisan will appreciate that the arrangements and to make white . Others employ more traditional red / green / details noted above are arbitrary . Actual choices of arrange blue clusters , or blue / UV LEDs with phosphors . ment and detail will depend on the particular application In one particular implementation , a processing stage 38 65 being served , and most likely will be different than those monitors , e.g. , the average intensity , redness , greenness or noted . ( To cite but a trivial example , FFTs need not be other coloration of the image data contained in the bodies of performed on 16x16 blocks , but can be done on 64x64 ,', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content=\"US 10,922,957 B2 \\n 47 48 \\n 256x256 , the whole image , etc. ) Similarly , it will be recog- CKing ( the N70 model , distributed by China Vision ) and nized that the body of a packet can convey an entire frame Samsung ( the MPB200 ) . LG and others have shown proto of data , or just excerpts ( e.g. , a 128x128 block ) . Image data types . ( These projectors are understood to use Texas Instru from a single captured frame may thus span a series of ments electronically - steerable digital micro - mirror arrays , in several packets . Different excerpts within a common frame 5 conjunction with LED or laser illumination . ) Microvision may be processed differently , depending on the packet with offers the PicoP Display Engine , which can be integrated which they are conveyed . into a variety of devices to yield projector capability , using Moreover , a processing stage 38 may be instructed to a micro - electro - mechanical scanning mirror ( in conjunction break a packet into multiple packets — such as by splitting with laser sources and an optical combiner ) . Other suitable image data into 16 tiled smaller sub - images . Thus , more 10 projection technologies include 3M's liquid crystal on sili packets may be present at the end of the system than were produced at the beginning . con ( LCOS ) and Displaytech's ferroelectric LCOS systems . \\n In like fashion , a single packet may contain a collection Use of two projectors , or two cameras , gives differentials \\n of data from a series of different images ( e.g. , images taken of projection or viewing , providing additional information \\n sequentially with different focus , aperture , or shutter set- 15 about the subject . In addition to stereo features , it also \\n tings ; a particular example is a set of focus regions from five enables regional image correction . For example , consider\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content=\"images taken with focus bracketing , or depth of field brack two cameras imaging a digitally watermarked object . One eting overlapping , abutting , or disjoint . ) This set of data camera's view of the object gives one measure of a trans may then be processed by later stages either as a set , or form that can be discerned from the object's surface ( e.g. , by through a process that selects one or more excerpts of the 20 encoded calibration signals ) . This information can be used to packet payload that meet specified criteria ( e.g. , a focus correct a view of the object by the other camera . And vice sharpness metric ) . versa . The two cameras can iterate , yielding a comprehen In the particular example detailed , each processing stage sive characterization of the object surface . ( One camera may 38 generally substituted the result of its processing for the view a better - illuminated region of the surface , or see some data originally received in the body of the packet . In other 25 edges that the other camera can't see . One view may thus arrangements this need not be the case . For example , a stage reveal information that the other does not . ) may output a result of its processing to a module outside the If a reference pattern ( e.g. , a grid ) is projected onto a depicted processing chain , e.g. , on an output 33. ( Or , as surface , the shape of the surface is revealed by distortions of noted , a stage may maintain — in the body of the output the pattern . The FIG . 16 architecture can be expanded to packet — the data originally received , and augment it with 30 include a projector , which projects a pattern onto an object , further data — such as the result ( s ) of its processing . ) for capture by the camera system . ( Operation of the projec Reference was made to determining focus by reference to tor can be synchronized with operation of the camera , e.g. , DCT frequency spectra , or edge detected data . Many con- by control processor module 36 — with the projector acti sumer cameras perform a simpler form of focus check- vated only as necessary , since it imposes a significant battery simply by determining the intensity difference ( contrast ) 35 drain . ) Processing of the resulting image by modules 38 between pairs of adjacent pixels . This difference peaks with ( local or remote ) provides information about the surface correct focus . Such an arrangement can naturally be used in topology of the object . This 3D topology information can be the detailed arrangements . ( Again , advantages can accrue used as a clue in identifying the object . from performing such processing on the sensor chip . ) In addition to providing information about the 3D con Each stage typically conducts a handshaking exchange 40 figuration of an object , shape information allows a surface to with an adjoining stage each time data is passed to or be virtually re - mapped to any other configuration , e.g. , flat . received from the adjoining stage . Such handshaking is Such remapping serves as a sort of normalization operation . routine to the artisan familiar with digital system design , so In one particular arrangement , system 30 operates a is not belabored here . projector to project a reference pattern into the camera's The detailed arrangements contemplated a single image 45 field of view . While the pattern is being projected , the sensor . However , in other embodiments , multiple image camera captures a frame of image data . The resulting image sensors can be used . In addition to enabling conventional is processed to detect the reference pattern , and therefrom stereoscopic processing , two or more image sensors enable characterize the 3D shape of an imaged object . Subsequent or enhance many other operations . processing then follows , based on the 3D shape data . One function that benefits from multiple cameras is 50 ( In connection with such arrangements , the reader is distinguishing objects . To cite a simple example , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content=\", based on the 3D shape data . One function that benefits from multiple cameras is 50 ( In connection with such arrangements , the reader is distinguishing objects . To cite a simple example , a single referred to the Google book - scanning patent , U.S. Pat . No. camera is unable to distinguish a human face from a picture 7,508,978 , which employs related principles . That patent of a face ( e.g. , as may be found in a magazine , on a billboard , details a particularly useful reference pattern , among other or on an electronic display screen ) . With spaced - apart sen- relevant disclosures . ) sors , in contrast , the 3D aspect of the picture can readily be 55 If the projector uses collimated laser illumination ( such as discerned , allowing a picture to be distinguished from a the PicoP Display Engine ) , the pattern will be in focus person . ( Depending on the implementation , it may be the 3D regardless of distance to the object onto which the pattern is aspect of the person that is actually discerned . ) projected . This can be used as an aid to adjust focus of a cell Another function that benefits from multiple cameras is phone camera onto an arbitrary subject . Because the pro refinement of geolocation . From differences between two 60 jected pattern is known in advance by the camera , the images , a processor can determine the device's distance captured image data can be processed to optimize detection from landmarks whose location may be precisely known . of the pattern such as by correlation . ( Or the pattern can be This allows refinement of other geolocation data available to selected to facilitate detection such as a checkerboard that the device ( e.g. , by WiFi node identification , GPS , etc. ) appears strongly at a single frequency in the image fre Just as a cell phone may have one , two ( or more ) sensors , 65 quency domain when properly focused . ) Once the camera is such a device may also have one , two ( or more ) projectors . adjusted for optimum focus of the known , collimated pat Individual projectors are being deployed in cell phones by tern , the projected pattern can be discontinued , and the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content='15 US 10,922,957 B2 \\n 49 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content='camera can then capture a properly focused image of the projected from different points differ when presented on an underlying subject onto which the pattern was projected . object and viewed by the camera , stereoscopic information Synchronous detection can also be employed . The pattern can again be discerned . may be projected during capture of one frame , and then off Many usage models are enabled through use a projector , for capture of the next . The two frames can then be sub- 5 including new sharing models ( c.f. , Greaves , “ View & tracted . The common imagery in the two frames generally Share : Exploring Co - Present Viewing and Sharing of Pic cancels leaving the projected pattern at a much higher tures using Personal Projection , ” Mobile Interaction with the signal to noise ratio . Real World 2009 ) . Such models employ the image created A projected pattern can be used to determine correct focus by the projector itself as a trigger to initiate a sharing for several subjects in the camera\\'s field of view . A child 10 session , either overtly through a commonly understood may pose in front of the Grand Canyon . The laser - projected symbol ( \" open ” sign ) , to covert triggers that are machine pattern allows the camera to focus on the child in a first readable . Sharing can also occur through ad hoc networks frame , and on the background in a second frame . These utilizing peer to peer applications , or a server hosted appli frames can then be composited taking from each the cation . portion properly in focus . Other output from mobile devices can be similarly shared . If a lens arrangement is used in the cell phone\\'s projector Consider keyvectors . One user\\'s phone may process an system , it can also be used for the cell phone\\'s camera image with Hough transform and other eigenface extraction system . A mirror can be controllably moved to steer the techniques , and then share the resulting keyvector of eigen camera or the projector to the lens . Or a beam - splitter face data with others in the user\\'s social circle ( either by arrangement 80 can be used ( FIG . 20 ) . Here the body of a 20 pushing same to them , or allowing them to pull it ) . One or cell phone 81 incorporates a lens 82 , which provides a light more of these socially - affiliated devices may then perform to a beam - splitter 84. Part of the illumination is routed to the facial template matching that yields an identification of a camera sensor 12. The other part of the optical path goes to formerly - unrecognized face in the imagery captured by the a micro - mirror projector system 86 . original user . Such arrangement takes a personal experience , Lenses used in cell phone projectors typically are larger 25 and makes it a public experience . Moreover , the experience aperture than those used for cell phone cameras , so the can become a viral experience , with the keyvector data camera may gain significant performance advantages ( e.g. , shared — essentially without bounds — to a great number of enabling shorter exposures ) by use of such a shared lens . Or , further users . reciprocally , the beam splitter 84 can be asymmetrical — not Selected Other Arrangements equally favoring both optical paths . For example , the beam- 30 In addition to the arrangements earlier detailed , another splitter can be a partially - silvered element that couples a hardware arrangement suitable for use with certain imple smaller fraction ( e.g. , 2 % , 8 % , or 25 % ) of externally inci- mentations of the present technology uses the Mali - 400 dent light the sensor path 83. The am - splitter may thus ARM graphics multiprocessor architecture , which includes serve to couple a larger fraction ( e.g. , 98 % , 92 % , or 75 % ) of plural fragment processors that can be devoted to the dif illumination from the micro - mirror projector externally , for 35 ferent types of image processing tasks referenced in this projection . By this arrangement the camera sensor 12', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content=\"devoted to the dif illumination from the micro - mirror projector externally , for 35 ferent types of image processing tasks referenced in this projection . By this arrangement the camera sensor 12 document . receives light of a conventional — for a cell phone camera- The standards group Khronos has issued OpenGL ES2.0 , intensity ( notwithstanding the larger aperture lens ) , while which defines hundreds of standardized graphics function the light output from the projector is only slightly dimmed calls for systems that include multiple CPUs and multiple by the lens sharing arrangement . 40 GPUs ( a direction in which cell phones are increasingly In another arrangement , a camera head is separate migrating ) . OpenGL ES2.0 attends to routing of different detachable — from the cell phone body . The cell phone body operations to different of the processing units with such is carried in a user's pocket or purse , while the camera head details being transparent to the application software . It thus is adapted for looking out over a user's pocket ( e.g. , in a provides a consistent software API usable with all manner of form factor akin to a pen , with a pocket clip , and with a 45 GPU / CPU hardware . battery in the pen barrel ) . The two communicate by Blu- In accordance with another aspect of the present technol etooth or other wireless arrangement , with capture instruc- ogy , OpenGL ES2 . standard is extended to provide a stan tions sent from the phone body , and image data sent from the dardized graphics processing library not just across different camera head . Such configuration allows the camera to CPU / GPU hardware , but also across different cloud pro constantly survey the scene in front of the user — without 50 cessing hardware again with such details being transparent requiring that the cell phone be removed from the user's to the calling software . Increasingly , Java service requests pocket / purse . ( JSRs ) have been defined to standardize certain Java - imple In a related arrangement , a strobe light for the camera is mented tasks . JSRs increasingly are designed for efficient separate or detachable — from the cell phone body . The implementations on top of OpenGL ES2.0 class hardware . light ( which may incorporate LEDs ) can be placed near the 55 In accordance with a still further aspect of the present image subject , providing illumination from a desired angle technology , some or all of the image processing operations and distance . The strobe can be fired by a wireless command noted in this specification ( facial recognition , SIFT process issued by the cell phone camera system . ing , watermark detection , histogram processing , etc. ) can be ( Those skilled in optical system design will recognize a implemented as JSRs providing standardized implemen number of alternatives to the arrangements particularly 60 tations that are suitable across diverse hardware platforms . noted . ) In addition to supporting cloud - based JSRs , the extended Some of the advantages that accrue from having two standards specification can also support the Query Router cameras can be realized by having two projectors ( with a and Response Manager functionality detailed earlier - in single camera ) . For example , the two projectors can project cluding both static and auction - based service providers . alternating or otherwise distinguishable patterns ( e.g. , simul- 65 Akin to OpenGL is OpenCV — a computer vision library taneous , but of differing color , pattern , polarization , etc ) into available under an open source license , permitting coders to the camera's field of view . By noting how the two patterns invoke a variety of functions — without regard to the par or\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content=\"US 10,922,957 B2 \\n 51 52 \\n ticular hardware that is being utilized to perform same . ( An based on “ costs , ” including bandwidth costs , external ser O'Reilly book , Learning OpenCV , documents the language vice provider costs , power costs to the cell phone battery , extensively . ) A counterpart , NokiaCV , provides similar func- intangible costs in consumer ( dis- ) satisfaction by delaying tionality specialized for the Symbian operating system ( e.g. , processing , etc. For example , if the user is running low on Nokia cell phones ) . 5 battery power , and is at a location far from a cell tower ( so OpenCV provides support for a large variety of opera- that the cell phone runs its RF amplifier at maximum output tions , including high level tasks such as facial recognition , when transmitting ) , then sending a large block of data for gesture recognition , motion tracking / understanding , seg- remote processing may consume a significant fraction of the mentation , etc. , as well as an extensive assortment of more battery's remaining life . In such case , the phone may decide atomic , elemental vision / image processing operations . 10 to process the data locally , or to forward it for remote CMVision is another package of computer vision tools processing when the phone is closer to the cell site or the that can be employed in certain embodiments of the present battery has been recharged . A set of stored rules can be technology — this package compiled by researchers at Car- applied to the relevant variables to establish a net “ cost negie Mellon University . function ” for different approaches ( e.g. , process locally , Still another hardware architecture makes use of a field 15 process remotely , defer processing ) , and these rules may programmable object array ( FPOA ) arrangement , in which indicate different outcomes depending on the states of these hundreds of diverse 16 - bit “ objects ” are arrayed in a gridded variables . node fashion , with each being able to exchange data with An appealing “ cloud ” resource is the processing capabil neighboring devices through very high bandwidth channels . ity found at the edges of wireless networks . Cellular net ( The PicoChip devices referenced earlier are of this class . ) 20 works , for example , include tower stations that are , in large The functionality of each can be reprogrammed , as with part , software - defined radios - employing processors to per FPGAs . Again different of the image processing tasks can be form— digitally — some or all of the operations traditionally performed by different of the FPOA objects . These tasks can performed by analog transmitting and receiving radio cir be redefined on the fly , as needed ( e.g. , an object may cuits , such as mixers , filters , demodulators , etc. Even perform SIFT processing in one state ; FFT processing in 25 smaller cell stations , so - called “ femtocells , ” typically have another state ; log - polar processing in a further state , etc. ) . powerful signal processing hardware for such purposes . The ( While many grid arrangements of logic devices are based PicoChip processors noted earlier , and other field program on “ nearest neighbor ” interconnects , additional flexibility mable object arrays , are widely deployed in such applica can be achieved by use of a “ partial crossbar ” interconnect . tions . See , e.g. , U.S. Pat . No. 5,448,496 ( Quickturn Design Sys- 30 Radio signal processing , and image signal processing ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='tems ) . ) have many commonalities , e.g. , employing FFT processing Also in the realm of hardware , certain embodiments of the to convert sampled data to the frequency domain , applying present technology employ “ extended depth of field ” imag- various filtering operations , etc Cell station equipment , ing systems ( see , e.g. , U.S. Pat . Nos . 7,218,448 , 7,031,054 including processors , are designed to meet peak consumer and 5,748,371 ) . Such arrangements include a mask in the 35 demands . This means that significant processing capability imaging path that modifies the optical transfer function of is often left unused . the system so as to be insensitive to the distance between the In accordance with another aspect of the present technol object and the imaging system . The image quality is then ogy , this spare radio signal processing capability at cellular uniformly poor over the depth of field . Digital post process- tower stations ( and other edges of wireless networks ) is ing of the image compensates for the mask modifications , 40 repurposed in connection with image ( and / or audio or other ) restoring image quality , but retaining the increased depth of signal processing for consumer wireless devices . Since an field . Using such technology , the cell phone camera can FFT operation is the same — whether processing sampled capture imagery having both nearer and further subjects all radio signals or image pixels — the repurposing is often in focus ( i.e. , with greater high frequency detail ) , without straightforward : configuration data for the hardware pro requiring longer exposures — as would normally be required . 45 cessing cores needn\\'t be changed much , if at all . And ( Longer exposures exacerbate problems such as hand - jitter , because 3G / 4G networks are so fast , a processing task can and moving subjects . ) In the arrangements detailed here , be delegated quickly from a consumer device to a cell station shorter exposures allow higher quality imagery to be pro- processor , and the results returned with similar speed . In vided to image processing functions without enduring the addition to the speed and computational muscle that such temporal delay created by optical / mechanical focusing ele- 50 repurposing of cell station processors affords , another ben ments , or requiring input from the user as to which elements efit is reducing the power consumption of the consumer of the image should be in focus . This provides for a much devices . more intuitive experience , as the user can simply point the Before sending image data for processing , a cell phone imaging device at the desired target without worrying about can quickly inquire of the cell tower station with which it is focus or depth of field settings . Similarly , the image pro- 55 communicating to confirm that it has enough unused capac cessing functions are able to leverage all the pixels included ity sufficient to undertake the intended image processing in the image / frame captured , as all are expected to be operation . This query can be sent by the packager / router of in - focus . In addition , new metadata regarding identified FIG . 10 ; the local / remote router of FIG . 10A , the query objects or groupings of pixels related to depth within the router and response manager of FIG . 7 ; the pipe manager 51 frame can produce simple \" depth map ” information , setting 60 of FIG . 16 , etc. the stage for 3D video capture and storage of video streams Alerting the cell tower / base station of forthcoming pro using emerging standards on transmission of depth infor- cessing requests , and / or bandwidth requirements , allows the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='mation . cell site to better allocate its processing and bandwidth In some embodiments the cell phone may have the resources in anticipation of meeting such needs . capability to perform a given operation locally , but may 65 Cell sites are at risk of becoming bottlenecked : undertak decide instead to have it performed by a cloud resource . The ing service operations that exhaust their processing or band decision of whether to process locally or remotely can be width capacity . When this occurs , they must triage by', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='as \\n users . US 10,922,957 B2 \\n 53 54 unexpectedly throttling back the processing / bandwidth pro- possible scenarios are sufficiently improbable that they may vided to one or more users , so others can be served . This be disregarded in bandwidth allocations . However , on the sudden change in service is undesirable , since changing the rare occasions when such improbable scenarios occur parameters with which the channel was originally estab- when thousands of subscribers sent cell phone picture mes lished ( e.g. , the bit rate at which video can be delivered ) , 5 sages from Washington D.C. during the Obama inaugura forces data services using the channel to reconfigure their tion , some subscribers may simply not receive service . ) respective parameters ( e.g. , requiring ESPN to provide a The statistical models on which site bandwidth allocations', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content=\"lower quality video feed ) . Renegotiating such details once are based , are understood to treat subscribers — in part — as the channel and services have been originally setup invari- unpredictable actors . Whether a particular subscriber ably causes glitches , e.g. , video delivery stuttering , dropped 10 requests service in the forthcoming seconds ( and what syllables in phone calls , etc. particular service is requested ) has a random aspect . To avoid the need for these unpredictable bandwidth The larger the randomness in a statistical model , the larger slowdowns , and resulting service impairments , cell sites the extremes tend to be . If reservations , or forecasts of future tend to adopt a conservative strategy — allocating band- demands , are routinely submitted by , e.g. , 15 % of subscrib width / processing resources parsimoniously , in order to 15 ers , then the behavior of those subscribers is no longer reserve capacity for possible peak demands . But this random . The worst case peak bandwidth demand on a cell approach impairs the quality of service that might otherwise site does not involve 100 % of the subscribers acting ran be normally provided sacrificing typical service in antici- domly , but only 85 % . Actual reservation information can be pation of the unexpected . employed for the other 15 % . Hypothetical extremes in peak In accordance with this aspect of the present technology , 20 bandwidth usage are thus moderated . a cell phone sends alerts to the cell tower station , specifying With lower peak usage scenarios , more generous alloca bandwidth or processing needs that it anticipates will be tions of present bandwidth can be granted to all subscribers . forthcoming . In effect , the cell phone asks to reserve a bit of That is , if a portion of the user base sends alerts to the site future service capacity . The tower station still has a fixed reserving future capacity , then the site may predict that the capacity . However , knowing that a particular user will be 25 realistic peak demand that may be forthcoming will still needing , e.g. , a bandwidth of 8 Mbit / s for 3 seconds , leave the site with unused capacity . In this case it may grant commencing in 200 milliseconds , allows the cell site to take a camera cell phone user a 12 Mbit / s channel - instead of the such anticipated demand into account as it serves other 8 Mbit / s channel stated in the reservation request , and / or may grant a video user a 15 Mbit / s channel instead of the Consider a cell site having an excess ( allocable ) channel 30 normal 10 Mbit / s channel . Such usage forecasting can thus capacity of 15 Mbit / s , which normally allocates to a new allow the site to grant higher quality services than would video service user a channel of 10 Mbit / s . If the site knows normally be the case , since bandwidth reserves need be held that a cell camera user has requested reservation for a 8 for a lesser number of unpredictable actors . Mbit / s channel starting in 200 milliseconds , and a new video Anticipatory service requests can also be communicated service user meanwhile requests service , the site may allo- 35 from the cell phone ( or the cell site ) to other cloud processes cate the new video service user a channel of 7 Mbit / s , rather that are expected to be involved in the requested services , than the usual 10 Mbit / s . By initially setting up the new allowing them to similarly allocate their resources anticipa video service user's channel at the slower bit rate , service torily . Such anticipatory service requests may also serve to impairments associated with cutting back bandwidth during alert the cloud process to pre - warm associated processing . an ongoing channel session are avoided . The capacity of the 40 Additional information may be provided from the cell cell site is the same , but it is now allocated in manner that phone , or elsewhere , for this purpose , such as encryption reduces the need for reducing\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content='information may be provided from the cell cell site is the same , but it is now allocated in manner that phone , or elsewhere , for this purpose , such as encryption reduces the need for reducing the bandwidth of existing keys , image dimensions ( e.g. , to configure a cloud FPOA to channels , mid - transmission . serve as an FFT processor for a 1024x768 image , to be In another situation , the cell site may determine that it has processed in 16x16 tiles , and output coefficients for 32 excess capacity at present , but expects to be more heavily 45 spectral frequency bands ) , etc. burdened in a half second . In this case it may use the present In turn , the cloud resource may alert the cell phone of any excess capacity to speed throughput to one or more video information it expects might be requested from the phone in subscribers , e.g. , those for whom it has collected several performance of the expected operation , or action it might packets of video data in a buffer memory , ready for delivery . request the cell phone to perform , so that the cell phone can These video packets may be sent through the enlarged 50 similarly anticipate its own forthcoming actions and prepare channel now , in anticipation that the video channel will be accordingly . For example , the cloud process may , under slowed in a half second . Again , this is practical because the certain conditions , request a further set of input data , such as cell site has useful information about future bandwidth if it assesses that data originally provided is not sufficient for demands . the intended purpose ( e.g. , the input data may be an image The service reservation message sent from the cell phone 55 without sufficient focus resolution , or not enough contrast , may also include a priority indicator . This indicator can be or needing further filtering ) . Knowing , in advance , that the used by the cell site to determine the relative importance of cloud process may request such further data can allow the meeting the request on the stated terms , in case arbitration cell phone to consider this possibility in its own operation , between conflicting service demands is required . e.g. , keeping processing modules configured in a certain Such anticipatory service requests from cell phones can 60 filter manner longer than may otherwise be the case , reserv also allow the cell site to provide higher quality sustained ing an interval of sensor time to possibly capture a replace service than would normally be allocated . ment image , etc. Cell sites are understood to employ statistical models of Anticipatory service requests ( or the possibility of con usage patterns , and allocate bandwidth accordingly . The ditional service requests ) generally relate to events that may allocations are typically set conservatively , in anticipation of 65 commence in few tens or hundreds of milliseconds realistic worst case usage scenarios , e.g. , encompassing sionally in a few single seconds . Situations in which the scenarios that occur 99.99 % of the time . ( Some theoretically action will commence tens or hundreds of second in the occa', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content='more . US 10,922,957 B2 \\n 55 56', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"future will be rare . However , while the period of advance The system may perform a FFT on captured image data to warning may be short , significant advantages can be obtain frequency domain information , and then feed that derived : if the randomness of the next second is reduced information to several watermark decoders operating in each second , then system randomness can be reduced con- parallel - each applying a different decoding algorithm . siderably . Moreover , the events to which the requests relate 5 When one of the applications extracts valid watermark data can , themselves , be of longer duration such as transmis- ( e.g. , indicated by ECC information computed from the sion of a large image file , which may take ten seconds or payload ) , the data is sent to a database corresponding to that format / technology of watermark . Plural such database point Regarding advance set - up ( pre - warming ) , desirably any ers can be included in a packet , and used conditionally operation that takes more than a threshold interval of time to 10 depending on which watermark decoding operation ( or complete ( e.g. , a few hundred microseconds , a millisecond , barcode reading operation , or fingerprint calculation , etc. ) ten microseconds , etc.— depending on implementation ) yields useful data . should be prepped anticipatorily , if possible . ( In some Similarly , the system may send a facial image to an instances , of course , the anticipated service is never intermediary cloud service , in a packet containing an iden requested , in which case such preparation may be for 15 tifier of the user ( but not containing the user's Apple iPhoto , naught . ) In another hardware arrangement , the cell phone or Picasa , or Facebook user name ) . The intermediary cloud processor may selectively activate a Peltier device or other service can take the provided user identifier , and use it to thermoelectric cooler coupled to the image sensor , in cir- access a database record from which the user's names on cumstances when thermal image noise ( Johnson noise ) is a these other services are obtained . The intermediary cloud potential problem . For example , if a cell phone detects a low 20 service can then route the facial image data to an Apple's light condition , it may activate a cooler on the sensor to try server — with the user's iPhoto user name ; to Picasa's ser and enhance the image signal to noise ratio . Or the image vice with the user's Google user name ; and to Facebook's processing stages can examine captured imagery for artifacts server with the user's Facebook user name . Those respective associated with thermal noise , and if such artifacts exceed a services can then perform facial recognition on the imagery , threshold , then the cooling device can be activated . ( One 25 and return the names of identified persons identified from approach captures a patch of imagery , such as a 16x16 pixel the user's iPhoto / Picasa / Facebook accounts ( directly to the region , twice in quick succession . Absent random factors , user , or through the intermediary service ) . The intermediate the two patches should be identical perfectly correlated . cloud service — which may serve large numbers of users The variance of the correlation from 1.0 is a measure of can keep informed of the current addresses for relevant noise — presumably thermal noise . ) A short interval after the 30 servers ( and alternate proximate servers , in case the user is cooling device is activated , a substitute image can be cap- away from home ) , rather than have each cell phone try to tured — the interval depending on thermal response time for keep such data in updated fashion . the cooler / sensor . Likewise if cell phone video is captured , Facial recognition applications can be used just to a cooler may be activated , since the increased switching identify persons , but also to identify relationships between activity by circuitry on the sensor increases its\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content='can be used just to a cooler may be activated , since the increased switching identify persons , but also to identify relationships between activity by circuitry on the sensor increases its temperature , 35 individuals depicted in imagery . For example , data main and thus its thermal noise . ( Whether to activate a cooler can tained by iPhoto / Picasa / Facebook may contain not just also be application dependent , e.g. , the cooler may be facial recognition features , and associated names , but also activated when capturing imagery from which watermark terms indicating relationships between the named faces and data may be read , but not activated when capturing imagery the account owner ( e.g. , father , boyfriend , sibling , pet , from which barcode data may be read . ) 40 roommate , etc. ) . Thus , instead of simply searching a user\\'s As noted , packets in the FIG . 16 arrangement can convey image collection for , e.g. , all pictures of “ David Smith ” the a variety of instructions and data in both the header and the user\\'s collection may also be searched for all pictures packet body . In a further arrangement a packet can addi- depicting \" sibling . \" tionally , or alternatively , contain a pointer to a cloud object , The application software in which photos are reviewed or to a record in a database . The cloud object / database record 45 can present differently colored frames around different rec may contain information such as object properties , useful for ognized faces in accordance with associated relationship object recognition ( e.g. , fingerprint or watermark properties data ( e.g. , blue for siblings , red for boyfriends , etc. ) . for a particular object ) . In some arrangements , the user\\'s system can access such If the system has read a watermark , the packet may information stored in accounts maintained by the user\\'s contain the watermark payload , and the header ( or body ) 50 network “ friends . ” A face that may not be recognized by may contain one or more database references where that facial recognition data associated with the user\\'s account at payload can be associated with related information . A water- Picasa , may be recognized by consulting Picasa facial rec mark payload read from a business card may be looked - up ognition data associated with the account of the user\\'s friend in one database ; a watermark decoded from a photograph “ David Smith . ” Relationship data indicated by David may be looked - up in another database , etc. A system may 55 Smith\\'s account can be similarly used to present , and apply multiple different watermark decoding algorithms to a organize , the user\\'s photos . The earlier unrecognized face single image ( e.g. , MediaSec , Digimarc ImageBridge , Civo- may thus be labeled with indicia indicating the person is lution , etc. ) . Depending on which application performed a David Smith\\'s roommate . This essentially remaps the rela particular decoding operation , the resulting watermark pay- tionship information ( e.g. , mapping “ roommate ” – as indi', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"load may be sent off to a corresponding destination database . 60 cated in David Smith's account , to “ David Smith's room ( Likewise with different barcodes , fingerprint algorithms , mate ” in the user's account ) . eigenface technologies , etc. ) The destination database The embodiments detailed above were generally address can be included in the application , or in configura- described in the context of a single network . However , plural tion data . ( Commonly , the addressing is performed indi- networks may commonly be available to a user's phone rectly , with an intermediate data store containing the address 65 ( e.g. , WiFi , Bluetooth , possibly different cellular networks , of the ultimate database , permitting relocation of the data- etc. ) The user may choose between these alternatives , or the base without changing each cell phone application . ) system may apply stored rules to automatically do so . In\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"US 10,922,957 B2 \\n 57 58 \\n some instances , a service request may be issued ( or results growing / evolving sets of service providers , can be set to the returned ) across several networks in parallel . tasks of deriving meaning from input stimuli ( audio as well Reference Platform Architecture as visual , e.g. , speech recognition ) through use of such an The hardware in cell phones was originally introduced for adaptable architecture . specific purposes . The microphone , for example , was used 5 Arasan Chip Systems , Inc. offers a Mobile Industry only for voice transmission over the cellular network : feed Processor Interface UniPro Software Stack , a layered , ker ing an A / D converter that fed a modulator in the phone's nel - level stack that aims to simplify integration of certain radio transceiver . The camera was used only to capture technologies into cell phones . That arrangement may be snapshots . Etc. As additional applications arose employing extended to provide the functionality detailed above . ( The such hardware , each application needed to develop its own 10 Arasan protocol is focused primarily on transport layer way to talk to the hardware . Diverse software stacks arose each specialized so a particular application could interact issues , but involves layers down to hardware drivers as well . \\n with a particular piece of hardware . This poses an impedi The Mobile Industry Processor Interface Alliance is a large \\n ment to application development . industry group working to advance cell phone technologies . ) This problem compounds when cloud services and / or 15 Leveraging Existing Image Collections , E.g. , for Metadata specialized processors are added to the mix . To alleviate Collections of publicly - available imagery and other con such difficulties , some embodiments of the present technol- tent are becoming more prevalent . Filch , YouTube , Photo ogy can employ an intermediate software layer that provides bucket ( MySpace ) , Picasa , Zooomr , FaceBook , Webshots\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='a standard interface with which and through which hardware and Google Images are just a few . Often , these resources can and software can interact . Such an arrangement is shown in 20 also serve as sources of metadata— either expressly identi FIG . 20A , with the intermediate software layer being labeled fied as such , or inferred from data such as file names , “ Reference Platform . \" descriptions , etc. Sometimes geo - location data is also avail In this diagram hardware elements are shown in dashed able . boxes , including processing hardware on the bottom , and An illustrative embodiment according to one aspect of the peripherals on the left . The box “ IC HW ” is “ intuitive 25 present technology works as follows . A captures a cell phone computing hardware , \" and comprises the earlier - discussed picture of an object , or scene perhaps a desk telephone , as hardware that supports the different processing of image shown in FIG . 21. ( The image may be acquired in other related data , such as modules 38 in FIG . 16 , the configurable manners as well , such as transmitted from another user , or hardware of FIG . 6 , etc. DSP is a general purpose digital downloaded from a remote computer . ) signal processor , which can be configured to perform spe- 30 As a preliminary operation , known image processing cialized operations ; CPU is the phone\\'s primary processor ; operations may be applied , e.g. , to correct color or contrast , GPU is a graphics processor unit . OpenCL and OpenGL are to perform ortho - normalization , etc. on the captured image . APIs through which graphics processing services ( per- Known image object segmentation or classification tech formed on the CPU and / or GPU ) can be invoked . niques may also be used to identify an apparent subject Different specialized technologies are in the middle , such 35 region of the image , and isolate same for further processing . as one or more digital watermark decoders ( and / or encod- The image data is then processed to determine character ers ) , barcode reading software , optical character recognition izing features that are useful in pattern matching and rec software , etc. Cloud services are shown on the right , and ognition . Color , shape , and texture metrics are commonly applications are on the top . used for this purpose . Images may also be grouped based on The reference platform establishes a standard interface 40 layout and eigenvectors ( the latter being particularly popular through which different applications can interact with hard- for facial recognition ) . Many other technologies can of ware , exchange information , and request services ( e.g. , by course be employed , as noted elsewhere in this specification . API calls ) . Similarly , the platform establishes a standard ( Uses of vector characterizations / classifications and other interface through which the different technologies can be image / video / audio metrics in recognizing faces , imagery , accessed , and through which they can send and receive data 45 video , audio and other patterns are well known and suited to other of the system components . Likewise with the cloud for use in connection with certain embodiments of the services , for which the reference platform may also attend to present technology . See , e.g. , patent publications', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='details of identifying a service provider — whether by reverse 20060020630 and 20040243567 ( Digimarc ) , 20070239756 auction , heuristics , etc. In cases where a service is available and 20020037083 ( Microsoft ) , 20070237364 ( Fuji Photo both from a technology in the cell phone , and from a remote 50 Film ) , U.S. Pat . Nos . 7,359,889 and 6,990,453 ( Shazam ) , service provider , the reference platform may also attend to 20050180635 ( Corel ) , U.S. Pat . Nos . 6,430,306 , 6,681,032 weighing the costs and benefits of the different options , and and 20030059124 ( L - 1 Corp. ) , U.S. Pat . Nos . 7,194,752 and deciding which should handle a particular service request . 7,174,293 ( Iceberg ) , U.S. Pat . No. 7,130,466 ( Cobion ) , U.S. By such arrangement , the different system components do Pat . No. 6,553,136 ( Hewlett - Packard ) , and U.S. Pat . No. not need to concern themselves with the details of other parts 55 6,430,307 ( Matsushita ) , and the journal references cited at of the system . An application may call for the system to read the end of this disclosure . When used in conjunction with text from an object in front of the cell phone . It needn\\'t recognition of entertainment content such as audio and concern itself with the particular control parameters of the video , such features are sometimes termed content “ finger image sensor , nor the image format requirements of the OCR prints ” or “ hashes . \" ) engine . An application may call for a read of the emotion of 60 After feature metrics for the image are determined , a a person in front of the cell phone . A corresponding call is search is conducted through one or more publicly - accessible passed to whatever technology in the phone supports such image repositories for images with similar metrics , thereby functionality , and the results are returned in a standardized identifying apparently similar images . ( As part of its image form . When an improved technology becomes available , it ingest process , Flickr and other such repositories may cal can be added to the phone , and through the reference 65 culate eigenvectors , color histograms , keypoint descriptors , platform the system takes advantages of its enhanced capa- FFTs , or other classification data on images at the time they bilities . Thus , growing / changing collections of sensors , and are uploaded by users , and collect same in an index for', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='10 \\n 15 \\n 25 US 10,922,957 B2 \\n 59 60 \\n public search . ) The search may yield the collection of 7960 ( 2 ) apparently similar telephone images found in Flickr , Facsimile Machine ( 2 ) depicted in FIG . 22 . 7920 ( 1 ) Metadata is then harvested from Flickr for each of these 7950 ( 1 ) images , and the descriptive terms are parsed and ranked by 5 Best Buy ( 1 ) frequency of occurrence . In the depicted set of images , for Desk ( 1 ) example , the descriptors harvested from such operation , and Ethernet ( 1 ) their incidence of occurrence , may be as follows : IP - phone ( 1 ) Cisco ( 18 ) Office ( 1 ) Phone ( 10 ) Pricey ( 1 ) \\n Telephone ( 7 ) Sprint ( 1 ) \\n VOIP ( 7 ) Telecommunications ( 1 ) IP ( 5 ) Uninett ( 1 ) 7941 ( 3 ) Work ( 1 ) Phones ( 3 ) The list of inferred metadata can be restricted to those Technology ( 3 ) terms that have the highest apparent reliability , e.g. , count 7960 ( 2 ) values . A subset of the list comprising , e.g. , the top N terms , 7920 ( 1 ) or the terms in the top Mth percentile of the ranked listing , 7950 ( 1 ) may be used . This subset can be associated with the FIG . 21 Best Buy ( 1 ) 20 image in a metadata repository for that image , as inferred Desk ( 1 ) metadata . \\n Ethernet ( 1 ) In the present example , if N = 4 , the terms Telephone , IP - phone ( 1 ) Cisco , Phone and VOIP are associated with the FIG . 21 Office ( 1 ) image . \\n Pricey ( 1 ) Once a list of metadata is assembled for the FIG . 21 image Sprint ( 1 ) ( by the foregoing procedure , or others ) , a variety of opera \\n Telecommunications ( 1 ) tions can be undertaken .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content='Uninett ( 1 ) One option is to submit the metadata , along with the Work ( 1 ) captured content or data derived from the captured content From this aggregated set of inferred metadata , it may be 30 ( e.g. , the FIG . 21 image , image feature data such as eigen assumed that those terms with the highest count values ( e.g. , vectors , color histograms , keypoint descriptors , FFTS , those terms occurring most frequently ) are the terms that machine readable data decoded from the image , etc ) , to a most accurately characterize the user\\'s FIG . 21 image . service provider that acts on the submitted data , and pro The inferred metadata can be augmented or enhanced , if vides a response to the user . Shazam , Snapnow ( now desired , by known image recognition / classification tech- 35 LinkMe Mobile ) , ClusterMedia Labs , Snaptell ( now part of niques . Such technology seeks to provide automatic recog- Amazon\\'s A9 search service ) , Mobot , Mobile Acuity , Nokia nition of objects depicted in images . For example , by Point & Find , Kooaba , idée TinEye , iVisit\\'s SeeScan , Evo recognizing a TouchTone keypad layout , and a coiled cord , lution Robotics \\' ViPR , IQ Engine’s oMoby , and Digimarc such a classifier may label the FIG . 21 image using the terms Mobile , are a few of several commercially available services Telephone and Facsimile Machine . 40 that capture media content , and provide a corresponding If not already present in the inferred metadata , the terms response ; others are detailed in the earlier - cited patent returned by the image classifier can be added to the list and publications . By accompanying the content data with the given a count value . ( An arbitrary value , e.g. , 2 , may be metadata , the service provider can make a more informed used , or a value dependent on the classifier\\'s reported judgment as to how it should respond to the user\\'s submis confidence in the discerned identification can be employed . ) 45 sion . If the classifier yields one or more terms that are already The service provider or the user\\'s device can submit present , the position of the term ( s ) in the list may be the metadata descriptors to one or more other services , e.g. , elevated . One way to elevate a term\\'s position is by increas- a web search engine such as Google , to obtain a richer set ing its count value by a percentage ( e.g. , 30 % ) . Another way of auxiliary information that may help better discern / infer / is to increase its count value to one greater than the next- 50 intuit an appropriate desired by the user . Or the information above term that is not discerned by the image classifier . obtained from Google ( or other such database resource ) can ( Since the classifier returned the term “ Telephone ” but not be used to augment / refine the response delivered by the the term “ Cisco , \" this latter approach could rank the term service provider to the user . ( In some cases , the metadata Telephone with a count value of “ 19 ” _one above Cisco . ) A possibly accompanied by the auxiliary information received variety of other techniques for augmenting / enhancing the 55 from Google can allow the service provider to produce an inferred metadata with that resulting from the image clas- appropriate response to the user , without even requiring the sifier are straightforward to implement . image data . ) A revised listing of metadata , resulting from the forego- In some cases , one or more images obtained from Flickr ing , may be as follows : may be substituted for the user\\'s image . This may be done , Telephone ( 19 ) 60 for example , if a Flickr image appears to be of higher quality Cisco ( 18 ) ( using sharpness , illumination histogram , or other mea Phone ( 10 ) sures ) , and if the image metrics are sufficiently similar VOIP ( 7 ) ( Similarity can be judged by a distance measure appropriate IP ( 5 ) to the metrics being used . One embodiment checks whether 7941 ( 3 ) 65 the distance measure is below a', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content='similar VOIP ( 7 ) ( Similarity can be judged by a distance measure appropriate IP ( 5 ) to the metrics being used . One embodiment checks whether 7941 ( 3 ) 65 the distance measure is below a threshold . If several alter Phones ( 3 ) nate images pass this screen , then the closest image is used . ) Technology ( 3 ) Or substitution may be used in other circumstances . The', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content=\"US 10,922,957 B2 \\n 61 62 \\n substituted image can then be used instead of ( or in addition The above examples searched Flickr for images based on to ) the captured image in the arrangements detailed herein . similarity of image metrics , and optionally on similarity of \\n In one such arrangement , the substitute image data is textual ( semantic ) metadata . Geolocation data ( e.g. , GPS submitted to the service provider . In another , data for several tags ) can also be used to get a metadata toe - hold . substitute images are submitted . In another , the original 5 If the user captures an arty , abstract shot of the Eiffel image data— together with one or more alternative sets of tower from amid the metalwork or another unusual vantage image data are submitted . In the latter two cases , the point ( e.g. , FIG . 29 ) , it may not be recognized from image service provider can use the redundancy to help reduce the metrics as the Eiffel tower . But GPS info captured with the chance of error - assuring an appropriate response is pro image identifies the location of the image subject . Public vided to the user . ( Or the service provider can treat each 10 submitted set of image data individually , and provide plural databases ( including Flickr ) can be employed to retrieve textual metadata based on GPS descriptors . Inputting GPS responses to the user . The client software on the cell phone can then assess the different responses , and pick between descriptors for the photograph yields the textual descriptors \\n Paris and Eiffel . them ( e.g. , by a voting arrangement ) , or combine the responses , to help provide the user an enhanced response . ) 15 Google Images , or another database , can be queried with \\n Instead of substitution , one or more related public the terms Eiffel and Paris to retrieve other , more perhaps \\n image ( s ) may be composited or merged with the user's cell conventional images of the Eiffel tower . One or more of phone image . The resulting hybrid image can then be used those images can be submitted to the service provider to \\n in the different contexts detailed in this disclosure . drive its process . ( Alternatively , the GPS information from A still further option is to use apparently - similar images 20 the user's image can be used to search Flickr for images gleaned from Flickr to inform enhancement of the user's from the same location ; yielding imagery of the Eiffel Tower image . Examples include color correction / matching , con- that can be submitted to the service provider . ) trast correction , glare reduction , removing foreground / back- Although GPS is gaining in camera - metadata - deploy ground objects , etc. By such arrangement , for example , such ment , most imagery presently in Flickr and other public a system may discern that the FIG . 21 image has foreground 25 databases is missing geolocation info . But GPS info can be components ( apparently Post - It notes ) on the telephone that automatically propagated across a collection of imagery that should be masked or disregarded . The user's image data can share visible features ( by image metrics such as eigenvec be enhanced accordingly , and the enhanced image data used tors , color histograms , keypoint descriptors , FFTs , or other\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content=\"thereafter . classification techniques ) , or that have a metadata match . Relatedly , the user's image may suffer some impediment , 30 To illustrate , if the user takes a cell phone picture of a city e.g. , such as depicting its subject from an odd perspective , fountain , and the image is tagged with GPS information , it or with poor lighting , etc. This impediment may cause the can be submitted to a process that identifies matching user's image not to be recognized by the service provider Flickr / Google images of that fountain on a feature - recogni ( i.e. , the image data submitted by the user does not seem to tion basis . To each of those images the process can add GPS match any image data in the database being searched ) . Either 35 information from the user's image . in response to such a failure , or proactively , data from A second level of searching can also be employed . From similar images identified from Flickr may be submitted to the set of fountain images identified from the first search the service provider as alternatives — hoping they might based on similarity of appearance , metadata can be har\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content=\"work better . vested and ranked , as above . Flickr can then be searched a Another approach one that opens up many further pos- 40 second time , for images having metadata that matches sibilities is to search Flickr for one or more images with within a specified threshold ( e.g. , as reviewed above ) . To similar image metrics , and collect metadata as described those images , too , GPS information from the user's image herein ( e.g. , Telephone , Cisco , Phone , VOIP ) . Flickr is then can be added . searched a second time , based on metadata . Plural images Alternatively , or in addition , a first set of images in with similar metadata can thereby be identified . Data for 45 Flickr / Google similar to the user's image of the fountain can these further images ( including images with a variety of be identified — not by pattern matching , but by GPS - match different perspectives , different lighting , etc. ) can then be ing ( or both ) . Metadata can be harvested and ranked from submitted to the service provider notwithstanding that these GPS - matched images . Flickr can be searched a second they may “ look ” different than the user's cell phone image . time for a second set of images with similar metadata . To When doing metadata - based searches , identity of meta- 50 this second set of images , GPS information from the user's data may not be required . For example , in the second search image can be added . of Flickr just - referenced , four terms of metadata may have Another approach to geolocating imagery is by searching been associated with the user's image : Telephone , Cisco , Flickr for images having similar image characteristics ( e.g. , Phone and VOIP . A match may be regarded as an instance in gist , eigenvectors , color histograms , keypoint descriptors , which a subset ( e.g. , three ) of these terms is found . 55 FFTs , etc. ) , and assessing geolocation data in the identified Another approach is to rank matches based on the rank- images to infer the probable location of the original image . ings of shared metadata terms . An image tagged with See , e.g. , Hays , et al , IM2GPS : Estimating geographic Telephone and Cisco would thus be ranked as a better match information from a single image , Proc . of the IEEE Conf . on than an image tagged with Phone and VOIP . One adaptive Computer Vision and Pattern Recognition , 2008. Techniques way to rank a “ match ” is to sum the counts for the metadata 60 detailed in the Hays paper are suited for use in conjunction descriptors for the user's image ( e.g. , 19 + 18 + 10 + 7 = 54 ) , and with certain embodiments of the present technology ( includ then tally the count values for shared terms in a Flickr image ing use of probability functions as quantizing the uncertainty ( e.g. , 35 , if the Flickr image is tagged with Cisco , Phone and of inferential techniques ) . VOIP ) . The ratio can then be computed ( 35/54 ) and com- When geolocation data is captured by the camera , it is pared to a threshold ( e.g. , 60 % ) . In this case , a “ match ” is 65 highly reliable . Also generally reliable is metadata ( location found . A variety of other adaptive matching techniques can or otherwise ) that is authored by the proprietor of the image . be devised by the artisan . However , when metadata descriptors ( geolocation or seman\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content='5 US 10,922,957 B2 \\n 63 64', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content='tic ) are inferred or estimated , or authored by a stranger to the The sub - metadata may indicate , for example , that the tag image , uncertainty and other issues arise . \" football ” was contributed by a 21 year old male in Brazil Desirably , such intrinsic uncertainty should be memori- on Jun . 18 , 2008. It may further indicate that the tags alized in some fashion so that later users thereof ( human or “ afternoon , ” “ evening ” and “ morning “ were contributed by machine ) can take this uncertainty into account . an automated image classifier at the University of Texas that One approach is to segregate uncertain metadata from made these judgments on Jul . 2 , 2008 based , e.g. , on the device - authored or creator - authored metadata . For example , angle of illumination on the subjects . Those three descriptors different data structures can be used . Or different tags can be may also have associated probabilities assigned by the used to distinguish such classes of information . Or each classifier , e.g. , 50 % for afternoon , 30 % for evening , and metadata descriptor can have its own sub - metadata , indicat- 10 20 % for morning ( each of these percentages may be stored ing the author , creation date , and source of the data . The as a sub - metatag ) . One or more of the metadata terms author or source field of the sub - metadata may have a data contributed by the classifier may have a further sub - tag string indicating that the descriptor was inferred , estimated , pointing to an on - line glossary that aids in understanding the deduced , etc. , or such information may be a separate sub- assigned terms . For example , such as sub - tag may give the metadata tag . 15 URL of a computer resource that associates the term “ after Each uncertain descriptor may be given a confidence noon ” with a definition , or synonyms , indicating that the metric or rank . This data may be determined by the public , term means noon to 7 pm . The glossary may further indicate either expressly or inferentially . An example is the case a probability density function , indicating that the mean time when a user sees a Flickr picture she believes to be from meant by “ afternoon ” is 3:30 pm , the median time is 4:15 Yellowstone , and adds a “ Yellowstone ” location tag , 20 pm , and the term has a Gaussian function of meaning together with a “ 95 % ” confidence tag ( her estimation of spanning the noon to 7 pm time interval . certainty about the contributed location metadata ) . She may Expertise of the metadata contributors may also be add an alternate location metatag , indicating “ Montana , ” reflected in sub - metadata . The term “ fescue ” may have together with a corresponding 50 % confidence tag . ( The sub - metadata indicating it was contributed by a 45 year old confidence tags needn\\'t sum to 100 % . Just one tag can be 25 grass seed farmer in Oregon . An automated system can contributed — with a confidence less than 100 % . Or several conclude that this metadata term was contributed by a tags can be contributed possibly overlapping , as in the person having unusual expertise in a relevant knowledge case with Yellowstone and Montana ) . domain , and may therefore treat the descriptor as highly If several users contribute metadata of the same type to an reliable ( albeit maybe not highly relevant ) . This reliability image ( e.g. , location metadata ) , the combined contributions 30 determination can be added to the metadata collection , so can be assessed to generate aggregate information . Such that other reviewers of the metadata can benefit from the information may indicate , for example , that 5 of 6 users who automated system\\'s assessment . contributed metadata tagged the image as Yellowsto with Assessment of the contributor\\'s expertise can also be an average 93 % confidence ; that 1 of 6 users tagged the self - made by the contributor . Or it can be made otherwise , image as Montana , with a 50 % confidence , and 2 of 6', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content=\"expertise can also be an average 93 % confidence ; that 1 of 6 users tagged the self - made by the contributor . Or it can be made otherwise , image as Montana , with a 50 % confidence , and 2 of 6 users 35 e.g. , by reputational rankings using collected third party tagged the image as Glacier National park , with a 15 % assessments of the contributor's metadata contributions . confidence , etc. ( Such reputational rankings are known , e.g. , from public Inferential determination of metadata reliability can be assessments of sellers on EBay , and of book reviewers on performed , either when express estimates made by contribu- Amazon ) Assessments may be field - specific , so a person tors are not available , or routinely . An example of this is the 40 may be judged ( or self - judged ) to be knowledgeable about FIG . 21 photo case , in which metadata occurrence counts are grass types , but not about dog breeds . Again , all such used to judge the relative merit of each item of metadata information is desirably memorialized in sub - metatags ( in ( e.g. , Telephone = 19 or 7 , depending on the methodology cluding sub - sub - metatags , when the information is about a used ) . Similar methods can be used to rank reliability when sub - metatag ) . More information about crowd - sourcing , several metadata contributors offer descriptors for a given 45 including use of contributor expertise , etc. , is found in image . Digimarc's published patent application 20070162761 . Crowd - sourcing techniques are known to parcel image- Returning to the case of geolocation descriptors ( which identification tasks to online workers , and collect the results . may be numeric , e.g. , latitude / longitude , or textual ) , an However , prior art arrangements are understood to seek image may accumulate over time a lengthy catalog of simple , short - term consensus on identification . Better , it 50 contributed geographic descriptors . An automated system seems , is to quantify the diversity of opinion collected about ( e.g. , a server at Flickr ) may periodically review the con image contents ( and optionally its variation over time , and tributed geotag information , and distill it to facilitate public information about the sources relied - on ) , and use that richer use . For numeric information , the process can apply known data to enable automated systems to make more nuanced clustering algorithms to identify clusters of similar coordi decisions about imagery , its value , its relevance , its use , etc. 55 nates , and average same to generate a mean location for each To illustrate , known crowd - sourcing image identification cluster . For example , a photo of a geyser may be tagged by techniques may identify the FIG . 35 image with the identi- some people with latitude / longitude coordinates in Yellow fiers “ soccer ball ” and “ dog . ” These are the consensus terms stone , and by others with latitude / longitude coordinates of from one or several viewers . Disregarded , however , may be Hells Gate Park in New Zealand . These coordinates thus information about the long tail of alternative descriptors , 60 form distinct two clusters that would be separately averaged . e.g. , summer , Labrador , football , tongue , afternoon , eve- If 70 % of the contributors placed the coordinates in Yel ning , morning , fescue , etc. Also disregarded may be demo- lowstone , the distilled ( averaged ) value may be given a graphic and other information about the persons ( or pro- confidence of 70 % . Outlier data can be maintained , but cesses ) that served metadata identifiers , or the given a low probability commensurate with its outlier status . circumstances of their assessments . A richer set of metadata 65 Such distillation of the data by a proprietor can be stored in may associate with each descriptor a set of sub - metadata metadata fields that are readable by the public , but not detailing this further information . writable . as\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content=\"5 US 10,922,957 B2 \\n 65 66 \\n The same or other approach can be used with added instructions may advise that the user can press the up - arrow textual metadatae.g . , it can be accumulated and ranked button 116a of the controller to view these metadata - similar based on frequency of occurrence , to give a sense of relative images ( 134 , FIG . 45A ) . \\n confidence . Thus , by pressing the right , left , and up buttons , the user The technology detailed in this specification finds numer- can review images that are similar to the captured image in ous applications in contexts involving watermarking , bar- appearance , location , or metadata descriptors . coding , fingerprinting , OCR - decoding and other Whenever such review reveals a picture of particular \\n approaches for obtaining information from imagery . Con interest , the user can press the down button 116c . This action \\n sider again the FIG . 21 cell phone photo of a desk phone . identifies the currently viewed picture to the service pro \\n Flickr can be searched based on image metrics to obtain a 10 vider , which then can repeat the process with the currently viewed picture as the base image . The process then repeats collection of subject - similar images ( e.g. , as detailed above ) . with the user - selected image as the base , and with button A data extraction process ( e.g. , watermark decoding , finger presses enabling review of images that are similar to that print calculation , barcode- or OCR - reading ) can be applied base image in appearance ( 16b ) , location ( 16d ) , or metadata to some or all of the resulting images , and information 15 ( 16a ) . gleaned thereby can be added to the metadata for the FIG . This process can continue indefinitely . At some point the 21 image , and / or submitted to a service provider with image user can press the center button 118 of the four - way con data ( either for the FIG . 21 image , and / or for related troller . This action submits the then - displayed image to a images ) . service provider for further action ( e.g. , triggering a corre From the collection of images found in the first search , 20 sponding response , as disclosed , e.g. , in earlier - cited docu text or GPS metadata can be harvested , and a second search ments ) . This action may involve a different service provider can be conducted for similarly - tagged images . From the text than the one that provided all the alternative imagery , or they tags Cisco and VOIP , for example , a search of Flickr may can be the same . ( In the latter case the finally selected image find a photo of the underside of the user's phone with need not be sent to the service provider , since that service OCR - readable data as shown in FIG . 36. Again , the 25 provider knows all the images buffered by the cell phone , extracted information can be added to the metadata for the and may track which image is currently being displayed . ) FIG . 21 image , and / or submitted to a service provider to The dimensions of information browsing just - detailed enhance the response it is able to provide to the user . ( similar - appearance images ; similar - location images ; simi As just shown , a cell phone user may be given the ability lar - metadata images ) can be different in other embodiments . to look around corners and under objects — by using one 30 Consider , for example , an embodiment that takes an image image as a portal to a large collection of related images . of a house as input ( or latitude / longitude ) , and returns the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"User Interface following sequences of images : ( a ) the houses for sale Referring FIGS . 44 and 45A , cell phones and related nearest in location to the input - imaged house ; ( b ) the houses portable devices 110 typically include a display 111 and a for sale nearest in price to the input - imaged house ; and ( c ) keypad 112. In addition to a numeric ( or alphanumeric ) 35 the houses for sale nearest in features ( e.g. , bedrooms / baths ) keypad there is often a multi - function controller 114. One to the input - imaged house . ( The universe of houses dis popular controller has a center button 118 , and four sur- played can be constrained , e.g. , by zip - code , metropolitan rounding buttons 116a , 116b , 116c and 116d ( also shown in area , school district , or other qualifier . ) FIG . 37 ) . Another example of this user interface technique is pre An illustrative usage model is as follows . A system 40 sentation of search results from EBay for auctions listing responds to an image 128 ( either optically captured or Xbox 360 game consoles . One dimension can be price ( e.g. , wirelessly received ) by displaying a collection of related pushing button 116b yields a sequence of screens showing images to the user , on the cell phone display . For example , Xbox 360 auctions , starting with the lowest - priced ones ) ; the user captures an image and submits it to a remote service . another can be seller's geographical proximity to user ( clos The service determines image metrics for the submitted 45 est to furthest , shown by pushing button 116d ) ; another can image ( possibly after pre - processing , as detailed above ) , and be time until end of auction ( shortest to longest , presented by searches ( e.g. , Flickr ) for visually similar images . These pushing button 116a ) . Pressing the middle button 118 can images are transmitted to the cell phone ( e.g. , by the service , load the full web page of the auction being displayed . or directly from Flickr ) , and they are buffered for display . A related example is a system that responds to a user The service can prompt the user , e.g. , by instructions pre- 50 captured image of a car by identifying the car ( using image sented on the display , to repeatedly press the right - arrow features and associated database ( s ) ) , searching EBay and button 116b on the four - way controller ( or press - and - hold ) Craigslist for similar cars , and presenting the results on the to view a sequence of pattern - similar images ( 130 , FIG . screen . Pressing button 116b presents screens of information 45A ) . Each time the button is pressed , another one of the about cars offered for sale ( e.g. , including image , seller buffered apparently - similar images is displayed . 55 location , and price ) based on similarity to the input image By techniques like those earlier described , or otherwise , ( same model year / same color first , and then nearest model the remote service can also search for images that are similar years / colors ) , nationwide . Pressing button 116d yields such in geolocation to the submitted image . These too can be sent a sequence of screens , but limited to the user's state ( or to and buffered at the cell phone . The instructions may metropolitan region , or a 50 mile radius of the user's advise that the user can press the left - arrow button 116d of 60 location , etc ) . Pressing button 116a yields such a sequence the controller to review these GPS - similar images ( 132 , FIG . of screens , again limited geographically , but this time pre 45A ) . sented in order of ascending price ( rather than closest model Similarly , the service can search for images that are year / color ) . Again , pressing the middle button loads the full similar in metadata to the submitted image ( e.g. , based on web page ( EBay or Craigslist ) of the car last - displayed . textual metadata inferred from other images , identified by 65 Another embodiment is an application that\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"image ( e.g. , based on web page ( EBay or Craigslist ) of the car last - displayed . textual metadata inferred from other images , identified by 65 Another embodiment is an application that helps people pattern matching or GPS matching ) . Again , these images can recall names . A user sees a familiar person at a party , but be sent to the phone and buffered for immediate display . The can't remember his name . Surreptitiously the user snaps a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"screen . \\n screen US 10,922,957 B2 \\n 67 68 \\n picture of the person , and the image is forwarded to a remote the system to its original state . Instead , pressing the right service provider . The service provider extracts facial recog- button gives , e.g. , a first similar - appearing image , and press nition parameters and searches social networking sites ( e.g. , ing the left button gives the first similarly - located image . FaceBook , MySpace , Linked - In ) , or a separate database Sometimes it is desirable to navigate through the same containing facial recognition parameters for images on those 5 sequence of screens , but in reverse of the order just - re sites , for similar - appearing faces . ( The service may provide viewed . Various interface controls can be employed to do the user's sign - on credentials to the sites , allowing searching this .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content='of information that is not otherwise publicly accessible . ) One is a \" Reverse \" button . The device 110 in FIG . 44 Names and other information about similar - appearing per- includes a variety of buttons not - yet discussed ( e.g. , buttons sons located via the searching are returned to the user\\'s cell 10 120a - 120f , around the periphery of the controller 114 ) . Any phone — to help refresh the user\\'s memory . of these if pressed can serve to reverse the scrolling Various UI procedures are contemplated . When data is order . By pressing , e.g. , button 120a , the scrolling ( presen returned from the remote service , the user may push button tation ) direction associated with nearby button 116b can be 116b to scroll thru matches in order of closest - similarity- reversed . So if button 116b normally presents items in order regardless of geography . Thumbnails of the matched indi- 15 of increasing cost , activation of button 120a can cause the viduals with associated name and other profile information function of button 116b to switch , e.g. , to presenting items can be displayed , or just full screen images of the person can in order of decreasing cost . If , in reviewing screens resulting be presented with the name overlaid . When the familiar from use of button 116b , the user “ overshoots ” and wants to person is recognized , the user may press button 118 to load reverse direction , she can push button 120a , and then push the full FaceBook / MySpace / Linked - In page for that person . 20 button 116b again . The screen ( s ) earlier presented would Alternatively , instead of presenting images with names , just then appear in reverse order — starting from the present a textual list of names may be presented , e.g. , all on a single ordered by similarity of face - match ; SMS text Or , operation of such a button ( e.g. , 120a or 1208 ) can messaging can suffice for this last arrangement . cause the opposite button 116d to scroll back thru the screens Pushing button 116d may scroll thru matches in order of 25 presented by activation of button 116b , in reverse order . closest - similarity , of people who list their residence as A textual or symbolic prompt can be overlaid on the within a certain geographical proximity ( e.g. , same metro- display screen in all these embodiments informing the user politan area , same state , same campus , etc. ) of the user\\'s of the dimension of information that is being browsed , and present location or the user\\'s reference location ( e.g. , home ) . the direction ( e.g. , browsing by cost : increasing ) . Pushing button 116a may yield a similar display , but limited 30 In still other arrangements , a single button can perform to persons who are “ Friends ” of the user within a social multiple functions . For example , pressing button 116b can network ( or who are Friends of Friends , or who are within cause the system to start presenting a sequence of screens , another specified degree of separation of the user ) . e.g. , showing pictures of houses for sale near the user\\'s A related arrangement is a law enforcement tool in which location — presenting each for 800 milliseconds ( an interval an officer captures an image of a person and submits same 35 set by preference data entered by the user ) . Pressing button to a database containing facial portrait / eigenvalue informa- 116b a second time can cause the system to stop the tion from government driver license records and / or other sequence displaying a static screen of a house for sale . sources . Pushing button 116b causes the screen to display a Pressing button 116b a third time can cause the system to sequence of images / biographical dossiers about persons present the sequence in reverse order , starting with the static nationwide having the closest facial matches . Pushing but- 40 screen and going backwards thru the screens earlier pre ton 116d causes the screen to display a similar sequence , but sented . Repeated operation of', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content=\"the closest facial matches . Pushing but- 40 screen and going backwards thru the screens earlier pre ton 116d causes the screen to display a similar sequence , but sented . Repeated operation of buttons 116a , 116b , etc. , can limited to persons within the officer's state . Button 116a operate likewise ( but control different sequences of infor yields such a sequence , but limited to persons within the mation , e.g. , houses closest in price , and houses closest in metropolitan area in which the officer is working . features ) . Instead of three dimensions of information browsing 45 In arrangements in which the presented information stems ( buttons 116b , 1160 , 116a , e.g. , for similar - appearing from a process applied to a base image ( e.g. , a picture images / similarly located images / similar metadata - tagged snapped by a user ) , this base image may be presented images ) , more or less dimensions can be employed . FIG . throughout the display - e.g . , as a thumbnail in a corner of 45B shows browsing screens in just two dimensions . ( Press- the display . Or a button on the device ( e.g. , 126a , or 1206 ) ing the right button yields a first sequence 140 of informa- 50 can be operated to immediately summon the base image tion screens ; pressing the left button yields a different back to the display . sequence 142 of information screens . ) Touch interfaces are gaining in popularity , such as in Instead of two or more distinct buttons , a single UI control products available from Apple and Microsoft ( detailed , e.g. , can be employed to navigate in the available dimensions of in Apple's patent publications 20060026535 , 20060026536 ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content=\"information . A joystick is one such device . Another is a 55 20060250377 , 20080211766 , 20080158169 , 20080158172 , roller wheel ( or scroll wheel ) . Portable device 110 of FIG . 44 20080204426 , 20080174570 , and Microsoft's patent publi has a roller wheel 124 on its side , which can be rolled - up or cations 20060033701 , 20070236470 and 20080001924 ) . rolled - down . It can also be pressed - in to make a selection Such technologies can be employed to enhance and extend ( e.g. , akin to buttons 116c or 118 of the earlier - discussed the just - reviewed user interface concepts allowing greater controller ) . Similar controls are available on many mice . 60 degrees of flexibility and control . Each button press noted In most user interfaces , opposing buttons ( e.g. , left button above can have a counterpart gesture in the vocabulary of 116b , and right button 116d ) navigate the same dimension of the touch screen system . information just in opposite directions ( e.g. , forward / re- For example , different touch - screen gestures can invoke verse ) . In the particular interface discussed above , it will be display of the different types of image feeds just reviewed . recognized that this is not the case ( although in other 65 A brushing gesture to the right , for example , may present a implementations , it may be so ) . Pressing the right button rightward - scrolling series of image frames 130 of imagery 116b , and then pressing the left button 116d , does not return having similar visual content ( with the initial speed of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content='US 10,922,957 B2 \\n 69 70 \\n scrolling dependent on the speed of the user gesture , and collection of images . The images thus identified can be with the scrolling speed decelerating or not- over time ) . A presented to the user using the arrangements noted above . brushing gesture to the left may present a similar leftward- Certain embodiments of the present technology may be scrolling display of imagery 132 having similar GPS infor regarded as employing an iterative , recursive process by mation . A brushing gesture upward may present images an 5 which information about one set of images ( a single image upward - scrolling display of imagery 134 similar in meta in many initial cases ) is used to identify a second set of data . At any point the user can tap one of the displayed images , which may be used to identify a third set of images , images to make it the base image , with the process repeating . etc. The function by which each set of images is related to Other gestures can invoke still other actions . One such the next relates to a particular class of image information , action is displaying overhead imagery corresponding to the 10 GPS location associated with a selected image . The imagery e.g. , image metrics , semantic metadata , GPS , decoded info , \\n etc. can be zoomed in / out with other gestures . The user can In other contexts , the relation between one set of images select for display photographic imagery , map data , data from different times of day or different dates / seasons , and / or and the next is a function not just of one class of information , \\n various overlays ( topographic , places of interest , and other 15 but two or more . For example , a seed user image may be \\n data , as is known from Google Earth ) , etc. Icons or other examined for both image metrics and GPS data . From these graphics may be presented on the display depending on two classes of information a collection of images can be', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content=\"contents of particular imagery . One such arrangement is determined — images that are similar in both some aspect of detailed in Digimarc's published application 20080300011 . visual appearance and location . Other pairings , triplets , etc. , “ Curbside ” or “ street - level ” imagery — rather than over- 20 of relationships can naturally be employed in the determi head imagery can be also displayed . nation of any of the successive sets of images . It will be recognized that certain embodiments of the Further Discussion present technology include a shared general structure . An Some embodiments of the present technology analyze initial set of data ( e.g. , an image , or metadata such as consumer cell phone picture , and heuristically determine descriptors or geocode information , or image metrics such 25 information about the picture’s subject . For example , is it a as eigenvalues ) is presented . From this , a second set of data person , place , or thing ? From this high level determination , ( e.g. , images , or image metrics , or metadata ) are obtained . the system can better formulate what type of response might From that second set of data , a third set of data is compiled be sought by the consumer making operation more intui ( e.g. , images with similar image metrics or similar metadata , tive . or image metrics , or metadata ) . Items from the third set of 30 For example , if the subject of the photo is a person , the data can be used as a result of the process , or the process may consumer might be interested in adding the depicted person continue , e.g. , by using the third set of data in determining as a FaceBook “ friend . ” Or sending a text message to that fourth data ( e.g. , a set of descriptive metadata can be compiled from the images of the third set ) . This can con person . Or publishing an annotated version of the photo to tinue , e.g. , determining a fifth set of data from the fourth 35 a web page . Or simply learning who the person is . ( e.g. , identifying a collection of images that have metadata If the subject is a place ( e.g. , Times Square ) , the consumer \\n terms from the fourth data set ) . A sixth set of data can be might be interested in the local geography , maps , and nearby \\n obtained from the fifth ( e.g. , identifying clusters of GPS data attractions . \\n with which images in the fifth set are tagged ) , and so on . If the subject is a thing ( e.g. , the Liberty Bell or a bottle\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content=\"with which images in the fifth set are tagged ) , and so on . If the subject is a thing ( e.g. , the Liberty Bell or a bottle \\n The sets of data can be images , or they can be other forms 40 of beer ) , the consumer may be interested in information of data ( e.g. , image metrics , textual metadata , geolocation about the object ( e.g. , its history , others who use it ) , or in data , decoded OCR- , barcode- , watermark - data , etc ) . buying or selling the object , etc. Any data can serve as the seed . The process can start with Based on the image type , an illustrative system / service image data , or with other information , such as image met- can identify one or more actions that it expects the consumer rics , textual metadata ( aka semantic metadata ) , geolocation 45 will find most appropriately responsive to the cell phone information ( e.g. , GPS coordinates ) , decoded OCR / barcode / image . One or all of these can be undertaken , and cached on watermark data , etc. From a first type of information ( image the consumer's cell phone for review . For example , scrolling metrics , semantic metadata , GPS info , decoded info ) , a first a thumbwheel on the side of the cell phone may present a set of information - similar images can be obtained . From that succession of different screens each with different infor first set , a second , different type of information ( image 50 mation responsive to the image subject . ( Or a screen may be metrics / semantic metadata / GPS / decoded info , etc. ) can be presented that queries the consumer as to which of a few gathered . From that second type of information , a second set possible actions is desired . ) of information - similar images can be obtained . From that In use , the system can monitor which of the available second set , a third , different type of information ( image actions is chosen by the consumer . The consumer's usage metrics / semantic metadata / GPS / decoded info , etc. ) can be 55 gathered . From that third type of information , a third set of history can be employed to refine a Bayesian model of the \\n information - similar images can be obtained . Etc. consumer's interests and desires , so that future responses \\n Thus , while the illustrated embodiments generally start can be better customized to the user . \\n with an image , and then proceed by reference to its image These concepts will be clearer by example ( aspects of \\n metrics , and so on , entirely different combinations of acts are 60 which are depicted , e.g. , in FIGS . 46 and 47 ) . also possible . The seed can be the payload from a product Processing a Set of Sample Images barcode . This can generate a first collection of images Assume a tourist snaps a photo of the Prometheus statue depicting the same barcode . This can lead to a set of at Rockefeller Center in New York using a cell phone or common metadata . That can lead to a second collection of other mobile device . Initially , it is just a bunch of pixels . images based on that metadata . Image metrics may be 65 What to do ? computed from this second collection , and the most preva- Assume the image is geocoded with location information lent metrics can be used to search and identify a third ( e.g. , latitude / longitude in XMP- or EXIF - metadata ) .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content='US 10,922,957 B2 \\n 71 72 \\n From the geocode data , a search of Flickr can be under- narrative description about the photograph . Placement of the taken for a first set of images taken from the same ( or placement - related metadata is another metric . nearby ) location . Perhaps there are 5 or 500 images in this Consideration can also be given to the particularity of the \\n first set . place - related descriptor . A descriptor “ New York ” or “ USA ” Metadata from this set of images is collected . The meta- 5 may be less indicative that an image is place - centric than a data can be of various types . One is words / phrases from a more particular descriptor , such as “ Rockefeller Center ” or title given to an image . Another is information in metatags “ Grand Central Station . ” This can yield a third metric . assigned to the image_usually by the photographer ( e.g. , A related , fourth metric considers the frequency of occur naming the photo subject and certain attributes / keywords ) , rence ( or improbability ) of a term — either just within the but additionally by the capture device ( e.g. , identifying the 10 collected metadata , or within a superset of that data . “ RCA camera model , the date / time of the photo , the location , etc ) . Building ” is more relevant , from this standpoint , than Another is words / phrases in a narrative description of the “ Rockefeller Center ” because it is used much less fre photo authored by the photographer . quently . Some metadata terms may be repeated across different These and other metrics can be combined to assign each images . Descriptors common to two or more images can be 15 image in the set with a place score indicating its potential identified ( clustered ) , and the most popular terms may be place - centric - ness . ranked . ( Such as listing is shown at “ A ” in FIG . 46A . Here , The combination can be a straight sum of four factors , and in other metadata listings , only partial results are given each ranging from 0 to 100. More likely , however , some for expository convenience . ) metrics will be weighted more heavily . The following equa From the metadata , and from other analysis , it may be 20 tion employing metrics M1 , M2 , M2 and M4 can be possible to determine which images in the first set are likely employed to yield a score S , with the factors A , B , C , D and person - centric , which are place - centric , and which are thing- exponents W , X , Y and Z determined experimentally , or by centric . Bayesian techniques : \\n Consider the metadata with which a set of 50 images may be tagged . Some of the terms relate to place . Some relate to 25 S = A * M1 ) W + ( B * M2 ) + ( C * M3 ) * + ( D * M4 ) 2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='Consider the metadata with which a set of 50 images may be tagged . Some of the terms relate to place . Some relate to 25 S = A * M1 ) W + ( B * M2 ) + ( C * M3 ) * + ( D * M4 ) 2 \\n persons depicted in the images . Some relate to things . Person - Centric Processing Place - Centric Processing A different analysis can be employed to estimate the Terms that relate to place can be identified using various person - centric - ness of each image in the set obtained from techniques . One is to use a database with geographical Flickr . information to look - up location descriptors near a given 30 As in the example just - given , a glossary of relevant terms geographical position . Yahoo\\'s GeoPlanet service , for can be compiled — this time terms associated with people . In example , returns a hierarchy of descriptors such as “ Rock- contrast to the place name glossary , the person name glos efeller Center , \" “ 10024 ” ( a zip code ) , “ Midtown Manhat- sary can be global — rather than associated with a particular tan , ” “ New York , ” “ Manhattan , ” “ New York , ” and “ United locale . ( However , different glossaries may be appropriate in States , \" when queried with the latitude / longitude of the 35 different countries . ) Rockefeller Center . Such a glossary can be compiled from various sources , The same service can return names of adjoining / sibling including telephone directories , lists of most popular names , neighborhoods / features on request , e.g. , “ 10017 , ” “ 10020 , \" and other reference works where names appear . The list may “ 10036 , ” “ Theater District , ” “ Carnegie Hall , ” “ Grand Cen- start , “ Aaron , Abigail , Adam , Addison , Adrian , Aidan , tral Station , ” “ Museum of American Folk Art , ” etc. , etc. 40 Aiden , Alex , Alexa , Alexander , Alexandra , Alexis , Allison , Nearby street names can be harvested from a variety of Alyssa , Amelia , Andrea , Andrew , Angel , Angelina , Anna , mapping programs , given a set of latitude / longitude coor- Anthony , Antonio , Ariana , Arianna , Ashley , Aubrey , Audrey ,', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='dinates or other location info . Austin , Autumn , Ava , Avery A glossary of nearby place - descriptors can be compiled in First names alone can be considered , or last names can be such manner . The metadata harvested from the set of Flickr 45 considered too . ( Some names may be a place name or a images can then be analyzed , by reference to the glossary , to person name . Searching for adjoining first / last names and / or identify the terms that relate to place ( e.g. , that match terms adjoining place names can help distinguish ambiguous in the glossary ) . cases . E.g. , Elizabeth Smith is a person ; Elizabeth N.J. is a Consideration then turns to use of these place - related place . ) metadata in the reference set of images collected from 50 Personal pronouns and the like can also be included in Flickr . such a glossary ( e.g. , he , she , him , her , his , our , her , I , me , Some images may have no place - related metadata . These myself , we , they , them , mine , their ) . Nouns identifying images are likely person - centric or thing - centric , rather than people and personal relationships can also be included ( e.g. , place - centric . uncle , sister , daughter , gramps , boss , student , employee , Other images may have metadata that is exclusively 55 wedding , etc ) Adjectives and adverbs that are usually place - related . These images are likely place - centric , rather applied to people may also be included in the person - term than person - centric or thing - centric . glossary ( e.g. , happy , boring , blonde , etc ) , as can the names In between are images that have both place - related meta- of objects and attributes that are usually associated with data , and other metadata . Various rules can be devised and people ( e.g. , t - shirt , backpack , sunglasses , tanned , etc. ) . utilized to assign the relative relevance of the image to place . 60 Verbs associated with people can also be employed ( e.g. , One rule looks at the number of metadata descriptors surfing , drinking ) . associated with an image , and determines the fraction that is In this last group , as in some others , there are some terms found in the glossary of place - related terms . This is one that could also apply to thing - centric images ( rather than metric . person - centric ) . The term “ sunglasses ” may appear in meta Another looks at where in the metadata the place - related 65 data for an image depicting sunglasses , alone ; “ happy ” may descriptors appear . If they appear in an image title , they are appear in metadata for an image depicting a dog . There are likely more relevant than if they appear at the end of a long also some cases where a person - term may also be a place', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='US 10,922,957 B2 \\n 73 74', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99}), Document(page_content=\"term ( e.g. , Boring , Oreg . ) . In more sophisticated embodi- lacking any face , increases the likelihood that the image ments , glossary terms can be associated with respective relates to a place named Elizabeth , or a thing named confidence metrics , by which any results based on such Elizabeth — such as a pet . ) terms may be discounted or otherwise acknowledged to have Still more confidence in the determination can be assumed different degrees of uncertainty . ) As before , if an image is 5 if the facial recognition algorithm identifies a face as a not associated with any person - related metadata , then the female , and the metadata includes a female name . Such an image can be adjudged likely not person - centric . Con- arrangement , of course , requires that the glossary — or other versely , if all of the metadata is person - related , the image is data structure have data that associates genders with at likely person - centric . least some names . For other cases , metrics like those reviewed above can be 10 ( Still more sophisticated arrangements can be imple assessed and combined to yield a score indicating the mented . For example , the age of the depicted person ( s ) can relative person - centric - ness of each image , e.g. , based on the be estimated using automated techniques ( e.g. , as detailed in number , placement , particularity and / or frequency / improb- U.S. Pat . No. 5,781,650 , to Univ . of Central Florida ) . Names ability of the person - related metadata associated with the found in the image metadata can also be processed to image . 15 estimate the age of the thus - named person ( s ) . This can be While analysis of metadata gives useful information about done using public domain information about the statistical whether an image is person - centric , other techniques can distribution of a name as a function of age ( e.g. , from also be employed either alternatively , or in conjunction published Social Security Administration data , and web sites with metadata analysis . that detail most popular names from birth records ) . Thus , One technique is to analyze the image looking for con- 20 names Mildred and Gertrude may be associated with an age tinuous areas of skin - tone colors . Such features characterize distribution that peaks at age 80 , whereas Madison and many features of person - centric images , but are less fre- Alexis may be associated with an age distribution that peaks quently found in images of places and things . at age 8. Finding statistically - likely correspondence between A related technique is facial recognition . This science has metadata name and estimated person age can further advanced to the point where even inexpensive point - and- 25 increase the person - centric score for an image . Statistically shoot digital cameras can quickly and reliably identify faces unlikely correspondence can be used to decrease the person within an image frame ( e.g. , to focus or expose the image centric score . ( Estimated information about the age of a based on such subjects ) . subject in the consumer's image can also be used to tailor the ( Face finding technology is detailed , e.g. , in U.S. Pat . No. intuited response ( s ) , as may information about the subject's 5,781,650 ( Univ . of Central Florida ) , U.S. Pat . No. 6,633 , 30 gender . ) ) 655 ( Sharp ) , U.S. Pat . No. 6,597,801 ( Hewlett - Packard ) and Just as detection of a face in an image can be used as a U.S. Pat . No. 6,430,306 ( L - 1 Corp. ) , and in Yang et al , “ plus ” factor in a score based on metadata , the existence of Detecting Faces in Images : A Survey , IEEE Transactions on person - centric metadata can be used as a “ plus ” factor to Pattern Analysis and Machine Intelligence , Vol . 24 , No. 1 , increase a person - centric score based on facial recognition January 2002 , pp . 34-58 , and Zhao , et al , Face Recognition : 35 data . A Literature Survey , ACM Computing Surveys , 2003 , pp . Of course , if no\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99}), Document(page_content=\"- centric score based on facial recognition January 2002 , pp . 34-58 , and Zhao , et al , Face Recognition : 35 data . A Literature Survey , ACM Computing Surveys , 2003 , pp . Of course , if no face is found in an image , this information 399-458 . ) can be used to reduce a person - centric score for the image Facial recognition algorithms can be applied to the set of ( perhaps down to zero ) . reference images obtained from Flickr , to identify those that Thing - Centric Processing have evident faces , and identify the portions of the images 40 A thing - centered image is the third type of image that may corresponding to the faces . be found in the set of images obtained from Flickr in the Of course , many photos have faces depicted incidentally present example . There are various techniques by which a within the image frame . While all images having faces could thing - centric score for an image can be determined . be identified as person - centric , most embodiments employ One technique relies on metadata analysis , using prin further processing to provide a more refined assessment . 45 ciples like those detailed above . A glossary of nouns can be One form of further processing is to determine the per- compiled either from the universe of Flickr metadata or centage area of the image frame occupied by the identified some other corpus ( e.g. , WordNet ) , and ranked by frequency face ( s ) . The higher the percentage , the higher the likelihood of occurrence . Nouns associated with places and persons can that the image is person - centric . This is another metric than be removed from the glossary . The glossary can be used in can be used in determining an image's person - centric score . 50 the manners identified above to conduct analyses of the Another form of further processing is to look for the images ' metadata , to yield a score for each . existence of ( 1 ) one or more faces in the image , together Another approach uses pattern matching to identify thing with ( 2 ) person - descriptors in the metadata associated with centric images matching each against a library of known the image . In this case , the facial recognition data can be thing - related images . used as a “ plus ” factor to increase a person - centric score of 55 Still another approach is based on earlier - determined an image based on metadata or other analysis . ( The “ plus ” scores for person - centric and place - centric . A thing - centric can take various forms . E.g. , a score ( in a 0-100 scale ) can score may be assigned in inverse relationship to the other be increased by 10 , or increased by 10 % . Or increased by two scores ( i.e. , if an image scores low for being person half the remaining distance to 100 , etc. ) centric , and low for being place - centric , then it can be Thus , for example , a photo tagged with “ Elizabeth ” 60 assigned a high score for being thing - centric ) . metadata is more likely a person - centric photo if the facial Such techniques may be combined , or used individually . recognition algorithm finds a face within the image than if In any event , a score is produced for each image — tending no face is found . to indicate whether the image is more- or less likely to be ( Conversely , the absence of any face in an image can be thing - centric . used as a “ plus ” factor to increase the confidence that the 65 Further Processing of Sample Set of Images image subject is of a different type , e.g. , a place or a thing . Data produced by the foregoing techniques can produce Thus , an image tagged with Elizabeth as metadata , but three scores for each image in the set , indicating rough\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99})]]\n"
     ]
    }
   ],
   "source": [
    "result_pdf = []\n",
    "for i in result_api:\n",
    "        loader = PyPDFLoader(i)\n",
    "        result_pdf.append(loader.load_and_split())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='(12) United States Patent \\n Belhumeur et al. US009275273B2 \\n US 9.275,273 B2 \\n Mar. 1, 2016 (10) Patent No.: \\n (45) Date of Patent: \\n (54) \\n (71) \\n (72) \\n (*) \\n (21) \\n (22) \\n (65) \\n (63) \\n (60) \\n (51) \\n (52) \\n (58) METHOD AND SYSTEM FOR LOCALIZING \\n PARTS OF AN OBJECT IN AN MAGE FOR \\n COMPUTER VISION APPLICATIONS (56) References Cited \\n U.S. PATENT DOCUMENTS \\n Applicant: Kri Beh Visi 5,828,769 A * 10/1998 Burns ........................... 382,118 pplicant: regman-Iseinurneur vision 6,252,974 B1* 6/2001 Martens et al. ............... 382/107 Technologies, LLC, San Diego, CA 6,502,082 B1* 12/2002 Toyama et al. ................. TO6, 16 \\n (US) 8,873,840 B2 * 10/2014 Krupka et al. ... ... 382,159 \\n 2004/000293.0 A1 1/2004 Oliver et al. .................... TO6/46 \\n Inventors: Peter N. Belhumeur, New York, NY 2004/001793.0 A1 1/2004 Kim et al. ..................... 382,103 \\n (US); David W. Jacobs, Bethesda, MD (Continued) (US); David J. Kriegman, San Diego, \\n CA (US); Neeraj Kumar, Seattle, WA OTHER PUBLICATIONS \\n (US) \\n Cristinace D. et al., Feature detection and tracking with constrained \\n Notice: Subject to any disclaimer, the term of this local models, British Machine Vision Conference pp. 231-240, patent is extended or adjusted under 35 2004. \\n Appl. No.: 14/324,991 \\n Filed: Jul. 7, 2014 Primary Examiner — Barry Drennan \\n Assistant Examiner — Aklilu Woldemariam \\n Prior Publication Data (74) Attorney, Agent, or Firm — Keller Jolley Preece \\n US 2015/OO78631 A1 Mar. 19, 2015 \\n Related U.S. Application Data \\n Continuation of application No. 13/488,415, filed on \\n Jun. 4, 2012, now Pat. No. 8,811,726. \\n Provisional application No. 61/492.774, filed on Jun. \\n 2, 2011. \\n Int. C. \\n G06K 9/00 (2006.01) \\n GO6K 9/62 (2006.01) \\n U.S. C. \\n CPC ........ G06K9/00281 (2013.01); G06K 9/00248 \\n (2013.01); G06K9/6278 (2013.01); G06K 9/00 \\n (2013.01); G06K 9/62 (2013.01) \\n Field of Classification Search \\n USPC ......... 382/118, 155, 159, 180, 224, 225, 107, \\n 382/222 \\n See application file for complete search history. \\n CCLECEMAGE l \\n EXEMPARS \\n 101 (57) ABSTRACT \\n A system is provided for localizing parts of an object in an \\n image by training local detectors using labeled image exem \\n plars with fiducial points corresponding to parts within the \\n image. Each local detector generates a detector score corre \\n sponding to the likelihood that a desired part is located at a given location within the image exemplar. A non-parametric \\n global model of the locations of the fiducial points is gener \\n ated for each of at least a portion of the image exemplars. An input image is analyzed using the trained local detectors, and \\n a Bayesian objective function is derived for the input image \\n from the non-parametric model and detector scores. The Bayesian objective function is optimized using a consensus of \\n global models, and an output is generated with locations of \\n the fiducial points labeled within the object in the image. \\n 20 Claims, 9 Drawing Sheets \\n INPUMAGE \\n FOR \\n EWAAON \\n 106 \\n GNERATE CJPJ \\n WHAR \\n LOCAONSLABEED \\n NIMAGE \\n 112 FA USE:0CA. \\n DETECORSTO \\n MARKFICAL MSED reAUre RAIN LOCAL GENERAE \\n poNS in RAINNG SLection otectors - DEECORSCORES \\n TRAININGIMAGE iMAGESINC 04 105 107 \\n EXEMPARS DAABASE \\n O2 03 \\n SEEC USEBAYESRUEO GENERAENON \\n JSE SLECED GO3A. OPTIEROBABY PARAWRC \\n MODE. O.ABEL MOWIE HAA GCBAMODEL GCEACDELS \\n AR LOCATONS BEST FIT . FIES HE DEECTOR -. FROMAGE \\n MIMAGE H 11 SCORES ExoMPARs \\n 111 109 O3 \\n CAL \\n RECOGNOM \\n SYSTEM', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 0}), Document(page_content='US 9,275,273 B2 \\n Page 2 \\n (56) References Cited 2010, 0077382 A1 3/2010 Sasaki ........................... 717/124 \\n 2011 0116690 A1* 5, 2011 ROSS et al. ... 382.118 \\n U.S. PATENT DOCUMENTS 2012/0022952 A1 1/2012 Cetin et al. ................. 705/14.73 \\n 2004/0181749 A1* 9/2004 Chellapilla et al. ........... 715,505 OTHER PUBLICATIONS \\n ck 28393. A. ck $39. SE al. .70743. Eckhardt Metal., Towards Practical facial feature detection, In, J. of \\n 2007/0258648 A1* 11/2007 Perronnin ..................... 382,224 Pattern Recognition and Artificial Intelligence 23 (3) 379-400, \\n 2008.0109041 A1* 5/2008 de Voir .............................. 6O7/7 2009.* \\n 2008/0267459 A1 * 10, 2008 Nakada et al. ................ 382,118 \\n 2010.0054592 A1 3/2010 Nanu et al. .................... 382, 167 * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 1}), Document(page_content='US 9,275.273 B2 Sheet 1 of 9 Mar. 1, 2016 U.S. Patent \\n - - -; \\n OLL LOETES SER HOOS RIO LOE LEGI \\n El LV-JENESOT\\\\/OOT NIV/>| 1 O L SÈJO LOE LEGI TV/OOT EST', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 2}), Document(page_content='US 9,275.273 B2 Sheet 2 of 9 Mar. 1, 2016 U.S. Patent \\n FIG. 2', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 3}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 3 of 9 US 9,275.273 B2 \\n s', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 4}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 4 of 9 US 9,275.273 B2 \\n o \\n N \\n s g P C C C C C C C C. O o \\n eoubSIO JenoO-Jeu / JOu ueeW', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 5}), Document(page_content='US 9,275.273 B2 Sheet 5 Of 9 Mar. 1, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 6}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 6 of 9 US 9.275,273 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 7}), Document(page_content='U.S. Patent Mar. 1, 2016 Sheet 7 Of 9 US 9,275.273 B2 \\n s', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 8}), Document(page_content='US 9,275.273 B2 Sheet 8 of 9 Mar. 1, 2016 U.S. Patent \\n O \\n 32 O LO r \\n O O O \\n O', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 9}), Document(page_content='US 9,275.273 B2 Sheet 9 Of 9 Mar. 1, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 10}), Document(page_content=\"US 9,275,273 B2 \\n 1. \\n METHOD AND SYSTEM FOR LOCALIZING \\n PARTS OF AN OBJECT IN AN IMAGE FOR \\n COMPUTER VISION APPLICATIONS \\n RELATED APPLICATIONS \\n This application is continuation of U.S. application Ser. \\n No. 13/488,415, filed Jun. 4, 2012, which claims the benefit of the priority of U.S. provisional application No. 61/492,774, \\n filed Jun. 2, 2011, which is incorporated herein by reference in its entirety. \\n GOVERNMENT RIGHTS \\n This invention was made with government funding from \\n the Office of the Chief Scientist of the Central Intelligence Agency. The government has certain rights in the invention. \\n FIELD OF THE INVENTION \\n The present invention relates to a method for computer aided analysis of images, and more specifically to a method \\n for localizing features within an image. \\n BACKGROUND OF THE INVENTION \\n Over the last decade, new applications in computer vision and computational photography have arisen due to earlier \\n advances in methods for detecting human faces in images. \\n These applications include face detection-based autofocus \\n and white balancing in cameras, Smile and blink detection, new methods for sorting and retrieving images in digital \\n photo management Software, obscuration of facial identity in digital photos, facial expression recognition, virtual try-on, \\n product recommendations, facial performance capture, ava \\n tars, controls, image editing software tailored for faces, and systems for automatic face recognition and verification. The first step of any face processing system is the detection \\n of locations in the images where faces are present. However, face detection from a single image is challenging because of \\n variability in Scale, location, orientation, and pose. Facial expression, occlusion, and lighting conditions also change \\n the overall appearance of faces. \\n Given an arbitrary image, the goal of face detection is to \\n determine whether or not there are any faces in the image and, \\n if present, return the image location and extent of each face. \\n The challenges associated with face detection can be attrib uted to the following factors: \\n Pose: The images of a face vary due to the relative camera face pose (frontal, 45 degree, profile, upside-down), and some \\n facial features such as an eye or the nose may become par tially or wholly occluded. \\n Presence or absence of structural components: Facial fea tures such as beards, moustaches, and glasses may or may not be present, and there is a great deal of variability among these components including shape, color, and size. \\n Facial expression: The appearance of faces is directly \\n affected by a person’s facial expression. \\n Occlusion: Faces may be partially occluded by other objects. In an image with a group of people, some faces may \\n partially occlude other faces. Image orientation: Face images vary directly for different \\n rotations about the camera's optical axis. Imaging conditions: When the image is formed, factors Such as lighting (spectra, Source distribution and intensity) \\n and camera characteristics (sensor response, lenses, filters) affect the appearance of a face. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n Camera Settings: The settings on the camera and the way \\n that is used can affect the image focus blur, motion blur, depth \\n of field, compression (e.g., jpeg) artifacts, and image noise. \\n Face detectors usually return the image location of a rect angular bounding box containing a face—this serves as the \\n starting point for processing the image. A part of the process \\n that is currently in need of improvement is the accurate detec \\n tion and localization of parts of the face, e.g., eyebrow cor \\n ners, eye corners, tip of the nose, earlobes, hair part, jawline, \\n mouth corners, chin, etc. These parts are often referred to as facial feature points or “fiducial points'. Unlike general inter\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 11}), Document(page_content=\"ners, eye corners, tip of the nose, earlobes, hair part, jawline, \\n mouth corners, chin, etc. These parts are often referred to as facial feature points or “fiducial points'. Unlike general inter \\n est or corner points, the fiducial point locations may not correspond to image locations with high gradients (e.g., tip of \\n the nose). As a result, their detection may require larger image \\n Support. \\n A number of approaches have been reported which have demonstrated great accuracy in localizing parts in mostly \\n frontal images, and often in controlled settings. \\n Early work on facial feature detection was often described as a component of a larger face processing task. For example, \\n Burl, et al. take a bottom-up approach to face detection, first \\n detecting candidate facial features over the whole image, then \\n selecting the most face-like constellation using a statistical \\n model of the distances between pairs of features. Other works detect large-scale facial parts such as each eye, the nose, and \\n the mouth and return a contour or bounding box around these components. \\n There is a long history of part-based object descriptions in \\n computer vision and perceptual psychology. Recent \\n approaches have shown a renewed emphasis on parts-based \\n descriptions and attributes because one can learn descriptions of individual parts and then compose them, generalizing to an \\n exponential number of combinations. The Poselets work by Bourdev and Malik, incorporated herein by reference, \\n describes a data-driven search for object parts that may be a \\n useful approach for addressing some of the described inad equacies of the prior art in order to achieve precise face \\n detection in uncontrolled image conditions. Many fiducial point detectors include classifiers that are trained to respond to a specific fiducial (e.g., left corner of the \\n left eye). These classifiers take as input raw pixel intensities \\n over a window or the output of a bank of filters (Gaussian \\n Derivative filters, Gabor filters, or Haar-like features). These local detectors are scanned over a portion of the image and \\n may return one or more candidate locations for the part or a \\n “score” at each location. This local detector is often a binary \\n classifier (feature or not-feature). For example, the Viola Jones style detector, which uses an image representation \\n called an “integral image' rather than working directly with \\n image intensities, has been applied to facial features. False \\n detections occur often, even for well-trained classifiers, because portions of the image have the appearance of a fidu \\n cial under some imaging condition. For example, a common \\n erroris for a “left corner of left eye' detector to respond to the \\n left corner of the right eye. Eckhart, et al. achieve robustness and handle greater pose variation by using a large area of \\n Support for the detector covering, e.g., an entire eye or the \\n nose with room to spare. Searching over a smaller region that \\n includes the actual part location reduces the chance of false detections with minimal impact of missing fiducials. While \\n this may be somewhat effective for frontal fiducial point \\n detection, the location of a part within the face detector box can vary significantly when the head rotates in three-dimen sions. For example, while the left eye is in the upper-left side \\n of the box when frontal, it can move to the right side when the face is seen in profile.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 11}), Document(page_content='US 9,275,273 B2 \\n 3 \\n To better handle larger variations in pose, constraints can \\n be established about the relative location of parts with respect \\n to each other rather than the actual location of each part to the detector box. This can be expressed as predicted locations, bounding regions, or as a conditional probability distribution \\n of one part location given another location. Alternatively, the \\n joint probability distribution of all the parts can be used, and \\n one model is that they form a multivariate normal distribution whose mean is the average location of each part. This is the model underlying Active Appearance Models and Active \\n Shape Models, which have been used for facial feature point detection in near frontal images. Saragih, et al. extend this to \\n use a Gaussian Mixture Model, whereas Everingham, et al. handle a wider range of pose, lighting and expression varia \\n tions by modeling the joint probability of the location of nine \\n fiducials relative to the bounding box with a mixture of Gaus sian trees. As pointed out in this work, a joint distribution of \\n part locations over a wide range of poses cannot be adequately modeled by a single Gaussian. \\n While a number of approaches balance local feature detec tor responses on the image with prior global information about the feature configurations, optimizing the resulting \\n objective function remains a challenge. The locations of some parts vary significantly with expression (e.g., the mouth, eye \\n brows) whereas others. Such as the eye corners and nose, are more stable. Consequently, Some detection methods organize \\n their search to first identify the stable points. The location of the mouth points are then constrained, possibly through a \\n conditional probability, by the locations of stable points. \\n However, this approach fails when these stable points cannot be reliably detected, for example, when the eyes are hidden by Sunglasses. \\n The need for the ability to reliably detect and identify \\n features within an image is not limited to human facial rec ognition. Many other disciplines rely on specific features \\n within an image to facilitate identification of an object within an image. For example, conservation organizations utilize \\n markings such as ear notches, Scars, tail patterns, etc., on wild \\n animals for identification of individual animals for study of migration patterns, behavior and survival. The ability to reli \\n ably locate and identify the unique features within an image \\n of an animal could provide expanded data for use in Such studies. Other applications of image analysis that could ben \\n efit from improved feature location capability include identi \\n fication of vehicles within images for military or law enforce \\n ment applications, and identification of structures in satellite images, to name a few. \\n SUMMARY OF THE INVENTION \\n According to the present invention, a method is provided \\n for facial feature detection by combining the outputs of a \\n plurality of local detectors with a consensus of non-paramet \\n ric global models for part locations. \\n The inventive method for face detection begins with a large collection of pre-specified (labeled) parts in images of human \\n faces taken under a variety of acquisition conditions, includ ing variability in pose, lighting, expression, hairstyle, Subject \\n age, Subject ethnicity, partial-occlusion of the face, camera \\n type, image compression, resolution, and focus. This collec tion captures both the variability of appearance of each part \\n and the variability in the relative positions of each part. \\n The collection of labeled exemplar images is used as a \\n training set for a local detector, which evaluates Small win \\n dows in the image to determine whether the area within the window contains the desired part. Using scale-invariant fea \\n tures within the image, the local detector, which is a sliding 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='tures within the image, the local detector, which is a sliding 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n window classifier, generates a detector score for each point in \\n the image, with the score corresponding to the likelihood that \\n the desired part is located at a given point in the image. The \\n sliding window classifier may be a neural network, Support \\n vector machine (SVM), Viola-Jones-style detector, or other learning machine that is appropriate for image analysis appli \\n cations. The sliding window classifier may be applied over the expected range of image locations for the particular part. \\n Next, a global detector is developed for a collection offiducial \\n points by combining the outputs of the local detectors with a \\n non-parametric prior model of face shape. By assuming that \\n global model images generate the part locations as hidden \\n variables, a Bayesian objective function can be derived. This \\n function is optimized using a consensus of models for the \\n hidden variables to determine the most likely values of the \\n hidden variables from which the part location can be deter \\n mined. \\n In a Bayesian sense, a generative probabilistic model \\n P(W) for the i-th fiducial can be constructed this is the probability distribution of the image feature W from the marked fiducials in the collection. Letting X={x\\', x, ... x\"} denote the locations of the n fiducials, the prior probability \\n distribution of the fiducial location PCX) can be estimated from the fiducial locations in the image collection. Detection of the parts in an input image can use the probabilistic models \\n on the appearance P(W) and fiducial locations POX) to find the fiducial locations in an input image by maximizing a Bayesian objective function. Alternatively, image features \\n that are not at the locations of the fiducials (negative \\n examples) and the features at the locations of fiducials (posi \\n tive examples used to construct P(W)) can be used to con struct classifiers or regressors for each fiducial. These classi fier outputs can be combined using the prior model on fiducial location POX) to detect fiducials in an input image. Existing \\n methods for detecting parts in faces that create a prior model \\n on the configuration of fiducials have used parametric forms \\n for PCX) such as a multivariate normal distribution or a mix \\n ture of Gaussian, whereas the inventive method uses a non parametric form that is dictated by the training collection. \\n In one aspect of the invention, the facial feature localiza tion method is formulated as a Bayesian inference that com \\n bines the output of local detectors with a prior model of face shape. The prior on the configuration of face parts is non \\n parametric, making use of a large collection of diverse, \\n labeled exemplars. Hidden (latent) variables are introduced for the identity and parts locations within the exemplar to \\n generate fiducial locations in a new image. The hidden vari ables are marginalized out, but they nonetheless provide valu \\n able conditional independencies between different parts. To marginalize efficiently, a RANdom SAmple Consensus \\n (RANSAC)-like process to sample likely values of the hidden \\n variables. This ultimately leads to part localization as a com \\n bination of local detector output and the consensus of a vari ety of exemplars and poses that fit this data well. \\n In another aspect of the invention, a method for localizing parts of an object in an input image comprises training a \\n plurality of local detectors using at least a portion of a plural ity of image exemplars as training images, wherein each \\n image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images \\n corresponding to a likelihood that a desired part is located at the location within the training image; generating a plurality of non-parametric global models using at least a portion of the plurality of image exemplars; inputting data corresponding to', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 12}), Document(page_content='US 9,275,273 B2 \\n 5 \\n the input image; applying the trained local detectors to the \\n input image to generate detector scores for the input image; \\n deriving a Bayesian objective function from the plurality of non-parametric global models using an assumption that loca \\n tions of fiducial points within the image exemplar are repre sented within its corresponding global model as hidden vari ables; optimizing the Bayesian objective function to obtain a \\n consensus set of global models for the hidden variables that best fits the data corresponding to the input image; and gen erating an output comprising locations of the fiducial points \\n within the object in the image. In one embodiment, the object \\n is a face. \\n In another aspect of the invention, a method for localizing \\n parts of an object in an input image comprises training a \\n plurality of local detectors using at least a portion of a plural \\n ity of image exemplars as training images, wherein each \\n image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detector \\n generates a detector score when applied at one location of a \\n plurality of locations of fiducial points in the training images \\n corresponding to a likelihood that a desired part is located at \\n the location within the training image; generating a non \\n parametric model of the plurality of locations of the fiducial \\n points in each of at least a portion of the plurality of image \\n exemplars; inputting data corresponding to the input image: \\n applying the trained local detectors to the input image to \\n generate detector scores for the input image; deriving a Baye \\n sian objective function for the input image from the non \\n parametric model and detector scores; and generating an out \\n put comprising locations of the fiducial points within the \\n object in the image. In one embodiment, the step of deriving \\n a Bayesian objective function comprises using an assumption \\n that locations offiducial points within the image exemplar are \\n represented within its corresponding global model as hidden \\n variables; and optimizing the Bayesian objective function to \\n obtain a consensus set of global models for the hidden vari \\n ables that best fits the data corresponding to the image. \\n In still another aspect of the invention, a computer-program product embodied on a non-transitory computer-readable \\n medium comprising instructions for receiving a plurality of image exemplars, and further comprises instructions for training a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, wherein \\n each image exemplar is labeled with fiducial points corre \\n sponding to parts within the image, and wherein each local \\n detector generates a detector score when applied at one loca tion of a plurality of locations offiducial points in the training images corresponding to a likelihood that a desired part is \\n located at the location within the training image; generating a non-parametric model of the plurality of locations of the fiducial points in each of at least a portion of the plurality of image exemplars; inputting data corresponding to the input \\n image; applying the trained local detectors to the input image to generate detector scores for the input image; deriving a Bayesian objective function for the input image from the \\n non-parametric model and detector scores; and generating an \\n output comprising locations of the fiducial points within the object in the image. In an exemplary embodiment, deriving a Bayesian objective function comprises using an assumption \\n that locations offiducial points within the image exemplar are represented within its corresponding global model as hidden variables; and optimizing the Bayesian objective function to \\n obtain a consensus set of global models for the hidden vari ables that best fits the data corresponding to the image. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 13}), Document(page_content='obtain a consensus set of global models for the hidden vari ables that best fits the data corresponding to the image. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 is a block diagram showing the steps of the inventive process for localizing parts of a face. \\n FIG. 2 is an image of a human face in the LFPW dataset, \\n where the left image shows hand-labeled fiducial points and the right image shows the points numbered to match FIG. 4 \\n FIG.3 is a set of photographs of human faces with fiducial \\n points indicated according to the inventive method. \\n FIG. 4 is a plot showing the mean error of the fiducial \\n detector on the LFPW dataset (gray bar of each pair) com pared to the mean variation in human labeling (black bar of \\n each pair) using the fiducial labels shown in FIG. 2. The error \\n is the fraction of inter-ocular distance. \\n FIG. 5 is a comparison of the performance of the inventive \\n method and the detector of Everingham, et al., showing that \\n the inventive method is roughly twice as accurate as both. FIG. 6 is a collection of photographic images from the \\n Labeled Face Parts in the Wild (LFPW) along with parts \\n located by the inventive detector. FIG. 7 is a set of images from BioID, with the parts local \\n ized by the inventive detector. FIG. 8 is a plot of cumulative error distribution comparing \\n the inventive method with methods described by others. \\n FIG. 9 is a plot of cumulative error distribution of the inventive method on the LFPW dataset compared to locations predicted using the face detector box or found with only the \\n local detectors. \\n DETAILED DESCRIPTION \\n A method is provided for localizing parts of an object \\n within an image by detecting fine-scale fiducial points or \\n microfeatures. Although the examples described herein relate to detecting parts of a human face, the inventive method may be used for detecting parts within images of many other \\n classes of objects, e.g., animals (dogs, cats, wild animals, \\n marine mammals, etc.) or animal faces, parts of bicycles, \\n vehicles, or structures. Thus, the described examples of face part detection are not intended to be limiting. \\n FIG.1 provides the basic steps of the inventive method 100. In step 101, a large number of image exemplars are collected, \\n preferably without constraint as to pose, expression, occlu \\n sion or other characteristics that would make the detection \\n process easier. In step 102, the image exemplars are marked \\n by hand with fiducial points. Examples of these fiducial \\n points are shown in FIG. 2, overlaid on an image obtained \\n from the Internet. In addition to the marked fiducials, a virtual fiducial location in each training image can be defined as a \\n mathematical function of a subset of the marked fiducial. For \\n example, a virtual fiducial in the center of the chin could be created by taking the average location of a fiducial located on \\n the lower lip and a fiducial located on the chin. In step 103, the marked image exemplars are downloaded \\n into a database that can be accessed by a processor that is programmed to execute a learning machine. The following steps are part of a computer program that \\n may be embodied within a non-transitory computer-readable \\n medium for causing a computer processor to execute the \\n specified steps. (For purposes of the present invention, a “computer processor includes a Supercomputing grid or \\n cluster, a network server, a private or dedicated server, a standard personal computer or laptop, a tablet computer, a \\n Smartphone, video game console or controller, digital cam \\n era, or any other known computing device (stand-alone or \\n embedded within another electronic device) that is capable of processing images.) In step 104, an optional pre-processing', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 13}), Document(page_content='US 9,275,273 B2 \\n 7 \\n or feature selection step, an algorithm is applied to select the \\n features that are most determinative of the patterns to be \\n recognized. In one example, known feature selection algo \\n rithms such as the well-known AdaBoost may be used to improve the performance of the Subsequent classifiers. In \\n another example, techniques for image feature detection and \\n characterization, e.g., Hough transforms, edge detection, \\n blob detection, histograms of Gradients (HOG), Speeded Up \\n Robust Feature (SURF), and others, may be applied. As \\n described below, in the preferred embodiment, a scale-invari \\n ant feature transform (SIFT) is used. \\n In step 105 local detectors are trained using the training \\n images selected from the set of image exemplars and features \\n extracted from the image. In the preferred embodiment, the \\n classifier is a non-linear support vector machine (SVM) with \\n a radial basis function (RBF) kernel. The local detectors return a score indicating the likelihood of an image location \\n being a particular fiducial point. The local detector is scanned \\n overan image and a score at each image location is computed; \\n a high score is a more likely location in the image for the \\n feature. The image locations that are local maxima of the detector output are referred to as “detector peaks.” \\n In step 106, data for an image to be evaluated is input into \\n the processor, and, in step 107, the trained local detectors are used to generate detector scores for each fiducial point type for each point in the image. Although not shown, the input image may require some pre-processing if the input image is \\n more that just a face. In this case, a face detector may be applied to the image to identify a region of the image that \\n contains a face, and this region would be the input to Subse quent steps. For example, in a group photograph, or in a \\n photograph where the Subject is Surrounded by structures or other background, a rectangle of an input image can be \\n cropped out and used as the input for face part detection. The \\n inventive method takes as input an image that contains a face. This optional face detector step can be bypassed when it is known that input contains only a single face at an approxi \\n mately known or specified location, orientation, and size (e.g., head shots in a yearbook). \\n In step 108, a plurality of non-parametric global models is generated using the image exemplars. (Note that step 108 can \\n follow either step 102 or step 103.) In step 109, the global \\n models from step 108 and the detector scores from step 107 are combined. A Bayesian objective function is derived by \\n assuming that the global models generate locations of the \\n parts as hidden variables. This function is optimized using a \\n consensus of global models for the hidden variables. The models that provide the best fit to the data are selected in step \\n 110. In step 111, the selected set of best fitting models is used to label the part locations in the image that was input in step 106. An output is generated in step 112, where the image 113 \\n may be stored in memory for further processing and/or dis played via agraphic display or printoutata user interface with \\n the part locations (fiducial points) labeled, as shown in FIG.2. \\n Note that the dots shown in the image are used to indicate that the parts were correctly found and accurately located. \\n In optional step 114, the labeled output image 113 can be further processed using the same or a different processor \\n programmed with facial recognition Software to identify the \\n individual whose face parts have been labeled by the inven \\n tive method. For Such processing, the marked locations will \\n be characterized as coordinates within the image, which will then be stored as an output. A face recognition system will use \\n the coordinates corresponding to the fiducial locations along with the input image for performing the recognition proce \\n dure. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content=\"the coordinates corresponding to the fiducial locations along with the input image for performing the recognition proce \\n dure. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n For purposes of this description, “facial features”, “face \\n parts”, “facial landmarks' and “parts' may be used inter \\n changeably to refer to a feature of a face. “Fiducial points' \\n refers to the location within an image, commonly seen as a \\n grid or array of pixels, at which the facial features, landmarks or parts may be found. \\n The dataset used for training and testing the inventive method was a collection of face images acquired from inter net search sites using simple text queries. More than a thou sand images were hand-labeled to create the dataset, which is referred to as “Labeled Face Parts in the Wild', or “LFPWA commercial, off-the-shelf (COTS) face detector was used to detect faces within the collected images. (No intentional fil tering was applied to exclude poor quality images.) Unlike datasets that are acquired systematically in laboratories, there were few preconditions in the LFPW dataset that would tend to make detection easier. Rather, images were included where the eyes might be occluded by glasses, Sunglasses, or hair; there may be heavy shadowing across features; the facial expression may be arbitrary; the face may have no makeup or be made up theatrically; the image may actually be an artistic rendering; the pose may be varied; there may be facial hair that occludes the fiducial points; and part of the face may be occluded by a hat, wall, cigarette, hand, or microphone. For example, FIGS. 3 and 6 illustrate a number of these condi tions. As a result, this dataset stands in contrast to datasets such as FERET or BioID which have been used for evaluating fiducial point detection in that the images are not restricted to frontal faces or collected in a controlled manner, e.g., using the same camera for all images. The inventive method consists of two basic processes. Given an input image containing a face, local detectors gen \\n erate detector scores for points in the image, where the score \\n provides an indication of the likelihood that a desired part is \\n located at a given point within the image. The detector scores \\n generated by the local detectors are then combined with a non-parametric prior model of a face shape. \\n For each local detector, a sliding window detector is scanned over a region of the image. The sliding window detectors may be a learning machine Such as a neural net work, linear or nonlinear support vector machine (SVM) or other learning machine. In a preferred embodiment, the detectors are SVM regressors with grayscale SIFT (Scale Invariant Feature Transform) descriptors as features. The SIFT descriptor window may be computed at two scales: \\n roughly /4 and /2 the inter-ocular distance. These two SIFT descriptors are then concatenated to form a single 256 dimen sional feature vector for the SVM regressor. In another embodiment, color SIFT is used to capture color variation of face parts. It will be recognized by practitioners with ordinary \\n skill in the art that there are other feature descriptors besides SIFT that could be used including histograms of gradients (HOG), color histograms, SURF ORB, Binary Robust Inde pendent Elementary Features (BRIEF), etc. It will also be recognized by those of ordinary skill in the art that other regressors that are trained with positive and negative training examples and take a feature descriptor as input and return a \\n score can be used. \\n For all of the training samples, the images were rescaled so \\n that the faces have an inter-ocular distance of roughly 55 pixels. Positive samples are taken at the manually annotated \\n part locations. Negative samples are taken at least 4 of the \\n inter-ocular distance away from the annotated locations. In addition, random image plane rotations within t20 are used to synthesize additional training samples.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content='inter-ocular distance away from the annotated locations. In addition, random image plane rotations within t20 are used to synthesize additional training samples. \\n These local detectors return a score at each point X in the image (or in some Smaller region of the face as inferred from \\n an earlier face detection step). The detector score d(x) indi \\n cates the likelihood that the desired part is located at point X in the image. This score is normalized to behave like a prob', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 14}), Document(page_content='US 9,275,273 B2 \\n 9 \\n ability by dividing by the sum of the scores in the detector \\n window. Once normalized, this score is written as P(xd), i.e., \\n the probability that the fiducial is at location X given all the \\n scores in the detection window. \\n As the local detectors are imperfect, the correct location \\n will not always be at the location with the highest detector \\n score. This can happen for many of the aforementioned rea \\n Sons, including occlusions due to head pose and visual \\n obstructions such as hair, glasses, hands, microphones, etc. \\n These mistakes in the local detector almost always happen at \\n places that are inconsistent with positions of the other, cor \\n rectly detected fiducial points. Nonetheless, the global detec \\n tors can be built to better handle the cases where the local \\n detectors are likely to go astray. \\n Although faces come in different shapes, present them \\n selves to the camera in many ways, and may possess extreme facial expressions, there are strong anatomical and geometric \\n constraints that govern the layout of face parts and their \\n location in images. These constraints are not modeled explic itly, but rather the training data dictates this implicitly. All the part locations are taken together to develop a global detector \\n for a collection offiducial points. A global model encodes the configuration of part locations. More formally, let X={x\\', x, ... x\"} denote the locations of n parts, where x\\' is the location of the i\\' part. Let D={d\\', d\\'. ... d\" the measured detector responses, where d\\' is the window of scores returned by the i\\' local detector. The goal is to find the value of X that maximizes the probability of X \\n given the measurements from the local detectors, i.e., \\n X* = argmax P(X D) (1) \\n X \\n Let X (where k=1,..., m) denote the locations of the n parts in the k\" of m exemplars, and let X be the locations of the parts in exemplark transformed by Some similarity trans \\n formation t. (Examples of similarity transformations include, \\n but are not limited to, reflection, rotation, translation, etc.) X is referred to as a “global model,” while k and t are the “hidden variables. \\n Assuming that each Xis generated by one of the global \\n models X, PCXD) can be expanded as follows: \\n i (2) P(X| D) =X IPX | X, D)P(X | D)dt sie \\n where the collection of m exemplars X along with similarity \\n transformations thave been introduced into the calculation of \\n PCXID) and then marginalized out. \\n By conditioning on the global model X, the locations of the parts x\\' can now be treated as conditionally independent \\n of one another and the first term of Eq. 2 can be rewritten as \\n (3) \\n 4 P(x; di) (4) \\n Since knowing the true location of the parts trumps any information provided by the detector, P(x,x)=P(x,x). 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n Also, since the relation between the transformed model fidu \\n cial and the true fiducial is translationally invariant, it should only depend on AX, -X-X\\'. With these observations, Eq.4 \\n can be rewritten as \\n P(X | X, D) = \\n Moving to the second term in Eq. 2, Bayes\\' rule can be used \\n to obtain \\n P(x,D) = f kit) (6) \\n P.X.) T = nil Pals. (7) \\n where again conditioning on the global model X, allows the detector responses d\\' to be treated as conditionally indepen \\n dent of one another. \\n A final application of Bayes\\' rule rewrites Eq. 7 as \\n \"Pips, \\n = CIP(x, Id) (9) \\n i=1 \\n Note that the terms within the square bracket in Eq. 8 that \\n depend only on Dare constant given the image. Also note that \\n the terms within the square bracket that depend only on X, \\n are also constant because a uniform distribution is assumed \\n on the global models. This allows all the terms within the \\n square bracket to be reduced to a single constant C. Combining Eqs. 1, 2, 5 and 9 yields', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 15}), Document(page_content='on the global models. This allows all the terms within the \\n square bracket to be reduced to a single constant C. Combining Eqs. 1, 2, 5 and 9 yields \\n where X* is the estimate for the part locations. The first term P(Ax) is taken to be a 2D Gaussian distri bution centered at the model location AX: though other probability distributions. Each part i has its own Gaussian \\n distribution. These distributions model how well the part \\n locations in the global model fit the true locations. If we had a large number of exemplars in our labeled dataset from \\n which to construct these global models—i.e., if m were very large—one would expect a close fit and low variances for \\n these distributions. The following steps are used to estimate \\n the covariance matrices for the part locations: For each exemplar X, from the labeled dataset of image exemplars, a sample X is obtained from the remaining exem \\n plars and a transformation t that gives the best La fit to X. Compute the difference X-X, and normalize the result by the inter-ocular distance. These normalized differences are \\n used to compute the covariance matrices for each part loca \\n tion.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 15}), Document(page_content='US 9,275,273 B2 \\n 11 \\n The second term P(x\\'ld)) is computed as follows. Take the estimated locationx for parti and lookup the response for the i\\' detector at that point in the image, i.e., d\\'(x). This value is then normalized to behave like a probability by dividing by \\n the sum of d\\'(x) for all x in the detector window. Computing the sum and integral in Eq. 10 is challenging, as \\n they are taken over all global models k and all similarity \\n transformations t. However, as noted from Eq. 2, if P(XID) is very small for a given k and t, it will be unlikely to contrib \\n ute much to the overall sum and integration. Thus, the strategy \\n is to consider only those global models k with transforma tions t for which P(XID) is large. In a sense, one performs a Monte Carlo integration of Eq. \\n 10 where the global models X chosen are those that are likely to contribute to the sum and integral. In the following, \\n the process is described for selecting a list of k and t that are used to compute this integration. \\n The goal is to optimize PCXID) over the unknownsk and t. This optimization is non-linear, and not amenable to gradi \\n ent descent-type algorithms. First, k is a discrete variable with \\n a large number of possible values. Second, even for a fixed k, different values oft can be expected to produce large numbers of local optima because the fiducial detectors usually produce \\n a multi-modal output. Transformations that align a model \\n with any subsets of these modes are likely to produce local optima in the optimization function. \\n To address this issue, a generate-and-test approach similar \\n to RANSAC (RANdom SAmple Consensus) can be used by generating a large number of plausible values for the hidden \\n variables k and t. Each of these values is evaluated using Eq. \\n 9, keeping track of them best global models, i.e., them best pairs k and t. This is done in the following steps: \\n 1. Select a random k. \\n 2. Select two random parts. Randomly match each model part to one of the ghighest peaks of the detector output \\n (i.e., highest detector scores) for that part. \\n 3. Set t to be the similarity transformation that aligns the two model fiducial points with two matching peaks from \\n Step 2. \\n 4. Evaluate Eq.9 for this k, t. 5. Repeat Steps 1 to 4 r times. \\n 6. Record in a set -- the m pairs k and t for which Eq.9 in Step 4 is largest. \\n In the experimental system, the values r-10,000, g=2, and \\n m=100 were used. \\n Estimating X \\n In the previous subsection, a RANSAC-like procedure was \\n used to find a list v? of m global models X for which P(XID) is largest. With these in hand, an approximate opti mization for X in Eq. 10 is \\n where the sum is now only taken over those k, te v. To find the best X*, first find an initial estimate X\" for each partias \\n k,te Wi \\n This is equivalent to solving for Xo by setting all P(AX.) and P(x\\'id) to a constant in Eq. 10 for allizi. To compute each 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n Xo, multiply the detector output by a Gaussian function cen \\n tered at x\\', with the covariances calculated as described above. Next, find the image location X, where the sum of the resulting products is maximized. The initial estimates, X\\', ie1 ... n can then be used to initialize an optimization of Eq. 11 to find the final estimates x* that make up X*. For some uses of the fiducial output, X\\', ie 1 . . . n may be \\n Sufficiently accurate estimates of the fiducial location and further optimization may be unnecessary. \\n It will be recognized by one of ordinary skill in the art that \\n the method and system for localizing parts of faces can be applied for localizing parts of many other object classes. The \\n method described hereintakes as training input a collection of images with marked part locations and then given a new \\n image, the method will find the location of the parts in the new image. Thus, the method could be applied to find face parts of', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 16}), Document(page_content='image, the method will find the location of the parts in the new image. Thus, the method could be applied to find face parts of \\n non-humans such as cats, dogs and other animals, as well as parts of inanimate objects, as well as for identification of distinct features within Some other image. For example, indi vidual humpback whales are identified by color patterns, \\n notches and Scars on their tails, so a parts locator could assist in computer-aided identification of a whale within an image \\n where angle, rotation and other variables may make it difficult to locate the body parts, especially given the short periods of \\n time during which the whales are visible Rhinoceros in the \\n wild are identified by features such as ear notches, horn characteristics, and tail length and curvature, so the ability to automatically locate these body parts within a photograph of \\n these shy animals would be helpful for conservation studies. \\n In another example, the inventive method may be used to find parts of a bicycle Such as the front hub, rear hub, peddles, seat, \\n left handlebar, right handlebar, crank arms, and brake levers. The method would be trained with images of bicycles where \\n the locations of these parts are marked. Local detectors are trained in the same way and the global model can be directly \\n used. The inventive method can be applied to find parts of many other objects of interest for use in various computer vision applications. \\n EXAMPLES \\n The present invention focuses on localizing parts in natural face images, taken under a wide range of poses, lighting \\n conditions, and facial expressions, in the presence of occlud ing objects such as Sunglasses or microphones. Existing \\n datasets for evaluating part localization do not contain the range of conditions. \\n Since researchers have recently reported results on BioID, the results produced by the present invention are compared to \\n prior results on BioID. Like most datasets used to evaluate part localization on face images, BioID contains near-frontal \\n views and less variation in viewing conditions than LFPW. \\n LFPW consists of 3,000 faces from images downloaded from the web using simple text queries on sites such as \\n google.com, flickr.com, and yahoo.com. The 3,000 faces \\n were detected using a commercial, off-the-shelf (COTS) face detection system. Faces were excluded only if they were \\n incorrectly detected by the COTS detector or if they con \\n tained text on the face. Note also that the COTS face detector \\n does not detect faces in or near profile, and so these images are implicitly excluded from the dataset. To obtain ground truth data, thirty-five fiducial points on \\n each face were labeled by workers on Amazon Mechanical Turk (MTurk), a crowdsourcing Internet marketplace that \\n enables computer programmers to coordinate the use of human intelligence to perform tasks that computers are cur \\n rently unable to do. The fiducial points are as follows:', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 16}), Document(page_content='US 9,275,273 B2 \\n 13 \\n left eyebrow out right eyebrow out \\n left eyebrow in right eyebrow in \\n left eyebrow center top \\n left eyebrow center bottom right eyebrow center top \\n right eyebrow center bottom \\n left eye out right eye out \\n left eye in right eye in \\n left eye center top \\n left eye center bottom right eye center top \\n right eye center bottom left eye pupil right eye pupil \\n left nose out \\n right nose out \\n nose center top \\n nose center bottom \\n left mouth out \\n right mouth out \\n mouth center top lip top \\n mouth center top lip bottom \\n mouth center bottom lip top \\n mouth center bottom lip bottom left ear top right ear top \\n left ear bottom \\n right ear bottom \\n left ear canal \\n right ear canal \\n chin \\n Of these third-five points, only twenty-nine were used in \\n the example shown here—the six points associated with the \\n ears were excluded. FIG. 2 illustrates the location of the 29 \\n points. Each point was labeled by three different MTurk workers. The average location was used as ground truth for \\n the fiducial point. FIG. 6 shows example images from LFPW, along with the \\n results. There is a degree of Subjectivity in the way humans \\n label the location of fiducial points in the images, and this is \\n seen in FIG.4, which shows the variation amongst the MTurk \\n workers. Some parts like the eye corners are more consis \\n tently labeled whereas the brows and chin are labeled less accurately. \\n The publicly available BioID dataset contains 1,521 images, each showing a frontal view of a face of one of 23 \\n different subjects. Seventeen fiducial points that had been \\n marked for the FGNet project were used, and the me, error \\n measure as defined by Cristinacce and Cootes was used to compare detected locations from ground truth locations. This \\n dataset has been widely used, allowing the results to be benchmarked with prior work. Note that training was per \\n formed using the LFPW dataset, and while testing was done \\n using the BioID data. There are considerable differences in the viewing conditions of these two datasets. Furthermore, \\n the locations of parts in LFPW do not always match those of BioID, and so a fixed offset was computed between parts that were defined differently. For example, where the left and right \\n nose points are outside of the nose in LFPW, they are below the nose in BioID). FIG. 7 shows some sample images, along \\n with the results obtain using the inventive method. The LFPW dataset was randomly split into 1,100 training \\n images and 300 test images. (An additional 1,600 images 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n were held out for subsequent evaluations at future dates.) Training images were used to train the SVM-based fiducial detectors and served as the exemplars for computing the \\n global models X. \\n The results of each localization were evaluated by measur ing the distance from each localized part to the average of \\n three locations supplied by MTurk workers. Error is mea \\n Sured as a fraction of the inter-ocular distance, to normalize for image size. FIG. 4 shows the resulting error broken down by part. This figure also compares the error in the inventive \\n method to the average distance between points marked by one MTurk worker and the average of the points marked by the \\n other two. As shown, this distance almost always exceeds the distance from points localized by the inventive approach to \\n the average of the points marked by humans. It is worth noting \\n that the eye points (9-18) are the most accurate, the nose and mouth points (19-29) are slightly worse, and the chin and eye \\n brows (1-8, 29) are least accurate. This trend is consistent between human and automatic labeling. FIGS. 3 and 6 show results on some representative images. \\n To highlight a few characteristics of these results, these images include non-frontal images including viewpoints \\n from below (FIG.3: Row 1, Col. 2 and FIG. 6: Row 2, Col. 2), difficult lighting (FIG. 6: Row 4, Col. 1), glasses (FIG. 6: Row', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 17}), Document(page_content='from below (FIG.3: Row 1, Col. 2 and FIG. 6: Row 2, Col. 2), difficult lighting (FIG. 6: Row 4, Col. 1), glasses (FIG. 6: Row \\n 1, Col. 5), sunglasses (FIG. 6: Row 2, Col. 4 and FIG. 6: Row 4, Col. 3), partial occlusion (FIG. 6: Row 2, Col. 5 by a pipe \\n and FIG. 6: Row 3, Col. 4 by hair), an artist drawing (FIG. 6: \\n Row 1, Col. 3), theatrical makeup (FIG. 6: Row 2, Col. 1), etc. \\n The localizer requires less than one second per fiducial on an \\n INTEL(R) Core i7 3.06 GHz machine; most of the time is spent evaluating the local detectors. \\n FIG.5 provides a comparison of the LFPW results from the \\n inventive method against those of the detector of Everingham \\n et al. At roughly 3% mean error rate, the results for the \\n inventive method are roughly twice as accurate as those of the prior art detector. \\n FIG. 6 shows a few examples of errors in the inventive \\n system. In Row 1, Cols. 2 and 5, local cues for the chin are \\n indistinct, and the chin is not localized exactly. Row 2, Col. 4 shows an example in which the lower lip is incorrectly local \\n ized. This can happen when the mouth is open and a row of \\n teeth are visible. It is believed that these errors can be prima \\n rily attributed to the local detectors and should be overcome by employing color-based representations that can more eas \\n ily distinguish between lips and teeth. In Row 4, Col. 1, the \\n left corner of the left eyebrow is too low, presumably due to \\n occlusion from the hair. \\n FIG. 7 illustrates results from the application of the inven \\n tive part localizer to the BioID faces. Results have been reported on this dataset by a number of authors. FIG. 8 shows \\n the cumulative error distribution of the me, error measure (mean error of 17 fiducials) defined by Cristinacce and Cootes, and compares the results of the inventive method to those reported by Cristinacce and Cootes, Milborrow and \\n Nicolls, Valstaret al., and Vukadinovic and Pantic. The results \\n of the inventive part localizer are similar to, but slightly better \\n than, those of Valstar et al., who have reported the best current \\n results on this dataset. It should be noted that training of the inventive system occurred using a very different dataset \\n (LFPW), and that locations of some fiducials were defined a bit differently. \\n FIG.9 shows the cumulative error distribution of the me, error measure for the inventive method applied to LFPW. Even though LFPW is a more challenging dataset, the cumu \\n lative error distribution curve on LFPW is almost identical to \\n the cumulative error distribution curve on BioID. (Note that the two figures have different scales along the x-axis.) FIG.9', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 17}), Document(page_content='US 9,275,273 B2 \\n 15 \\n also shows the cumulative error distribution when only the \\n local detectors are used and when locations are predicted \\n solely from the face box. While the local detectors are effec tive for most fiducial points, there is a clear benefit from using the consensus of global models. Many of the occluded fidu cial points are incorrectly located by the local detectors, as \\n evidenced by the slow climb toward 1.0 of the local detectors \\n CUV. \\n The inventive method provides a new approach to localiz ing parts in face images. The method utilizes a Bayesian \\n model that combines local detector outputs with a consensus of non-parametric global models for part locations, computed \\n from exemplars. The inventive parts localizer is accurate over a large range of real-world variations in pose, expression, lighting, makeup, and image quality, providing a significant \\n improvement over the limitations of existing approaches. \\n REFERENCES \\n 1. 1st Intl. Workshop on Parts and Attributes. 2010. 546 \\n 2. L. Bourdev and J. Malik. Poselets: body part detectors \\n trained using 3d human pose annotations. In IEEE Confer ence on Computer Vision and Pattern Recognition, page \\n 1365 1372, 2009. 546 \\n 3. M. Burl, T. Leung, and P. Perona. Face localization via shape statistics. In Workshop on Automatic Face and Ges \\n ture Recognition, 1995. 546 \\n 4. P. Campadelli, R. Lanzarotti, and G. Lipori. Automatic \\n facial feature extraction for face recognition. In Face Rec ognition. I-Tech Education and Publishing, 2007. 546 \\n 5. D. Cristinacce and T. Cootes. Feature detection and track \\n ing with constrained local models. In BMWC, pages 929 \\n 938, 2006. 546, 549, 550 \\n 6. D. Cristinacce, T. Cootes, and I. Scott. A multi-stage approach to facial feature detection. In BMWC, pages 231 \\n 240, 2004. 546 \\n 7. L. Ding and A. M. Martinez. Precise detailed detection of faces and facial features. In IEEE Computer Vision and Pattern Recognition (CVPR), 2008. 546 \\n 8. M. Eckhardt, I. Fasel, and J. Movellan. Towards practical \\n facial feature detection. Int. J. of Pattern Recognition and Artificial Intelligence, 23(3):379-400, 2009. 546 \\n 9. M. Everingham, J. Sivic, and A. Zisserman. “Hello! My \\n name is ... Buffy’ automatic naming of characters in TV \\n video. In BMVC, 2006. 546, 547, 550 \\n 10. N. Gourier, D. Hall, and J. L. Crowley. Facial features detection robust to pose, illumination and identity. In Int. Confon Systems, Man and Cybernetics, 2004. 546 11. L. Gu and T. Kanade. A generative shape regularization \\n model for robust face alignment. In European Conference \\n on Computer Vision (ECCV), pages 413-426, 2008. 546 \\n 12. E. Holden and R. Owens. Automatic facial point detec tion. In Asian ConfComputer Vision, pages 731-736, 2002. \\n 546 \\n 13. Jesorsky, K. J. Kirchberg, and R. W. Frischholz. Robust \\n face detection using the Hausdorff distance. In Confon \\n Audio-and Video-Based Biometric Person Authentication, pages 90-95. Springer, 2001. 549 \\n 14. N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. \\n Attribute and simile classifiers for face verification. In \\n IEEE International Conference on Computer Vision, 2009. \\n 546 \\n 15. B. Leibe, A. Ettlin, and B. Schiele. Learning semantic object parts for object categorization. Image and Vision \\n Computing, 26:15-26, 1998. 546 \\n 16. D. Lowe. Distinctive image features from scale-invariant keypoints. Intl. Journal of Computer Vision, 2003. 547 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n 17. S. Milborrow and F. Nicolls. Locating facial features with \\n an extended active shape model. In European Conf On \\n Computer Vision, pages 504-513, 2008. 546, 550 \\n 18. M. Reinders, R. W. C. Koch, and J. Gerbrands. Locating facial features in image sequences using neural networks. \\n In Confon Automatic Face and Gesture Recognition, pages \\n 230-235, 1997.546 19. J. M. Saragih, S. Lucey, and J. Cohn. Face alignment \\n through Subspace constrained mean-shifts. In Interna', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 18}), Document(page_content='In Confon Automatic Face and Gesture Recognition, pages \\n 230-235, 1997.546 19. J. M. Saragih, S. Lucey, and J. Cohn. Face alignment \\n through Subspace constrained mean-shifts. In Interna \\n tional Conference of Computer Vision (ICCV), September \\n 2009. 546 \\n 20. M. Valstar, B. Martinez, X. Binefa, and M. Pantic. Facial \\n point detection using boosted regression and graph models. \\n In IEEE Computer Vision and Pattern Recognition \\n (CVPR), pages 2729-2736, 2010. 546, 547, 550, 552 \\n 21. P. Viola and M. Jones. Robust real-time face detection. \\n Intl. Journal of Computer Vision, 57:137-154, 2004. 545, \\n 546 \\n 22. D. Vukadinovic and M. Pantic. Fully automatic facial \\n feature point detection using Gabor feature based boosted \\n classifiers. In Int. Confon Systems, Man and Cybernetics, \\n pages 1692-1698, 2005. 546, 547, 550 23. M.-H. Yang, D. J. Kriegman, and N. Ahuja. Detecting \\n faces in images: A Survey. IEEE Trans. On Pattern Analysis \\n and Machine Intelligence, 24(1):34-58, 2002. 545 \\n 24. C. Zhan, W. Li, P. Ogunbona, and F. Safaei. Real-time \\n facial feature point extraction. In Advances in multimedia information processing, pages 88-97. Springer-Verlag, \\n 2007. 546 \\n The invention claimed is: \\n 1. A system for localizing parts of an object in an input \\n image, the System comprising: \\n at least one computing device configured to: \\n train a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, \\n wherein each image exemplar is labeled with fiducial points corresponding to parts within the image, and \\n wherein each local detector generates a detector score when applied at one location of a plurality of locations of fiducial points in the training images corresponding to a \\n likelihood that a desired part is located at the location within the training image: \\n generate a non-parametric model of the plurality of loca \\n tions of the fiducial points in each of at least a portion of the plurality of image exemplars, the non-parametric \\n model denoting locations of parts within each of the at least a portion of the plurality of image exemplars; \\n receive data corresponding to an input image; \\n apply the trained local detectors to the input image to generate detector scores for the input image; derive a Bayesian objective function for the input image \\n from the non-parametric model and detector scores by using an assumption that locations of the fiducial points \\n in each of the at least a portion of the plurality of image exemplars are represented within the non-parametric \\n model as hidden variables; and generate an output comprising locations of the fiducial \\n points within the object in the image based on results of the Bayesian objective function for the input image. \\n 2. The system of claim 1 wherein the object is a face. 3. The system of claim 1, wherein generating the output \\n comprising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image further comprises:', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 18}), Document(page_content='US 9,275,273 B2 \\n 17 \\n optimizing the Bayesian objective function to obtain a \\n consensus set of one or more global models for the \\n hidden variables that best fits the data corresponding to the image. \\n 4. The system of claim 3, wherein deriving the Bayesian objective function for the input image from the non-paramet \\n ric model and detector scores further comprises: Selecting a random image exemplar from the plurality of image exemplars; \\n Selecting from within the random image exemplar at least \\n two random fiducial points; applying a similarity transform to align the at least two \\n random fiducial points with peaks of the detector output; evaluating a fit of the random image exemplar and similar \\n ity transform to the data; and repeating the steps of selecting a random image exemplar, Selecting at least two random fiducial points, applying a similarity transform and evaluating a fit for a plurality of \\n iterations until a desired fit to the data is obtained. \\n 5. The system of claim 1, wherein the plurality of local detectors comprise sliding window detectors. 6. The system of claim 5, wherein the sliding window detectors comprise support vector machines. 7. The system of claim 5, wherein the sliding window detectors use features comprising scale invariant feature transform descriptors. 8. The system of claim 1, wherein generating the output comprises displaying on a monitor or printout the image of the object with markings indicating the fiducial points within the input image. 9. The system of claim 1, wherein generating the output comprises storing the output in a memory for further process ing by an image recognition system. \\n 10. The system of claim 3, wherein the consensus set contains only one global model for each of the hidden vari \\n ables. \\n 11. The system of claim 1, wherein the input image com prises features in addition to the object and further compris ing pre-processing the input image to select and extract an area within the input image which contains only the object. 12. A method for localizing parts of an object in an input image, the method comprising: training, using at least one processor, a plurality of local detectors using at least a portion of a plurality of image exemplars as training images, wherein each image exemplar is labeled with fiducial points corresponding \\n to parts within the image, and wherein each local detec tor generates a detector score when applied at one loca tion of a plurality of locations of fiducial points in the training images corresponding to a likelihood that a \\n desired part is located at the location within the training image; \\n generating, using the at least one processor, a non-paramet \\n ric model of the plurality of locations of the fiducial points in each of at least a portion of the plurality of image exemplars, the non-parametric model denoting 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 18 \\n locations of parts within each of the at least a portion of the plurality of image exemplars; receiving data corresponding to an input image: applying, using the at least one processor, the trained local detectors to the input image to generate detector scores for the input image: deriving, using the at least one processor, a Bayesian objec tive function for the input image from the non-paramet ric model and detector scores by using an assumption that locations of the fiducial points in each of the at least a portion of the plurality of image exemplars are repre sented within the non-parametric model as hidden vari \\n ables; and generating, using the at least one processor, an output com prising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image. \\n 13. The method as recited in claim 12, wherein the object is \\n a face.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 19}), Document(page_content='13. The method as recited in claim 12, wherein the object is \\n a face. \\n 14. The method as recited in claim 12, wherein generating the output comprising locations of the fiducial points within the object in the image based on results of the Bayesian objective function for the input image further comprises opti mizing the Bayesian objective function to obtain a consensus \\n set of one or more global models for the hidden variables that best fits the data corresponding to the image. 15. The method as recited in claim 14, whereinderiving the Bayesian objective function for the input image from the non-parametric model and detector scores further comprises: Selecting a random image exemplar from the plurality of image exemplars; \\n Selecting from within the random image exemplar at least \\n two random fiducial points: applying a similarity transform to align the at least two random fiducial points with peaks of the detector output; evaluating a fit of the random image exemplar and similar \\n ity transform to the data; and repeating the steps of selecting a random image exemplar, \\n selecting at least two random fiducial points, applying a similarity transform and evaluating a fit for a plurality of \\n iterations until a desired fit to the data is obtained. \\n 16. The method as recited inclaim 12, wherein the plurality of local detectors comprise sliding window detectors. 17. The method as recited in claim 16, wherein the sliding window detectors comprise Support vector machines. \\n 18. The method as recited in claim 16, wherein the sliding window detectors use features comprising scale invariant fea ture transform descriptors. \\n 19. The method as recited in claim 14, wherein the consen Sus set contains only one global model for each of the hidden \\n variables. \\n 20. The method as recited in claim 12, wherein the input image comprises features in addition to the object and further comprising pre-processing the input image to select and \\n extract an area within the input image which contains only the object.', metadata={'source': 'https://patentimages.storage.googleapis.com/a2/95/38/bcc3b288e56669/US9275273.pdf', 'page': 19})]\n",
      "[Document(page_content='(12) United States Patent \\n Cheng et al. USOO9432632B2 \\n US 9.432,632 B2 \\n Aug. 30, 2016 (10) Patent No.: \\n (45) Date of Patent: \\n (54) ADAPTIVE MULTI-MODAL INTEGRATED \\n BOMETRIC IDENTIFICATION AND \\n SURVELLANCE SYSTEMS \\n (71) Applicant: Proximex Corporation, Cupertino, CA \\n (US) \\n (72) Inventors: Ken P. Cheng, Saratoga, CA (US); \\n Edward Y. Chang, Santa Barbara, CA (US); Yuan-Fang Wang, Goleta, CA \\n (US) \\n (73) Assignee: Proximex Corporation, Cupertino, CA \\n (US) \\n Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 0 days. \\n (21) Appl. No.: 14/607,201 \\n (22) Filed: Jan. 28, 2015 \\n (65) Prior Publication Data \\n US 2015/O138332 A1 May 21, 2015 \\n Related U.S. Application Data \\n (60) Division of application No. 13/738,655, filed on Jan. \\n 10, 2013, now Pat. No. 8,976,237, and a continuation of application No. 13/101,149, filed on May 5, 2011, \\n now Pat. No. 8.373,753, and a division of application (*) Notice: \\n (Continued) \\n (51) Int. Cl. \\n H04N 7/8 (2006.01) \\n A6 IB I/00 (2006.01) \\n (Continued) \\n (52) U.S. Cl. \\n CPC H04N 7/18 (2013.01); G06K 9/00 (2013.01); G06K 9/00288 (2013.01); \\n (Continued) \\n (58) Field of Classification Search \\n USPC ........... 348/77; 340/506; 358/143, 147, 161, \\n 358/169; 707/4, 103: 382/103, 209, 276, 382/277, 289, 291, 293, 294, 295, 282,305, \\n 382/115, 107, 190 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 5,258,837 A \\n 5,473,369 A 11/1993 Gormley \\n 12, 1995 Abe \\n (Continued) \\n FOREIGN PATENT DOCUMENTS \\n WO 2007/044037 A1 4/2007 \\n OTHER PUBLICATIONS \\n PCT/US05/44656 International Search Report and Written Opinion, \\n Jun. 26, 2006. \\n (Continued) \\n Primary Examiner — Jerome Grant, II \\n (74) Attorney, Agent, or Firm — Dean D. Small: The Small \\n Patent Law Group, LLC. \\n (57) ABSTRACT \\n A Surveillance system is provided that includes at least one sensor disposed in a security area of a Surveillance region to sense an occurrence of a potential security breach event; a plurality of cameras is disposed in the Surveillance region; at \\n least one camera thereof has a view of the security area and can be configured to automatically gather biometric infor mation concerning at least one subject person in the vicinity of the security area in response to the sensing of a potential \\n security breach event; one or more other of the plurality of cameras can be configured to search for the at least one Subject person; a processing system is programmed to \\n produce a dossier corresponding to the at least one subject \\n person to match biometric information of one or more persons captured by one or more of the other cameras with corresponding biometric information in the dossier. \\n 35 Claims, 8 Drawing Sheets \\n Apply Monitoring Rule on Security Area \\n configure device and Security Area specific parameters (Door, \\n restricted area, etc.) \\n Environment \\n (Map) Admin \\n Security Area \\n Admin display a sist of available Monitoring rules to apply to security area (Based on device characteristics Display a list of available devices to selecticeselect for applying Monitoring rule Select Security Area to configure Monitoring rule \\n Monitoring rule (Security Function) \\n Admin Configure Schedule & \\n Eable Rule to start monitoring configured \\n area Setect Cotstation Monitoring rule', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 0}), Document(page_content='US 9,432.632 B2 \\n Page 2 \\n (60) \\n (51) \\n (52) \\n (56) Related U.S. Application Data \\n No. 1 1/231,353, filed on Sep. 19, 2005, now Pat. No. \\n 7.956,890. \\n Provisional application No. 60/610,998, filed on Sep. \\n 17, 2004. \\n Int. C. \\n G06K 9/00 (2006.01) \\n G06K 9/62 (2006.01) \\n G08B I3/196 (2006.01) \\n G06T 7/40 (2006.01) \\n U.S. C. \\n CPC ..... G06K 9/00771 (2013.01); G06K 9/00885 \\n (2013.01); G06K 9/00892 (2013.01); G06K 9/6293 (2013.01); G06T 7/408 (2013.01); \\n G08B 13/196 (2013.01); H04N 7/181 (2013.01) \\n References Cited \\n U.S. PATENT DOCUMENTS \\n 5,479,574 A 12/1995 Glier et al. \\n 5,701,398 A 12/1997 Glier et al. \\n 5,769,074 A 6, 1998 Barnhill et al. \\n 5,835,901 A 11/1998 Duvoisin et al. \\n 5,838,465. A 11/1998 Satou et al. \\n 5,912,980 A 6, 1999 Hunke \\n 6,008,912 A 12/1999 Sato et al. \\n 6,072,496 A 6, 2000 Guenter et al. \\n 6,157.469 A 12/2000 Mestha \\n 6,248,063 B1 6, 2001 Barnhill et al. \\n 6,306,087 B1 10/2001 Barnhill et al. 6,335,985 B1 1/2002 Sambonsugi et al. \\n 6,404,900 B1 6/2002 Qian et al. 6,408.404 B1 6/2002 Ladwig \\n 6,591.224 B1 7/2003 Sullivan et al. \\n 6,604,093 B1 8, 2003 Etzion et al. \\n 6,609, 198 B1 8, 2003 Wood et al. \\n 6,625,315 B2 9/2003 Laumeyer et al. \\n 6,628,829 B1 9, 2003 Chasen \\n 6,697,103 B1 2, 2004 Fernandez et al. \\n 6,707.487 B1 3/2004 Aman et al. \\n 6,711,587 B1 3, 2004 Dufaux \\n 6,757,668 B1 6, 2004 Goebel et al. \\n 6,906,709 B1 6, 2005 Larkin et al. \\n 6,909,745 B1 6, 2005 Puri et al. \\n 6,944,319 B1 9/2005 Huang et al. \\n 6,963,899 B1 1 1/2005 Fernandez et al. 6,968,006 B1 11/2005 Puri et al. 6,970,513 B1 11/2005 Puri et al. 6,970,582 B2 11/2005 Langley 6,976.207 B1 12/2005 Rujan et al. \\n 7,023.913 B1 4/2006 Monroe et al. \\n 7,028,034 B2 4/2006 Wesinger et al. \\n 7,058,204 B2 6, 2006 Hildreth et al. \\n 7,116,353 B2 10/2006 Hobson et al. 7,127.464 B2 10/2006 Wesinger et al. \\n 7,136,524 B1 1 1/2006 Goh et al. \\n 7,187,783 B2 3/2007 Moon et al. \\n 7,196,722 B2 3/2007 White et al. \\n 7,213,174 B2 5/2007 Dahlquist et al. \\n 7,242.423 B2 7, 2007 Lin \\n 7,242,810 B2 7/2007 Chang 7,269,591 B2 9/2007 Wesinger et al. \\n 7,277,485 B1 10/2007 Puri 7.286,687 B2 10/2007 Lindwurm et al. 7,295.228 B2 11/2007 Roberts et al. 7,319,796 B1 1/2008 Sharp \\n 7.366,174 B2 4/2008 MacFaden et al. \\n 7,382.249 B2 6, 2008 Fancella \\n 7,384,590 B2 6/2008 Kelly et al. \\n 7,385,590 B2 6, 2008 Millar et al. 7,412,112 B2 * 8/2008 Yamasaki .......... GO6K9/00785 \\n 345,626 \\n 7,456,727 B2 11/2008 Pinter et al. 7,522, 186 B2 4/2009 Arpa et al. \\n 7,525,576 B2 4/2009 Kannermark et al. \\n 7,596,240 B2 * 9/2009 Ito .................... G08B 13, 196O2 \\n 382,103 \\n 7,746,380 B2 6/2010 Maruya et al. \\n 7,777,783 B1 8, 2010 Chin et al. \\n 8,373,753 B2 * 2/2013 Cheng ................ GO6K9/00771 \\n 348/143 \\n 2001/0048765 A1 12/2001 Yi et al. \\n 2002fOO97322 A1 7/2002 Monroe et al. \\n 2002/0138768 A1 9/2002 Murakami et al. \\n 2002/O1901 19 A1 12/2002 Huffman \\n 2003/0107653 A1 6/2003 Utsumi et al. \\n 2003. O1699.08 A1 9, 2003 Kim et al. \\n 2003/0202102 A1 10, 2003 Shiota et al. \\n 2004/0001149 A1 1/2004 Smith \\n 2004/0022442 A1 2/2004 Kim \\n 2004/0081338 A1 4/2004 Takenaka \\n 2004/0100563 A1 5, 2004 Sablak et al. \\n 2004/01 17638 A1* 6/2004 Monroe ............. GO6K9/00221 \\n T13, 186 \\n 2004/O136574 A1 7/2004 Kozakaya et al. \\n 2005/0052532 A1 3/2005 ElooZ. et al. \\n 2005, 0100209 A1 5/2005 Lewis et al. \\n 2005, 0132414 A1 6/2005 Bentley et al. \\n 2005/022281.0 A1 10, 2005 Jakobson et al. \\n 2005/0265607 A1 12/2005 Chang \\n 2006, OO17807 A1 1/2006 Lee et al. \\n 2006/O112039 A1 5/2006 Wang et al. \\n 2006/0203090 A1 9/2006 Wang et al. \\n 2006/0221 184 A1 10, 2006 Vallone et al. \\n 2006/0279628 A1 12/2006 Fleming 2006/0284978 A1 12/2006 Girgensohn et al. \\n 2007, 0146484 A1 6/2007 Horton et al. \\n 2007. O154088 A1 7/2007 Goh et al. \\n 2008.00684-64 A1 3/2008 Kitagawa et al. \\n 2008.OO795.54 A1 4/2008 Boice \\n 2008, OO887O6 A1 4/2008 Girgensohn et al. \\n 2008/0106597 A1 5/2008 Amini et al. \\n 2008. O130949 A1 6/2008 Ivanov et al. \\n 2008, 0218590 A1 9, 2008 Park et al.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 1}), Document(page_content='2008.OO795.54 A1 4/2008 Boice \\n 2008, OO887O6 A1 4/2008 Girgensohn et al. \\n 2008/0106597 A1 5/2008 Amini et al. \\n 2008. O130949 A1 6/2008 Ivanov et al. \\n 2008, 0218590 A1 9, 2008 Park et al. \\n 2008/0294.588 A1 11/2008 Morris et al. \\n 2009/0322873 A1* 12/2009 Reilly ................ GOS 7,411 \\n 348/143 \\n 2010.0002082 A1 1/2010 Buehler et al. \\n OTHER PUBLICATIONS \\n PCT/US05/43808 International Search Report and Written Opinion, \\n Oct. 10, 2007. \\n PCT/US05/33378 International Search Report and Written Opinion, \\n Apr. 26, 2006. \\n PCT/US05/33750 International Search Report and Written Opinion, \\n May 2, 2007. \\n PCT/US05/16961 International Search Report and Written Opinion, \\n Oct. 17, 2006. \\n VidSys. Inc. Complaint filed in the US District Court for the Eastern \\n district of Virginia on Oct. 19, 2010. \\n Redstone Integrated Solutions Documents, 2003, Dec. 2003. \\n 95/001,525 Reexamination Request for 7,777,783, filed Jan. 21, \\n 2011. \\n Goh et al., “Robust Perceptual Color Identification\\' U.S. Appl. No. \\n 11/229,091, filed Sep. 16, 2005. \\n Dec. 1997, Belhumeur, A., et al. (1997), “Eigenfaces vs. Fisherfaces: recognition using class specific linear projection\\'. \\n IEEE Transactions on Pattern Analysis and Machine Intelligence \\n 19(7): 711-720. \\n Brunei, R. and D. Falavigna (1995), \"Person Identification using \\n multiple clues.” IEEE Transactions on Pattern Analysis and \\n Machine Intelligence 17(10): 955-966, Dec. 1995.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 1}), Document(page_content='US 9,432.632 B2 \\n Page 3 \\n (56) References Cited \\n OTHER PUBLICATIONS \\n Brunelli, R., et al. (1995), “Automatic Person Recognition by Using Acoustic and Geometric Features\\'. Machine Vision and Applica \\n tions 8:317-325, Dec. 1995. Hong, Lin and Anil K. Jain. (1998). “Integrating faces and finger prints for personal identification.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20(12): 1295-1307. International Search Report mailed on Apr. 2006 for PCT Patent Application No. PCT/US05/33378 filed on Sep. 19, 2005, one page. Jain, A.K., et al., (1997). \"On-Line fingerprint Verification\\', IEEE Transactions on Pattern Analysis and Machine Intelligence archive 19{4): 302-314, Dec. 1997. Kittler, J. et al. (1998). “On Combining classifiers\\', IEEE Trans actions on Pattern Analysis and Machine Intelligence 20 (3): \\n 226-239, Dec. 1998. Lu X et al. (2003). “Combing Classifiers for face recognition\\'. \\n IEEE International Conference on Multimedia Systems and Expo, \\n Baltimore, MD, Jul. Dec. 2003. \\n Maio, D. et al. (2002). “FVC2000: fingerprint verification compe \\n tition\\', IEEE Transactions on Pattern Analysis and Machine Intel \\n ligence 24(3): 402-412, Dec. 2002. \\n Phillips, P.J. et al. (2000). “The FERET evaluation methodology for \\n face-recognition algorithms\\'. IEEE Transactions on Pattern Analy \\n sis and Machine Intelligence 22(10): 1090-1104, Dec. 2000. \\n Senior, A. (2001). A combination fingerprint classifier. IEEE Trans \\n actions on Pattern Analysis and Machine Intelligence 23(10): 1165 \\n 1174, Dec. 2001. \\n Turk, A. and A. Pentland. (1991). “Eigenfaces for Recognition\\'. \\n Journal of Cognitive Neuroscience 3 (1): 71-86. \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 2}), Document(page_content='US 9,432,632 B2 Sheet 2 of 8 Aug. 30, 2016 U.S. Patent \\n(u?upy Queuuuou?AuE) Sde N eun6??uOO', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 4}), Document(page_content='US 9,432,632 B2 Sheet 3 of 8 Aug. 30, 2016 U.S. Patent \\neeuw K??unoes uo a[nx] 6u?uo??uOIN ÁIddy', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 5}), Document(page_content='US 9,432,632 B2 Sheet 4 of 8 Aug. 30, 2016 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 6}), Document(page_content='US 9,432,632 B2 Sheet 6 of 8 Aug. 30, 2016 U.S. Patent \\n ••?: ( ) … Jessam, \\n ****', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 8}), Document(page_content='US 9,432,632 B2 Sheet 7 of 8 Aug. 30, 2016 U.S. Patent \\n Z ‘61-I \\n–', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 9}), Document(page_content='US 9,432,632 B2 Sheet 8 of 8 Aug. 30, 2016 U.S. Patent \\n ; - & ) {}} }} \\n K', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 10}), Document(page_content='US 9,432,632 B2 \\n 1. \\n ADAPTIVE MULT-MODAL INTEGRATED \\n BIOMETRIC IDENTIFICATION AND \\n SURVELLANCE SYSTEMS \\n CROSS REFERENCE TO RELATED \\n APPLICATIONS \\n This application is a divisional application and claims \\n priority to and the benefit of the filing date of U.S. continu ation patent application Ser. No. 13/738,655, filed Jan. 10, \\n 2013, and entitled \"Adaptive Multi-Modal Integrated Bio \\n metric Identification Detection and Surveillance Systems.” which is a continuation patent application of Ser. No. \\n 13/101,149, filed May 5, 2011, now U.S. Pat. No. 8,373,753, and entitled \"Adaptive Multi-Modal Integrated Biometric \\n Identification Detection and Surveillance Systems\\', which is a divisional patent application of U.S. patent application Ser. \\n No. 1 1/231,353 filed on Sep. 19, 2005, now U.S. Pat. No. 7.956,890 and entitled “Adaptive Multi-Modal Integrated \\n Biometric Identification Detection and Surveillance Sys tems’ which claims the benefit of U.S. Provisional Appli \\n cation No. 60/610,998, filed on Sep. 17, 2004, and entitled “Adaptive Multi-Modal Integrated Biometric Identification Detection Systems, all of which are hereby incorporated by \\n reference in their entirety as if fully set forth herein. \\n BACKGROUND OF THE INVENTION \\n 1. Field of the Invention \\n The invention relates in general to biometric identifica tion, and more particularly, to a Surveillance system using \\n biometric identification. \\n 2. Brief Description of the Related Art \\n The state of the art of applying biometric technologies to authenticate and positively determine the identity of a per \\n son is still faced with several technical challenges. Specifi cally, the challenges can be categories into two aspects: data \\n acquisition and data matching. Data acquisition deals with \\n acquiring biometric data from individuals. Data matching \\n deals with matching biometric data both quickly and accu rately. These challenges can be explained by a port-entry \\n scenario. In Such a setting, it is difficult to obtain certain \\n biometric data such as DNA and voice samples of individu als. For biometric data that can be more easily acquired. Such as face images and fingerprints, the acquired data quality can \\n vary greatly depending on acquisition devices, environmen \\n tal factors (e.g., lighting condition), and individual corpo \\n ration. Tradeoffs exist between intrusiveness of data collec \\n tion, data collection speed, and data quality. \\n Once after the needed data have been acquired, conduct ing matching in a very large database can be very time consuming. It goes without saying that unless a system can \\n acquire and match data both timely and accurately, the system is practically useless in improving public security, \\n where the inconvenience due to the intrusive data-acquisi tion process and the time-consuming matching process \\n ought to be minimized. \\n A biometric system typically aims to address either one of the following issues: 1) Authentication: is the person the one \\n he/she claims to be? 2) Recognition: who a person is? In the first case, data acquisition is Voluntary and matching is done \\n in a one-to-one fashion—matching the acquired data with \\n the data stored on an ID card or in a database. In the second \\n case, individuals may not be cooperating, and the system \\n must conduct searches in a very large repository. \\n The prior art in biometric can be discussed in two parts: \\n single-modal Solutions and multi-modal solutions. Several 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n systems have been built to use one of the following single \\n modal: facial data, voice, fingerprint, iris or DNA. The effectiveness of these single-modal approaches can be evalu \\n ated in three metrics: the degree of intrusiveness, speed and accuracy. From the perspective of a user, acquiring face \\n modal can be the most noninvasive method, when video \\n cameras are mounted in the distance. However, the same convenience nature often compromises data quality. An', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 11}), Document(page_content='modal can be the most noninvasive method, when video \\n cameras are mounted in the distance. However, the same convenience nature often compromises data quality. An \\n intrusive face acquisition method is to acquire frontal face \\n features, which requires corporation from individuals. Voice \\n is another popular modal. However, traditional Voice-rec ognition fails miserable when Voice samples of multiple \\n individuals are simultaneously captured or when back \\n ground noise exists. Even when the acquired Voice data can be \"pure.” existing signal processing and matching tech \\n niques can hardly achieve recognition accuracy of more than \\n 50%. The next popular modal is fingerprint, which can achieve much higher recognition accuracy at the expense of \\n intrusive data acquisition and time-consuming data match \\n ing. Finally, DNA is by far the most accurate recognition technique, and the accompanying inconvenience in data acquisition and the computational complexity are both exceedingly high. Summarizing the single model approach, \\n non-intrusive data-acquisition techniques tend to Suffer from low recognition accuracy, and intrusive data-acquisition \\n techniques tend to Suffer from long computational time \\n As to multimodal techniques, there have been several prior art United States patents and patent applications dis \\n close techniques. However, as will be further discussed \\n below, these disclosures do not provide scalable means to \\n deal with tradeoffs between non-intrusiveness, speed and accuracy requirements. These disclosures may fix their system configuration for a particular application, and cannot \\n adapt to queries of different requirements and of different applications. \\n Wood et al. disclose in U.S. Pat. No. 6,609,198 a security architecture using the information provided in a single sign-on in multiple information resources. Instead of using \\n a single authentication scheme for all information resources, the security architecture associates trust-level requirements \\n with information resources. Authentication schemes (e.g., those based on passwords, certificates, biometric techniques, \\n Smart cards, etc.) are employed depending on the trust-level \\n requirement(s) of an information resource (or information \\n resources) to be accessed. Once credentials have been obtained for an entity and the entity has been authenticated to a given trust level, access is granted, without the need for \\n further credentials and authentication, to information \\n resources for which the authenticated trust level is sufficient. \\n The security architecture also allows upgrade of credentials \\n for a given session. The credential levels and upgrade \\n scheme may be useful for a log-on session; however, such architecture and method of operations do not provide a resolution for high speed and high accuracy applications \\n Such as passenger security check in an airport. \\n Sullivan et al. disclose in U.S. Pat. No. 6,591,224 a \\n method and apparatus for providing a standardized measure \\n of accuracy of each biometric device in a biometric identity authentication system having multiple users. A statistical \\n database includes continually updated values of false accep \\n tance rate and false rejection rate for each combination of \\n user, biometric device and biometric device comparison \\n score. False acceptance rate data are accumulated each time a user Successfully accesses the system, by comparing the \\n user\\'s currently obtained biometric data with stored tem \\n plates of all other users of the same device. Each user is treated as an “impostor with respect to the other users, and', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 11}), Document(page_content=\"US 9,432,632 B2 \\n 3 \\n the probability of an impostor's obtaining each possible \\n comparison score is computed with accumulated data each \\n time a successful access is made to the system. The statis \\n tical database also contains a false rejection rate, accumu lated during a test phase, for each combination of user, \\n biometric device and biometric device comparison score. By \\n utilizing a biometric score normalizer, Sullivan's method and apparatus may be useful for improving the accuracy of \\n a biometric device through acquiring more training data. \\n Murakami et al. disclose in U.S. Pre-Grant Publication \\n 2002-0138768 entitled “Method for biometric authentica \\n tion through layering biometric traits, a portable biometric authentication system having a single technology for mea Suring multiple, varied biological traits to provide individual \\n authentication based on a combination of biological traits. At least one of these biometric traits is a live physiological \\n trait, such as a heartbeat waveform, that is Substantially— but not necessarily completely unique to the population of \\n individuals. Preferably, at least one of the identifying aspects \\n of the biological traits is derived from a measurement taken by reflecting light off the subdermal layers of skin tissue. \\n The Murakami et al. approach is limited by the more \\n intrusive measurement techniques to obtain data such as \\n heartbeat waveform and reflecting light off the subdermal \\n layers of skin tissue. These data are not immediately avail able in a typical security check situation to compare with the \\n biometric data, e.g., heart beat waveforms and reflection light from subdermal layers from the skin of a targeted \\n searching object. Furthermore, the determination or the filtering of persons’ identity may be too time consuming and \\n neither appropriate for nor adaptive to real time applications. \\n Langley discloses in U.S. Pre-Grant Publication 2002 0126881, entitled “Method and system for identity verifi cation using multiple simultaneously scanned biometric images, a method to improve accuracy and speed of bio \\n metric identity verification process by use of multiple simul \\n taneous scans of biometric features of a user, such as multiple fingerprints, using multiple scanners of Smaller size \\n than would be needed to accommodate all of the fingerprints in a single Scanner, and using multiple parallel processors, or a single higher speed processor, to process the fingerprint \\n data more efficiently. Obtaining biometric data from mul \\n tiple user features by use of multiple scanners increases \\n verification accuracy, but without the higher cost and slower processing speed that would be incurred if a single large \\n scanner were to be used for improved accuracy. The meth ods according to Langley may provide the advantages of \\n speed and accuracy improvements. However, the nature of requiring multiple scans makes data acquisition time-con \\n Suming and intrusive. \\n On the academia side, much research effort has been geared toward analyzing data from individual biometric \\n channels (e.g., voice, face, fingerprint, please see the refer ence list for a partial list), less emphasis has been placed on comparing the performance of different approaches or \\n combing information from multiple biometric channels to \\n improve identification. Some notable exceptions are dis \\n cussed below. In Hong Lin, Jain A. K., Integrating faces and fingerprints for personal identification, IEEE Transactions \\n on Pattern Analysis and Machine Intelligence, Vol. 20, No. \\n 12, December 1998, pp. 1295-1307, the authors report an \\n automated person identification system that combines face and fingerprint information. The face recognition method \\n employed is the traditional eigen face approach, M. Turk and A. Pentland, Eigenfaces for Recognition, J. Cognitive Neu \\n roscience Vol. 3, No. 1, 1991, pp. 71-96, which computes a \\n set of orthonormal bases (eigen faces) of the database 10 \\n 15 \\n 25 \\n 30 \\n 35\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 12}), Document(page_content=\"roscience Vol. 3, No. 1, 1991, pp. 71-96, which computes a \\n set of orthonormal bases (eigen faces) of the database 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n images using the principal component analysis. Face images \\n are then approximated by their projection onto the ortho \\n normal Eigen face bases, and compared using Euclidean \\n distances. For fingerprint, the authors extend their previous \\n work, Jain, A. K.; Lin Hong; Bolle, R.; On-line fingerprint \\n verification, Pattern Analysis and Machine Intelligence, Vol. \\n 19, No. 4, April 1997, pp. 302-314, to extract minutiaes from fingerprint images. They then align two fingerprint images \\n by computing the transformation (translation and rotation) \\n between them. Minutiaes are strung together into a string representation and a dynamic programming-based algorithm \\n is used to compute the minimum edit distance between the \\n two input fingerprint strings. Decision fusion is achieved by \\n cross validation of the top matches identified by the two \\n modules, with matching results weighed by their confidence \\n or accuracy levels. The performance of the system is vali \\n dated on a database of about 640 face and 640 fingerprint images. \\n In Phillips, Henson Moon; Rive, S E A.; Russ. The FERRET evaluation methodology for face-recognition algo \\n rithms, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, No. 10, October 2000, pp. 1090-1104, the Michigan State University research group extends their \\n information fusion framework to include more modalities. \\n In particular, images of a Subject's right hand were captured, \\n and fourteen features comprising the lengths of the fingers, \\n widths of the fingers, and widths of the palm at various \\n locations of the hand. Euclidean distance metric was used to \\n compare feature vectors. Simple Sum rules, decision tree and \\n linear discriminant function are used for classification. It is \\n observed that a personal ID system using three modules \\n outperforms that uses only two of the three modules. While this is an interesting experiment, the data set used is Small \\n and there is no accepted universal standard in using hand \\n images in biometrics. In R. Brunelli, D. Falavigna, T. Poggio and L. Stringa, \\n Automatic Person Recognition by Using Acoustic and Geo \\n metric Features, Machine Vision and Applications 1995, Vol.8 pp. 317-325, an automated person recognition system using voice and face signatures is presented. The speaker recognition Subsystem utilizes acoustic parameters (log \\n energy outputs and their first-order time derivatives from 24 triangular band-pass filters) computed from the spectrum of \\n short-time windows of the speech signal. The face recogni tion Subsystem is based on geometric data represented by a \\n vector describing discriminant facial features such as posi \\n tions and widths of the nose and mouth, chin shape, thick ness and shape of the eyebrows, etc. The system captures \\n static images of the test Subjects and the test Subjects are also \\n asked to utter ten digits from Zero to nine for use in the speaker ID Subsystem. Each Subsystem then computes the \\n distances of the test Subject's speech and face signatures \\n with those stored in the databases. Decisions from the two \\n ID modules are combined by computing a joint matching \\n score that is the Sum of the two individual matching scores, weighted by the corresponding variance. Experimental \\n results show that integration of visual and acoustic infor mation enhances both performance and reliability of the separate systems. The above system was later improved \\n upon in Brunelli, R.; Falavigna, D., Person identification using multiple cues, IEEE Transactions on Pattern Analysis \\n and Machine Intelligence, Vol. 17, No. 10, October 1995, \\n pp. 955-966, where multiple classifiers are used in the face recognition Subsystems, and the matching score normaliza \\n tion process is made more robust using robust statistical \\n methods.\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 12}), Document(page_content='US 9,432,632 B2 \\n 5 \\n In Kittler, J.; Hatef, M.; Duin, R. P. W.; Matas, J., On combining classifiers, IEEE Transactions on Pattern Analy \\n sis and Machine Intelligence, Vol. 20, No. 3, March 1998, pp. 226-239, a performance study of various ensemble \\n classification scheme is presented. It is shown that many existing decision aggregation rules are actually simplifica \\n tions based on the more general Bayesian rule. The authors compare the performance of different decision aggregation \\n rules (max, min, median, and majority Voting rule) by \\n performing an experiment in biometrics. Three modules are used: frontal faces, face profiles, and Voiceprints. Simple \\n correlation-based and distance-based matching is performed on frontal faces and face profiles, respectively, by finding a \\n geometric transformation that minimizes the differences in intensity. It is shown that a simple aggregation Scheme by \\n Summing the results from individual classifiers actually perform the best. \\n In Lu X; Wang Y; and Jain A, Combing classifiers for face \\n recognition, IEEE International Conference on Multimedia Systems and Expo, Baltimore, Md., July 2003, three well known appearance-based face recognition methods, namely \\n PCA, M. Turk and A. Pentland, Eigenfaces for Recognition, \\n J. Cognitive Neuroscience Vol. 3, No. 1, 1991, pp. 71-96, \\n ICA, and LDA, Belhumeur, P. N.: Hespanha, J. P.; Krieg man, D. J. Eigenfaces vs. Fisherfaces: recognition using \\n class specific linear projection, IEEE Transactions on Pat tern Analysis and Machine Intelligence, Vol. 19, No. 7, July \\n 1997, pp. 711-720, are used for face image classification. \\n Two combination strategies, the sum rule and RBF network, are used to integrate the outputs from these methods. Experi \\n mental results show that while individual methods achieve \\n recognition rates between 80% and 88%, the ensemble classifier boosts the performance to 90%, using either the \\n sum rule or RBF network. In Senior, A., A combination fingerprint classifier, IEEE Transactions on Pattern Analysis \\n and Machine Intelligence, Vol. 23, No. 10, October 2001, \\n pp. 1165-1174, a similar multi-classifier scheme, this time for fingerprint classification, is proposed. Hidden Markov \\n Models and decision trees are used to recognize ridge structures of the fingerprint. The accuracy of the combina \\n tion classifier is shown to be higher than that of two state-of-the-art systems tested under the same condition. These studies represent encouraging results that validate our \\n multi-modal approach, though only a single biometric chan \\n nel, either face or fingerprint, not a combination of biometric \\n channels, is used in these studies. Maio, D.; Maltoni, D.; Cappelli, R.; Wayman, J. L.; Jain, \\n A. K., FVC2000: fingerprint verification competition, IEEE \\n Transactions on Pattern Analysis and Machine Intelligence, \\n Vol. 24, No. 3, March 2002, pp. 402-412, documents a fingerprint verification competition that was carried out in \\n conjunction with the International Conference on Pattern \\n Recognition (ICPR) in 2000 (a similar contest was held \\n again in 2002). The aim is to take the first step towards the \\n establishment of a common basis to better understand the \\n state-of-the-art and what can be expected from the finger print technology in the future. Over ten participants, includ \\n ing entries from both academia and industry, took part. Four \\n different databases, two created with optical sensors, one with a capacitive sensor, and one synthesized, were used in \\n the validation. Both the enrollment error (if a training image \\n can be ingested into the database or not) and the matching \\n error (if a test image can be assigned the correct label or not) \\n and the average time of enrollment and matching are docu \\n mented. \\n A study, that is similar in spirit but compares the perfor mance of face recognition algorithms, is reported in Phillips, 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 13}), Document(page_content='mented. \\n A study, that is similar in spirit but compares the perfor mance of face recognition algorithms, is reported in Phillips, 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n P. J.; Hyeonjoon Moon; Rizvi, S. A.; Rauss, P. J., The FERET evaluation methodology for face-recognition algo \\n rithms, IEEE Transactions on Pattern Analysis and Machine \\n Intelligence, Vol. 22, No. 10, October 2000, pp. 1090-1104. \\n A subset of the Feret database (a gallery of over 3000 \\n images) was used in the study. Ten different algorithms, \\n using a wide variety of techniques, such as PCA and Fischer \\n discriminant, were tested. Cumulative matching scores as a \\n function of matching ranks in the database are tabulated and used to compare the performance of different algorithms. \\n This study was repeated three times, in August 1994, March \\n 1995, and July 1996. What is significant about this study is \\n that the performance of the face recognition algorithms \\n improved over the three tests, while the test condition \\n became more challenging (with increasingly more images in \\n the test datasets). \\n As can be seen from the above brief survey, multi-modal \\n biometrics holds a lot of promise. It is likely that much more \\n accurate classification results can be obtained by intelli gently fusing the results from multiple biometric channels given performance requirements. While it is important to \\n keep on improving the accuracy and applicability of indi \\n vidual biometric sensors and recognizers, the performance \\n of a biometric system can be boosted significantly by judiciously and intelligently employing and combining mul \\n tiple biometric channels. While there have seen significant research activities in single- and multi-channel biometry over the past decade, the \\n state-of-the-art is still wanting in terms of speed and accu \\n racy. Therefore, a need still exists in the art to provide new and improved methods and system configurations to \\n increase the speed and accuracy of biometric identity veri \\n fication and determinations such that the above-mentioned \\n difficulties and limitations may be resolved. The present \\n invention meets this need. \\n SUMMARY \\n One embodiment of the invention provides a novel sur \\n veillance method. An event sensor Such as, a camera, \\n chemical sensor, motion detector, unauthorized door access \\n sensor, for example, is disposed to sense an occurrence of a \\n potential security breach event. A camera with a view of the \\n area in which an event is sensed gathers biometric informa tion concerning a subject person in the vicinity of the event \\n at about the time the event is sensed. A subject dossier is produced containing biometric information relating to the \\n subject person sensed by the camera with the view of the \\n area. Biometric information of persons captured on one or \\n more other Surveillance cameras in the general vicinity of the event is matched against corresponding biometric infor \\n mation in the Subject dossier. \\n Another embodiment of the invention provides a new Surveillance system. A sensor is disposed in a Surveillance \\n region to sense an occurrence of a security breach event. The \\n system includes a plurality of cameras. At least one camera \\n of the plurality has a view of the security area and can be configured to automatically gather biometric information \\n concerning a Subject person in the vicinity of an area where \\n the event occurred in response to the sensing of the event. \\n One or more of the plurality of cameras can be configured to search for the subject person. The surveillance system also includes a processing system which can be programmed to produce a subject dossier corresponding to the Subject person. The processing system also can be programmed to \\n match biometric information of one or more persons cap', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 13}), Document(page_content='US 9,432,632 B2 \\n 7 \\n tured by one or more of the cameras with corresponding \\n biometric information in the subject dossier. \\n These and other features and advantages of the invention \\n sill be apparent from the following description of embodi \\n ments thereof in conjunction with the drawings. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 is an illustrative showing a map of an airport passenger terminal and its immediate vicinity protected by a \\n surveillance system of one embodiment of the invention and also showing several pop-up views relating to event alerts in \\n accordance With the embodiment. \\n FIG. 2 is another view of the map of FIG. 1 showing Zoom to detail maps of different portions of the overall passenger \\n terminal map. FIG. 3 is an illustrative drawing of example security areas \\n within the surveillance region of FIGS. 1-2 outfitted with \\n event SensOrS. \\n FIG. 4 is an illustrative block level hardware diagram of \\n a Surveillance system in accordance with an embodiment of \\n the invention. \\n FIG. 5 is an illustrative block diagram level drawing of a \\n system architecture of an embodiment of the invention that incorporates the system hardware of FIG. 4. FIG. 6 is an illustrative flow diagram showing gathering \\n and conversion of facial feature data to a facial feature signature. \\n FIG. 7 is an illustrative flow diagram showing gathering \\n and conversion of fingerprint feature data to a fingerprint signature. \\n FIG. 8 is an illustrative flow diagram showing gathering \\n and conversion of DNA data to a DNA signature. One embodiment of the invention may employ a DNA fingerprint \\n for identification purposes. \\n DETAILED DESCRIPTION OF THE \\n INVENTION \\n The following description is presented to enable any \\n person skilled in the art to make and use the invention, and is provided in the context of particular applications and their \\n requirements. Various modifications to the preferred \\n embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied \\n to other embodiments and applications without departing \\n from the spirit and scope of the invention. Moreover, in the following description, numerous details are set forth for the purpose of explanation. However, one of ordinary skill in the \\n art will realize that the invention can be practiced without \\n the use of those specific details. In other instances, well known structures and devices are shown in block diagram \\n from in order not, to obscure the description of the invention with unnecessary detail. Thus, the present invention is not \\n intended to be limited to the embodiments shown, but is to be accorded the widest scope consistent with the principles \\n and features disclosed herein. \\n System Overview \\n One embodiment of the invention involves an intelligent \\n Surveillance system. A plurality of cameras, Some with and \\n some without overlapping fields of view, are distributed throughout a Surveillance region. Intelligent computer soft \\n ware based agents process information captured by one or \\n more of the cameras to produce a subject dossier indicative of the identity of a person whose images have been captured \\n by one or more of the cameras. Information for a subject dossier also may be gathered through other modalities Such 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n as voice recognition, iris Scan, or fingerprint, for example. \\n The system includes multiple event sensors, which may \\n include the cameras, chemical sensors, infrared sensors, or \\n other security alarm sensors that trigger an alert, upon \\n sensing an occurrence of a predetermined category of event \\n requiring heightened vigilance. For example, an alarm may \\n be triggered when a locked door is opened without proper \\n access permission or when an unauthorized person enters a \\n restricted area or when a vehicle is parked in a restricted area. More specifically, a Subject dossier is produced for', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 14}), Document(page_content=\"access permission or when an unauthorized person enters a \\n restricted area or when a vehicle is parked in a restricted area. More specifically, a Subject dossier is produced for \\n individuals in the vicinity of the location of an alarm \\n triggering event. For instance, a Subject dossier may be \\n produced for persons captured in a video camera image at or \\n near a door in the Surveillance region at about the time when \\n an unauthorized opening of the door is detected by an event \\n SSO. \\n A subject dossier may include soft biometric information, \\n also referred to as “soft’ features such as clothing color, \\n estimated height and weight. A subject dossier also may \\n include temporal information, Such as walking speed or \\n direction of travel. In addition, a Subject dossier also may \\n include more permanent information Such as facial features, fingerprint, iris Scan, Voiceprint and DNA. Soft features may \\n be selected to be especially useful for relocating an indi \\n vidual within the Surveillance region, especially in a crowd, \\n for example. For instance, it may be relatively easy to \\n identify individuals based upon clothing color or estimated \\n height and weight. However, soft features have the disad \\n vantage of not being as reliable or permanent over time. If \\n a person takes off his jacket, then an identifying color feature \\n may be lost. If a person sits down, then it may become \\n impossible to use height and weight information to pick that \\n person out of a crowd. \\n System sensors continually monitor the Surveillance \\n region for the occurrence of one or more Suspicious events. \\n In one embodiment, the system directs a live video feed \\n from one or more cameras having the location of an alert \\n triggering event in their field of view to a console in a \\n manned control center. The system also may direct video \\n images captured just before the event to the control center \\n console. Thus, an operator at the console can observe \\n behavior of suspicious individuals at the scene of the event in real time and immediately prior to the event. A subject \\n dossier produced for individuals at the scene of the event can be used to automatically identify and track a suspect indi \\n vidual present at the scene of the event within the surveil \\n lance area after the occurrence of the event. \\n The system may employ information in a subject dossier incrementally. For instance, the system may prioritize infor \\n mation in the Subject dossier. Certain information in the Subject dossier Such as clothing color, estimated height and weight, walking pattern or gait and certain key facial fea \\n tures such as facial shape, facial hair, skin color, or hair color may be used to make an initial estimate of which persons in \\n a camera's field of view are candidates for a match to a \\n Suspicious person identified in response to an alert. Other \\n features from a subject dossier then may be added incre \\n mentally to make a more careful assessment of whether identified candidates actually match the Suspect. Alterna tively, as more information concerning a Suspicious person \\n becomes available, additional features may be added incre mentally to a suspects Subject dossier for that person. This \\n additional information then may be used to more effectively \\n locate and track the individual within the surveillance \\n region.\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 14}), Document(page_content='US 9,432,632 B2 \\n 9 \\n Surveillance Region \\n One embodiment of the invention is configured for use in airport security. In this embodiment, the Surveillance region comprises an airport passenger terminal and the Surrounding passenger ground transport loading/unloading Zone directly \\n outside the terminal and the aircraft parking area adjacent \\n the terminal. FIG. 1 is an illustrative drawing of a map of an airport passenger terminal and its immediate vicinity pro \\n tected by a surveillance system of one embodiment of the \\n invention. The system includes multiple cameras, each with \\n an associated field of view, some of which are overlapping. The Surveillance region has multiple areas including pas senger arrival and departure areas, a passenger departure \\n shops and a terrace. Groups of cameras with overlapping \\n fields of view are deployed to capture images within differ ent regions of the passenger arrival and passenger departure \\n aaS. \\n FIG. 2 is another view of the map of FIG. 1 showing Zoom to detail maps of different portions of the overall passenger \\n terminal map. The illustrative maps of FIGS. 1-2 can be displayed on a control terminal so that an operator can easily \\n correlate an alert to a specific area an airport Surveillance region. For instance, if an alert is triggered in the arrivals region shown in FIG. 1, then an operator may request the \\n left-most Zoom shown in FIG. 2 in order to quickly picture the airport layout in the vicinity of the alert. Additional Zoom \\n maps (not shown) may be provided for numerous locations Such as security gates, check-in counters, airport fairway, \\n parking area, access entrance, check-in counters, etc. Each different area may be associated with a group of cameras and \\n event SensOrS. \\n Event sensors are disposed at selected locations within the surveillance region. FIG. 3 is an illustrative drawing of example security areas within the Surveillance region of \\n FIGS. 1-2 outfitted with event sensors. A first security area comprises a door. The door may be equipped with a sensor, \\n Such as a mechanical sensor, that detects unauthorized opening of the door. A second security area comprises a \\n window. The window may be associated with a mechanical \\n sensor that detects when the window has been broken. A \\n third security represents a threshold to a restricted area. The \\n restricted area may be equipped with motion detectors that \\n detect the presence of persons in a restricted area. Cameras \\n situated throughout the Surveillance region also may serve as event sensors. For example, the system may employ a \\n monitoring rule whereby a camera monitors a particular area \\n of the passenger terminal. If a person is loitering in that area, \\n defined by failing to move beyond a 15 foot radius for more \\n than 60 seconds, then a low level alert is declared, the \\n camera Zooms in, and the face of the loitering person is \\n matched against the faces of persons on a watch list, for example. \\n Landmarks are defined in the security areas for purpose of estimating height and weight and direction and speed of \\n travel of a Suspect individual. For instance, a landmark Such as a countertop may be identified, and processing of a \\n camera image may be calibrated to estimate a person’s \\n height relative to the land marked countertop. A group of multiple structures, such as telephone booths, lounge areas, \\n signs or countertops, within a field of view of one or more of a group of cameras covering a security area may be \\n identified. Processing of camera images from the group of \\n cameras may be used to estimate the direction and speed at which a suspect is moving based upon the sequence and \\n timing of his passing the land marked structures. Although the Surveillance region in this one example is \\n described in terms of an airport passenger terminal, it will be 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n appreciated that the invention is not restricted to an airport', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 15}), Document(page_content='described in terms of an airport passenger terminal, it will be 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n appreciated that the invention is not restricted to an airport \\n terminal. Moreover, the surveillance region need not be a \\n continuous local area. Event sensors and Surveillance cam \\n eras may be disposed over disparate areas and be in com \\n munication with a control center via a network Such as the \\n internet, for example. \\n System Architecture \\n FIG. 4 is an illustrative block level hardware diagram of \\n a Surveillance system in accordance with an embodiment of the invention. The system includes multiple data collection agents, a knowledge server, a local knowledge server data \\n base, an application server, a middle-tier database, web \\n servers, a browser based control console and one or more client applications such as Computer Aided Dispatch sys \\n tem, building management system, access control system, \\n etc. It should be understood that the various components shown are merely illustrative. Each agent may gather infor \\n mation from numerous sources, such as the cameras shown in FIG. 1, distributed throughout a surveillance region. Moreover, for example, the knowledge server and the appli \\n cation server can be implemented across multiple hardware \\n systems or as different processes within a single hardware \\n system. \\n A security agent is a process that spans many tasks to \\n collect information about Subject(s). For example, a security agent may spawn multiple data collection agents include a \\n facial features, fingerprint, DNA, clothing color, Subject \\n gait, Subject height and weight, skin color/tone, hair color/ \\n tone, Subject direction and Voiceprint, for example. Each \\n data collection task produces different information about an individual. More specifically, each produces a signature \\n indicative of some identifying aspect of a person under \\n Surveillance. For instance, a facial features agent uses facial information captured by one or more cameras to produce a \\n signature indicative of an individual’s facial features. Simi larly, for example, a clothing color agent uses clothing color \\n information captured by one or more cameras to produce a \\n signature indicative of the color of an individual’s clothing color. Thus, the multiple agents can produce multiple dif \\n ferent signatures, each indicative of one or more different identifying feature of an individual. The agents provide the signatures to the knowledge \\n server, which aggregates signatures for each given person \\n under surveillance into a subject dossier for that person. The knowledge server indexes the signatures within a given \\n Subject dossier to permit incremental searches for individu als within the search region. The knowledge server also may perform classification and matching. The local knowledge \\n server database stores the digital signatures and correspond \\n ing indexing information. \\n The web services is the component that provides the \\n interfaces via Web Server which is usually part of an operating system. For example, web services provides the \\n interfaces for our internal components or external systems \\n via Web Server (such as Microsoft IIS on Windows, or Apache on Linux). All the interfaces to the system are via HTTP or HTTPS using port 80. Doing so, our system can \\n run across firewall. Basically, the Web Services component \\n just exposes our system interface to the outside world via \\n Web Server. \\n The application server is the component that provides that \\n database access to the user interface component, and per \\n forms session management which includes authentication \\n and authorization. The middle-tier database serves as the \\n local database for the application server. FIG. 5 is an illustrative block diagram level drawing of a \\n system architecture of an embodiment of the invention that', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 15}), Document(page_content='US 9,432,632 B2 \\n 11 \\n incorporates the system hardware of FIG. 4. A user interface \\n (UI) provides an operator of the system with real-time \\n information concerning alert events within the Surveillance \\n region. The UI may provide maps of the entire surveillance \\n region, including Zoom maps. It can display alerts from \\n different sensors including cameras, digital video recorders, \\n access control, bio-chemical detectors, etc. It may display \\n Videos of a security area in which an alert has been triggered, \\n detailed images of Suspect individuals and details of one or \\n more alerts that have been triggered. \\n Referring again to FIG. 1, there is show an example of a \\n UI display Screen in with pop-up display showing various \\n images relating to one or more alerts. In the center of the \\n screen is map of a Surveillance region. The operator can be \\n selectively enlarge, minimize or close each pop-up. A Video \\n Review display provides a video image of the security \\n region at about the time of an alert. An Incident Detection \\n display provides detailed information concerning an alert \\n event. In this example, the alert event involved an individual \\n tailgating at a commuter door. A Suspect Description display \\n provides identifying information concerning an individual \\n under Surveillance based upon information gathered into a \\n Subject dossier produced for the person. A Detailed Images \\n display provides pictures of a suspect individual captured by \\n one or more surveillance cameras. A Potential Identification \\n display provides images of the Suspect together with images \\n of one or more people whose facial features closely match \\n those of the Suspect. The potential matches are based upon \\n a facial feature signature provided by the facial feature \\n agent. Across the bottom of the map, there is a chart listing \\n briefly Summarizing multiple alert situations. The operator \\n may selectively access pop-up screens for these alert situ \\n ation. \\n Thus, the UI advantageously displays a variety of infor \\n mation aggregated in response to one or more alerts. In a typical airport security region, for example, there may be \\n several hundred cameras dispersed throughout a large physi \\n cal area. Moreover, there may be only a few operators monitoring one or more UI consoles. Depending upon the \\n rules for monitoring and declaring alerts, alerts may occur \\n frequently or infrequently. The UI of one embodiment of the \\n invention directs an operator to areas of a Surveillance region that are subject to alert and provides pertinent infor \\n mation concerning the alert So that the operator can effi \\n ciently manage security from a control center. The UI also allows an operator to quickly investigate and simultaneously \\n keep abreast of multiple alert events. \\n Furthermore, as explained more fully below, information \\n from different sensing devices is correlated to facilitate tracking of a suspect within a security region. For instance, \\n Soft biometric information and temporal information is used to locate a suspect as he or she travels within the security region. In one embodiment, a dashed line can be produced on a map on the display showing a path followed by a \\n Suspect within the Surveillance region. Information from \\n different data collection agents may be fused in order to more accurately identify and track an individual. Therefore, \\n the operator can use the UI to evaluate an alert event, to identify and track a suspect. The operator may use this \\n information as a basis to send information to a responder to \\n intercede or deal with an alert incident. \\n Knowledge Services are implemented as an application \\n running on the knowledge server. Knowledge Services cor \\n relate and analyze signature information provided by differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n ent sensory devices (i.e., data gathering agents). The Knowl', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 16}), Document(page_content='relate and analyze signature information provided by differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n ent sensory devices (i.e., data gathering agents). The Knowl \\n edge Services assemble and index subject dossiers, and when appropriate, fuse signature information for improved \\n classification results. The Knowledge Services also gener \\n ate, activate or deactivate rules and send/control rules and instruction to the Rules and Agent Manager. The Rules and Agent Manager also is implemented on the \\n knowledge server. The Rules and Agent Manager manages \\n all other agents and manages rules that can be sent to each \\n agent. It correlates information from agents. It can also \\n escalate an alert if the alert is not acknowledged by an \\n operator within a given timeframe and/or similar alerts happen repeatedly within a given time span (e.g. within 2 \\n hours). Both the Knowledge Service and the Rules and Agent Manager are the primary components for aggregating, \\n categorizing biometric signatures which are parts of object \\n dossiers. It also performs other tasks such as task assign ment/tracking, load balancing tasks among agents, and inter \\n acting with data access components. \\n The following are examples of rules that may be imple \\n mented by the system. \\n Rules: \\n Actor Action \\n Person Walk through lane against \\n direction of traffic \\n Person Tailgating \\n Person Loitering \\n Person Piggyback \\n Person Traveler screening \\n Person Walk in restricted area \\n Vehicle Park overtime \\n Vehicle Park in restricted area \\n The Person-Loitering rule involves the following criteria: \\n Radius 15 foot \\n Duration 20 seconds \\n Alert Severity Low \\n Response Zoom in face to match “watch list \\n The Person-Tailgating Rule involves the following crite \\n ria: \\n Critical \\n Acknowledge Loitering and \\n Tailgating alerts and deliver \\n alarm to operator console Alert severity Response \\n The correlation Monitoring Rule for the occurrence of a Person-Loitering event AND a Person-Tailgating event \\n involving the same person is as follows: \\n Critical \\n Acknowledge Loitering and \\n Tailgating alerts and deliver \\n alarm to operator console Alert Severity Response \\n As described above the UI, may display several categories \\n of information concerning an alert. The Knowledge Service and the Rules and Agent Manager provide the correlation \\n between events and data sources and Subject dossiers that permit an operator to view a map of the location of an alert, \\n soft-biometric data of a suspect and video playback, for example. More particularly, these components provide a link', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 16}), Document(page_content='US 9,432,632 B2 \\n 13 \\n to a map and Zoom stored in the middle tier database, link \\n to video feeds for video view real-time monitoring or \\n playback of recorded video clips and stored in a Digital \\n Video Recorder system and provide the subject dossier \\n information. \\n The Middle Tier Data Access runs on the application \\n server. It controls the database including functions such as \\n query, add, delete, index. Indexing biometric signatures and \\n updating Subject dossiers are done by this component. \\n A (Security) Agent is implemented as an application \\n running on the knowledge server that controls and manages \\n the data gathering sensors. In the case of cameras or DVRs, it can also perform video analytic using Computer Vision \\n technology. Those tasks include background Subtraction, \\n image stabilization, object detection, object classification, \\n object tracking, and object identification. It can also control \\n the movement of Pan/Tilt/Zoom (PTZ) cameras, manage \\n areas of interest within the field of view of the camera \\n (called Mouse/Man Trap), and collect video streams from \\n DVR or cameras. It also has a scheduler that controls when \\n rules or video analytic are performed. \\n A Sensory Device Directory Access and Video Server is \\n implemented as an application that has access to the knowl \\n edge server manages and provides information regarding \\n sensor devices or other Subsystems. Basically, it is a soft \\n ware layer that enables the overall system to handle different \\n makes/models of sensor devices. \\n The Web Services is the component provided by operating \\n systems or web servers. It manages other components, \\n spawns or deletes services as necessary. It can also listen to \\n messages from other systems. The Web Services provides \\n interfaces to the system via Web Services running as part of \\n a Web Server. The system provides a library resided on a \\n specific directory, and the Web Server (which is usually part \\n of the operating system) will use it to interpret interface \\n requests to our system. \\n Tracking, Facial Recognition, Fingerprint recognition, \\n and other biometric identification are done at the (Security) \\n agents. Biometric signatures are collected and generated at the agents, and sent to the Rules-and-Agent Manger. The Knowledge Services and the Rule-and-Agent Manager col \\n lectively collect biometric signatures and object tracking \\n locations, and then generate and manage subject dossiers. A \\n described above, a subject dossier includes information about object (e.g., person) Such as, biometric information/ \\n signatures, soft biometric information (hair color, skin tone/ color, weight or build, height, etc.) and other temporal \\n information (e.g., speed, direction, location, past activities, \\n information that the operator is looking for, etc.). Data fusion is performed by the Knowledge Services and the Rules and Agent Manager. Data required or generated by \\n each of the components are saved and retrievable via the \\n Middle-tier/Data Access component, which in turn utilizes a \\n relational database such as Microsoft SQL Server. \\n Subject Dossier \\n Data gathering agents collect data concerning a subject \\n person from different sources. The Knowledge Services aggregate the data into Subject dossier. The data aggregated \\n into a given dossier may include different digital signatures produced by different data gathering agents. A subject dos \\n sier also may include fused data signatures produced by the \\n fusion of data gathered from multiple data Sources having \\n different data modalities. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n The following is an example of information in a subject \\n dossier. \\n Subject Dossier: \\n Facial Features Signature (e.g., nose shape and size, face \\n width, distance between eye corners, skin color (light, \\n medium, dark), nose angle (profile view) Soft Biometrics Signature (e.g., clothing color, height, weight)', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 17}), Document(page_content='width, distance between eye corners, skin color (light, \\n medium, dark), nose angle (profile view) Soft Biometrics Signature (e.g., clothing color, height, weight) \\n Temporal Information Signature (e.g., direction of travel, speed, past places visited path) Fingerprint Signature \\n Voice Print Signature \\n Iris Scan Signature DNA Analysis Signature \\n Fingerprint Signature Voice Print Signature Iris Scan \\n Signature DNA Analysis Signature. \\n The information in a subject dossier is indexed so that it \\n can be used to more efficiently identify and track Suspects \\n and to avoid false alarms. More particularly, a dossier is \\n indexed so that certain information Such as Soft biometrics \\n can be used to Screen candidates within a Surveillance for \\n closer study and also to predict likely places within a \\n Surveillance region to look for a suspect. For instance, soft \\n biometric information Such as clothing color, height and \\n weight may be employed to select candidates for further investigation. For example, the Knowledge Services may be \\n programmed to cause the Security Agents to search for a \\n match between clothing color in a subject dossier of a Suspect and clothing color of unidentified persons in a \\n surveillance region. If a match is found, then the Knowledge Service may cause the Security Agents to perform an \\n analysis of whether facial features in the subject dossier match facial features of the person with matching color clothing. Moreover, temporal information provided in a \\n Subject dossier Such as direction and speed of travel of a Suspect may trigger the Knowledge Services to alert only \\n certain sensory devices, such as a group of cameras in an area of the Surveillance region where the Suspect is headed, \\n to be on the lookout for the suspect. \\n A Subject dossier may be incremented as more informa tion concerning a suspect is gathered. For example, initially, \\n only soft biometric information Such as clothing color and estimated height and weight might be available. Subse \\n quently, more information Such as a facial feature signature \\n or a voice print may become available and will be added to the subject dossier. Newly received data from these multiple sources may be fused with previous data by the Knowledge \\n Services as it is received. \\n A subject dossier is a record stored in a computer readable medium that can be easily accessed by security agents and \\n a console operator. The dossier is structured to separate soft \\n biometric information and temporal data from other biomet \\n ric information. Soft biometric and temporal information generally can be characterized as being easier to obtain and useful for tracking purpose, but not very reliable for defini \\n tive identification purposes. Other biometric information, Such as fingerprints, voiceprints and an iris Scan are more \\n reliable, but more difficult to obtain. Thus, soft biometric and temporal data can be used advantageously to track an \\n individual until more reliable information, such as detailed facial features or fingerprints can be obtained to provide a \\n more reliable identification. \\n Data Gathering Agents \\n The surveillance system of one embodiment employs \\n multiple streams of data including one or more of facial', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 17}), Document(page_content=\"US 9,432,632 B2 \\n 15 \\n features, vocal, fingerprint, iris Scan, DNA data, Soft bio metric data, temporal data and fused data. FIG. 6 is an illustrative flow diagram showing gathering \\n and conversion of facial feature data to a facial feature \\n signature. Facial feature A comprises a front image of a face \\n that is segmented into a plurality of local areas as an \\n irreducible set of image building elements to extract a set of \\n local features that can be mapped into a mathematical formula. Facial feature B comprises is a side image that is also separated into a set of irreducible image building \\n elements for extracting local features. Facial feature C comprises a side profile curve that is also collected for use \\n in the identity check and authentication processes. Facial \\n features D and E comprise skin color and tone and hair color. \\n These facial feature data are collected from several video \\n key frames taken from a parallax camera. \\n These facial feature data are used to produce a facial features signature. In one embodiment, the Knowledge Services which applies an MPEG-7 descriptor, e.g., a facial recognition descriptor, representing a projection of a face \\n vector onto a set of basis vectors that span the space of possible face vectors and the projection of the face from a side view defined by a profile curve. The face recognition \\n feature sets are extracted from a normalized face image and a normalized profile curve. The normalized face image \\n includes 56 lines with 46 intensity values in each line. The \\n centers of the two eyes in each face image are located on the 24.sup.throw and the 16.sup.th and 30. Sup.st column for the \\n right and left eye respectively. This normalized image is then \\n used to extract the one dimensional face vector that includes \\n the luminance pixel values from the normalized face image \\n arranged into a one dimensional vector using a raster Scan starting at the top-left corner of the image and finishing at \\n the bottom right corner of the image. The face recognition \\n feature set is then calculated by projecting the one-dimen \\n sional face vector onto the space defined by a set of basis vectors. By using the front image, the side image, the profile \\n curve, the skin color and tone and the hair color, the accuracy of identity authentication is significantly improved. A Voiceprint signature also can be produced for identity \\n check and authentication over a telephone, for example. A Voiceprint is particularly useful because it is totally nonin \\n vasive. In one embodiment, a multi-dimensional voice iden tification process may be employed to generate a speaker's \\n Voice signature by processing pitch contour vectors, time \\n signature, beat number vector and Voice shape defined by \\n audio waveforms of the speaker. For example, one embodi \\n ment applies pitch models for different pitch intervals, which \\n are defined to be the difference between the semitones of two \\n adjacent nodes: \\n Pitch Interval=(log(current pitch)-log(previous \\n pitch) log 2.Sup.1/12 \\n FIG. 7 is an illustrative flow diagram showing gathering \\n and conversion of fingerprint feature data to a fingerprint signature. A raw image of a fingerprint is converted into a set \\n of fingerprint codes. The set of codes has a more compact \\n format, e.g., IKENDI Fingerprint Pattern Format, which is based on encoding the friction ridges into a set of direction codes. The coded fingerprint is converted to fingerprint signature in an MPEG-7 descriptor. \\n FIG. 8 is an illustrative flow diagram showing gathering \\n and conversion of DNA data to a DNA signature. One embodiment of the invention may employ a DNA fingerprint \\n for identification purposes. A complete DNA profile includes \\n 13 short tandem repeats (STRs) with repeats of four or five \\n nucleotides in addition to a sex marker. Each STR has 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n various expected length and is located on different chromo\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 18}), Document(page_content='nucleotides in addition to a sex marker. Each STR has 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n various expected length and is located on different chromo \\n Somes or different ends of the same chromosome and each \\n is independently inherited. FIG. 8 show respectively the \\n human chromosomes with STR names and locations and \\n three or four different polymorphisms labeled with each of \\n four fluorescent dyes. The DNAs of different lengths are separated by gel electrophoresis. Since it is desirable to \\n detect all different DNAs in one signal identification pro \\n cess, different colors of dyes are used to mark different DNAS that have same length. Appropriate dyes are \\n employed in a PCR operation with STR primers to separate \\n the DNAs based on length and color to get accurate DNA \\n fingerprint in a single DNA identification process. The DNA profile signature is generated in the present invention by \\n using STRs and STR types, e.g., STR Name, Type}, {STR \\n Name, Type} where STR Names are {TPOX, DSS1358, \\n FGA, D5S818, CSF1PO, D7S820, D8S1179, THO1, VWA, \\n D13S317, D16S539, D18S51, D21S11, SEX, etc. Types are required to make Sure other DNA sequences may use the \\n repeat number of alleles instead of hetero/homozygous, e.g., {Heterozygous, Homozygous. DNA samples for identity \\n check and authentication may include hair, saliva, and blood. Samples are collected and their signatures are stored \\n in a database. New Sample can be collected and analyzed (but not in real time) using DNA arrays/chips, GeneChip, \\n Verigene ID, traditional PCR, or Forensic STR Analysis \\n methods. The result signature will be matched with the signatures in the database. \\n FIG. 8 illustrates genomic barcodes based on a standard Universal Product Codes for identifying retailed products by \\n employing ten alternate numerals at eleven positions to \\n generate one hundred billion unique identifiers. One \\n embodiment of the invention applies the barcode techniques for DNA fingerprint identification process. Special consid \\n erations are focused on the facts that the repeat polymor phisms are found mainly in intergenic (nongene) regions of \\n chromosomes, especially near the centromeres and that the polymorphisms always exist in a pair in this case, one from \\n each cop of chromosome 1. At a polymorphic locus (loca \\n tion), different numbers of a repeated unit create different alleles. Furthermore, repeated sequences of 9-80 nucleotides \\n are referred to as Variable Number Tandem Repeats (VN TRs). This VNTR has a 16 nucleotide repeat. Repeated \\n sequences of 2 to 8 nucleotides are referred to as Short \\n Tandem. Repeats (STRs). This STR has four nucleotide repeat. In a general genomic barcode system, huge number \\n of string of sites are generated with four alternate nucleo tides, i.e., adenine, guanine, cytosine, thymine, at each \\n position. A survey of just fifteen of these nucleotide posi \\n tions would create a possibility of 4.sup.15, i.e., one billion \\n codes. In the present invention, only fourteen STRs and types are employed to generate barcodes that are easier to \\n analyze with much smaller amount of data to process and \\n that can be more conveniently searched with existing search engine, e.g., Google search engine. \\n Soft biometric information, such as clothing color may be \\n captured using cameras calibrated in accordance with a process disclosed in commonly assigned co-pending U.S. \\n patent application Ser. No. Not Yet Known, filed Sep. 16, \\n 2005, entitled “Robust Perceptual Color Identification.” invented by K. Gob, E. Y. Chang and Y. F Wang, which is expressly incorporated by reference in its entirety into this \\n application through this reference. This patent application \\n addresses a problem of camera-based sensors perceiving an article of clothing as having a slightly different color when \\n viewed from different angles or under different lighting', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 18}), Document(page_content=\"US 9,432,632 B2 \\n 17 \\n conditions. The patent application proposes the representing \\n color of an article of clothing using a “robust perceptual \\n color. \\n Data from different modalities may be fused by the Knowledge Services for classification and identification purposes without Suffering the “curse of dimensionality using techniques taught in commonly assigned co-pending \\n U.S. patent application Ser. No. 11/129,090, filed May 13, \\n 2005, entitled, Multimodal High-Dimensional Data Fusion \\n for Classification and Identification, invented by E. Y. \\n Chang, now U.S. Pat. No. 7,242,810 issued Jul. 10, 2007, which is expressly incorporated herein in its entirety by this \\n reference. Data may be incrementally added to a classifica tion and identification process by the Knowledge Services using techniques taught by commonly assigned co-pending \\n U.S. patent application Ser. No. 1 1/230.932, filed Sep. 19, \\n 2005, entitled Incremental Data Fusion and Decision Mak ing, invented by Yuan-Fang Wang, now U.S. Pat. No. \\n 7.467,116 issued Nov. 25, 2008, which is expressly incor porated herein in its entirety by this reference. \\n While the invention has been described with reference to \\n various illustrative features, aspects and embodiments, it will be appreciated that the invention is susceptible of \\n various modifications and other embodiments, other than \\n those specifically shown and described. The invention is therefore to be broadly construed as including all such \\n alternative variations, modifications and other embodiments within the spirit and scope as hereinafter claimed. \\n It is to be understood that the above description is \\n intended to be illustrative, and not restrictive. For example, \\n the above-described embodiments (and/or aspects thereof) \\n may be used in combination with each other. In addition, many modifications may be made to adapt a particular \\n situation or material to the teachings of the invention with out departing from its scope. Dimensions, types of materials, \\n orientations of the various components, and the number and positions of the various components described herein are \\n intended to define parameters of certain embodiments, and are by no means limiting and are merely exemplary embodi \\n ments. Many other embodiments and modifications within the spirit and scope of the claims will be apparent to those of skill in the art upon reviewing the above description. The \\n scope of the invention should, therefore, be determined with reference to the appended claims, along with the full scope \\n of equivalents to which such claims are entitled. In the appended claims, the terms “including and “in which are used as the plain-English equivalents of the respective terms \\n “comprising and “wherein.” Moreover, in the following \\n claims, the terms “first,' 'second,' and “third,' etc. are used \\n merely as labels, and are not intended to impose numerical requirements on their objects. Further, the limitations of the following claims are not written in means—plus-function \\n format and are not intended to be interpreted based on 35 U.S.C. S112, sixth paragraph, unless and until Such claim limitations expressly use the phrase “means for followed by \\n a statement of function void of further structure. \\n What is claimed is: \\n 1. A Surveillance system comprising: multiple data gathering agents disposed throughout a \\n Surveillance region to monitor the Surveillance region, the data gathering agents to provide corresponding signatures identifying aspects of a Subject person in the \\n Surveillance region, the signatures including one or \\n both of biometric information or temporal information; a database storing a map of a layout for the Surveillance region; 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 18 \\n a processing System programmed to aggregate the signa \\n tures associated with the Subject person to produce a \\n subject dossier including one or both of biometric\", metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 19}), Document(page_content='40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 18 \\n a processing System programmed to aggregate the signa \\n tures associated with the Subject person to produce a \\n subject dossier including one or both of biometric \\n information or temporal information corresponding to the Subject person, the processing system programmed \\n to correlate the data gathering agents, the Subject dos \\n sier and the map; and \\n a control terminal to display the map of the layout for at \\n least a portion of the Surveillance region and produce \\n indicia on the map tracking a path followed by the \\n Subject person within the Surveillance region, the indi \\n cia based on the signatures in the Subject dossier and \\n the data gathering agents. \\n 2. The system of claim 1, wherein the indicia represent a \\n dashed line produced on the map on the display tracking the \\n path followed by the subject person within the surveillance region. \\n 3. The system of claim 1, wherein the processing system \\n configured to correlate different types of the data gathering \\n agents to facilitate tracking of the Subject person and to \\n display a location of the data gathering agents on the layout. \\n 4. The system of claim 1, wherein the data gathering \\n agents include at least one of a camera, digital video \\n recorder, access control or bio-chemical detector. \\n 5. The system of claim 1, wherein the database stores Zoom maps to be displayed by the control terminal, the Zoom \\n maps associated with locations in the Surveillance region, the Zoom maps illustrating at least one of security gates, \\n airport fairway, parking area, access entrance, or check-in \\n counters in the layout of the Surveillance region. \\n 6. The system of claim 1, wherein the data gathering agents monitor a large Surveillance area through which \\n multiple individuals travel. 7. The system of claim 1, wherein the processing system \\n is programmed to index the signatures within the Subject \\n dossier to permit incremental searches for individuals within the Surveillance region. 8. The system of claim 1, wherein the processing system is programmed to perform classification and matching of the signatures. \\n 9. The system of claim 1, wherein the data gathering \\n agents collect information about at least one of a walking pattern or gait of the Subject, the processing system pro \\n grammed to analyze the at least one of a walking pattern or gait of the Subject person. \\n 10. The system of claim 1, wherein the data gathering \\n agents collect information about at least one of an iris Scan and facial features of the Subject person. \\n 11. The system of claim 1, wherein the data gathering \\n agents collect information in connection with multiple indi viduals, the processing system programmed to prioritize the \\n information and analyzing a portion of the information to \\n make an initial estimate of which of the individuals are \\n candidate individuals for a potential match to the subject person, the processing system programmed to incrementally \\n add other portions of the information to the analysis to assess \\n which of the candidate individuals match the subject person. 12. The system of claim 1, wherein the processing system \\n is programmed to apply a monitoring rule wherein at least \\n one of the data gathering agents monitors a particular area in \\n the surveillance region and when an individual loiters in the particular area for more than a select period of time, the processing system declares an alert. \\n 13. The system of claim 1, wherein the processing system \\n is programmed to direct at least one of the data gathering \\n agents to Zoom in on a face of the individual, and the', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 19}), Document(page_content='agents comprise at least one of a camera, chemical sensor, \\n motion detector, door access sensor, capacitive sensor, bio metric sensor, optical sensor, infrared sensor, or security alarm sensor, having a corresponding view of the first or \\n second areas in the surveillance region. \\n ing signatures from different types of the data gathering agents to facilitate tracking of the subject person and dis playing a location of the data gathering agents on the layout. \\n recorder, access control or bio-chemical detector. US 9,432,632 B2 \\n 19 \\n processing system programmed to attempt to match the face \\n of the individual to a watch list. \\n 14. The system of claim 1, wherein at least one of the data \\n gathering agents scans data on at least one of a smart card \\n or an identification (ID) card. 5 15. The system of claim 1, wherein the data gathering agents collect biometric signatures, temporal information and object tracking locations, the biometric signatures including at least one of hair color, skin tone/color, weight, build, or height, the temporal information including at least one of speed, direction, location, past places visited, path, or past activities. 10 \\n 16. The system of claim 1, wherein the data gathering agents comprises first and second data gathering agents that \\n are positioned to monitor different first and second areas in the surveillance region, wherein the subject person travels out of a view of the first data gathering agent and into a view of the second data gathering agent as the subject person \\n travels out of the first area and into the second area. 15 \\n 17. The system of claim 16, wherein the data gathering 20 agents collect multiple data streams including one or more of facial features, vocal, fingerprint, iris scan, DNA data, soft biometric data, temporal data and used data. 18. The system of claim 16, wherein the data-gathering \\n 25 \\n 19. A surveillance method comprising: 30 monitoring a surveillance region with multiple data gath ering agents disposed throughout the surveillance region to provide corresponding signatures identifying \\n aspects of a subject person in the surveillance region, \\n the signatures including one or both of biometric infor mation or temporal information: storing a map of a layout for the surveillance region; aggregating the signatures associated with the subject person to produce a subject dossier including one or \\n both of biometric information or temporal information corresponding to the subject person; \\n correlating the data gathering agents, the subject dossier \\n and the map; and displaying the map of the layout for at least a portion of the Surveillance region and producing indicia on the 45 map tracking a path followed by the subject person \\n within the surveillance region, the indicia based on the signatures in the subject dossier and the data gathering \\n agents. \\n 20. The method of claim 19, wherein the indicia represent a dashed line produced on the map when displayed tracking 35 \\n 40 \\n 50 \\n the path followed by the subject person within the surveil lance region. \\n 21. The method of claim 19, further comprising correlat \\n 55 \\n 22. The method of claim 19, wherein the data gathering agents including at least two of a camera, digital video \\n 60 \\n 23. The method of claim 19, further comprising storing and displaying Zoom maps associated with locations in the 20 \\n Surveillance region, the Zoom maps illustrating at least one of security gates, check-in counters, airport fairway, parking \\n area, access entrance, or check-in counters in the layout of the surveillance region. 24. The method of claim 19, further comprising indexing the signatures within the subject dossier and performing \\n incremental searches for individuals within the surveillance region. \\n 25. The method of claim 19, further comprising perform ing classification and matching of the signatures.', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 20}), Document(page_content='incremental searches for individuals within the surveillance region. \\n 25. The method of claim 19, further comprising perform ing classification and matching of the signatures. \\n 26. The method of claim 19, wherein the information in the signatures includes collecting information about at least one of a walking pattern or gait of the subject, the method analyzing the at least one of a walking pattern or gait of the Subject person. \\n 27. The method of claim 19, wherein the information in \\n the signatures includes information about at least one of an iris scan and facial features of the subject person. 28. The method of claim 19, further comprising collecting information in connection with multiple individuals, priori tizing the information, analyzing a portion of the informa \\n tion to make an initial estimate of which of the individuals \\n are candidate individuals for a potential match to the subject person, and incrementally adding other portions of the \\n information to the analysis to assess which of the candidate individuals match the subject person. 29. The method of claim 19, further comprising\\\\applying a monitoring rule wherein at least one of the data gathering agents monitors a particular area in the surveillance region and when an individual loiters in the particular area for more than a select period of time, declaring an alert. 30. The method of claim 19, further comprising directing at least one of the data gathering agents to Zoom in on a face of the individual, and attempting to match the face of the \\n individual to a watch list. \\n 31. The method of claim 19, further comprising scanning \\n data on at least one of a smart card or an identification (ID) \\n card. \\n 32. The method of claim 19, further comprising collecting biometric signatures, temporal information and object track ing locations, the biometric signatures including at least one of hair color, skin tone/color, weight, build, or height, the temporal information including at least one of speed, direc tion, location, past places visited, path, or past activities. 33. The method of claim 19, further comprising position ing first and second data gathering agents, from the multiple \\n data gathering agents, to monitor different first and second areas in the Surveillance region, wherein the subject person travels out of a view of the first data gathering agent and into a view of the second data gathering agent as the subject \\n person travels out of the first area and into the second area. 34. The method of claim 19, wherein the data gathering agents collect multiple data streams including one or more \\n of facial features, Vocal, fingerprint, iris scan, DNA data, soft biometric data, temporal data and used data. 35. The method of claim 19, wherein the data-gathering agents comprise at least one of a camera, chemical sensor, motion detector, door access sensor, capacitive sensor, bio metric sensor, optical sensor, infrared sensor, or security alarm sensor, having a corresponding view of the first or \\n Second areas in the surveillance region. \\n ck ck ck ck ck', metadata={'source': 'https://patentimages.storage.googleapis.com/98/f0/22/ae169ef67be168/US9432632.pdf', 'page': 20})]\n",
      "[Document(page_content='| HAO WANATHA DLA US009971920B2 \\n ( 12 ) United States Patent \\n Derakhshani et al . ( 10 ) Patent No . : US 9 , 971 , 920 B2 ( 45 ) Date of Patent : * May 15 , 2018 \\n ( 54 ) SPOOF DETECTION FOR BIOMETRIC AUTHENTICATION ( 56 ) References Cited \\n U . S . PATENT DOCUMENTS ( 71 ) Applicant : EyeVerify , LLC , Kansas City , KS ( US ) 5 , 291 , 560 A 5 , 303 , 709 A 3 / 1994 Daugman \\n 4 / 1994 Dreher et al . \\n ( Continued ) ( 72 ) Inventors : Reza R . Derakhshani , Shawnee , KS ( US ) ; Casey Hughlett , Lenexa , KS ( US ) ; Jeremy Paben , Prairie Village , \\n KS ( US ) ; Joel Teply , Shawnee , KS ( US ) ; Toby Rush , Roeland Park , KS \\n ( US ) FOREIGN PATENT DOCUMENTS \\n CN CN 101119679 A 2 / 2008 101404059 A 4 / 2009 \\n ( Continued ) ( 73 ) Assignee : EyeVerify LLC , Kansas City , MO ( US ) \\n OTHER PUBLICATIONS ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 \\n U . S . C . 154 ( b ) by 0 days . days . \\n This patent is subject to a terminal dis \\n claimer . Bowyer , K . W . , et al . , “ Image Understanding for Iris Biometrics : A Survey , ” Computer Vision and Image Understanding ; 110 ( 2 ) : 281 \\n 307 ; May 2008 . \\n ( Continued ) \\n ( 21 ) Appl . No . : 14 / 790 , 863 Primary Examiner — Shefali Goradia ( 74 ) Attorney , Agent , or Firm — Goodwin Procter LLP ( 22 ) Filed : Jul . 2 , 2015 \\n ( 65 ) Prior Publication Data \\n US 2016 / 0132735 A1 May 12 , 2016 \\n Related U . S . Application Data \\n ( 63 ) Continuation of application No . 14 / 335 , 345 , filed on Jul . 18 , 2014 , now Pat . No . 9 , 104 , 921 , which is a \\n ( Continued ) ( 57 ) ABSTRACT \\n This specification describes technologies relating to biomet ric authentication based on images of the eye . In general , one aspect of the subject matter described in this specification can be embodied in methods that include obtaining images of a subject including a view of an eye . The methods may \\n further include determining a behavioral metric based on detected movement of the eye as the eye appears in a \\n plurality of the images , determining a spatial metric based \\n on a distance from a sensor to a landmark that appears in a plurality of the images each having a different respective \\n focus distance , and determining a reflectance metric based \\n on detected changes in surface glare or specular reflection \\n patterns on a surface of the eye . The methods may further include determining a score based on the behavioral , spatial , and reflectance metrics and rejecting or accepting the one or \\n more images based on the score . ( 51 ) Int . CI . \\n G06K 9 / 00 ( 2006 . 01 ) ( 52 ) U . S . Cl . CPC . . . . . . . . . . G06K 9 / 00 ( 2013 . 01 ) ; G06K 9 / 00597 \\n ( 2013 . 01 ) ; G06K 9 / 00899 ( 2013 . 01 ) ; G06K 9700906 ( 2013 . 01 ) ( 58 ) Field of Classification Search ??? . . . . . . . . . . . . . . . . . . . . . GO6K 9 / 00597 ; G06K 9 / 00899 See application file for complete search history . 23 Claims , 10 Drawing Sheets \\n 5400 \\n 420 \\n LIGHT SENSOR \\n 4101 \\n 32 \\n 424 \\n 440 \\n AUTHENTICATION \\n MODULE \\n 1450 SECURED DEVICE \\n ACTUATOR \\n 460', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 0}), Document(page_content='US 9 , 971 , 920 B2 Page 2 \\n WO WO - 2008 / 155447 A2 12 / 2008 \\n WO - 2010 / 129074 A111 / 2010 \\n OTHER PUBLICATIONS Related U . S . Application Data \\n continuation of application No . 14 / 216 , 964 , filed on \\n Mar . 17 , 2014 , now Pat . No . 8 , 787 , 628 , which is a continuation of application No . 14 / 059 , 034 , filed on \\n Oct . 21 , 2013 , now Pat . No . 8 , 675 , 925 , which is a continuation of application No . 13 / 888 , 059 , filed on May 6 , 2013 , now abandoned , which is a continuation \\n of application No . 13 / 572 , 097 , filed on Aug . 10 , 2012 , \\n now Pat . No . 8 , 437 , 513 . Bowyer , K . W . , et al . , “ A Survey of Iris Biometrics Research : 2008 - 2010 , ” Handbook of Iris Recognition 2013 ; pp . 15 - 54 ; Jan . \\n 2013 . \\n ( 56 ) References Cited \\n U . S . PATENT DOCUMENTS \\n 5 , 632 , 282 A 5 / 1997 Hay et al . 6 , 095 , 989 A 8 / 2000 Hay et al . \\n 6 , 542 , 624 B1 4 / 2003 Oda \\n 6 , 760 , 467 B1 7 / 2004 Min et al . 6 , 839 , 151 B1 1 / 2005 Andree et al . 7 , 031 , 539 B2 4 / 2006 Tisse et al . 7 , 287 , 013 B2 10 / 2007 Schneider et al . 7 , 327 , 860 B2 2 / 2008 Derakhshani et al . 7 , 336 , 806 B2 2 / 2008 Schonberg et al . 7 , 668 , 351 B1 2 / 2010 Soliz et al . 7 , 801 , 335 B2 9 / 2010 Hanna et al . 7 , 925 , 058 B2 4 / 2011 Lee et al . 8 , 079 , 711 B2 12 / 2011 Stetson et al . \\n 8 , 090 , 246 B2 1 / 2012 Jelinek 8 , 235 , 529 B18 / 2012 Raffle et al . 8 , 251 , 511 B2 8 / 2012 Stetson et al . 8 , 279 , 329 B2 10 / 2012 Shroff et al . 8 , 345 , 935 B2 1 / 2013 Angell et al . 8 , 345 , 945 B2 1 / 2013 Song et al . \\n 8 , 369 , 595 B1 2 / 2013 Derakhshani et al . 8 , 437 , 513 B1 5 / 2013 Derakhshani et al . 8 , 457 , 367 B1 6 / 2013 Sipe et al . 8 , 553 , 948 B2 10 / 2013 Hanna \\n 2005 / 0281440 Al 12 / 2005 Pemer 2005 / 0286801 A112 / 2005 Nikiforov \\n 2006 / 0058682 A1 3 / 2006 Miller et al . 2006 / 0110011 A1 5 / 2006 Cohen et al . 2006 / 0132790 A1 6 / 2006 Gutiin \\n 2006 / 0140460 A16 / 2006 Coutts 2007 / 0110285 Al 5 / 2007 Hanna et al . \\n 2007 / 0286462 Al 12 / 2007 Usher et al . \\n 2007 / 0291277 Al 12 / 2007 Everett et al . 2008 / 0025574 AL 1 / 2008 Morikawa et al . 2008 / 0298642 Al 12 / 2008 Meenen 2010 / 0094262 A1 4 / 2010 Tripathi et al . \\n 2010 / 0128117 A1 5 / 2010 Dyer \\n 2010 / 0142765 Al 6 / 2010 Hamza 2010 / 0158319 Al 6 / 2010 Jung et al . \\n 2010 / 0271471 Al 10 / 2010 Kawasaki et al . 2011 / 0033091 A1 2 / 2011 Fujii et al . \\n 2011 / 0058712 Al 3 / 2011 Sanchez Ramos 2012 / 0163678 A1 6 / 2012 Du et al . 2012 / 0300990 A1 11 / 2012 Hanna et al . 2016 / 0125178 A1 * 5 / 2016 Danikhno . . . . . . . . . . . . . . . GO6F 21 / 32 \\n 726 / 18 Cesar Jr . and Costa , \" Neurcal Cell Classification by Wavelets and Multiscale Curvature , ” Biol Cybern ; 79 : 347 - 360 ; Oct . 1998 . \\n Chen , Z . , et al . , \" A Texture - Based Method for Classifying Cracked Concrete Surfaces From Digital Images Using Neural Networks , ” \\n Proc . of Int \\' l Joint Conference on Neural Networks , IEEE , San Jose , CA ; pp . 2632 - 2637 ; Jul . 31 - Aug . 5 , 2011 . \\n Choras , R . S . , “ Ocular Biometrics - Automatic Feature Extraction From Eye Images , ” Recent Res . in Telecommunications , Informat ics , Electronics and Signal Processing ; pp . 179 - 183 ; May 2011 . \\n Clausi , D . A . and Jernigan , M . E . , “ Designing Gabor Filters for Optimal Texture Separability , ” Pattern Recog ; 33 ( 11 ) : 1835 - 1849 ; \\n Nov . 2000 . Costa , L . d . F . and Cesar Jr , R . M . , \" Shape Analysis and Classifica \\n tion : Theory and Practice , ” CRC Press ; pp . 608 - 615 ; Dec . 2000 . Crihalmeanu , S . , et al . , “ Enhancement and Registration Schemes for Matching Conjunctival Vasculature , ” Proc . of the 3rd IAPR / IEEE International Conference on Biometrics ( ICB ) , Alghero , Italy ; pp . \\n 1247 - 1256 ; Jun . 2009 . \\n Crihalmeanu , S . and Ross , A . , \" Multispectral Scleral Patterns for Ocular Biometric Recognition , ” Pattern Recognition Lett ; \\n 33 ( 14 ) : 1860 - 1869 ; Oct . 2012 .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 1}), Document(page_content='Crihalmeanu , S . and Ross , A . , \" Multispectral Scleral Patterns for Ocular Biometric Recognition , ” Pattern Recognition Lett ; \\n 33 ( 14 ) : 1860 - 1869 ; Oct . 2012 . \\n Crihalmeanu , S . and Ross , A . , \" On the Use of Multispectral \\n Conjunctival Vasculature as a Soft Biometric , ” Proc . IEEE Work shop on Applications of Computer Vision ( WACV ) , Kona , HI ; pp . \\n 204 - 211 ; Jan . 2011 . Derakhshani , R . R . , et al . , “ A New Biometric Modality Based on \\n Conjunctival Vasculature , ” Proc . of the Artificial Neural Networks in Engineering Conference ( ANNIE ) , St . Louis , MO ; pp . 497 - 504 , \\n Nov . 2006 . Derakhshani , R . R . , et al . , \" A Texture - Based Neural Network Clas sifier for Biometric Identification Using Ocular Surface \\n Vasculature , \" Proceedings of International Joint Conference on \\n Neural Networks ( IJCNN ) , Orlando , FL ; Opgs . , Aug . 2007 . Derakhshani , et al . , \" A Vessel Tracing Algorithm for Human Con junctival Vasculature , ” Abstract in Proc . of the Kansas City Life \\n Sciences Institute Research Day Conference , Kansas City , MO ; p . \\n 150 ; Mar . 2006 . Derakhshani , et al . , “ Computational Methods for Objective Assess \\n ment of Conjunctival Vascularity , ” Engineering in Medicine and Biology Society ( EMBC ) , Proc . of the 2012 IEEE Engineering in \\n Medicine and Biology Conference , San Diego , CA ; 4pgs ; Aug . 28 - Sep . 1 , 2012 . \\n Gottemukkula , V . , et al . , \" A Texture - Based Method for Identifica tion of Retinal Vasculature , ” 2011 IEEE International Conference \\n on Technologies for Homeland Security , Waltham , MA ; pp . 434 \\n 439 ; Nov . 15 - 17 , 2011 . Grigorescu , S . E . , et al . , \" Comparison of Texture Features Based on Gabor Filters , ” IEEE Transactions on Image Processing ; 11 ( 10 ) : 1160 - 1167 ; Oct . 2002 . He , X . , et al . , Three - Class ROC Analysis — A Decision Theoretic Approach Under the Ideal Observer Framework ; IEEE Transactions on Med Imaging ; 25 ( 5 ) : 571 - 581 , May 2006 . http : / / appbank . us / healthcare / pulse - phone - put - your - finger - against \\n the - camer - a - to - check - your - pulse - check - your - pulse - every - day - and \\n keep - control - of - your - health , accessed Aug . 8 , 2012 . \\n Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / \\n US2013 / 042889 ; 11pgs . ; dated Oct . 18 , 2013 . Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / US2013 / 043901 ; 11pgs . , dated Aug . 22 , 2013 . Int \\' l Search Report and Written Opinion of the ISA / EP in PCT / \\n US2013 / 043905 ; 9pgs . , dated Nov . 15 , 2013 FOREIGN PATENT DOCUMENTS \\n CN \\n CN \\n CN GB GB \\n WO \\n WO \\n WO \\n WO 101923640 A \\n 102037488 A 102436591 A 2465881 A 2471192 A \\n WO - 01 / 88857 Al WO - 2006 / 119425 A2 WO - 2007 / 127157 A2 \\n WO - 2008 / 030127 AL 12 / 2010 \\n 4 / 2011 \\n 5 / 2012 \\n 6 / 2010 \\n 12 / 2010 11 / 2001 11 / 2006 11 / 2007 \\n 3 / 2008', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 1}), Document(page_content='US 9 , 971 , 920 B2 Page 3 \\n ( 56 ) References Cited \\n OTHER PUBLICATIONS \\n Kalka , N . D . , et al . “ Estimating and Fusing Quality Factors for Iris \\n Biometric Images , ” IEEE Transactions on Systems , Man and Cybernetics , Part A : Systems and Humans ; 40 ( 3 ) : 509 - 524 ; May \\n 2010 . Kirbas , C . and Quek , F . K . H . , “ Vessel Extraction Techniques and Algorithms : A Survey , ” Proc . of the Third IEEE Symposium on Bioinformatics and Bioengineering , Computer Society , Bethesda , \\n MD ; 8pgs ; Mar . 10 - 12 , 2003 . Kollreider , K . , et al . , “ Verifying Liveness by Multiple Experts in Face Biometrics , ” IEEE Comp Soc Conference on Computer Vision and Pattern Recognition Workshops , Anchorage , AK ; pp . 1 - 6 ; Jun . \\n 24 - 26 , 2008 . \\n Li , H . and Chutatape , O . , “ Automated Feature Extraction in Color Retinal Images by a Model Based Approach , ” IEEE Transactions on Biomed Eng ; 51 ( 2 ) : 246 - 254 ; Feb . 2004 . Liu , Z . , et al . , “ Finger Vein Recognition With Manifold Learning , ” J Network and Computer Appl ; 33 ( 3 ) : 275 - 282 ; May 2010 . Liu , C . , et al . , “ Gabor Feature Based Classification Using the Enhanced Fisher Linear Discriminant Model for Face Recognition , \" IEEE Transactions on Image Processing ; 11 ( 4 ) : 467 - 476 ; Apr . 2002 . Owen , C . G . , et al . , \" Optimal Green ( Red - free ) Digital Imaging of Conjunctival Vasculature , ” Opthal Physiol Opt ; 22 ( 3 ) : 234 - 243 ; May 2002 . Proenca , H . , “ Quality Assessment of Degraded Iris Images \\n Acquired in the Visible Wavelength , ” IEEE Transactions on Infor \\n mation Forensics and Security ; 6 ( 1 ) : 82 - 95 ; Mar . 2011 . Randen , T . and Husøy , J . H . , “ Filtering for Texture Classification : A Comparative Study , \" IEEE Transactions on Pattern Analysis and \\n Machine Intell . ; 21 ( 4 ) : 291 - 310 ; Apr . 1999 . Saad , M . A . , et al . , “ Model - Based Blind Image Quality Assessment Using Natural DCT Statistics , ” IEEE Transactions on Image Pro cessing ; X ( X ) : 1 - 12 ; Dec . 2010 . Schwartz , G . and Berry , M . J . , “ Sophisticated Temporal Pattern Recognition in Retinal Ganglion Cells , \" J Neurophysiol ; 99 ( 4 ) : 1787 - 1798 ; Feb . 2008 . Sun , Z . , et al . , “ Improving Iris Recognition Accuracy via Cascaded Classifiers , ” IEEE Transactions on Systems , Man , and Cybernet ics — Part C : Applications and Reviews ; 35 ( 3 ) : 435 - 441 ; Aug . 2005 . Tanaka , T . and Kubo , N . , “ Biometric Authentication by Hand Vein Patterns , \" Soc . of Instrument and Control Engineers ( SICE ) Annual Conference , Sapporo , Japan ; pp . 249 - 253 ; Aug . 4 - 6 , 2004 . Tanaka , H . , et al . , Texture Segmentation Using Amplitude and \\n Phase Information of Gabor Filters , Elec . Comm . in Japan , Part . 3 ; \\n 87 ( 4 ) : 66 - 79 ; Apr . 2004 . Tankasala , S . P . , et al . , “ Biometric Recognition of Conjunctival \\n Vasculature Using GLCM Features , ” 2011 Int \\' l Conference on Image Information Processing ( ICIIP ) , Waknaghat , India ; 6pgs ; \\n Nov . 3 - 5 , 2011 . \\n Tankasala , S . P . et al . , “ Classification of Conjunctival Vasculature Using GLCM Features , ” 2011 Int \\' l Conference on Image Informa \\n tion Processing ( ICIIP ) , Waknaghat , India ; opgs ; Nov . 2011 . Tankasala , S . P . , et al . , “ Visible Light , Bi - Modal Ocular Biometrics , ” \\n Proc . of the 2012 2nd International Conference on Communication Computing and Security ( ICCCS ) , Rourkela , India ; 9pgs ; Oct . 6 - 8 , \\n 2012 . Thomas , N . L . , et al . , \" A New Approach for Sclera Vein Recogni tion , ” Proc . SPIE 7708 , Mobile Multimedia / Image Processing , Security , and Applications 2010 ; 7708 ( 1 ) : 1 - 10 ; Apr . 2010 . Toth , B . , “ Liveness Detection : Iris , ” Encyclo . Biometrics ; 2 : 931 \\n 938 ; Jan . 2009 . Wu , C . and Harada , K . , “ Extraction and Digitization Method of \\n Blood Vessel in Scleraconjunctiva Image , ” Int \\' l J Computer Sci and', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 2}), Document(page_content='938 ; Jan . 2009 . Wu , C . and Harada , K . , “ Extraction and Digitization Method of \\n Blood Vessel in Scleraconjunctiva Image , ” Int \\' l J Computer Sci and \\n Network Security ; 11 ( 7 ) : 113 - 118 ; Jul . 2011 . Zhou , Z . , et al . , “ A Comprehensive Sclera Image Quality Measure , ” \\n 2010 11th Int \\' l . Conf . Control , Automation , Robotics and Vision ( ICARCV ) , Singapore ; pp . 638 - 643 ; Dec . 7 - 10 , 2010 . Zhou , Z . , et al . , \" A New Human Identification Method : Sclera Recognition , ” IEEE Transactions on Systems , Man , and Cybernet ics Part A : Systems and Humans ; 42 ( 3 ) : 571 - 583 ; May 2012 . Zhou , Z . , et al . , “ Multi - angle Sclera Recognition System , ” 2011 IEEE Workshop on Computational Intelligencein Biometrics and Identity Management ( CIBIM ) , Paris ; 6pgs ; Apr . 11 , 2011 . \\n Zhou , Z . , et al . , “ Multimodal Eye Recognition , ” Mobile Multime dia / Image Processing , Security , and Applications 2010 ; 7708 : 1 - 10 ; Apr . 2010 . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 2}), Document(page_content='bols US 9 , 971 , 920 B2 \\n Emman \\n WALTER AR \\n T S01141140109101011001012 \\n A1401411 Sheet 1 of 10 \\n - OLI Wenu APA ZOT May 15 , 2018 \\n WANIUM Wwwwwww \\n W \\n WW . KAN PROPURU \\n 091 OZI OZI \\n M \\n ELEKLE OET \\n OVT \\n OTL \\n OSL U . S . Patent \\n 00L', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 3}), Document(page_content='U . S . Patent May 15 , 2018 Sheet 2 of 10 US 9 , 971 , 920 B2 \\n 210 \\n kannt ZA woor \\n 220 \\n 225 mmmmmmmmmmmmmmmmmmmmmmm FIG . 2 \\n Town wwwwwwwwwwwwww MORENOMENOCH 240 . \\n . ITTELIER \\n - mememmning \\n 002 230', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 4}), Document(page_content='€ \\' O13 US 9 , 971 , 920 B2 \\n * * LEKULLA \\n R \\n I w \\n MATEMATIKENANCE \\n ww . mim immer * \\n HOTEL * * * \\n www \\n var ve ww www . wwww wamewaong \\n A \\n - \\n 22€ € \\n 07€ her Sony \\n w \\n ww \\n \" \\n wy wielu www \\n YOK ww www ww Lebanon WAUWAPENANGANI \\n wys \\n . \\n com 088 Z€€ \\n CASAAAAAAA Sheet 3 of 10 \\nwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww SI DAN KAWA \\n CERNING \\n AU P May 15 , 2018 \\n OSTURE \\n WWWWWWWWWW som many \\n WAND thichchh \\n 01€ U . S . Patent \\n even 00€', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 5}), Document(page_content='400 \\n ????? … ” U . S . Patent \\n 420 \\n 4 - 4 - toc - dette LGT SERS \\n MAMAHA Monorms MAHAMAHANNA \\n ??MATH were 432 May 15 , 2018 \\n 44 f reen 434 \\n ????? My Wii - ? : m - aris Hom PyrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrranPHPWITH \\n ) MAN \\n Airiti 44 \\n MASTERSE 4 4 AMAHAHAHAMAHANNAHAMAHAMAHA Sheet 4 of 10 \\n with the is the in AUTHENTICATION MODULE \\n EELEPHTHERLANTINAIL14 on on \\n HAHAMAHHHHHHHHH 450 \\n SECURED DEVICE ACTUATOR TEL US 9 , 971 , 920 B2 \\n 4466 \\n FG . 4', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 6}), Document(page_content='?? 5302 \\n 5 . ) \" c } { } . \" U . S . Patent \\n 5 16 \\n punsume ven mur ) 4 . 7 3 \\n 8 Ep??? 8 . \\n 524 . 504 ???? •Views ?? ?? ?? ? sta 523 \\n ~ ~ . . SECURE TRANSACTION SERVICE May 15 , 2018 \\npurposuerzburupitizen Network . - . - . \\n ? 535 \\n 526 - » » ~ ~ ~ ~ ~ AUTHENTICATION MODULE | 4 . AA - - - SI - ha . l - ela - . - . - . - 4 . 1 . \\n 1 . A 11 \\n 58 \\n 528 » Sheet 5 of 10 \\n uhs » x ??? » Fre ?K???????????????????????????????????? = 543 \\n ??? 550 . . . . ? » - - l ru . AUTHENTICATION MODULE ???????????????????????????????????? \\n \" + to 1 ~ r ?? ?? ??? \\n i | . . \\n : » \" . \" saw ta - - . \\n • \\n » » \\n » \\n 530 » » AUTHENT CATEG8 | \\n | APPY | \\n » \\n » » - - 114 1 - - - - - - \\n ?? ?? ~ - \\n - » ? 3 ??? xx » ? . ??? ??? ??? ???? . ? . ?? ?? | US 9 , 971 , 920 B2 \\n FIG . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 7}), Document(page_content='U . S . Patent May 15 , 2018 Sheet 6 of 10 US 9 , 971 , 920 B2 \\n 600 WWW Wh 75 602 Obtain image ( s ) of eye \\n 5 604 WWW Determine liveness score for image ( s ) \\n 5 606 BORDERS 616 yes Live eye depicted ? 608 \\n Accept image ( s ) Reject image ( s ) \\n 610 \\n 620 Report spoof attack Segment image ( s ) into regions \\n OOOO \\n Preprocess the images regions \\n Wwwwwwwwwwwwwwwwwww na UHE \\n Determine features for each region \\n Determine match score based on the features and reference features 5 626 \\n 5 628 \\n yes \\n wwwwwwwwwwwwwwww Match ? 6327 25 www Accept Reject Reject \\n Fig . 6', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 8}), Document(page_content='Fig . 7 US 9 , 971 , 920 B2 \\n Return liveness score W \\n WW . \\n MAMAN \\n wwwwwwwww \\n ww liveness metrics Determine liveness score based on \\n VODOOOOOOOOOO wwwxxxwwwwwwwwwwxxx \\n xxxXW Sheet 7 of 10 \\n MUDOVAVAVAATLUND A mananananana Bont Determine reflectance metric \\n Determine spatial metric \\n were \\n Determine behavioral metric May 15 , 2018 \\n wwwwwwwwwwww 914 \\n 714 \\n TIL ST CondoKOFFREDOXXO9FCARDO \\n SESROCK AS a \\nwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww \\n wwww \\n ADAZKORXX SERADECKERXX TOFFORSIKK OASICKOR WIR WKWW . ROMWWWWWWWWWWWWWW \\n * * \\n OK \\n * * \\n * \\n * * * * \\n * * * * * * \\n * * * * * * * * * * * * * * * * * * * * \\n * \\n * \\n * \\n * * * \\n * * \\n * * * \\n * * * \\n * * * * \\n * * * * * Determine liveness metric ( s ) for the image ( s ) OIL ) \\n image ( s ) determination for the Start liveness score ZOL ) \\n 700 U . S . Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 9}), Document(page_content='800 U . S . Patent \\nCOELLOOOOOOOOOOOOOOOOOORGELOODIKSLUOSTELUSSOCIOECENT COLOULOOSIOS S 810 \\n Apply photic stimuli \\n + F TFFFFFFF + 7 + 7 1 \\n Capture sequence of images 2 May 15 , 2018 \\n w w wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww wwwwwww pues 814 \\n Determine diameter of pupil in each image \\n wwwwwwwuuuuuuuuuuuuuuuuwwww anninen M 15 816 \\n Determine motion parameters from sequence of pupil diameters Sheet 8 of 10 \\n int Determine behavioral metric as distance between motion parameters and expected motion parameters \\n W * Fig . 8A US 9 , 971 , 920 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 10}), Document(page_content='U . S . Patent \\n 820 0 . 922020 . O O O . . 27 . 222 . 22 . 22 . 22 . 2 . 4 . 0 . 0 . 24 . 04 . 830 \\n Apply external stimuli 22 \\n wwwwwwwwww Capture sequence of images May 15 , 2018 \\nwwwwwwwwwwwwwwwwwwwwwwwww www 834 \\n Determine location of iris in each image 15836 \\n Determine motion parameters from sequence of iris locations Sheet 9 of 10 \\n ODOOOOOOOOOOOOO O OOOOOOOOOOOOOOOOD moodcocoonozcocacoon Determine behavioral metric as distance between motion parameters and expected motion parameters US 9 , 971 , 920 B2 \\n Fig . 8B', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 11}), Document(page_content='Fig . 9 US 9 , 971 , 920 B2 \\n gsi 7 \\n 796 m 054 \\n 896 - 096 m2 \\n 2480 000 0000 / \\n orientation \\n 60000000000000000 wwwwwwwwwww \\n 0000000000pxogo d o ??????????????? ?????? , ansens meracun seramai meme news season \\n C 996 ON \\nZL186 KWD \\n m \\n ange won \\n plan \\n poso m \\n costo 2000 \\n widoo \\n D www \\n moooooooooooooo o \\n 96€ 7 9707 5968 \\n 9747 972 . 9507 Sheet 10 of 10 \\n C086 \\n MATEKOHALIKIB OUR WWWWWWWWWWW \\n MODO MMMMM \\n 2 \\n 924 \\n - 914 wwwwwwwwwww \\n wwwww \\n w \\n concorso corpo conosconocoooooooooooooowwwwww 00000 \\n 0 \\n00000000000000000000000000000000000000000000000 \\n 000000000000000 \\n 00000600000000000000 \\n 0 . U 1 \\n UEVA YORK EVO \\n * * * * \\n * * * * * * * * * * * * * * 000000 \\n * * * * * * * \\n * * * * * * * 000 \\n 0 \\n 0 00000 \\n 0 0000 May 15 , 2018 \\n V HODIN W \\n OL6 114 youw \\n wwwwwwingo 922 \\n w \\n wwwwwwwwwww rimm ing HY \\n 06 ONDOR \\n ? \\n runninn \\n * * * * * * * * * manning \\n NAMANG AWAY WWWWWwwwwwwwww \\n w wwwwwwwwwwwwwwwWWWWWWWWWWWWWWWWWWwwwwwwwwwwwwwwwww \\n . S \\n port \\n A NAKKOR KERAMIK asiaan \\n www Die \\n wwwNKA \\n 76 \\n 2 706 \\n 6806 cm 806 U . S . Patent \\n 2006 4 006 \\n C 946', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 12}), Document(page_content='US 9 , 971 , 920 B2 \\n SPOOF DETECTION FOR BIOMETRIC a sensor configured to capture two or more images of a \\n AUTHENTICATION subject including a view of an eye , wherein the images collectively include a plurality of focus distances . The \\n CROSS REFERENCE TO RELATED system may further include an illumination element provide \\n APPLICATION 5 photic stimuli in synchronization with the capture of one or more images by the sensor . The system may further include \\n This application is a continuation of , and claims priority a means for determining a behavioral metric based on , at to . patented U . S . patent application Ser . No . 14 / 335 . 345 . least , detected movement of the eye as the eye appears in a \\n filed on Jul . 18 , 2014 , entitled “ Spoof Detection for Bio plurality of the images . The behavioral metric is a measure \\n metric Authentication . ” which is a continuation of , and 10 of deviation of detected movement and timing from claims priority to , patented U . S . patent application Ser . No . expected movement of the eye . The system may further \\n 14 / 216 , 964 , filed on Mar . 17 , 2014 , entitled “ Spoof Detec include a module configured to determine a spatial metric \\n tion for Biometric Authentication , \" which is a continuation based on , at least , a distance from a sensor to a landmark that \\n of , and claims priority to , patented U . S . patent application appears in a plurality of the images each having a different appe Ser . No . 14 / 059 . 034 . filed on Oct . 21 . 2013 . entitled “ Spoof 15 respective focus distance . The system may further include a \\n Detection for Biometric Authentication , ” which is a con module configured to determine a reflectance metric based \\n tinuation of , and claims priority to , abandoned U . S . patent on , at least , detected changes in surface glare or specular application Ser . No . 13 / 888 , 059 , filed on May 6 , 2013 , reflection patterns on a surface of the eye as the eye appears \\n entitled “ Spoof Detection for Biometric Authentication , \" in a plurality of the images , wherein the reflectance metric \\n which is a continuation of , and claims priority to , patented 20 is a measure of changes in glare or specular reflection \\n U . S . patent application Ser . No . 13 / 572 , 097 , filed on Aug . patches on the surface of the eye . The system may further \\n 10 , 2012 , and entitled \" Spoof Detection for Biometric include a module configured to determine a score based on , \\n Authentication . ” The disclosures of the foregoing applica at least , the behavioral , spatial , and reflectance metrics . The system may further include an interface configured to reject tions are incorporated herein in their entirety . 25 or accept the one or more images based on the score . \\n TECHNICAL FIELD In general , one aspect of the subject matter described in this specification can be embodied in a system that includes The present disclosure relates to biometric authentication a data processing apparatus and a memory coupled to the \\n based on images of the eye . data processing apparatus . The memory having instructions \\n 30 stored thereon which , when executed by the data processing \\n BACKGROUND apparatus cause the data processing apparatus to perform \\n operations including obtaining two or more images of a It is often desirable to restrict access to property or subject including a view of an eye , wherein the images resources to particular individuals . Biometric systems may collectively include a plurality of focus distances . The be used to authenticate the identity of an individual to either 35 operations may further include determining a behavioral grant or deny access to a resource . For example , iris scanners metric based on , at least , detected movement of the eye as may be used by a biometric security system to identify an the eye appears in a plurality of the images . The behavioral', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 13}), Document(page_content=\"individual based on unique structures in the individual ' s iris . metric may be a measure of deviation of detected movement \\n and timing from expected movement of the eye . The opera \\n SUMMARY 40 tions may further include determining a spatial metric based \\n on , at least , a distance from a sensor to a landmark that This specification describes technologies relating to bio appears in a plurality of the images each having a different \\n metric authentication based on images of the eye . In general , respective focus distance . The operations may further \\n one aspect of the subject matter described in this specifica include determining a reflectance metric based on , at least , \\n tion can be embodied in a method that includes obtaining 45 detected changes in surface glare or specular reflection two or more images of a subject including a view of an eye , patterns on a surface of the eye as the eye appears in a \\n wherein the images collectively include a plurality of focus plurality of the images , wherein the reflectance metric is a distances . The method may further include determining a measure of changes in glare or specular reflection patches on behavioral metric based on , at least , detected movement of the surface of the eye . The operations may further include \\n the eye as the eye appears in a plurality of the images . The 50 determining a score based on , at least , the behavioral , \\n behavioral metric may be a measure of deviation of detected spatial , and reflectance metrics . The operations may further movement and timing from expected movement of the eye . include rejecting or accepting the one or more images based The method may further include determining a spatial metric on the score . \\n based on , at least , a distance from a sensor to a landmark that In general , one aspect of the subject matter described in \\n appears in a plurality of the images each having a different 55 this specification can be embodied in a non - transient com \\n respective focus distance . The method may further include puter readable media storing software including instructions determining a reflectance metric based on , at least , detected executable by a processing device that upon such execution changes in surface glare or specular reflection patterns on a cause the processing device to perform operations that surface of the eye as the eye appears in a plurality of the include obtaining two or more images of a subject including images , wherein the reflectance metric is a measure of 60 a view of an eye , wherein the images collectively include a \\n changes in glare or specular reflection patches on the surface plurality of focus distances . The operations may further \\n of the eye . The method may further include determining a include determining a behavioral metric based on , at least , score based on , at least , the behavioral , spatial , and reflec detected movement of the eye as the eye appears in a tance metrics . The method may further include rejecting or plurality of the images . The behavioral metric may be a \\n accepting the one or more images based on the score . 65 measure of deviation of detected movement and timing from In general , one aspect of the subject matter described in expected movement of the eye . The operations may further \\n this specification can be embodied in a system that includes include determining a spatial metric based on , at least , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 13}), Document(page_content='US 9 , 971 , 920 B2 \\n distance from a sensor to a landmark that appears in a reliably authenticating individuals . Some implementations plurality of the images each having a different respective may prevent spoofing of an eye biometric based authenti focus distance . The operations may further include deter - cation system using objects that are not a living human eye . \\n mining a reflectance metric based on , at least , detected The details of one or more embodiments of the invention changes in surface glare or specular reflection patterns on a 5 are set forth in the accompanying drawings and the descrip \\n surface of the eye as the eye appears in a plurality of the tion below . Other features , aspects , and advantages of the \\n images , wherein the reflectance metric is a measure of invention will become apparent from the description , the changes in glare or specular reflection patches on the surface drawings , and the claims . of the eye . The operations may further include determining \\n a score based on , at least , the behavioral , spatial , and 10 BRIEF DESCRIPTION OF THE DRAWINGS reflectance metrics . The operations may further include rejecting or accepting the one or more images based on the FIG . 1 is a diagram of the anatomy of a human eye . \\n score . FIG . 2 is a diagram of an example image including These and other embodiments can each optionally include portions showing vasculature of the white of an eye . one or more of the following features . Determining the 15 FIG . 3 is a diagram of an example image that is segmented behavioral metric may include determining an onset , dura - for analysis . tion , velocity , or acceleration of pupil constriction in FIG . 4 is a block diagram of example security system that response to photic stimuli . The photic stimuli may include a is configured to authenticate an individual based in part on flash pulse . The photic stimuli may include a change in the one or more images of the white of an eye . \\n intensity of light output by a display . The determining the 20 FIG . 5 is a block diagram of an example online environ behavioral metric may include determining an onset , dura ment . tion , or acceleration of gaze transition in response to external FIG . 6 is a flow chart of an example process for authen stimuli . The external stimuli may include prompts for ticating an individual based on one or more images of the instructing a user to direct gaze . The external stimuli may white of an eye , where the liveness of the eye in the obtained include an object depicted in a display that moves within the 25 images for authentication is checked . \\n display . The spatial metric may be a measure of deviation of FIG . 7 is a flow chart of an example process for deter the subject from a two - dimensional plane . The spatial metric mining a liveness score for one or more images of an eye . may be a measure of deviation of the subject from an FIG . 8A is a flow chart of an example process for expected three - dimensional shape . Determining the spatial determining a behavioral metric based on constriction of a metric may include determining parallax of two or more 30 pupil in response to photic stimulus . \\n landmarks that appear in a plurality of the images . Half - FIG . 8B is a flow chart of an example process for tones may be detected in an image captured using reduced determining a behavioral metric based on gaze transition of dynamic range and the images may be rejected based at least an iris in response to external stimulus . in part on the half - tones . Determining the behavioral metric FIG . 9 shows an example of a computer device and a may include detecting blood flow of the eye as the eye 35 mobile computer device that can be used to implement the appears in a plurality of the images . Determining the score techniques described here . \\n may include using a trained function approximator to deter', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 14}), Document(page_content=\"may include using a trained function approximator to deter \\n mine the score . The landmark may be a portion of a face DETAILED DESCRIPTION depicted in the images . Determining the reflectance metric may include pulsing a flash to illuminate the subject while 40 Distinctive features of an individual ' s visible vasculature one or more of the images are being captured , detecting the in the whites of the eyes may be used to identify or \\n appearance of glare on the eye from the flash in the images , authenticate the individual . For example , images of the \\n and measuring the time difference between the pulsing of the white of a user ' s eye can be obtained and analyzed to flash and the appearance of a corresponding glare on the eye compare features of the eye to reference record in order to \\n in the images . Determining the reflectance metric may 45 authenticate the user and grant or deny the user access to a \\n include pulsing a flash to illuminate the subject while one or resource . Adversaries or intruders could attempt spoof a \\n more of the images are being captured and detecting fine security system using such an authentication method by three dimensional texture of a white of the eye by measuring presenting something other than a live eye ( e . g . , a picture of \\n uniformity of a pattern of glare on the eye from the flash in an authorized user ' s face or a plastic model of an authorized \\n the images . A sensor setting that controls focus may be 50 user ' s eye ) to the security system ' s light sensor . Some spoof \\n adjusted to a plurality of different settings during capture of attempts may be frustrated by configuring a security system \\n two or more of the images . The images captured with to analyze the obtained images to discriminate images of \\n different focus settings may be compared to determine live eyes from images of props . \\n whether these images reflect their respective focus settings . One or more liveness metrics can be calculated that reflect A sensor setting that controls exposure may be adjusted to a 55 properties a live eye is expected to exhibit that may not be plurality of different settings during capture of two or more exhibited by certain spoof attempts . For example , stimuli of the images . The images captured with different exposure can be applied to a user during the image acquisition process \\n settings may be compared to determine whether these and the response of an eye depicted in the images may be images reflect their respective exposure settings . A sensor quantified with a metric compared to an expected response \\n setting that controls white balance may be adjusted to a 60 of a live eye to those stimuli . In some implementations , the \\n plurality of different settings during capture of two or more obtained images can be checked at a plurality of focus \\n of the images . The images captured with different white distances to determine if the eye depicted in the images is balance settings may be compared to determine whether three dimensional ( e . g . , does it have landmarks that appear these images reflect their respective white balance settings . to be positioned at distances from the sensor that deviated Particular embodiments of the invention can be imple - 65 from a single plane ) . In some implementations , a metric \\n mented to realize none , one or more of the following related to the reflectance of the eye may be determined . A advantages . Some implementations may provide security by live eye has unique reflectance properties caused by its three\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 14}), Document(page_content=\"US 9 , 971 , 920 B2 \\n dimensional shape and its fine surface texture and moisture be preprocessed and segmented to isolate regions of interest \\n that may not be exhibited by many spoof attack props . For within the image and enhance the view of vasculature in the example , a flash device may be used to illuminate the subject whites of the eyes . For example , the regions of interest may during a portion of the image acquisition process and the be tiled portions that form grids covering some or all the \\n timing and quality of the reflection of the flash pulse on the 5 whites of the eyes . A portion 320 of the corresponding to the subject ' s eye may analyzed to determine if it is indeed a live white of the right eye left of the iris may be isolated , for eyeball being imaged in real time . example , by identifying the corneal limbus boundary and the In some implementations , a plurality of liveness metrics edges of the eyelids . Similarly , a portion 322 corresponding may be combined to determine a liveness score or decision to the white of the left eye left of the iris may be isolated . that reflects the likelihood that the images depict a live eye , 10 D Preprocessing may be used to enhance the view of the as opposed to , for example , an image of model or a two vasculature in this region , for example , by selecting a dimensional picture of an eye . For example , a trained component color from the image data that maximizes the function approximator ( e . g . , a neural network ) can be used to determine , based on a plurality of liveness metrics , a contrast between the vasculature and the surrounding white \\n liveness score . The images obtained can then be accepted or 15 por 5 portions of the whites of the eyes . In some implementations , \\n rejected based on the liveness score . In some implementa these portions 320 , 322 of the image may be further seg \\n tions , a spoof attempt may be reported when the liveness mented into tiles forming grids 330 , 332 that divide an \\n score indicates that the images do not depict a live eye . exposed surface area of the whites of the eyes into smaller \\n FIG . 1 is a diagram of the anatomy of a human eye 100 . regions for analysis purposes . Features of the vasculature in The diagram is a cross - section of the eye with a blowup 102 20 these regions of interest may be used for identification , of the anatomy near the corneal limbus boundary of the eye verification , or authentication of an individual . that separates the colored iris 110 from the surrounding FIG . 4 is a block diagram of example security system 400 \\n white of the eye . The white of the eye includes a complex that is configured to authenticate an individual based in part vascular structure which is not only readily visible and on one or more images of the white of an eye 410 . A user of scannable from outside of the eye , but in addition that 25 the security system 400 may present their eye 410 to a light \\n vascular structure is unique and varies between individuals . sensor 420 . In this manner one or more images of the white \\n Thus , these vascular structures of the white of the eye , of the eye 410 may be captured . A digital camera , a \\n mostly due to vasculature of conjunctiva and episclera , can three - dimensional ( 3D ) camera , and a light field sensor are be scanned and advantageously used as a biometric . This examples of light sensors that may be employed . The light biometric can be used to authenticate a particular individual , 30 sensor 420 may employ a variety of technologies , e . g . , or , identify an unknown individual . digital charge - coupled devices ( CCD ) or complementary\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content='The white of the eye has a number of layers . The sclera metal - oxide - semiconductors ( CMOS ) . In some implemen 120 is an opaque , fibrous , protective , layer of the eye tations , the user may be prompted via messages shown on containing collagen and elastic fiber . The sclera 120 is display 424 to make certain poses to expose portions of the covered by the episclera 130 , which has a particularly large 35 white of the eye 410 and facilitate image acquisition . For \\n number of blood vessels and veins that that run through and example , the user may be prompted to direct their gaze in \\n over it . The episclera 130 is covered by the bulbar conjunc order to roll the iris of their eye 410 left , right , up , up - left , tiva 140 , which is a thin clear membrane that interfaces with and roll up - right . In some implementations , not shown , the the eyelid 150 or the environment when the eyelid is opened . user may be prompted to assume poses though messages Blood vessels and veins run through all of these layers of the 40 played through a speaker , through indicator lights ( e . g . white of the eye and can be detected in images of the eye . LEDs ) , or not prompted at all . The eye also includes eyelashes 160 that may sometimes In some implementations , the sensor 420 can be config obscure portions of the white of the eye in an image . ured to detect when the eye 410 has been properly positioned FIG . 2 is a diagram of an example image 200 including in the field of view of the sensor . Alternatively , software or \\n portions showing vasculature of the white of an eye . Such an 45 firmware implemented on a computing device 430 can \\n image 200 may be captured with a sensor ( e . g . , a camera ) analyze one or more images produced by the light sensor \\n that is integrated into a computing device such as , for 420 to determine whether the eye 410 has been properly \\n example , a smart phone , a tablet computer , a television , a positioned . In some implementations , the user may manually laptop computer , or a personal computer . For example , a indicate when the eye 410 is properly positioned through a \\n user may be prompted through a display or audio prompt to 50 user interface ( e . g . , button , keyboard , keypad , touchpad , or look to the left while the image is captured , thus exposing a touch screen ) . \\n larger area of the white of the eye to the right of the iris to An authentication module 440 implemented on the com \\n the view of the sensor . Similarly , a user may be prompted to puting device 430 may obtain one or more images of the look right , up , down , straight , etc . while an image is cap - white of the eye through the light sensor 420 . In some \\n tured . The example image includes a view of an iris 220 with 55 implementations , the computing device 430 is integrated \\n a pupil 210 at its center . The iris 220 extends to the corneal with or electrically coupled to the light sensor 420 . In some \\n limbus boundary 225 of the eye . The white 230 of the eye implementations , the computing device 430 may communi \\n is external to a corneal limbus boundary 225 of the eye . An cate with the light sensor 420 through a wireless interface extensive vasculature 240 of the white of the eye is visible ( e . g . , an antenna ) . in the image 100 . This vasculature 240 may be distinctive for 60 The authentication module 440 processes images an individual . In some implementations , distinctive features obtained through the light sensor 420 to control access to a of the vasculature 240 may be used as a basis for identifying , secured device 450 . For example , the authentication module verifying , or authenticating an individual user . 440 may implement authentication processes described in FIG . 3 is a diagram of an example image 300 , including relation to FIG . 6 . In some implementations , the secured', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content='portions showing vasculature of the whites of two eyes , that 65 device 450 may include an actuator 460 ( e . g . , a locking is segmented for analysis . A captured image 310 may be mechanism ) that affects the access control instructions from obtained in a variety of ways . The captured image 310 may the authentication module 440 . nu', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 15}), Document(page_content=\"US 9 , 971 , 920 B2 \\n The computing device may be integrated with or interface configured to communicate with a first server system 512 \\n with the secured device 450 in a variety of ways . For and / or a second server system 514 over a network 511 . example , the secured device 450 may be an automobile , the Computing devices 502 , 504 , 506 , 508 , 510 have respective light sensor 420 may be a camera integrated in the steering users 522 , 524 , 526 , 528 , 530 associated therewith . The first wheel or dashboard of the automobile , and the computing 5 and second server systems 512 , 514 each include a comput \\n device 430 may be integrated in the automobile and elec - ing device 516 , 517 and a machine - readable repository , or \\n trically connected to the camera and an ignition locking database 518 , 519 . Example environment 500 may include \\n system that serves as the security actuator 460 . A user may many thousands of Web sites , computing devices and serv present views of the whites of their eye to the camera in ers , which are not shown . order to be authenticated as an authorized driver of the 10 Network 511 may include a large computer network , \\n automobile and start the engine . examples of which include a local area network ( LAN ) , In some implementations , the secured device 450 may be wide area network ( WAN ) , the Internet , a cellular network , \\n a real estate lock box , the light sensor 420 may be a camera or a combination thereof connecting a number of mobile integrated with the user ' s mobile device ( e . g . , a smartphone computing devices , fixed computing devices , and server \\n or tablet device ) , and the processing of the authentication 15 systems . The network ( s ) included in network 511 may module 440 may be performed in part by the user ' s mobile provide for communications under various modes or proto device and in part by a computing device integrated with the cols , examples of which include Transmission Control Pro \\n lock box that controls a power locking mechanism . The two tocol / Internet Protocol ( TCP / IP ) , Global System for Mobile computing devices may communicate through a wireless communication ( GSM ) voice calls , Short Electronic mes interface . For example , the user ( e . g . , a realtor giving a 20 sage Service ( SMS ) , Enhanced Messaging Service ( EMS ) , showing of a property ) may use the camera on their mobile or Multimedia Messaging Service ( MMS ) messaging , Eth \\n device to obtain one or more images and submit data based ernet , Code Division Multiple Access ( CDMA ) , Time Divi on the images to the lock box in order to be authenticated as sion Multiple Access ( TDMA ) , Personal Digital Cellular authorized user and granted access to keys stored in the lock ( PDC ) , Wideband Code Division Multiple Access \\n box . 25 ( WCDMA ) , CDMA2000 , or General Packet Radio System In some implementations , the secured device 450 is a gate ( GPRS ) , among others . Communication may occur through or door that controls access to a property . The light sensor a radio - frequency transceiver . In addition , short - range com 420 may be integrated in the door or gate or positioned on munication may occur , e . g . , using a BLUETOOTH , WiFi , or \\n a wall or fence near the door or gate . The computing device other such transceiver system . 430 may be positioned nearby and may communicate 30 Computing devices 502 , 504 , 506 , 508 , 510 enable through a wireless interface with the light sensor 420 and a respective users 522 , 524 , 526 , 528 , 530 to access and to \\n power locking mechanism in the door or gate that serves as view documents , e . g . , web pages included in web sites . For \\n an actuator 460 . In some implementations , the secured example , user 522 of computing device 502 may view a web \\n device 450 may be a rifle and the light sensor 420 may be page using a web browser . The web page may be provided\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"device 450 may be a rifle and the light sensor 420 may be page using a web browser . The web page may be provided \\n integrated with a scope attached to the rifle . The computing 35 to computing device 502 by server system 512 , server \\n device 430 may be integrated in the butt of the rifle and may system 514 or another server system ( not shown ) . electronically connect to the light sensor 420 and a trigger or In example environment 500 , computing devices 502 , hammer locking mechanism that serves as an actuator 460 . 504 , 506 are illustrated as desktop - type computing devices , In some implementations , the secured device 450 may be a computing device 508 is illustrated as a laptop - type com piece of rental equipment ( e . g . , a bicycle ) . 40 puting device 508 , and computing device 510 is illustrated The computing device 430 may include a processing as a mobile computing device . It is noted , however , that device 432 ( e . g . , as described in relation to FIG . 9 ) and a computing devices 502 , 504 , 506 , 508 , 510 may include , machine - readable repository , or database 434 . In some e . g . , a desktop computer , a laptop computer , a handheld \\n implementations , the machine - readable repository may computer , a television with one or more processors embed \\n include flash memory . The machine - readable repository 434 45 ded therein and / or coupled thereto , a tablet computing \\n may be used to store one or more reference records . A device , a personal digital assistant ( PDA ) , a cellular tele \\n reference record may include data derived from one or more phone , a network appliance , a camera , a smart phone , an \\n images of the white of an eye for a registered our authorized enhanced general packet radio service ( EGPRS ) mobile \\n user of the secured device 450 . In some implementations , phone , a media player , a navigation device , an electronic \\n the reference record includes complete reference images . In 50 messaging device , a game console , or a combination of two \\n some implementations the reference record includes features or more of these data processing devices or other appropriate \\n extracted from the reference images . In some implementa data processing devices . In some implementations , a com \\n tions the reference record includes encrypted features puting device may be included as part of a motor vehicle extracted from the reference images . In some implementa - ( e . g . , an automobile , an emergency vehicle ( e . g . , fire truck , \\n tions the reference record includes identification keys 55 ambulance ) , a bus ) . encrypted by features extracted from the reference images . Users interacting with computing devices 502 , 504 , 506 , \\n To create a reference record for a new user and enrollment 508 , 510 can interact with a secure transaction service 523 \\n or registration process may be carried out . An enrollment hosted , e . g . , by the server system 512 , by authenticating \\n process may include the capture of one or more reference themselves and issuing instructions or orders through the images of the white of a new registered user ' s eye . In some 60 network 511 . The secure transactions may include , e . g . , implementations , the enrollment process may be performed e - commerce purchases , financial transactions ( e . g . , online using the light sensor 420 and processing device 430 of banking transactions , credit or bank card transactions , loy \\n authentication system 400 . alty reward points redemptions ) , or online voting . The FIG . 5 is a block diagram showing an example of a secured transaction service may include an authentication \\n network environment 500 on which the techniques described 65 module 525 that coordinates authentication of users from the \\n herein may be implemented . Network environment 500 secured server ' s side of the interaction . In some implemen\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"herein may be implemented . Network environment 500 secured server ' s side of the interaction . In some implemen \\n includes computing devices 502 , 504 , 506 , 508 , 510 that are tations , authentication module 525 may receive image data\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 16}), Document(page_content=\"US 9 , 971 , 920 B2 \\n 10 \\n from a user device ( e . g . , computing devices 502 , 504 , 506 , processing may be transmitted to an authentication module \\n 508 , 510 ) that includes one or more images of the eye of a ( e . g . , authentication modules 525 or 540 ) . In this manner , the \\n user ( e . g . , users 522 , 524 , 526 , 528 , 530 ) . The authentication authentication functions may be distributed between the module may then process the image data to authenticate the client and the server side processes in a manner suited a user by determining if the image data matches a reference 5 particular application . For example , in some implementa \\n record for a recognized user identity that has been previ - tions , the authentication application 550 determines liveness \\n ously created based on image data collected during an scores for captured images and rejects any images with \\n enrollment session . liveness scores that indicate a spoof attack . If a liveness In some implementations , a user who has submitted a score indicates a live eye , image data , based on the accepted \\n request for service may be redirected to an authentication 10 images , may be transmitted to a server side authentication module 540 that runs on separate server system 514 . Authen module ( e . g . , authentication modules 525 or 540 ) for further tication module 540 may maintain reference records for analysis . registered or enrolled users of the secure transaction service In some implementations , the authentication application \\n 523 and may also include reference records for users of other accesses a reference record for a user identity and conducts \\n secure transaction services . Authentication module 540 can 15 a full authentication process , before reporting the result \\n establish secure sessions with various secure transaction ( e . g . , user accepted or rejected ) to a server side authentica services ( e . g . , secure transaction service 523 ) using tion module . encrypted network communications ( e . g . , using a public key The authentication application 550 may be implemented encryption protocol ) to indicate to the secure transaction as software , hardware or a combination of software and \\n service whether the user has been authenticated as a regis - 20 hardware that is executed on a processing apparatus , such as tered or enrolled user . Much like authentication module 525 , one or more computing devices ( e . g . , a computer system as authentication module 540 may receive image data from the illustrated in FIG . 9 ) . requesting user ' s computing device ( e . g . , computing devices FIG . 6 is a flow chart of an example process 600 for \\n 502 , 504 , 506 , 508 , 510 ) and may process the image data to authenticating an individual based on one or more images of \\n authenticate the user . In some implementations , the authen - 25 the white of an eye . A liveness score is determined for the tication module may determine liveness scores for images obtained images and used to accept or reject the images . \\n received from a user and may accept or reject the images When an image of a live eye is detected and accepted , the \\n based on the liveness scores . When an image is rejected as image is further analyzed to determine a match score by \\n a spoof attempt presenting something other than a live eye , extracting features from the image and comparing the fea \\n the authentication module 540 may send network commu - 30 tures to a reference record . The user is then accepted or nication messages to report the spoof attempt to the secure rejected based on the match score . \\n transaction service 523 or a relevant authority . The process 600 can be implemented , for example , by the The authentication module 540 may be implemented as authentication module 440 in the computing device 430 of\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content=\"software , hardware or a combination of software and hard - FIG . 4 . In some implementations , the computing device 430 ware that is executed on a processing apparatus , such as one 35 is a data processing apparatus that includes one or more \\n or more computing devices ( e . g . , a computer system as processors that are configured to perform actions of the \\n illustrated in FIG . 9 ) . process 600 . For example , the data processing apparatus A user device ( e . g . , computing device 510 ) may include may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In an authentication application 550 . The authentication appli some implementations , process 600 may be implemented in cation 550 may facilitate the authentication of the user as a 40 whole or in part by the authentication application 550 that is registered or enrolled user identity for the purpose of access executed by a user computing device ( e . g . , computing ing secured services ( e . g . , secure transaction service 523 ) device 510 ) . For example , the user computing device may be \\n through a network 511 . For example , the authentication a mobile computing device ( e . g . , mobile computing device \\n application 550 may be a mobile application or another type 950 of FIG . 9 ) . In some implementations , process 600 may \\n client application for interacting with a server - side authen - 45 be implemented in whole or in part by the authentication \\n tication module ( e . g . , authentication module 540 ) . The module 540 that is executed by a user server system ( e . g . , authentication application 550 may drive a sensor ( e . g . , a server system 514 ) . In some implementations , the server camera connected to or integrated with a user computing system 514 is a data processing apparatus that includes one device ) to capture one or more images of a user ( e . g . , user or more processors that are configured to perform actions of 530 ) that include views of the white of the user ' s eye . The 50 the process 600 . For example , the data processing apparatus \\n authentication application 550 may prompt ( e . g . , through a may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In display or speakers ) the user to pose for image capture . For some implementations , a computer readable medium can \\n example , the user may be prompted to face the sensor and include instructions that when executed by a computing direct their gaze left or right to expose large portions of the device ( e . g . , a computer system ) cause the device to perform \\n white of an eye to the sensor . 55 actions of the process 600 . In some implementations , the authentication application One or more images of an eye are obtained 602 . The \\n 550 transmits captured image data to an authentication images include a view of a portion of a vasculature of the eye \\n module ( e . g . , authentication modules 525 or 540 ) on a external to a corneal limbus boundary of the eye . The remote server ( e . g . , server systems 512 or 514 ) through the obtained images may be monochrome or represented in \\n network 511 . The collection of image data from user may 60 various color spaces ( e . g . , RGB , SRGB , HSV , HSL , or \\n facilitate enrollment and the creation of a reference record YCbCr ) . In some implementations , an image may be for the user . The collection of image data from user may also obtained using a light sensor ( e . g . , a digital camera , a 3D facilitate authentication against a reference record for a user camera , or a light field sensor ) . The sensor may be sensitive identity . to light in various ranges of wavelength . For example , the In some implementations , additional processing of the 65 sensor may be sensitive to the visible spectrum of light . In image data for authentication purposes may be performed by some implementations , the sensor is paired with a flash or\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content='the authentication application 550 and the results of that torch that can be pulsed to illuminate objects in view of the', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 17}), Document(page_content='11 US 9 , 971 , 920 B2 \\n 12 \\n sensor . The capture of images can be synchronized or with different settings , it may indicate that the sensor has time - locked with pulsing of a flash . In some implementa been bypassed by a spoof attack . For example , sensor tions , the sensor captures a sequence of images that can be configuration settings controlling focus , exposure time , or \\n used to track motion of objects within the field of view of the white balance may be adjusted in this manner . If correspond \\n sensor . The sensor can include one more settings that control 5 ing changes in the obtained image data are not detected , the image capture ( e . g . , focus distance , flash intensity , exposure , obtained images may be rejected 608 . and white balance ) . The images can collectively include a If the liveness score indicates a high likelihood that live plurality of focus distances . For example , a sequence of eye is depicted in the images , the one or more images are images may be captured , each image captured with a dif - accepted 616 and subjected to further analysis to complete \\n ferent focus distance settings for the sensor and / or some 10 the authentication process , \\n sensors ( e . g . , a light field sensor ) can capture an image that The one or more images may be segmented 620 to \\n is focused at a plurality of distances from the sensor . In some identify regions of interest that include the best views of \\n implementations , the one or more images can be obtained vasculature in the white of an eye . In some implementations , 502 by reception through a network interface ( e . g . , a net anatomical landmarks ( e . g . , an iris , its center and corneal work interface of server system 514 ) . 15 limbus boundary , eye corners , and the edges of eyelids ) may \\n A liveness score can then be determined 604 for the one be identified in the one or more images . Regions of interest \\n or more images . In some implementations , image data within the image may be identified and selected based on elements ( e . g . , a voxel , a pixel , a ray , or a red , green or blue their location in relation to the identified anatomical land channel value ) are input directly to a trained function marks . For example , regions of interest may be located in the \\n approximator that outputs a liveness score . The function 20 white of eye to the left , right , above , or below the iris . In approximator can be trained using data corresponding to some implementations , the selected regions of interest are training images of both live eyes and spoof props that are tiled to form a grid covering a larger portion of the white of \\n paired with ideal scores ( e . g . , 1 for live eyes and 0 for spoof the eye . In some implementations , the selected regions of the props ) . The function approximator or classifier models the image are noncontiguous ( e . g . , neighboring regions may \\n mapping from input data ( i . e . , the training image data or 25 overlap or neighboring regions may have space between \\n features ) to output data ( i . e . , the resulting liveness score or them ) . The selected regions of interest may correspond to \\n binary decision ) with a set of model parameters . The model regions of interest selected from a reference image on which \\n parameter values are selected using a training algorithm that data in a reference record is based . is applied to the training data . For example , the function In some implementations , eye corners are found by fitting \\n approximator can be based the following models : linear 30 curves on the detected portions of the eyelid over sclera , and \\n regression , Volterra series , Wiener series , radial basis func - then extrapolating and finding the intersection of those', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content=\"regression , Volterra series , Wiener series , radial basis func - then extrapolating and finding the intersection of those \\n tions , kernel methods , polynomial methods ; piecewise linear curves . If one intersection ( corner ) cannot be found due to models , Bayesian classifiers , k - nearest neighbor classifiers , the fact that the iris was too close ( e . g . , due to gaze \\n neural networks , support vector machines , or fuzzy function direction ) , then a template from the same corner area but approximator . Other models are possible . In some imple - 35 from the opposite gaze direction photo can be derived and \\n mentations , the liveness score may be binary . applied to the problematic corner neighborhood in the image \\n In some implementations , the liveness score is determined at hand , and the maximum correlation location can be tagged \\n 604 based on one or more liveness metrics that in turn are as the corner . determined based on the obtained images . Some examples In some implementations , eyelids are found by adaptive of such a process are described in relation to FIG . 7 . 40 thresholding methods that find the white of the eye from the For example , the liveness score can be determined 604 by image , which border the eyelids . The sclera mask itself can the authentication module 440 , the authentication applica be corrected by morphological operations ( e . g . , convex hull ) tion 550 , authentication module 525 , or the authentication to take out aberrations . \\n module 540 . In some implementations , the limbic boundary is found \\n The liveness score is checked 606 to determine whether 45 from the sclera mask as where the sclera ends due to its \\n the images are likely to include a view of a live eye . In some termination at the iris limbic boundary . implementations , the liveness score can be compared to a In some implementations , the iris center is found through threshold multiple methods . If the eye color is light , the center of the \\n If the liveness score indicates a low likelihood of a live pupil can be found as the iris center . If the iris is too dark , \\n eye and thus a high likelihood of a spoof attack , the one or 50 then the center of the ellipsoid fitted to the limbic boundary \\n more images are rejected 608 . In some implementations , a and its center is found , or it is determined as the focal point \\n spoof attack may then be reported 610 . In some implemen - of normal rays ( i . e . , lines perpendicular to tangents to the \\n tations , the spoof attack is reported 610 through a display or limbic boundary ) converging around the iris center , or a \\n speaker ( e . g . , with an alarm sound or flashing display ) . In combination of the above methods . some implementations , the spoof attack is reported 610 by 55 The image regions may be preprocessed 622 to enhance \\n transmitting one or messages over a network using a net - the view of a vasculature within an image . In some imple \\n work interface . The user may then be rejected 630 and mentations , preprocessing 622 includes Color Image \\n denied access to secured device or service . Enhancement and Contrast Limited Adaptive Histogram In some implementations ( not shown ) , a check may be Equalization ( CLAHE ) which enhances the contrast of the performed to verify that obtained images were captured 60 intensity image . CLAHE operates in small regions of the \\n from a particular sensor and that that the particular sensor image called tiles . Each tile ' s contrast is enhanced such that \\n has not been bypassed by the submission of spoofed image the histogram of the output approximately matches the\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content='has not been bypassed by the submission of spoofed image the histogram of the output approximately matches the \\n data . For example , during image capture , one or more sensor histogram specified by particular distribution ( e . g . , uniform , configuration settings may be adjusted to take on different exponential , or Rayleigh distribution ) . The neighboring tiles settings during capture of two or more of the images . These 65 are then combined using bilinear interpolation to eliminate different settings are expected to be reflected in the obtained the artificially induced boundaries . In some implementa image data . If changes in the image data between images tions , the images may be enhanced by selecting one of the', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 18}), Document(page_content=\"14 US 9 , 971 , 920 B2 \\n 13 \\n red , green or blue color components which has the best images . For example , the kernel parameters may be derived contrast between the vessels and the background . The green for eye images captured at a distance of 6 - 12 centimeters component may be preferred because it may provide the best away from the eye using a particular sensor ( e . g . a back \\n contrast between vessels and background . camera on a smartphone ) and the segmented sclera region In some implementations , preprocessing 622 includes 5 can be resized to a resolution of ( e . g . , 401x501 pixels ) for \\n application of a multi - scale enhancement filtering scheme to the analysis . Visible eye surface vasculature may be spread enhance the intensity of the images thereby facilitating in all the directions on white of the eye . For example , the \\n detection and subsequent extraction features of the vascular Gabor kernels may be aligned across six different angles structure . The parameters of the filter may be determined ( Angle = 0 , 30 , 60 , 90 , 120 , and 150 degrees ) . The phase of \\n empirically so as to account for variations in the girth of the 10 the Gabor - filtered images may vary from - n to tu radians . \\n blood vessels . The algorithm used may have good sensitiv . Phase values above 0 . 25 and below - 0 . 25 radians may \\n ity , good specificity for curves and suppresses objects of correspond to vascular structures . To binarize the phase \\n other shapes . The algorithm may be based on the second image using thresholding , all values of phase above 0 . 25 or \\n derivatives of the image . First , since the second derivatives below - 0 . 25 may be set to one and the remaining values to \\n are sensitive to noise , an image segment is convolved with 15 zero . This may result in a sharp vasculature structure in a Gaussian function . The parameter o of the Gaussian corresponding phase image . This operation can be per function may correspond to the thickness of a blood vessel . formed for images resulting from applications of all six \\n Next , for each image data element , a Hessian matrix may be Gabor kernels at different angles . All the six binarized built and eigenvalues 21 and 22 may be computed . In each images may be added , to reveal a fine and crisp vascular \\n Hessian matrix ridges are defined as points where the image 20 structure . In some implementations , a vector of the elements has an extremum in the direction of the curvature . The of the binarized phase images may be used as a feature direction of the curvature is the eigenvector of the second vector for comparing the image to a reference record . In order derivatives of the image that corresponds to the largest some implementations , differences in textural features \\n absolute eigenvalue à . The sign of the eigenvalue deter - between image regions of interest may be used as a feature \\n mines if it is a local minimum a > 0 or maximum à < 0 . The 25 vector . The sum of all the l ' s in a binarized image area \\n computed eigenvalues are then used to filter the blood vessel divided by the area of region of interest may reflect the line with the equations : extent of the visible vasculature . A match score is determined 626 based on the features and 1 _ line ( 1 , 12 ) = 411 - 1421 if a1 < 0 and 1 _ line ( 11 , 12 ) = 0 corresponding features from a reference record . The refer if N120 30 ence record may include data based at least in part on one or The diameter of the blood vessels varies but the algorithm more reference images captured during an enrollment or assumes the diameter is within an interval , [ do , d1 ] . Gauss - registration process for a user . In some implementations , a \\n ian smoothing filters may be employed in the scale range of match score may be determined 626 as a distance ( e . g . , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 19}), Document(page_content=\"ian smoothing filters may be employed in the scale range of match score may be determined 626 as a distance ( e . g . , a \\n [ d0 / 4 , d1 / 4 ] . This filtering may be repeated N times based on Euclidian distance , a correlation coefficient , modified Haus the smoothing scales : 35 dorff distance , Mahalanobis distance , Bregman divergence , cosine similarity , Kullback - Leibler distance , and Jensen gl = d0 / 4 , 02 = r * 01 , 02 = r ^ 2 * 01 , . . . 02 = Shannon divergence ) between a vector of features extracted 1 ( N - 1 ) * ol = d1 / 4 from the one or more obtained images and a vector of This final output may be the maximum value from the output features from the reference record . In some implementa \\n of all individual filters of N scales . 40 tions , the match score may be determined 626 by inputting \\n Features are determined 624 for each image region that features extracted from the one or more obtained images and reflect structure or properties of the vasculature visible in features from the reference record to a trained function that region of the user ' s eye . In some implementations , approximator . \\n minutia detection methods may be used to extract features of In some implementations , a quality based fusion match \\n the user ' s vasculature . Examples of minutia detection pro - 45 score is determined 626 based on match scores for multiple \\n cesses are described in U . S . Pat . No . 7 , 327 , 860 . images of the same vasculature . In some implementations , In some implementations , features may be determined match scores for multiple images are combined by adding \\n 624 in part by applying a set of filters to the image regions the match scores together in weighted linear combination \\n that correspond to texture features of those image regions . with weights that respectively depended on quality scores \\n For example , features may be determined in part by applying 50 determined for each of the multiple images . Other examples \\n a set of complex Gabor filters at various angles to the image of techniques that may be used to combine match scores for The parameters of the filter can be determined empirically so multiple images based on their respective quality scores as to account for variations in the spacing , orientation , and include hierarchical mixtures , sum rule , product rule , gated girth of the blood vessels . The texture features of an image fusion , Dempster - Shafer combination , and stacked general can be measured as the amount of sharp visible vasculature 55 ization , among others . in the region of interest . This quality can be determined with In some implementations , the match score is determined the ratio of area of sharp visible vasculature to the area of 626 by an authentication module ( e . g . , authentication mod region of interest . The phase of Gabor filtered image , when ule 440 running on computing device 430 ) . \\n binarized using a threshold , may facilitate detection and The match score may be checked 628 to determine \\n reveal sharp visible vasculature . 60 whether there is a match between the one or more obtained The phase of complex Gabor filtered image reflects the images and the reference record . For example the match vascular patterns at different angles when the Gabor filter score may be compared to a threshold . A match may reflect kernel is configured with Sigma = 2 . 5 Pixel , Frequency = 6 ; a high likelihood that the user whose eye is depicted in the and Gamma = 1 . The choice of frequency may be dependent one or more obtained images is the same as an individual \\n on the distance between vessels , which in turn depends on 65 associated with the reference record . the resolution and distance between image acquisition sys - If there is no match , then the user may be rejected 630 . As tem and the subject . These parameters may be invariant to a result , the user may be denied access to a secure device or\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 19}), Document(page_content='15 US 9 , 971 , 920 B2 \\n 16 \\n service ( e . g . , secured device 450 or secure transaction ser - In some implementations , photic stimuli ( e . g . , a flash vice 523 ) . In some implementations , the user may be pulse , a change brightness of an LCD display ) are applied to \\n informed of the rejection 630 through a message that is a subject while the images are being captured . In response \\n shown on a display or played through a speaker . In some to these photic stimuli , a pupil of a live eye is expected to \\n implementations , the rejection may be affected by transmit - 5 constrict to adapt to the change in illumination . Further the \\n ting a message through a network reflecting the status of the pupil is expected to constrict in a certain way over time with , \\n user as rejected . For example , the authentication module an onset time that depends on the reaction time of a user , a 540 , upon rejecting user 530 may transmit a rejection duration of the constriction movement required to reach a \\n message to the secure transaction server 523 using a network new steady state pupil diameter , an average velocity of \\n interface of server system 514 . The authentication module 10 constriction , and a particular acceleration curve for the 540 may also send a rejection message to user computing constriction motion . By examining a sequence of images \\n device 510 in this scenario . captured before and after the start of a photic stimulus , one If there is a match , then the user may be accepted 632 . AS or more parameters of a detected motion may be determined a result , the user may be granted access to a secure device and compared to one or more parameters of the expected \\n or service ( e . g . , secured device 450 or secure transaction 15 motion . A substantial deviation from the expected motion in \\n service 523 ) . In some implementations , the user may be response to the photic stimuli may indicate the subject in \\n informed of the acceptance 632 through a message that is view of the camera is not a live eye and there is spoof attack \\n shown on a display or played through a speaker . In some occurring . An example of this implementation is described \\n implementations , the acceptance may be affected by trans - in relation to FIG . 8A . \\n mitting a message through a network reflecting the status of 20 In some implementations , a behavioral metric may be \\n the user as accepted . For example , the authentication module determined 712 by applying external stimuli ( e . g . , prompts 540 , upon accepting user 530 may transmit an acceptance instructing a user to direct their gaze or a display showing a \\n message to the secure transaction server 523 using a network moving object that user follows with their eyes ) to a subject \\n interface of server system 514 . The authentication module during image capture and tracking the gaze transitions that \\n 540 may also send an acceptance message to user computing 25 may result . In response to these external stimuli , a live eye \\n device 510 in this scenario . is expected to move in a certain way over time . Some FIG . 7 is a flow chart of an example process 700 for parameters of an expected gaze transition motion may \\n determining a liveness score for one or more images of an include an onset time that depends on the reaction time of a \\n eye . One or more liveness metrics are determined 710 for the user , a duration of the gaze transition movement required to \\n images and the liveness score is determined 730 based on the 30 reach a new steady state gaze direction , an average velocity , \\n one or more liveness metrics . and a particular acceleration curve for the gaze transition The process 700 can be implemented , for example , by the motion . By examining a sequence of images captured before', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content=\"authentication module 440 in the computing device 430 of and after the start of an external stimulus , one or more FIG . 4 . In some implementations , the computing device 430 parameters of a detected motion may be determined and \\n is a data processing apparatus that includes one or more 35 compared to one or more parameters of the expected motion . \\n processors that are configured to perform actions of the A substantial deviation from the expected motion in process 700 . For example , the data processing apparatus response to the external stimuli may indicate the subject in may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In view of the camera is not a live eye and there is spoof attack some implementations , process 700 may be implemented in occurring . An example of this implementation is described \\n whole or in part by the authentication application 550 that is 40 in relation to FIG . 8B . executed by a user computing device ( e . g . , computing In some implementations , determining 712 a behavioral device 510 ) . For example , the user computing device may be metric may include detecting flow of blood in a vasculature \\n a mobile computing device ( e . g . , mobile computing device of the white of the eye ( e . g . vasculature in the episclera ) . A 950 of FIG . 9 ) . In some implementations , process 700 may sequence of images may be analyzed to detect changes in \\n be implemented in whole or in part by the authentication 45 hue and changes in visible width of veins and blood vessels \\n module 540 that is executed by a user server system ( e . g . , in the white of the eye that occur over time . The vasculature \\n server system 514 ) . In some implementations , the server of a live eye is expected to exhibit regular changes in vessel \\n system 514 is a data processing apparatus that includes one widths and hue that correspond to a user ' s pulse . A substan or more processors that are configured to perform actions of tial deviation from the expected blood flow pattern may \\n the process 700 . For example , the data processing apparatus 50 indicate the subject in view of the camera is not a live eye \\n may be a computing device ( e . g . , as illustrated in FIG . 9 ) . In and there is spoof attack occurring . some implementations , a computer readable medium can For example , consider a section of vasculature between \\n include instructions that when executed by a computing two branching points or sharp bends . The tubular body of device ( e . g . , a computer system ) cause the device to perform that vessel change shape and color when the heart is pump \\n actions of the process 700 . 55 ing blood through it . In some implementations , 300 frames Process 700 starts 702 when one or more images are or images may be captured over a 10 second period . Image received for processing . For example , the one or more regions may be registered from one capture instance to the \\n images may be encoded as two , three , or four dimensional next . The blood flow may then be measured by comparing arrays of data image elements ( e . g . , a pixel , a voxel , a ray , the physical dimensions ( 2d or 3d ) of points of interest along \\n or a red , green or blue channel value ) . 60 blood vessels over time , as well as the coloration of those One or more liveness metrics may then be determined 710 vessels over time . In this manner , changes consistent with \\n based on the one or more images . In this example , a pulse can be detected . For example if the measure “ pulse ” behavioral metric is determined 712 based on detected signal resembled a square wave that would not be consistent movement of the eye as the eye appears in a plurality of the with a natural circulatory system . If it consisted of spikes \\n images . The behavioral metric can be a measure of deviation 65 ( both vessel dilation and appropriate coloration change ) at\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content='images . The behavioral metric can be a measure of deviation 65 ( both vessel dilation and appropriate coloration change ) at \\n of detected movement and timing from expected movement regular intervals over time within normal range for a human of the eye . user , possibly even for the specific user , then the input is', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 20}), Document(page_content=\"17 US 9 , 971 , 920 B2 \\n 18 \\n likely to correspond to a real live pulse . A distance between dimensional shape . A three - dimensional model including \\n the measure pulse signal and an expected pulse signal may locations of landmarks corresponding to an expected shape \\n be determined to assess the likelihood that the subject is a for a subject including the live eye of a user may be used for \\n live eye rather than a spoof attack . comparison to the detected landmark locations . In some In some implementations , the expected motion param - 5 implementations , the relative positions of landmarks on a \\n eters are specific to a particular user and are determined particular user ' s face may be determined during an enroll \\n during an enrollment session and stored as part of a refer ment session and used generate a three - dimensional model \\n ence record for the particular user . In some implementations , that is stored as part of a reference record . In some imple \\n the expected motion parameters are determined for a popu - mentations , three - dimensional model for a population of \\n lation based on a large collection of user data or oftline 10 users may be determined based on an aggregation of mea \\n studies . surements or studies of a large number of people . Various For example , a behavioral metric may be determined 712 types of metrics can be used as a spatial metric to compare \\n by an authentication module or application ( e . g . , authenti - the detected landmark positions to the expected shape ( e . g . , \\n cation module 440 ) . a Euclidian distance , a correlation coefficient , modified In this example , a spatial metric is determined 714 based 15 Hausdorff distance , Mahalanobis distance , Bregman diver \\n on a distance from a sensor to a landmark that appears in a gence , Kullback - Leibler distance , and Jensen - Shannon plurality of the images each having a different respective divergence ) . focus distance . Focus distance is the distance from a sensor In some implementations , determining 714 the spatial to a point in its field of view that is perfectly in focus . For metric comprises determining parallax of two or more some sensors , the focus distance may be adjusted for dif - 20 landmarks that appear in a plurality of the images . Parallax ferent images by adjusting a focus configuration setting for is the apparent displacement of an observed object due to a the sensor . For example , a landmark ( e . g . , an iris , an eye change in the position of the observer . A plurality of images corner , a nose , an ear , or a background object ) may be taken from different perspectives on the subject may result identified and located in the plurality of images with differ - in landmarks within the images appearing to move by \\n ent focus distances . A landmark ' s representation in a par - 25 different amounts because of differences in their distance \\n ticular image has a degree of focus that depends on how far from the sensor . This parallax effect may be measured and \\n the object corresponding to the landmark is from an in focus used as a spatial metric that reflects the three - dimensional point in the field of view of the sensor . Degree of focus is a nature of a subject in the view of the sensor . If all the \\n measure of the extent to the image of the landmark is blurred landmarks in the images undergo the same apparent dis \\n by optical effects in the light sensor ( e . g . , due to diffraction 30 placement due to relative motion of the sensor , i . e . , the \\n and convolution with the aperture shape ) . The degree of difference in the parallax effect for the landmarks is small , \\n focus for a landmark in a particular image may be estimated then the subject viewed by the camera has higher likelihood by determining the high frequency components of the image of being a two - dimensional spoof attack . In some imple\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content=\"signal in the vicinity of the landmark . When the landmark is mentations , the sensor is moved about the subject during in focus , more high frequency components in its vicinity are 35 image capture to collect image data from different orienta \\n expected . When the degree of focus is low for a landmark , tions relative to the subject . For example , a single camera smaller high frequency components are expected . By com may be rotated or slid slightly or multiple cameras at paring the degree of focus for a landmark in images with different positions may be used for image capture . In some \\n different focus distances , the distance from the sensor to the implementations , a user is prompted to move in order to \\n landmark may be estimated . In some implementations , dis - 40 change the relative orientation of the subject and the sensor . tances from the sensor ( e . g . a camera ) for multiple land - In some implementations , it is assumed that sensor will marks are estimated to form a topological map ( consisting of naturally move relative to the subject . For example , where \\n a set of three - dimensional landmark positions ) of the subject the sensor is a camera in hand - held user device ( e . g . a in the view of the sensor . The positions of these landmarks smartphone or tablet ) the sensor may naturally move relative \\n in the space viewed by the camera may be compared to a 45 to the users face due to involuntary haptic motion . \\n model by determining a spatial metric ( e . g . , the mean square For example , a spatial metric may be determined 714 by \\n difference between the detected location of one or more an authentication module or application ( e . g . , authentication landmarks and the corresponding modeled locations of the module 440 ) . one or more landmarks ) that reflects deviation from the In this example , a reflectance metric is determined 716 \\n model . 50 based on detected change in surface glare or specular \\n In some implementations , the spatial metric is a measure reflection patterns on a surface of the eye as the eye appears of the deviation of the subject from a two - dimensional plane . in a plurality of the images . The reflectance metric may be One possible spoofing strategy is to present a two dimen - a measure of changes in glare or specular reflection patches \\n sional image ( e . g . , a photograph ) of a registered user ' s eye on the surface of the eye . As the illumination of an eye in the to the sensor . However the locations of landmarks ( e . g . , an 55 view of the sensor changes , due to relative motion of the eye \\n eye , nose , mouth , and ear ) in the two dimensional image will and a light source or to changes in a dynamic light source \\n occur in a two dimensional plane , unlike landmarks in and ( e . g . , a flash , LCD screen , or other illumination element ) , the around a real live eye . For example , the locations of multiple glare and specular reflection patterns visible on the eye are landmarks may be fit to the closest two dimensional plane expected to change by appearing , disappearing , growing , and the average distance of the landmarks from this fit plane 60 shrinking , or moving . In some implementations , changes in \\n can be determined as the spatial metric . A high value for this the illumination are induced during image capture by photic spatial metric may indicate a three - dimensional subject and stimuli ( e . g . a flash pulse ) or external stimuli ( e . g . a prompt a higher likelihood that the subject is a live eye , while a low instructing a user to change gaze direction ) . For example , value may indicate a higher likelihood that the subject is a glare , including its boundaries , can be detected by thresh\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content='two - dimensional spoof attack . 65 olding a contrast enhanced image to find the whitest spots . In some implementations , the spatial metric is a measure Detected changes in the glare or specular reflection patterns of the deviation of the subject from an expected three - on the eye in the images may be compared to expected', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 21}), Document(page_content='US 9 , 971 , 920 B2 \\n 19 20 \\n changes in the these patterns by determining 716 a reflec dynamic range for the sensor ( e . g . , a camera ) so that a finer \\n tance metric that measures the deviation of the detected resolution in intensity of detected light is achieved in a range change from an expected change . within which it occurs in the captured images . In this We are looking for changes in the area and shape of this manner , the intensity or color scale can be zoomed in to \\n glare . One can also look at the ratio of circumference to area 5 reveal more subtle changes in the level of the detected image of the glare patch . signal . If the captured images are of a live eye , it is expected \\n In some implementations , a flash may be pulsed to that the range of color or intensity values detected will \\n illuminate the subject while one or more of the images are continue to vary continuously . In contrast , a spoofed image being captured . Glare from the flash may be detected on the ( e . g . a digital photograph presented to the sensor ) may eye as it appears in the images . The pulsing of the flash may 10 exhibit large discontinuous jumps corresponding to half \\n be synchronized with image capture so that the time differ - tones . The extent of halftones in the image may be measured ence between when the flash is pulsed and when the corre - in a variety of ways ( e . g . , as average or maximum eigen \\n sponding glare appears in the images can be measured . The values of a Hessian matrix evaluated in a region of the image \\n reflectance metric may be based on this time difference . or as high frequency components of the image signal ) . In \\n Large deviations from the expected synchronization or time - 15 some implementations , images with a halftone metric above \\n lock of the flash pulse and the onset of a corresponding glare a threshold are rejected . In some implementations , histo or specular reflection may indicate a spoof attack . For grams of gray shades in the image are generated and the example , a replay attack uses pre - recorded video of a uniformity of the distribution between grey level bins ( e . g . , \\n capturing scenario . Glare changes in the pre - recorded video 256 bins ) is measured . \\n are unlikely to be time - locked to a real - time flash event 20 In some implementations , the liveness metrics are deter during the current session . Another example is presenting a mined 710 in parallel . In some implementations , the liveness \\n printed image of an eye to the sensor , in which case glare metrics are determined 710 in series . may spread across the printed image in an unnaturally The liveness score may then be determined 730 based on uniform manner or may not change perceivably due to a lack the one or more liveness metrics . In some implementations , \\n of moisture on the viewed surface . If no corresponding glare 25 the liveness score is determined by inputting the one or more \\n or specular reflection is detected , the reflectance metric may liveness metrics to a trained function approximator . \\n be determined to be a large arbitrary number corresponding The function approximator may be trained using data \\n to poor synchronization or a lack of time - lock between the corresponding to training images of live eyes and various \\n flash and detected glare or specular reflection . spoof attacks that have been correctly labeled to provide a In some implementations , changes in illumination may be 30 desired output signal . The function approximator models the detected as changes as changes in the uniformity of a glare mapping from input data ( i . e . , the training image liveness pattern caused by greater amounts of fine three - dimensional metrics ) to output data ( i . e . , a liveness score ) with a set of texture of a white of the eye being revealed as the intensity model parameters . The model parameter values are selected', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 22}), Document(page_content='of the illumination is increased . For example , a flash may be using a training algorithm that is applied to the training data . \\n pulsed to illuminate the subject at higher intensity while one 35 For example , the function approximator may be based the \\n or more of the images are being captured . Fine three following models : linear regression , Volterra series , Wiener \\n dimensional texture of a white of the eye may be detected by series , radial basis functions , kernel methods , polynomial measuring uniformity of a pattern of glare on the eye in the methods ; piecewise linear models , Bayesian classifiers , \\n images before and after the onset of the flash pulse . For k - nearest neighbor classifiers , neural networks , support vec \\n example , the uniformity of the glare of specular reflection 40 tor machines , or fuzzy function approximator . In some pattern may be measured as the ratio of circumference to the implementations , the liveness score may be binary . area of the glare . The larger this number compared to 2 / R , For example , the liveness score may be determined 730 \\n the more non - circular and non - uniform the glare ( R is the based on one or more liveness metrics by an authentication \\n estimated radius of the glare patch ) . In some implementa - module or application ( e . g . , authentication module 440 ) . tions , a function approximator ( e . g . , a neural network ) is 45 The resulting liveness score may then be returned 740 and \\n trained to distinguish between specular reflection patterns may be used by an authentication system ( e . g . , authentica recorded from live eyeballs vs . synthesized eyeballs , such as tion system 400 ) in variety of ways . For example , the \\n 3D printed eyeballs , using a sensor with an illumination liveness score may be used to accept or reject the one or \\n element ( e . g . , a flash ) . more images . For example , a reflectance metric may be determined 716 50 FIG . 8A is a flow chart of an example process 800 for by an authentication module or application ( e . g . , authenti - determining a behavioral metric based on constriction of a \\n cation module 440 ) . pupil in response to photic stimulus . One or more photic In some implementations ( not shown ) , additional liveness stimuli are applied 810 to the scene viewed by a sensor ( e . g . metrics may be determined 710 . For example , a metric light sensor 420 ) . For example , the photic stimuli may reflecting the extent of saccadic motion of the eye in the 55 include a flash pulse or a change in the brightness of a view of the sensor may be determined . An iris of the eye may display ( e . g . , an LCD display ) . A sequence of images is be landmarked in a sequence of images so that its position captured 812 by the sensor before and after the start of the or orientation may be tracked . This sequence of positions or photic stimuli . For example , the sequence of images may be orientations may be analyzed to determine extent of saccadic captured at regularly spaced times ( e . g . , at 10 , 30 , or 60 Hz ) \\n motion , by filtering for motions at a particular frequency 60 in an interval ( e . g . , 2 , 5 , or 10 seconds ) that includes the start \\n associated with normal saccadic motion . of the photic stimuli . In some implementations , a liveness metric may be deter - In some implementations , a pupil is landmarked in each mined 710 that reflects the extent of halftones in a captured of the captured images and the diameter of the pupil is image . Halftones are artifacts of digital printed images that determined 814 in each captured image . The diameter may \\n may be used in a spoof attack and thus their presence may 65 be determined 814 relative to a starting diameter for the indicate a high likelihood of a spoof attack . For example , pupil that is measured in one or more images captured before one or more images may be captured using a reduced the start of the photic stimuli .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 22}), Document(page_content='21 US 9 , 971 , 920 B2 \\n 22 \\n The resulting sequence of pupil diameters measured in In some implementations , an iris is landmarked in each of response to the photic stimuli may be analyzed to determine the captured images and the position or orientation of the iris 816 one or more motion parameters for the constriction of is determined 834 in each captured image . The position may the pupil in response to the photic stimuli . In some imple be determined 834 relative to a starting position for the iris \\n mentations , motion parameters of the pupil constriction may 5 that is measured in one or more images captured before the \\n include an onset time of the constriction motion relative to start of the external stimuli . the start of the photic stimuli . Onset is the time delay T he resulting sequence of iris positions measured in between the start of the photic stimuli and the start of the response to the external stimuli may be analyzed to deter \\n constriction motion . In some implementations , motion mine 836 one or more motion parameters for the gaze \\n parameters of the pupil constriction may include a duration 10 transition in response to the external stimuli . In some of the constriction motion . Duration is the length of time implementations , motion parameters of the gaze transition between the start of the constriction motion and the end of may include an onset time of the gaze transition motion \\n the constriction motion , when the pupil diameter reaches a relative to the start of the external stimuli . Onset is the time \\n new steady state value ( e . g . , after which the diameter does delay between the start of the external stimuli and the start \\n not change for a minimum interval of time ) . In some 15 of the gaze transition motion . In some implementations , implementations , motion parameters of the pupil constric - motion parameters of the gaze transition may include a tion may include a velocity of pupil constriction . For duration of the gaze transition motion . Duration is the length example , the velocity may be determined as difference in of time between the start of the gaze transition motion and pupil diameters between two points in time divided by the the end of the gaze transition motion , when the iris reaches length of the time interval between them . In some imple - 20 a new steady state position ( e . g . , after which the iris does not mentations , motion parameters of the pupil constriction may move for a minimum interval of time ) . In some implemen include an acceleration of the pupil constriction in different tations , motion parameters of the gaze transition may time segments of constriction period . For example , the include a velocity of gaze transition . For example , the acceleration may be determined as a difference in velocities velocity may be determined as difference in iris positions \\n between two intervals . 25 between two points in time divided by the length of the time The behavioral metric may be determined 818 as a interval between them . In some implementations , motion distance between one or more determined motion param - parameters of the gaze transition may include an accelera \\n eters and one or more expected motion parameters . For tion of the gaze transition . For example , the acceleration \\n example , the behavior metric may include a difference may be determined as a difference in velocities between two \\n between a detected onset time and an expected onset time for 30 intervals . \\n a live eye . For example , the behavior metric may include a The behavioral metric may be determined 838 as a', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content=\"between a detected onset time and an expected onset time for 30 intervals . \\n a live eye . For example , the behavior metric may include a The behavioral metric may be determined 838 as a \\n difference between a detected duration and an expected distance between one or more determined motion param duration of pupil constriction for a live eye . In some eters and one or more expected motion parameters . For implementations , a sequence of pupil diameters is compared example , the behavior metric may include a difference to an expected sequence of pupil diameters by determining 35 between a detected onset time and an expected onset time for \\n a distance ( e . g . , a Euclidian distance , a correlation coeffi - a live eye . For example , the behavior metric may include a \\n cient , modified Hausdorff distance , Mahalanobis distance , difference between a detected duration and an expected Bregman divergence , Kullback - Leibler distance , and duration of pupil constriction for a live eye . In some Jensen - Shannon divergence ) between the two sequences . In implementations , a sequence of iris positions is compared to some implementations , a sequence of pupil constriction 40 expected sequence of iris positions by determining a dis \\n velocities for the constriction motion is compared to an tance ( e . g . , a Euclidian distance , a correlation coefficient , expected sequence of pupil constriction velocities by deter - modified Hausdorff distance , Mahalanobis distance , Breg mining a distance between the two sequences of velocities . man divergence , Kullback - Leibler distance , and Jensen In some implementations , a sequence of pupil constriction Shannon divergence ) between the two sequences . In some \\n accelerations for the constriction motion is compared to an 45 implementations , a sequence of transition velocities for the expected sequence of pupil constriction accelerations by gaze transition motion is compared to expected sequence of determining a distance between the two sequences of accel - transition velocities by determining a distance between the \\n erations . two sequences of velocities . In some implementations , a \\n For example , the process 800 may be implemented by an sequence of gaze transition accelerations for the constriction \\n authentication module or application ( e . g . , authentication 50 motion is compared to an expected sequence of gaze tran \\n module 440 ) controlling a light sensor ( e . g . light sensor 420 ) sition accelerations by determining a distance between the \\n and an illumination element . two sequences of accelerations . FIG . 8B is a flow chart of an example process 820 for For example , the process 820 may be implemented by an determining a behavioral metric based on gaze transition of authentication module or application ( e . g . , authentication an iris in response to external stimulus . One or more external 55 module 440 ) controlling a light sensor ( e . g . light sensor 420 ) stimuli are applied 830 to a user viewed by a sensor ( e . g . and a prompting device ( e . g . , a display , a speaker , or a haptic light sensor 420 ) . For example , the external stimuli may feedback device ) . \\n include prompts instructing a user to direct their gaze ( e . g . , FIG . 9 shows an example of a generic computer device look right , left , up , down , or straight ahead ) during image 900 and a generic mobile computing device 950 , which may \\n capture . Prompts may be visual , auditory , and / or tactile . In 60 be used with the techniques described here . Computing \\n some implementations , the external stimuli can include an device 900 is intended to represent various forms of digital object that moves within in display for user ' s eyes to follow . computers , such as laptops , desktops , workstations , personal\", metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content='A sequence of images is captured 832 by the sensor before digital assistants , servers , blade servers , mainframes , and and after the start of the external stimuli . For example , the other appropriate computers . Computing device 950 is \\n sequence of images may be captured at regularly spaced 65 intended to represent various forms of mobile devices , such \\n times ( e . g . , at 10 , 30 , or 60 Hz ) in an interval ( e . g . , 2 , 5 , or as personal digital assistants , cellular telephones , smart 10 seconds ) that includes the start of the external stimuli . phones , and other similar computing devices . The compo', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 23}), Document(page_content='23 US 9 , 971 , 920 B2 \\n 24 \\n nents shown here , their connections and relationships , and puting device 900 may be combined with other components \\n their functions , are meant to be exemplary only , and are not in a mobile device ( not shown ) , such as device 950 . Each of \\n meant to limit implementations of the inventions described such devices may contain one or more of computing device \\n and / or claimed in this document . 900 , 950 , and an entire system may be made up of multiple Computing device 900 includes a processor 902 , memory 5 computing devices 900 , 950 communicating with each \\n 904 , a storage device 906 , a high - speed interface 908 other . connecting to memory 904 and high - speed expansion ports Computing device 950 includes a processor 952 , memory \\n 910 , and a low speed interface 912 connecting to low speed 964 , an input / output device such as a display 954 , a com bus 914 and storage device 906 . Each of the components munication interface 966 , and a transceiver 968 , among 902 , 904 , 906 , 908 , 910 , and 912 , are interconnected using 10 other components . The device 950 may also be provided various busses , and may be mounted on a common moth with a storage device , such as a microdrive or other device , erboard or in other manners as appropriate . The processor to provide additional storage . Each of the components 950 , \\n 902 can process instructions for execution within the com - 952 , 964 , 954 , 966 , and 968 , are interconnected using \\n puting device 900 , including instructions stored in the various buses , and several of the components may be \\n memory 904 or on the storage device 906 to display graphi - 15 mounted on a common motherboard or in other manners as \\n cal information for a GUI on an external input / output device , appropriate . such as display 916 coupled to high speed interface 908 . In The processor 952 can execute instructions within the \\n other implementations , multiple processors and / or multiple computing device 950 , including instructions stored in the buses may be used , as appropriate , along with multiple memory 964 . The processor may be implemented as a memories and types of memory . Also , multiple computing 20 chipset of chips that include separate and multiple analog devices 900 may be connected , with each device providing and digital processors . The processor may provide , for \\n portions of the necessary operations ( e . g . , as a server bank , example , for coordination of the other components of the \\n a group of blade servers , or a multi - processor system ) . device 950 , such as control of user interfaces , applications \\n The memory 904 stores information within the computing run by device 950 , and wireless communication by device \\n device 900 . In one implementation , the memory 904 is a 25 950 . \\n volatile memory unit or units . In another implementation , Processor 952 may communicate with a user through the memory 904 is a non - volatile memory unit or units . The control interface 958 and display interface 956 coupled to a memory 904 may also be another form of computer - readable display 954 . The display 954 may be , for example , a TFT \\n medium , such as a magnetic or optical disk . LCD ( Thin - Film - Transistor Liquid Crystal Display ) or an The storage device 906 is capable of providing mass 30 OLED ( Organic Light Emitting Diode ) display , or other \\n storage for the computing device 900 . In one implementa - appropriate display technology . The display interface 956 tion , the storage device 906 may be or contain a computer may comprise appropriate circuitry for driving the display readable medium , such as a floppy disk device , a hard disk 954 to present graphical and other information to a user . The', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 24}), Document(page_content='device , an optical disk device , or a tape device , a flash control interface 958 may receive commands from a user memory or other similar solid state memory device , or an 35 and convert them for submission to the processor 952 . In \\n array of devices , including devices in a storage area network addition , an external interface 962 may be provided in or other configurations . A computer program product can be communication with processor 952 , so as to enable near area \\n tangibly embodied in an information carrier . The computer communication of device 950 with other devices . External program product may also contain instructions that , when interface 962 may provide , for example , for wired commu \\n executed , perform one or more methods , such as those 40 nication in some implementations , or for wireless commu \\n described above . The information carrier is a computer - or n ication in other implementations , and multiple interfaces \\n machine - readable medium , such as the memory 904 , the may also be used . storage device 906 , or a memory on processor 902 , for The memory 964 stores information within the computing example . device 950 . The memory 964 can be implemented as one or The high speed controller 908 manages bandwidth - inten - 45 more of a computer - readable medium or media , a volatile \\n sive operations for the computing device 900 , while the low memory unit or units , or a non - volatile memory unit or units . \\n speed controller 912 manages lower bandwidth - intensive Expansion memory 974 may also be provided and connected operations . Such allocation of functions is exemplary only . to device 950 through expansion interface 972 , which may In one implementation , the high - speed controller 908 is include , for example , a SIMM ( Single In Line Memory coupled to memory 904 , display 916 ( e . g . , through a graph - 50 Module ) card interface . Such expansion memory 974 may ics processor or accelerator ) , and to high - speed expansion provide extra storage space for device 950 , or may also store ports 910 , which may accept various expansion cards ( not applications or other information for device 950 . Specifi shown ) . In the implementation , low - speed controller 912 is cally , expansion memory 974 may include instructions to coupled to storage device 906 and low - speed expansion port carry out or supplement the processes described above , and \\n 914 . The low - speed expansion port , which may include 55 may include secure information also . Thus , for example , \\n various communication ports ( e . g . , USB , Bluetooth , Ether - expansion memory 974 may be provided as a security \\n net , wireless Ethernet ) may be coupled to one or more module for device 950 , and may be programmed with \\n input / output devices , such as a keyboard , a pointing device , instructions that permit secure use of device 950 . In addi \\n a scanner , or a networking device such as a switch or router , tion , secure applications may be provided via the SIMM \\n e . g . , through a network adapter . 60 cards , along with additional information , such as placing The computing device 900 may be implemented in a identifying information on the SIMM card in a non - hackable number of different forms , as shown in the figure . For manner . example , it may be implemented as a standard server 920 , or The memory may include , for example , flash memory \\n multiple times in a group of such servers . It may also be and / or NVRAM memory , as discussed below . In one imple implemented as part of a rack server system 924 . In addition , 65 mentation , a computer program product is tangibly embod \\n it may be implemented in a personal computer such as a ied in an information carrier . The computer program product laptop computer 922 . Alternatively , components from com contains instructions that , when executed , perform one or', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 24}), Document(page_content='US 9 , 971 , 920 B2 \\n 25 26 \\n ne - more methods , such as those described above . The infor - ( e . g . , a mouse or a trackball ) by which the user can provide mation carrier is a computer - or machine - readable medium , input to the computer . Other kinds of devices can be used to \\n such as the memory 964 , expansion memory 974 , memory provide for interaction with a user as well ; for example , \\n on processor 952 , or a propagated signal that may be feedback provided to the user can be any form of sensory \\n received , for example , over transceiver 968 or external 5 feedback ( e . g . , visual feedback , auditory feedback , or tactile \\n interface 962 . feedback ) ; and input from the user can be received in any Device 950 may communicate wirelessly through com - form , including acoustic , speech , or tactile input . \\n munication interface 966 , which may include digital signal The systems and techniques described here can be imple \\n processing circuitry where necessary . Communication inter mented in a computing system that includes a back end \\n face 966 may provide for communications under various 10 component ( e . g . , as a data server ) , or that includes a middle modes or protocols , such as GSM voice calls , SMS , EMS , ware component ( e . g . , an application server ) , or that or MMS messaging , CDMA , TDMA , PDC , WCDMA , includes a front end component ( e . g . , a client computer \\n CDMA2000 , or GPRS , among others . Such communication having a graphical user interface or a Web browser through may occur , for example , through radio - frequency trans which a user can interact with an implementation of the \\n ceiver 968 . In addition , short - range communication may 15 systems and techniques described here ) , or any combination \\n occur , such as using a Bluetooth , WiFi , or other such of such back end , middleware , or front end components . The transceiver ( not shown ) . In addition , GPS ( Global Position components of the system can be interconnected by any ing System ) receiver module 970 may provide additional form or medium of digital data communication ( e . g . , a \\n navigation - and location - related wireless data to device 950 , communication network ) . Examples of communication net \\n which may be used as appropriate by applications running 20 works include a local area network ( “ LAN ” ) , a wide area \\n on device 950 . network ( “ WAN ” ) , and the Internet . Device 950 may also communicate audibly using audio The computing system can include clients and servers . A \\n codec 960 , which may receive spoken information from a client and server are generally remote from each other and user and convert it to usable digital information . Audio typically interact through a communication network . The \\n codec 960 may likewise generate audible sound for a user , 25 relationship of client and server arises by virtue of computer such as through a speaker , e . g . , in a handset of device 950 . programs running on the respective computers and having a \\n Such sound may include sound from voice telephone calls , client - server relationship to each other . \\n may include recorded sound ( e . g . , voice messages , music A number of embodiments have been described . Never files , etc . ) and may also include sound generated by appli - theless , it will be understood that various modifications may \\n cations operating on device 950 . 30 be made without departing from the spirit and scope of the The computing device 950 may be implemented in a invention . number of different forms , as shown in the figure . For In addition , the logic flows depicted in the figures do not example , it may be implemented as a cellular telephone 980 . require the particular order shown , or sequential order , to \\n It may also be implemented as part of a smartphone 982 , achieve desirable results . In addition , other steps may be', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 25}), Document(page_content='It may also be implemented as part of a smartphone 982 , achieve desirable results . In addition , other steps may be \\n personal digital assistant , or other similar mobile device . 35 provided , or steps may be eliminated , from the described \\n Various implementations of the systems and techniques flows , and other components may be added to , or removed described here can be realized in digital electronic circuitry , from , the described systems . Accordingly , other embodi integrated circuitry , specially designed ASICs ( application ments are within the scope of the following claims . \\n specific integrated circuits ) , computer hardware , firmware , What is claimed is : software , and / or combinations thereof . These various imple - 40 1 . A computer - implemented method comprising : mentations can include implementation in one or more capturing , using a light sensor , a plurality of images of a \\n computer programs that are executable and / or interpretable subject including a view of an eye of the subject ; \\n on a programmable system including at least one program determining a reflectance metric by at least one of : mable processor , which may be special or general purpose , measuring a time difference between ( i ) an occurrence \\n coupled to receive data and instructions from , and to trans - 45 of a flash pulse captured by one or more of the \\n mit data and instructions to , a storage system , at least one images and ( ii ) an appearance of a corresponding \\n input device , and at least one output device . glare detected on the eye in one or more of the These computer programs ( also known as programs , images ; and software , software applications or code ) include machine measuring uniformity of a pattern of glare on an outer \\n instructions for a programmable processor , and can be 50 surface of the eye visible in one or more of the implemented in a high - level procedural and / or object - ori images caused by one or more flash pulses captured ented programming language , and / or in assembly / machine by one or more of the images , wherein the uniformity \\n language . As used herein , the terms “ machine - readable of a pattern of glare on the eye is measured as a ratio \\n medium ” and “ computer - readable medium ” refer to any of circumference to an area of the glare ; and computer program product , apparatus and / or device ( e . g . , 55 rejecting or accepting the images based on , at least , the magnetic discs , optical disks , memory , Programmable Logic reflectance metric . \\n Devices ( PLDs ) ) used to provide machine instructions and 2 . The method of claim 1 , further comprising changing or data to a programmable processor , including a machine - one or more parameters at different times during the cap \\n readable medium that receives machine instructions as a turing of the images . machine - readable signal . The term \" machine - readable sig - 60 3 . The method of claim 2 , wherein a particular parameter \\n nal ” refers to any signal used to provide machine instruc - comprises a flash pulse , a focus setting of the light sensor , \\n tions and / or data to a programmable processor . brightness of a display , exposure of the light sensor , white \\n To provide for interaction with a user , the systems and balance of the light sensor , an illumination of the subject , or techniques described here can be implemented on a com - external stimuli . puter having a display device ( e . g . , a CRT ( cathode ray tube ) 65 4 . The method of claim 1 , wherein measuring the time or LCD ( liquid crystal display ) monitor ) for displaying difference comprises synchronizing the flash pulse with the information to the user and a keyboard and a pointing device capturing of one or more of the images .', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 25}), Document(page_content='US 9 , 971 , 920 B2 \\n 27 28 \\n 5 . The method of claim 1 , further comprising : 15 . The system of claim 12 , wherein measuring the time determining a spatial metric based on , at least , a distance difference comprises synchronizing the flash pulse with the from the light sensor to a landmark that appears in a capturing of one or more of the images . \\n plurality of the images each having a different respec 16 . The system of claim 12 , wherein the operations further \\n tive focus distance , 5 comprise : \\n wherein the rejecting or accepting the images is further determining a spatial metric based on , at least , a distance \\n based on the spatial metric . from the light sensor to a landmark that appears in a \\n 6 . The method of claim 5 , wherein the spatial metric is a plurality of the images each having a different respec \\n measure of deviation of the subject from an expected tive focus distance , \\n three - dimensional shape . 10 wherein the rejecting or accepting the images is further \\n 7 . The method of claim 5 , wherein determining the spatial based on the spatial metric . \\n metric comprises determining parallax of two or more 17 . The system of claim 16 , wherein the spatial metric is \\n landmarks that appear in a plurality of the images . a measure of deviation of the subject from an expected \\n 8 . The method of claim 1 , further comprising : three - dimensional shape . \\n determining a behavioral metric based on , at least , 15 at 15 18 . The system of claim 16 , wherein determining the \\n detected movement of the eye as the eye appears in a spatial metric comprises determining parallax of two or \\n plurality of the images , wherein the behavioral metric more landmarks that appear in a plurality of the images . \\n is a measure of deviation of detected movement and 19 . The system of claim 12 , wherein the operations further \\n timing from expected movement of the eye , comprise : \\n wherein the rejecting or accepting the images is further 20 her 20 determining a behavioral metric based on , at least , detected movement of the eye as the eye appears in a based on the spatial metric . \\n 9 . The method of claim 8 , wherein determining the plurality of the images , wherein the behavioral metric \\n behavioral metric comprises determining an onset , duration , is a measure of deviation of detected movement and \\n velocity , or acceleration of pupil constriction in response to timing from expected movement of the eye , 25 wherein the rejecting or accepting the images is further photic stimuli . \\n 10 . The method of claim 8 , wherein determining the based on the spatial metric . \\n behavioral metric comprises determining an onset , duration , 20 . The system of claim 19 , wherein determining the \\n or acceleration of gaze transition in response to external behavioral metric comprises determining an onset , duration , \\n stimuli . velocity , or acceleration of pupil constriction in response to \\n 11 . The method of claim 8 , wherein determining the 30 photic stimuli . \\n behavioral metric comprises detecting blood flow of the eye 21 . The system of claim 19 , wherein determining the \\n as the eye appears in a plurality of the images . behavioral metric comprises determining an onset , duration , \\n 12 . A system comprising data processing apparatus pro or acceleration of gaze transition in response to external \\n grammed to perform operations comprising : stimuli . \\n capturing , using a light sensor , a plurality of images of a 35 ces of a 35 22 . The system of claim 19 , wherein determining the \\n subject including a view of an eye of the subject ; behavioral metric comprises detecting blood flow of the eye', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 26}), Document(page_content='subject including a view of an eye of the subject ; behavioral metric comprises detecting blood flow of the eye \\n determining a reflectance metric by at least one of : as the eye appears in a plurality of the images . 23 . A non - transitory computer storage medium encoded measuring a time difference between ( i ) an occurrence with instructions that , when executed by a data processing of a flash pulse captured by one or more of the images and ( ii ) an appearance of a corresponding 40 app onding 40 apparatus , cause the data processing apparatus to perform \\n glare detected on the eye in one or more of the operations comprising : \\n images ; and capturing , using a light sensor , a plurality of images of a \\n measuring uniformity of a pattern of glare on an outer subject including a view of an eye of the subject ; \\n surface of the eye visible in one or more of the determining a reflectance metric by at least one of : \\n images caused by one or more flash pulses captured 45 measuring a time difference between ( i ) an occurrence \\n by one or more of the images , wherein the uniformity of a flash pulse captured by one or more of the images and ( ii ) an appearance of a corresponding of a pattern of glare on the eye is measured as a ratio \\n of circumference to an area of the glare ; and glare detected on the eye in one or more of the \\n rejecting or accepting the images based on , at least , the images ; and \\n reflectance metric . measuring uniformity of a pattern of glare on an outer \\n 13 . The system of claim 12 , wherein the operations further surface of the eye visible in one or more of the \\n comprise changing one or more parameters at different times images caused by one or more flash pulses captured \\n during the capturing of the images . by one or more of the images , wherein the uniformity \\n 14 . The system of claim 13 , wherein a particular param of a pattern of glare on the eye is measured as a ratio \\n eter comprises a flash pulse , a focus setting of the light 55 of circumference to an area of the glare ; and \\n sensor , brightness of a display , exposure of the light sensor , rejecting or accepting the images based on , at least , the \\n white balance of the light sensor , an illumination of the reflectance metric . \\n subject , or external stimuli . * * * * * 50', metadata={'source': 'https://patentimages.storage.googleapis.com/84/29/92/5fe2a153298d2d/US9971920.pdf', 'page': 26})]\n",
      "[Document(page_content='US01115139732 \\n ( 12 ) United States Patent ( 10 ) Patent No .: US 11,151,397 B2 \\n ( 45 ) Date of Patent : * Oct . 19 , 2021 Kim et al . \\n ( 54 ) LIVENESS TESTING METHODS AND APPARATUSES AND IMAGE PROCESSING \\n METHODS AND APPARATUSES \\n ( 71 ) Applicant : Samsung Electronics Co. , Ltd. , Suwon - si ( KR ) ( 52 ) U.S. CI . CPC G06K 9/00906 ( 2013.01 ) ; G06K 9/00228 ( 2013.01 ) ; GO6K 9/00268 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search CPC . GO6F 11/0718 ; G06F 21/32 ; G06K 9/00268 ; GO6K 9/00906 ; GO6K 9/4609 ; \\n ( Continued ) ( 72 ) Inventors : Wonjun Kim , Hwaseong - si ( KR ) ; Sungjoo Suh , Seoul ( KR ) ; Jaejoon Han , Seoul ( KR ) ; Wonjun Hwang , \\n Seoul ( KR ) ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS ( 73 ) Assignee : Samsung Electronics Co. , Ltd. , Gyeonggi - do ( KR ) 5,487,172 A * 1/1996 Hyatt B6OR 16/0373 \\n 700/8 \\n G06K 9/0004 \\n 340 / 5.53 7,620,212 B1 * 11/2009 Allen ( * ) Notice : \\n ( Continued ) Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 815 days . \\n This patent is subject to a terminal dis \\n claimer . FOREIGN PATENT DOCUMENTS \\n CN \\n CN ( 21 ) Appl . No .: 15 / 156,847 10-1499 164 A 8/2009 \\n 10-2938144 A 2/2013 \\n ( Continued ) \\n ( 22 ) Filed : May 17 , 2016 \\n OTHER PUBLICATIONS ( 65 ) Prior Publication Data \\n US 2016/0328623 A1 Nov. 10 , 2016 Q is for Quantum - An Encyclopedia of Particle Physics . * \\n ( Continued ) \\n Related U.S. Application Data \\n ( 63 ) Continuation of application No. 14 / 612,632 , filed on Feb. 3 , 2015 , now Pat . No. 9,679,212 . Primary Examiner Santiago Garcia ( 74 ) Attorney , Agent , or Firm — Harness , Dickey & Pierce , P.L.C. \\n ( 30 ) Foreign Application Priority Data \\n May 9 , 2014 \\n Jun . 24 , 2014 ( KR ) ( KR ) 10-2014-0055687 \\n 10-2014-0077333 ( 57 ) ABSTRACT \\n A user recognition method and apparatus , the user recogni tion method including performing a liveness test by extract ing a first feature of a first image acquired by capturing a user , and recognizing the user by extracting a second feature of the first image based on a result of the liveness test , is provided . a \\n ( 51 ) Int . Ci . GO6K 9/00 \\n G06T 7700 ( 2006.01 ) ( 2017.01 ) \\n ( Continued ) 18 Claims , 15 Drawing Sheets \\n 125 \\n 120 120 110 110 \\n 115 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 0}), Document(page_content=\"US 11,151,397 B2 Page 2 \\n FOREIGN PATENT DOCUMENTS ( 51 ) Int . Ci . G06T 7/55 ( 2017.01 ) G06K 9/46 ( 2006.01 ) ( 52 ) U.S. Ci . CPC G06K 9/4661 ( 2013.01 ) ; G06T 770002 ( 2013.01 ) ; G06T 7/55 ( 2017.01 ) ; G06K 9/00 ( 2013.01 ) ; GO6T 2207/10004 ( 2013.01 ) ; GO6T 2207/10016 ( 2013.01 ) ; G06T 2207/10028 ( 2013.01 ) ; G06T 2207/30201 ( 2013.01 ) \\n ( 58 ) Field of Classification Search CPC G06K 9/00899 ; G06K 9/00107 ; G06K \\n 9/00885 ; G06K 9/2018 ; G06K 9/00302 ; \\n G06K 9/00288 ; G06K 9/0004 ; GOOK 9/00228 ; G06K 9/00261 ; G06K 9/00597 ; G06K 9/00604 ; G06K 9/00617 ; G06K 9/00892 ; G06K 9/00912 ; G06T 2207/30201 ; G06T 7/0002 ; G06T 7/0048 ; G06T 770081 ; G06T 770083 ; GOOG 2360/16 ; GO9G 5/02 ; HO4L 9/3231 ; HO4L 63/0861 ; G06Q 20/40145 USPC 382/115 , 116 , 117 , 118 , 203 , 224 , 226 , \\n 382/227 , 274 See application file for complete search history . JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n JP \\n KR \\n KR \\n KR \\n KR \\n KR \\n KR H8-189819 A \\n H11-339048 A \\n 2002/532807 A 2006-259923 A \\n 2006-259931 A \\n 2009-187130 A \\n 2010-231398 A \\n 2012-069133 A \\n 0421221 \\n 0421221 B1 \\n 2005/0084448 A \\n 0887183 0887183 B1 2011/0092752 A 7/1996 \\n 12/1999 10/2002 \\n 9/2006 \\n 9/2006 \\n 8/2009 10/2010 4/2012 2/2004 \\n 2/2004 \\n 8/2005 \\n 2/2009 \\n 2/2009 \\n 8/2011 \\n OTHER PUBLICATIONS \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 8,503,800 B2 8/2013 Blonk et al . 8,675,926 B2 * 3/2014 Zhang G06K 9/00906 \\n 382/118 \\n 2004/0061777 A1 * 4/2004 Sadok GO8B 17/125 \\n 348/83 \\n 2006/0122834 A1 * 6/2006 Bennett GIOL 15/1822 \\n 704/256 \\n 2007/0268312 A1 11/2007 Marks et al . \\n 2008/0253622 A1 * 10/2008 Tosa ..... G06K 9/00604 \\n 382/117 2010/0097470 A1 * 4/2010 Yoshida GO8B 13/19641 \\n 348/159 2010/0189313 A1 * 7/2010 Prokoski A61B 5/0064 \\n 382/118 2011/0187715 A1 * 8/2011 Jacobs HO4N 13/339 \\n 345/426 2011/0299741 A1 * 12/2011 Zhang G06K 9/00228 \\n 382/117 2012/0013651 A1 * 1/2012 Trayner GO2B 5/32 \\n 345/690 2012/0155725 A1 * 6/2012 Bathe G06T 7/20 \\n 382/128 2013/0212655 A1 * 8/2013 Hoyos G06K 9/00107 \\n 726/5 \\n 2013/0219480 A1 * 8/2013 Bud G06F 21/32 \\n 726/7 2013/0321672 Al 12/2013 Silverstein et al . \\n 2014/0168453 A1 * 6/2014 Shoemake HO4N 5/23206 \\n 348 / 207.11 2014/0201126 A1 * 7/2014 Zadeh G06K 9/627 \\n 706/52 2014/0270404 Al * 9/2014 Hanna G06Q 30/0609 382/116 \\n 2014/0270409 A1 * 9/2014 Hanna G06Q 30/0609 382/118 \\n 2014/0283113 A1 * 9/2014 Hanna G06F 21/32 \\n 726/27 \\n 2015/0124072 A1 5/2015 Wei et al . 2015/0237273 A1 8/2015 Sawadaishi 2015/0324629 A1 * 11/2015 Kim G06K 9/00228 \\n 382/203 \\n 2015/0324993 Al 11/2015 Stein et al . 2016/0085958 A1 * 3/2016 Kang G06F 21/40 \\n 726/19 Notice of Allowance issued in U.S. Appl . No. 14 / 612,632 , dated \\n Jan. 25 , 2017 . \\n U.S. Office Action dated Oct. 17 , 2017 in U.S. Appl . No. 15 / 499,164 . T. Brox , “ A TV flow based local scale measure for texture discrimi nation , ” in Proc . ECCV , May 2004 , 12 pgs . J. Weickert , “ Efficient and reliable schemes for nonlinear diffusion filtering , ” IEEE Trans . Image Process . , vol . 7 , No. 3 , Mar. 1998 , 14 \\n pgs . H. Seo , “ Face verification using the lark representation , ” IEEE Tr . Information Forensics and Security , vol . 6 , No. 4 , Dec. 2011 , 12 pgs . B. Wang , “ Illumination Normalization Based on Weber's Law With Application to Face Recognition ” , IEEE Signal Processing Letters , \\n Aug. 2011 , 4 pgs . , vol . 18 , No. 8 . T. Chen , “ Total Variation Models for Variable Lighting Face Rec ognition ” , IEEE Transactions on Pattern Analysis and Machine Intelligence , Sep. 2006 , vol . 28 , No. 9 , IEEE Computer Society . W. Kim , “ Face Liveness Detection from a Single Image via Non linear Diffusion Speed Model ” , 1 Multimedia Processing Lab . , SAIT , SEC , 130 , 2014 , 2 pgs . Extended European Search Report issued in European Patent Appli\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 1}), Document(page_content='cation No. 15163789 , dated Nov. 20 , 2015 . Tan , et al . , “ Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model ” , Department of Computer Science and Technology , ECCV 2010 , Part VI , LNCS 6316 , pp . 504-517 . Kim , et al . , Face Liveness Detection From A Single Image Via Diffusion Speed Model , IEEE Transactions on Imafe Processing , vol . 24 , No. 8 , Aug. 2015 , pp . 2456-2465 . Scherzer , et al . , “ Variational Methods in Imaging ” , Springer Science & Business Media , Jan. 1 , 2009 , pp . 185-203 . U.S. Office Action dated Jun . 1 , 2017 in U.S. Appl . No. 15 / 499,164 . Final Office Action dated May 17 , 2018 in U.S. Appl . No. 15 / 499,164 . Office Action for corresponding U.S. Appl . No. 14 / 612,632 dated Aug. 30 , 2016 . Pierre Buyssens , Marinette Revenu . Label Diffusion on Graph for Face Identification . ICB , Mar. 2012 , New Delhi , India . pp . 46-51 , 2012 . \\n Office Action for corresponding U.S. Appl . No. 14 / 612,632 dated Mar. 14 , 2016 . Non - Final Office Action dated Oct. 23 , 2018 in U.S. Appl . No. 15 / 499,164 . Office Action dated Nov. 27 , 2018 in Japanese Patent Application \\n No. 2015-056767 . Extended European Search Report dated Sep. 22 , 2020 in European Application No. 20169859.4 . Yu - Jin Zhang , “ Advances in Face Image Analysis : Techniques and Technologies , ” Tsinghua University , Jan. 1 , 2020 , Beijing , China . Joachim Weickert , “ Anisotropic Diffusion in Image Processing , \" Department of Computer Science , University of Copenhagen , Jan. 1 , 1998 , Copenhagen , Denmark . Notice of Allowance dated Mar. 13 , 2019 , in U.S. Appl . No. 15 / 499,164 . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 1}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 1 of 15 US 11,151,397 B2 \\n FIG . 1A \\n 120 110 \\n 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 2}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 2 of 15 US 11,151,397 B2 \\n FIG . 1B \\n 125 120 110 \\n 115', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 3}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 3 of 15 US 11,151,397 B2 \\n FIG . 2 \\n Oo Oo \\n 225 \\n 215 \\n 210 \\n 220 211', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 4}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 4 of 15 US 11,151,397 B2 \\n FIG . 3 \\n 310 \\n 311 312 \\n INPUT \\n IMAGE RECEIVER TESTER TEST \\n RESULT', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 5}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 5 of 15 US 11,151,397 B2 \\n FIG . 4 \\n 410 420 430 \\n 412 432', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 6}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 6 of 15 US 11,151,397 B2 \\n FIG . 5 \\n 9 \\n # • \\n K \\n 510 520', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 7}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 7 of 15 US 11,151,397 B2 \\n FIG . 6 \\n 600 \\n 613 \\n 611 612 TESTER TEST \\n RESULT INPUT \\n IMAGE RECEIVER DIFFUSER', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 8}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 8 of 15 US 11,151,397 B2 \\n FIG . 7 \\n 710 720 ON \\n 715 \\n 730', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 9}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 9 of 15 US 11,151,397 B2 \\n FIG . 8', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 10}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 10 of 15 US 11,151,397 B2 \\n FIG . 9 \\n 910 920 \\n 913 \\n 911 912 FACE RECOGNITION \\n AND / OR \\n USER VERIFICATION CIRCUIT GENERATOR \\n INPUT \\n IMAGE RECEIVER H DIFFUSER', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 11}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 11 of 15 US 11,151,397 B2 \\n FIG . 10 \\n START \\n 1010 \\n RECEIVE INPUT IMAGE \\n 1020 \\n TEST LIVENESS \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 12}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 12 of 15 US 11,151,397 B2 \\n FIG . 11 \\n START \\n 1110 \\n RECEIVE FIRST IMAGE \\n 1120 \\n GENERATE SECOND IMAGE \\n 1130 \\n GENERATE THIRD IMAGE \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 13}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 13 of 15 US 11,151,397 B2 \\n FIG . 12 \\n START \\n 1210 \\n RECEIVE \\n FIRST IMAGE \\n 1220 \\n GENERATE \\n SECOND IMAGE 1240 \\n CALCULATE DIFFUSION VELOCITY 1230 \\n 1250 GENERATE \\n THIRD IMAGE EXTRACT DIFFUSION \\n VELOCITY BASED \\n STATISTICAL \\n INFORMATION \\n 1270 \\n NO LIVENESS \\n TEST SUCCESSFUL ? \\n YES \\n 1260 \\n PERFORM FACE RECOGNITION AND / OR USER VERIFICATION \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 14}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 14 of 15 US 11,151,397 B2 \\n FIG . 13 \\n 1300 \\n IMAGE \\n SENSOR 1302 \\n IMAGE \\n PROCESSOR \\n 1304 \\n DISPLAY \\n 1308 \\n MEMORY \\n 1306', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 15}), Document(page_content='U.S. Patent Oct. 19 , 2021 Sheet 15 of 15 US 11,151,397 B2 \\n FIG . 14 \\n START \\n 1410 \\n FIRST IMAGE DIFFUSE FIRST IMAGE \\n 1420 \\n NO LIVENESS \\n TEST \\n SUCCESSFUL ? \\n YES 1430 \\n PERFORM USER RECOGNITION \\n END', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 16}), Document(page_content='15 \\n a US 11,151,397 B2 \\n 1 2 \\n LIVENESS TESTING METHODS AND corresponding to the diffusion speed greater than or equal to APPARATUSES AND IMAGE PROCESSING the first threshold , among the diffusion speeds ; calculating at \\n METHODS AND APPARATUSES least one of an average or a standard deviation of the diffusion speeds ; or calculating a filter response based on the CROSS - REFERENCE TO RELATED 5 diffusion speeds . \\n APPLICATIONS The extracting of the first feature may include : extracting a first - scale region from the first image based on the diffu This application is a Continuation of and claims priority sion speeds ; and calculating an amount of noise components under 35 U.S.C. $ 120 to U.S. application Ser . No. 14/612 , included in the first - scale region based on a difference 632 , filed Feb. 3 , 2015 , which claims priority under 35 10 between the first - scale region and a result of applying U.S.C. $ 119 to Korean Patent Application No. 10-2014 median filtering to the first - scale region . 0055687 , filed on May 9 , 2014 , and Korean Patent Appli The performing may include : determining whether an cation No. 10-2014-0077333 , filed on Jun . 2014 , in the object included in the image has a planar property or a Korean Intellectual Property Office , the entire contents of which are incorporated herein by reference . three - dimensional ( 3D ) structural property , based on the first feature ; outputting a signal corresponding a failed test in a \\n BACKGROUND case in which the object is determined to have the planar property ; and outputting a signal corresponding to a suc \\n Field cessful test in a case in which the object is determined to One or more example embodiments relate to liveness 20 have the 3D structural property . testing methods , liveness testing apparatuses , image pro The performing may include : calculating a degree of cessing methods , image processing apparatuses , and / or elec- uniformity in a distribution of light energy included in a tronic devices including the same . plurality of pixels corresponding to an object included in the', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content=\"Description of the Related Art image based on the first feature ; outputting a signal corre Biometric technologies may identify a human based on 25 sponding to a failed test in a case in which the degree of unique biometric characteristics of each individual user . uniformity in the distribution of the light energy is greater Among conventional biometric technologies , a face recog- than or equal to a threshold ; and outputting a signal corre nition system may naturally recognize a user based on the sponding to a successful test in a case in which the degree user's face without requiring user contact with a sensor , such of uniformity in the distribution of the light energy is less as a fingerprint scanner or the like . However , conventional 30 than the threshold . face recognition systems may be vulnerable to imperson- The performing may further include at least one of : ations using a picture of a face of a registered target . determining whether the first feature corresponds to a fea ture related to a medium that displays a face or a feature SUMMARY related to an actual face ; outputting a signal corresponding 35 to a failed test in a case in which the first feature corresponds At least one example embodiment provides a user recog- to the feature related to a medium that displays a face ; or nition method including : receiving a first image acquired by outputting a signal corresponding to a successful test in a capturing a user ; performing a liveness test by extracting a case in which the first feature corresponds to the feature first feature of the first image ; and recognizing the user by related to an actual face . extracting a second feature of the first image based on a 40 The user recognition method may further include receiv result of the liveness test . The first image may correspond to ing a user verification request for approving an electronic a first frame in a video , and the new image may correspond commerce payment . The user recognition method may fur to a second frame in the video . ther include approving the electronic commerce payment in According to at least some example embodiments , the a case in which user verification succeeds in the recognizing . user recognition method further include , in a case in which 45 The user recognition method may further include receiv the result of the liveness test corresponds to a failed test : ing a user input which requires user verification . The user receiving a new image ; performing a liveness test by extract- recognition method may further include performing an ing a first feature of the new image ; and recognizing the user operation corresponding to the user input in a case in which by extracting a second feature of the new image based on a user verification succeeds in the recognizing . The user input result of the liveness test on the new image . 50 which requires user verification may include at least one of The performing may include : generating a second image a user input to unlock a screen , a user input to execute a by diffusing a plurality of pixels included in the first image ; predetermined application , a user input to execute a prede calculating diffusion speeds of the pixels based on a differ- termined function in an application , or a user input to access ence between the first image and the second image ; and a predetermined folder or file . extracting the first feature based on the diffusion speeds . The user recognition method may further include receiv The generating may include : iteratively updating value of ing a user input related to a gallery including a plurality of the pixels using a diffusion equation . items of contents . The user recognition method may further The extracting of the first feature may include : estimating include sorting content corresponding to a user among the a surface property related to an object included in the first plurality of items of contents in the gallery in a case in which image based on the diffusion\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='content corresponding to a user among the a surface property related to an object included in the first plurality of items of contents in the gallery in a case in which image based on the diffusion speeds . The surface property 60 the user is identified in the recognizing ; and providing the may include at least one of a light - reflective property of a sorted content to the user .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='surface of the object , a number of dimensions of the surface At least one other example embodiment provides of the object , or a material of the surface of the object . recognition method including : receiving a video acquired by The extracting of the first feature may include at least one capturing a user ; performing a liveness test based on at least of : calculating a number of pixels corresponding to a dif- 65 one first frame in the video ; and recognizing the user based fusion speed greater than or equal to a first threshold , among on at least one second frame in the video based on a result the diffusion speeds ; calculating a distribution of the pixels of the liveness test . 55 \\n user', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 17}), Document(page_content='a \\n a \\n ? \\n > US 11,151,397 B2 \\n 3 4 \\n The performing may include at least one of : determining ever , be embodied in many alternate forms and should not be the video to be a live video in a case in which the result of construed as limited to only the embodiments set forth the liveness test corresponds to a successful test ; determin herein . \\n ing the video to be a fake video in a case in which the result Accordingly , while example embodiments are capable of \\n of the liveness test corresponds to a failed test ; or determin- 5 various modifications and alternative forms , the embodi \\n ing the video to be a live video in a case in which the at least ments are shown by way of example in the drawings and will \\n one first frame includes a predetermined number of con be described herein in detail . It should be understood , \\n secutive frames and a result of a liveness test on the however , that there is no intent to limit example embodi \\n consecutive frames corresponds to a successful test . ments to the particular forms disclosed . On the contrary ,', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content='consecutive frames corresponds to a successful test . ments to the particular forms disclosed . On the contrary , \\n The at least one first frame may differ from the at least one 10 example embodiments are to cover all modifications , equivalents , and alternatives falling within the scope of this second frame . For example , the at least one first frame may disclosure . Like numbers refer to like elements throughout include at least one frame suitable for a liveness test , and the the description of the figures . at least one second frame may include at least one frame Although the terms first , second , etc. may be used herein suitable for user recognition . 15 to describe various elements , these elements should not be At least one other example embodiment provides a user limited by these terms . These terms are only used to distin recognition apparatus including a processor configured to guish one element from another . For example , a first element perform a liveness test by extracting a first feature of a first could be termed a second element , and similarly , a second image acquired by capturing a user , and recognize the user element could be termed a first element , without departing by extracting a second feature of the first image based on a 20 from the scope of this disclosure . As used herein , the term result of the liveness test . “ and / or , ” includes any and all combinations of one or more of the associated listed items . BRIEF DESCRIPTION OF THE DRAWINGS When an element is referred to as being “ connected , ” or “ coupled , ” to another element , it can be directly connected Example embodiments will become more apparent and 25 or coupled to the other element or intervening elements may more readily appreciated from the following description of be present . By contrast , when an element is referred to as the example embodiments shown in the drawings in which : being “ directly connected , ” or “ directly coupled , ” to another FIGS . 1A and 1B illustrate a liveness test according to element , there are no intervening elements present . Other example embodiments ; words used to describe the relationship between elements FIG . 2 illustrates a principle of a liveness test according 30 should be interpreted in a like fashion ( e.g. , “ between , \" to an example embodiment ; versus “ directly between , ” “ adjacent , ” versus “ directly adja FIG . 3 illustrates a liveness testing apparatus according to cent , \" etc. ) . an example embodiment ; The terminology used herein is for the purpose of describ FIG . 4 illustrates a diffusion process according to an ing particular embodiments only and is not intended to be example embodiment ; 35 limiting . As used herein , the singular forms “ a , ” “ an , ” and FIG . 5 illustrates an example small - scale region ( SR ) map “ the , ” are intended to include the plural forms as well , unless according to example embodiments ; the context clearly indicates otherwise . It will be further FIG . 6 illustrates a liveness testing apparatus according to understood that the terms “ comprises , \" \" comprising , \" \\n an example embodiment ; “ includes , \" and / or “ including , ” when used herein , specify FIG . 7 illustrates an example input image and example 40 the presence of stated features , integers , steps , operations , images processed according to an example embodiment ; elements , and / or components , but do not preclude the pres FIG . 8 illustrates example changes in an input image as a ence or addition of one or more other features , integers , result of changes in illumination , according to example steps , operations , elements , components , and / or groups \\n embodiments ; thereof .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content='embodiments ; thereof . \\n FIG . 9 illustrates an image processing apparatus accord- 45 It should also be noted that in some alternative imple ing to an example embodiment ; mentations , the functions / acts noted may occur out of the FIG . 10 illustrates a liveness testing method according to order noted in the figures . For example , two figures shown an example embodiment ; in succession may in fact be executed substantially concur FIG . 11 illustrates an image processing method according rently or may sometimes be executed in the reverse order , to an example embodiment ; 50 depending upon the functionality / acts involved . FIG . 12 illustrates an image processing and authentica- Specific details are provided in the following description tion / verification method according to another example to provide a thorough understanding of example embodi \\n embodiment ; ments . However , it will be understood by one of ordinary FIG . 13 is a block diagram illustrating an electronic skill in the art that example embodiments may be practiced system according to an example embodiment ; and 55 without these specific details . For example , systems may be FIG . 14 is a flowchart illustrating a user recognition shown in block diagrams so as not to obscure the example method according to an example embodiment . embodiments in unnecessary detail . In other instances , well known processes , structures and techniques may be shown DETAILED DESCRIPTION without unnecessary detail in order to avoid obscuring 60 example embodiments . Various example embodiments will now be described In the following description , example embodiments will more fully with reference to the accompanying drawings in be described with reference to acts and symbolic represen which some example embodiments are shown . tations of operations ( e.g. , in the form of flow charts , flow Detailed illustrative embodiments are disclosed herein . diagrams , data flow diagrams , structure diagrams , block However , specific structural and functional details disclosed 65 diagrams , etc. ) that may be implemented as program mod herein are merely representative for purposes of describing ules or functional processes include routines , programs , example embodiments . Example embodiments may , how- objects , components , data structures , etc. , that perform par > \\n 2', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 18}), Document(page_content=\"a \\n may be US 11,151,397 B2 \\n 5 6 \\n ticular tasks or implement particular abstract data types and sensor 115 that photographs the face of the user 120. The may be implemented using existing hardware at , for image sensor 115 may also be part of the liveness testing example , existing electronic devices , such as smartphones , apparatus 110 . personal digital assistants , laptop or tablet computers , etc. In one example , as shown in FIG . 1A , the input image is Such existing hardware may include one or more Central 5 generated by photographing the actual face of the user 120 . Processing Units ( CPUs ) , graphics processing units ( GPUs ) , In this example , the liveness testing apparatus 110 deter mines that the face included in the input image corresponds image processors , system - on - chip ( SOC ) devices , digital signal processors ( DSPs ) , application - specific - integrated to a real ( or living ) three - dimensional object , and outputs a \\n circuits , field programmable gate arrays ( FPGAs ) computers signal indicating that the face included in the input image \\n or the like . 10 corresponds to a real three - dimensional object . That is , for \\n Although a flow chart may describe the operations as a example , the liveness testing apparatus 110 tests whether the face included in the input image corresponds to the real ( or sequential process , many of the operations may be per formed in parallel , concurrently or simultaneously . In addi living ) three - dimensional object , and outputs a signal indi cating a successful test since the face in the input image does tion , the order of the operations may be re - arranged . A 15 indeed correspond to the real ( or living ) three - dimensional process may be terminated when its operations are com object . pleted , but may also have additional steps not included in the In another example , as shown in FIG . 1B , the input image figure . A process may correspond to a method , function , is generated by photographing a face displayed on a display procedure , subroutine , subprogram , etc. When a process medium 125 , rather than the actual face of the user 120 . corresponds to a function , its termination may correspond to 20 According to at least this example , the display medium 125 a return of the function to the calling function or the main refers to a medium that displays an object ( e.g. , a face ) in \\n function . two dimensions . The display medium 125 may include , for Reference will now be made in detail to the example example , a piece of paper on which a face of a user is printed embodiments illustrated in the accompanying drawings , ( e.g. , a photo ) , an electronic device displaying a face of a wherein like reference numerals refer to like elements 25 user , etc. In one example scenario , the user 120 may attempt throughout . Example embodiments are described below to to log into an electronic device ( e.g. , a smartphone or the \\n explain the present disclosure by referring to the figures . like ) with another user's account by directing a face dis \\n One or more example embodiments described below played on the display medium 125 toward the image sensor \\n applicable to various fields , for example , smartphones , lap 115. In FIG . 1B , the face displayed on the display medium top or tablet computers , smart televisions ( TVs ) , smart home 30 125 is marked with a broken line to indicate that the face \\n systems , smart cars , surveillance systems , etc. For example , displayed on the display medium 125 is directed toward the \\n one or more example embodiments may be used to test a image sensor 115 , rather than the user 120. In this example , the liveness testing apparatus 110 determines that the face liveness of an input image and / or authenticate a user to log into a smartphone or other device . In addition , one or more 35 dimensional representation of the object , and outputs a included in the input image corresponds to a fake two\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 19}), Document(page_content='example embodiments may be used to test a liveness of an signal indicating that the face included in the input image input image and / or authenticate a user for admission control corresponds to a fake two - dimensional representation of the and / or monitoring in a public area and / or a secured area . object . That is , for example , the liveness testing apparatus 110 tests whether the face included in the input image Liveness Test According to Example Embodiments 40 corresponds to the real ( or living ) three - dimensional object , and outputs a signal indicating a failed test since the face in FIGS . 1A and 1B illustrate a liveness test according to an the input image does not correspond to the real ( or living ) example embodiment . three - dimensional object , but rather the fake two - dimen According to at least some example embodiments , a sional representation of the object . In some cases , the term liveness test refers to a method of testing ( or determining ) 45 fake object may be used to refer to the fake two - dimensional whether an object included in an input image corresponds to representation of the object . a real three dimensional object . In one example , the liveness The liveness testing apparatus 110 may detect a face test may verify whether a face included in an input image region from the input image . In this example , liveness corresponds to ( or is obtained from ) a real three - dimensional testing method and apparatus may be applicable to the face ( 3D ) object ( e.g. , an actual face ) or a fake two - dimensional 50 region detected from the input image . ( 2D ) representation of the object ( e.g. , a picture of a face ) . FIG . 2 illustrates a principle of a liveness test according Through the liveness test , an attempt to verify a face of to an example embodiment . another using a forged and / or falsified picture may be A liveness testing apparatus according to at least this effectively rejected . example embodiment tests a liveness of an object included Referring to FIGS . 1A and 1B , according to at least one 55 in an input image based on whether the object has one or example embodiment , a liveness testing apparatus 110 more characteristics of a flat ( two - dimensional ) surface or a receives an input image including a face of a user 120 , and three - dimensional ( 3D ) structure . tests a liveness of the face included in the received input Referring to FIG . 2 , the liveness testing apparatus distin image . In one example , the liveness testing apparatus 110 guishes between a face 211 displayed on a medium 210 and may be ( or be included in ) a mobile device , for example , a 60 an actual face 220 of a user . The face 211 displayed on the mobile phone , a smartphone , a personal digital assistant medium 210 corresponds to a two - dimensional ( 2D ) flat ( PDA ) , a tablet computer , a laptop computer , etc. In another surface . When an input image is generated by photographing example , the liveness testing apparatus 110 may be ( or be the face 211 displayed on the medium 210 , an object included in ) a computing device , for example , a personal included in the input image has one or more characteristics computer ( PC ) , an electronic product , such as a TV , a 65 of a flat surface . Since a surface of the medium 210 security device for a gate control , etc. The liveness testing corresponds to a 2D flat surface , light 215 incident on the apparatus 110 may receive the input image from an image face 211 displayed on the medium 210 is more uniformly a \\n a a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 19}), Document(page_content='a \\n a US 11,151,397 B2 \\n 7 8 \\n reflected by the surface of the medium 210. Thus , light input image has one or more characteristics of a 3D struc energy is more uniformly distributed on the object included ture . In this case , the liveness testing apparatus determines in the input image . Even if the medium 210 is curved , a that the face of the input image corresponds to a real 3D surface of a curved display may still correspond to , and have object , and outputs a signal indicating the same ( e.g. , characteristics , of a 2D flat surface . 5 indicating a successful test ) . Conversely , the actual face 220 of the user is a 3D In one example , the threshold degree of uniformity may structure . When an input image is generated by photograph- be a value corresponding to a situation in which about 50 % ing the actual face 220 of the user , an object included in the or more of a number of pixels included in an image portion input image has characteristics of a 3D structure . Since the correspond to a face region ( e.g. , more generally , a region in actual face 220 of the user corresponds to a 3D structure 10 which a portion corresponding to a face region is indicated having various 3D curves and shapes , light 225 incident to by a bounding box ) . the actual face 220 of the user is less ( or non- ) uniformly The liveness testing apparatus may test the liveness of an reflected by a surface of the actual face 220 of the user . Thus , object based on a single input image . The single input image light energy is less ( or non- ) uniformly distributed on the may correspond to a single picture , a single image , a still object included in the input image . 15 image of a single frame , etc. The liveness testing apparatus According to at least one example embodiment , a liveness may test a liveness of an object included in a single input testing apparatus tests a liveness of an object included in an image by determining whether the object has one or more input image based on a distribution of light energy in the characteristics of a flat 2D surface or of a 3D structure . In object . In one example , the liveness testing apparatus ana- more detail , for example , the liveness testing apparatus may lyzes the distribution of light energy included in the object 20 test the liveness of the object included in the single input of the input image to determine whether the object included image by calculating a degree of uniformity in the distribu in the input image has characteristics of a flat 2D surface or tion of light energy included in the object . \\n of a 3D structure . FIG . 3 illustrates a liveness testing apparatus 310 accord Still referring to FIG . 2 , in one example , an input image ing to an example embodiment . may be generated by photographing the face 211 displayed 25 Referring to FIG . 3 , the liveness testing apparatus 310', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='on the medium 210. In this case , the liveness testing appa- includes a receiver 311 and a tester 312 . ratus analyzes a distribution of light energy included in the In example operation , the receiver 311 receives an input face of the input image , and determines that the face of the image . The receiver 311 may receive an input image gen input image has one or more characteristics of a flat surface . erated by an image sensor ( not shown ) . The receiver 311 The liveness testing apparatus then determines that the face 30 may be connected to the image sensor using a wire , wire of the input image corresponds to a fake object , and outputs lessly , or via a network . Alternatively , the receiver 311 may a signal indicating the same ( e.g. , indicating a failed test ) . receive the input image from a storage device , such as a In another example , an input image may be generated by main memory , a cache memory , a hard disk drive ( HDD ) , a photographing the actual face 220 of the user . In this case , solid state drive ( SSD ) , a flash memory device , a network the liveness testing apparatus analyzes a distribution of light 35 drive , etc. energy included in a face of the input image , and determines The tester 312 tests a liveness of an object included in the that the face of the input image has one or more character- input image . As discussed above , the tester 312 may test the istics of a 3D structure . The liveness testing apparatus then liveness of the object by determining whether the object has determines that the face of the input image corresponds to a one or more characteristics of a flat 2D surface or of a 3D real 3D object , and outputs a signal indicating the same ( e.g. , 40 structure . In one example , a successful test is one in which indicating a successful test ) . the object is determined to have one or more characteristics According to at least some example embodiments , the of a 3D structure , whereas a failed test is one in which the liveness testing apparatus may determine a liveness of an object is determined to have one or more characteristics of object included in an input image based on a degree of a flat 2D surface . In more detail , for example , the tester 312 uniformity in the distribution of light energy included in the 45 may test the liveness of the object by analyzing a distribution object in the input image . With regard again to FIG . 2 , in one of light energy included in the object of the input image . In example , since the light 215 incident to the face 211 dis- a more specific example , the tester 312 may test the liveness played on the medium 210 is reflected substantially uni- of the object by calculating a degree of uniformity in the formly , light energy included in the face of the input image distribution of the light energy included in the object of the is distributed substantially uniformly . When a degree of 50 input image , and comparing the degree of uniformity of the uniformity in the distribution of the light energy included in distribution of light energy in the object of the input image the face of the input image is greater than or equal to a given with a threshold value . If the determined degree of unifor ( or alternatively , desired or predetermined ) threshold degree mity is greater than or equal to a threshold value , then the of uniformity , the liveness testing apparatus determines that object in the input image is determined to correspond to the face of the input image has one or more characteristics 55 ( have been obtained from ) a flat 2D surface . On the other of a flat 2D surface . In this case , the liveness testing hand , if the determined degree of uniformity is less than or apparatus determines that the face of the input image cor- equal to the threshold value , then the object in the input responds to a fake object , and outputs a signal indicating the image is determined to correspond to ( have been obtained same ( e.g. , indicating a failed test )', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='value , then the object in the input responds to a fake object , and outputs a signal indicating the image is determined to correspond to ( have been obtained same ( e.g. , indicating a failed test ) . from ) a 3D structure . In another example with regard to FIG . 2 , since the light 60 According to at least some example embodiments , the 225 incident on the actual face 220 of the user is less ( or tester 312 may filter a plurality of pixels corresponding to non- ) uniformly reflected , light energy included in the face the object included in the input image to analyze the of the input image has a light distribution that is less ( or distribution of the light energy included in the plurality of non- ) uniform . When a degree of uniformity in the distri- pixels corresponding to the object included in the input bution of the light energy included in the face of the input 65 image . In one example , the tester 312 may filter the plurality image is less than the given threshold degree of uniformity , of pixels using a diffusion process . In this example , the tester the liveness testing apparatus determines that the face of the 312 may diffuse a plurality of pixels corresponding to the a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='a \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 20}), Document(page_content='a \\n 1 \\n a US 11,151,397 B2 \\n 9 10 \\n object included in the input image to analyze the distribution According to at least some example embodiments , the of the light energy included in the plurality of pixels liveness testing apparatus may reduce the final iteration corresponding to the object included in the input image . An count L using the AOS scheme to solve Equation 1. When example diffusion process will be described in more detail the AOS scheme is used , the reliability of ut , which is the below with reference to FIG . 4 . 5 value of the finally diffused pixel , may be sufficiently high Although example embodiments may be discussed in although a time step t of a given size is used . The liveness detail with regard to a diffusion process , it should be testing apparatus may increase efficiency of an operation for understood that any suitable filtering processes may be used the diffusion process using the AOS scheme to solve the in connection with example embodiments . In one example , diffusion equation . The liveness testing apparatus may per example embodiments may utilize bilateral filtering . 10 form the diffusion process using a relatively small amount of Because bilateral filtering is generally well - known , a processor and / or memory resources . detailed description is omitted . Moreover , any suitable fil- The liveness testing apparatus may effectively preserve a tering that preserves an edge region and blurs a non - edge texture of the input image using the AOS scheme to solve the region , in a manner similar or substantially similar to diffusion equation . The liveness testing apparatus may effec diffusion and bilateral filtering , may be used in connection 15 tively preserve the original texture of the input image even with example embodiments discussed herein . in relatively low - luminance and backlit environments . FIG . 4 illustrates a diffusion process according to an Referring to FIG . 4 , image 410 corresponds to an input example embodiment . image , image 420 corresponds to an intermediate diffusion According to at least some example embodiments , a image , and image 430 corresponds to a final diffusion image . liveness testing apparatus may diffuse a plurality of pixels 20 In this example , the final iteration count L is set to “ 20 ” . The corresponding to an object included in an input image . The image 420 was acquired after values of pixels included in the liveness testing apparatus may iteratively update values of input image were iteratively updated five times based on the plurality of pixels using a diffusion equation . In one Equation 3. The image 430 was acquired after the values of example , the liveness testing apparatus may diffuse the the pixels included in the input image were iteratively plurality of pixels corresponding to the object included in 25 updated 20 times based on Equation 3 . the input image according to Equation 1 shown below . According to at least this example embodiment , the \\n 26 + 1 = + * + div ( d ( 1Vukl ) Vu \" ) liveness testing apparatus may use a diffusion speed to [ Equation 1 ] determine whether the object included in the input image has In Equation 1 , k denotes an iteration count , uk denotes a one or more characteristics of a flat surface or of a 3D value of a pixel after a k - th iteration , and uk + 1 denotes a 30 structure . The diffusion speed refers to a speed at which each value of a pixel after a ( k + 1 ) -th iteration . A value of a pixel pixel value is diffused . The diffusion speed may be defined in the input image is denoted uº . as shown below in Equation 4 . Still referring to Equation 1 , V denotes a gradient opera tor , div ( ) denotes a divergence function , and d ( ) denotes a s ( x , y ) = \\\\ u + ( x , y ) -4 ° ( x , y ) [ Equation 4 ]', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 21}), Document(page_content=\"diffusivity function . In Equation 4 , s ( x , y ) denotes a diffusion speed of a pixel The diffusivity function do ) may be given ( or alterna- at coordinates ( x , y ) , uº ( x , y ) denotes a value of the pixel at tively , desired or predetermined ) function . In one example , coordinates ( x , y ) in an input image , and u ' ( x , y ) denotes a the diffusivity function may be defined as shown below in value of the pixel at coordinates ( x , y ) in a final diffusion Equation 2 . image . As shown in Equation 4 , as a difference between a \\n d ( Vul ) = 1 / ( \\\\ Vul + B ) [ Equation 2 ] 40 pixel value before diffusion and a pixel value after diffusion increases , a calculated diffusion speed also increase , In Equation 2 , B denotes a relatively small positive whereas when the difference between the pixel value before number ( e.g. , a minute value , such as about 10-6 ) . When the diffusion and the pixel value after diffusion decreases , the diffusivity function defined as shown above in Equation 2 is calculated diffusion speed decreases . used , a boundary of an object may be preserved relatively More broadly , the liveness testing apparatus may deter well during the diffusion process . When the diffusivity mine whether the object included in the input image has one function is a function of pixel gradient Vu as shown in or more characteristics of a flat surface or of a 3D structure Equation 2 , the diffusion equation is a nonlinear diffusion based on a magnitude of the change in pixel values in the equation . image after L number of iterations of the above - discussed The liveness testing apparatus may apply an additive 50 filtering process . operator splitting ( AOS ) scheme to solve Equation 1 , and the A face image may be divided into a small - scale region and liveness testing apparatus may diffuse the plurality of pixels a large - scale region . The small - scale region may refer to a corresponding to the object included in the input image region in which a feature point or a feature line is present . according to Equation 3 shown below . In one example , the small - scale region may include the eyes , 55 the eyebrows , the nose , and the mouth of the face . The large - scale region may refer to a region in which a relatively uk + 1 [ Equation 3 ] = ( -2 ( ( 1 – 27Ax ( ed ' ) * + ( 1 – 2tAy ( uck ) A Jecke large portion is occupied by skin of the face . In one example , the large - scale region may include the forehead and cheeks \\n of the face . \\n In Equation 3 , I denotes a value of a pixel in an input 60 Diffusion speeds of pixels belonging to the small - scale image , Ac denotes a horizontal diffusion matrix , A , denotes region may be greater than diffusion speeds of pixels belong a vertical diffusion matrix , T denotes a time step . A final ing to the large - scale region . Referring back to the example iteration count L and the time step t may be given ( or shown in FIG . 4 , a pixel 411 corresponding to eyeglass alternatively desired or predetermined ) . In general , when the frames in the image 410 differs from neighboring pixels time step t is set to be relatively small and the final iteration 65 corresponding to skin , and thus , a value of the pixel 411 may count L is set to be relatively large , a reliability of u ? change substantially ( e.g. , relatively greatly ) as a result of denoting a value of a finally diffused pixel may increase . diffusion . The value of the pixel 411 in the image 410 may 35 \\n a \\n 45 \\n a a \\n a \\n 1 \\n a \\n a\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 21}), Document(page_content='15 US 11,151,397 B2 \\n 11 12 \\n be updated to a value of a pixel 431 in the image 430 by differs from the SR map 520 acquired when the actual face diffusion . Conversely , a pixel 412 corresponding to a cheek of the same user is photographed . In the SR map 510 and the in the image 410 is similar to neighboring pixels , and thus , SR map 520 , portions marked with black color correspond a value of the pixel 412 may change less than the value of to pixels satisfying SR ( x , y ) = 1 , whereas portions marked the pixel 411 ( e.g. , relatively slightly ) by diffusion . The 5 with white color correspond to pixels satisfying SR ( x , y ) = 0 . \\n value of the pixel 412 in the image 410 may be updated to In this example , in the SR map 510 and the SR map 520 , the \\n a value of a pixel 432 in the image 430 as a result of black portions have relatively fast diffusion speeds , and the \\n diffusion . white portions have relatively slow diffusion speeds . \\n A difference in diffusion speeds may also result from a According to at least one example embodiment , the distribution of light energy in an image . When light energy 10 liveness testing apparatus may test a liveness of a face in an in the image is more uniformly distributed , a relatively small image by analyzing an SR map . For example , the liveness \\n diffusion speed may be calculated . Moreover , when the light testing apparatus may test the liveness of the face in the \\n energy is more uniformly distributed , the probability of image by extracting various features from the SR map . neighboring pixels having similar pixel values may be When an actual face of a user is photographed , various \\n higher ( e.g. , relatively high ) . Conversely , when light energy light reflections may occur due to curves on the actual face \\n of the user . in the image is less ( or non- ) uniformly distributed , a relatively high diffusion speed may be observed . Moreover , When light energy in the image is less ( or non- ) uniformly \\n when the light energy is less ( or non- ) uniformly distributed , distributed , a relatively large number of pixels having a pixel \\n the probability of neighboring pixels having different pixel value of “ 1 ” may be included in the SR map . In this example , \\n values may be relatively high . 20 the liveness testing apparatus may determine the liveness of \\n According to at least some example embodiments , a the face in the image based on Equation 6 shown below . \\n liveness testing apparatus may calculate a degree of unifor mity in the distribution of the light energy in the image based on statistical information related to diffusion speeds . The 1 , if N ( SR ( x , y ) = 1 ) < 6 [ Equation 6 ] \\n liveness testing apparatus may test a liveness of an object in the image based on the statistical information related to otherwise \\n diffusion speeds . To calculate the statistical information related to diffusion speeds , the liveness testing apparatus In Equation 6 , N ( SR ( x , y ) = 1 ) denotes a number of pixels may extract a small - scale region from an image according to 30 satisfying SR ( x , y ) = 1 , and denotes a threshold value , which Equation 5 shown below . may be given , desired , or alternatively preset . The liveness testing apparatus may determine that the face in the image \\n 1 , if s ( x , y ) > u to corresponds to a fake object when [ Equation 5 ] SR ( x , y ) 0 , otherwise 25 Fake = : ( x , y ) \\n 0 , \\n = { : 35 \\n ( x , y ) \\n a \\n Fake = { ( x , y )', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 22}), Document(page_content='1 , if s ( x , y ) > u to corresponds to a fake object when [ Equation 5 ] SR ( x , y ) 0 , otherwise 25 Fake = : ( x , y ) \\n 0 , \\n = { : 35 \\n ( x , y ) \\n a \\n Fake = { ( x , y ) \\n a N ( SR ( x , y ) = 1 ) < 6 In Equation 5 , SR ( x , y ) is an indicator indicating whether a pixel at coordinates ( x , y ) belongs to a small - scale region . In this example , when a value of SR ( x , y ) corresponds to “ 1 ” , is satisfied . the pixel at the coordinates ( x , y ) belongs to the small - scale 40 In another example , when light energy in the image is less region , whereas when the value of SR ( x , y ) corresponds to ( non- ) uniformly distributed , a relatively large amount of “ O ” , the pixel at the coordinates ( x , y ) does not belong to the small - scale region . noise components may be included in the SR map . In this case , the liveness testing apparatus may determine the The value of SR ( x , y ) may be determined based on a liveness of the face in the image based on Equation 7 shown diffusion speed for the pixel at coordinates ( x , y ) . For 45 below . example , when the diffusion speed s ( x , y ) is greater than a given ( or alternatively , desired or predetermined ) threshold value , the value of SR ( x , y ) is determined to be “ 1 ” . Other wise , the value of SR ( x , y ) is determined to be “ O ” . The 1 , if | SR ( x , y ) – SRM ( x , y ) ] < & [ Equation 7 ] \\n threshold value may be set based on an average y of the 50 0 , otherwise entire image and a standard deviation a of the entire image . The average u of the entire image may correspond to an average of diffusion speeds of pixels included in the entire In Equation 7 , SRx ( x , y ) denotes a value of a pixel at image , and the standard deviation a of the entire image may coordinates ( x , y ) in an image acquired by applying median correspond to a standard deviation of the diffusion speeds of 55 filtering to an SR map , and denotes a threshold value , which the pixels included in the entire image . may be given , desired , or alternatively preset . As the amount Hereinafter , an image in which a value of a pixel at of noise components increases , a number of pixels having a coordinates ( x , y ) corresponds to SR ( x , y ) will be referred to difference between a value of SR ( x , y ) and a value of SR M ( x , as a small - scale region ( SR ) map . Since each pixel included y ) increase . In this example , when in the SR map may have a value of “ O ” or “ 1 ” , the SR map 60 may also be referred to as a binary map . The SR map may effectively represent an underlying structure of a face in various illumination environments . SR ( x , y ) – SRM ( x , y ) < & \\n FIG . 5 illustrates two example SR maps according to example embodiments . Referring to FIG . 5 , as shown , the SR map 510 acquired is satisfied , the face in the image is determined to be a fake when a medium displaying a face of a user is photographed object . a \\n a \\n a a \\n ( x , y ) \\n 65', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 22}), Document(page_content='Live = ( x , y ) \\n ( x 20 \\n ( x , y ) \\n otherwise US 11,151,397 B2 \\n 13 14 \\n Equations 6 and 7 are provided only as examples . The the extracted features and the learned parameters , the clas liveness testing apparatus may test the liveness of the object sifier may output a signal indicating whether the object in the image based on a variety of diffusion speed based included in the input image corresponds to a real object or statistical information . In one example , the liveness testing a fake object \\n apparatus may use a distribution of pixels having diffusion 5 FIG . 6 illustrates a liveness testing apparatus 600 accord speeds greater than or equal to a given , desired or alterna- ing to an example embodiment . tively predetermined , threshold value . Referring to FIG . 6 , the liveness testing apparatus 600 \\n In more detail , the liveness testing apparatus may deter includes : a receiver 611 , a diffuser 612 ; and a tester 613. The \\n mine the liveness of the face in the image based on Equation receiver 611 may correspond to the receiver 311 shown in \\n 8 shown below . 10 FIG . 3 . In example operation , the receiver 611 receives an input image , and outputs the input image to the diffuser 612 and \\n the tester 613 . 1 , if N ( SR ( x , y ) = 1 ) 26 [ Equation 8 ] Although element 612 is referred to as a diffuser and the 15 example embodiment shown in FIG . 6 will be described 0 , otherwise with regard to a diffusion operation , the element 612 may be more generally referred to as a filter or filter circuit 612 . \\n The liveness testing apparatus may determine that the face Moreover , any suitable filtering operation may be used as \\n in the image corresponds to a live 3D object when E ( x , y ) N desired . \\n ( SR ( x , y ) = 125 is satisfied . The diffuser 612 diffuses a plurality of pixels correspond \\n In another example , the liveness testing apparatus may ing to an object included in the input image by iteratively \\n determine the liveness of the face in the image based on updating values of the plurality of pixels corresponding to \\n Equation 9 shown below . the object included in the input image based on a diffusion equation . In one example , the diffuser 612 may diffuse the 25 plurality of pixels corresponding to the object included in \\n 1 , if I ( SR ( x , y ) – SRM ( x , y ) 25 [ Equation 9 ] the input image using Equation 1 discussed above .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='Live = The diffuser 612 may iteratively update the values of the plurality of pixels corresponding to the object included in 0 , the input image by applying an AOS scheme to the diffusion 30 equation . In one example , the diffuser 612 may diffuse the In this example , when ISR ( x , y ) -SR ( x , y ) l2 is sat- plurality of pixels corresponding to the object included in isfied , the face in the image is determined to be a live 3D the input image using Equation 3 discussed above . The object . diffuser 612 may output a diffusion image generated when The liveness testing apparatus may also use diffusion the plurality of pixels are diffused . speed based statistical information without using an SR 35 Still referring to FIG . 6 , the tester 613 tests a liveness of map . In this example , the liveness testing apparatus may use the object included in the input image based on diffusion respective values of diffusion speeds of all pixels , an average speeds of the plurality of pixels . In one example , the tester of the diffusion speeds of all the pixels , and a standard 613 may test the liveness of the object by estimating a deviation of the diffusion speeds of all the pixels . The surface property related to the object based on the diffusion liveness testing apparatus may also use a filter response 40 speeds . The surface property refers to a property related to based on diffusion speeds . The liveness testing apparatus a surface of the object , and may include , for example , a may use a result of applying median filtering to diffusion light - reflective property of the surface of the object , a speeds of all pixels . number of dimensions of the surface of the object , and / or a According to at least some example embodiments , the material of the surface of the object . liveness testing apparatus may extract various features based 45 The tester 613 may analyze a distribution of light energy on diffusion speed based statistical information , and learn included in the input image to estimate the surface property the extracted features . The liveness testing apparatus may related to the object included in the input image . In one calculate diffusion speed based statistical information from example , the tester 613 may analyze the distribution of the various training images , and enable a classifier to learn the light energy included in the input image to determine features extracted from the statistical information , in a 50 whether the object included in the input image has a surface learning stage . The training images may include images of property ( one or more characteristics ) of a medium display a live 3D object and images of a fake 2D object . ing a face ( e.g. , a 2D flat surface ) or a surface property ( one A simply structured classifier may obtain a distance or more characteristics ) of an actual face of a user ( e.g. , a 3D between vectors ( e.g. , a Euclidian distance ) or a similarity structure ) . ( e.g. , a normalized correlation ) , and compare the distance 55 When the object included in the input image is determined between the vectors or the similarity to a threshold value . A to have a surface property of a medium displaying a face neural network , a Bayesian classifier , a support vector ( e.g. , a 2D flat surface ) , the tester 613 may output a signal machine ( SVM ) , or an adaptive boosting ( AdaBoost ) learn- corresponding to a failed test . That is , for example , when the ing classifier may be used as a more elaborate classifier . tester 613 determines that the object included in the input The liveness testing apparatus may calculate diffusion 60 image has a surface property of a medium displaying a face speed based statistical information from an input image , and ( e.g. , a 2D flat surface ) , the tester 613 may output a signal extract features from the statistical information using a indicative of a failed test . When the object included in the given , desired , or alternatively predetermined ,', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='613 may output a signal extract features from the statistical information using a indicative of a failed test . When the object included in the given , desired , or alternatively predetermined , method . The input image is determined to have a surface property of an method may correspond to the method used in the learning actual face of a user ( e.g. , a 3D structure ) , the tester 613 may stage . The liveness testing apparatus may input the extracted 65 output a signal corresponding to a successful test . That is , for features and learned parameters into the classifier to test a example , when the tester 613 determines that the object liveness of the object included in the input image . Based on included in the input image has a surface property of an ?', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 23}), Document(page_content='a \\n a \\n a \\n I = W : v US 11,151,397 B2 \\n 15 16 \\n actual face of a user ( e.g. , a 3D structure ) , the tester 613 may tially unaffected by illumination on an object when the input output a signal indicative of a successful test . image is obtained . One or more example embodiments may In another example , the tester 613 may test the liveness of provide technology that may generate an image less suscep the object by calculating statistical information related to the tible ( e.g. , impervious ) to changes in illumination , thereby diffusion speeds . As discussed above , a 2D object and a 3D 5 increasing reliability of face recognition and / or user verifi object have different light - reflective properties . And the cation , and / or reducing computational complexity of face different light - reflective properties between the 2D object recognition and / or user verification . and the 3D object may be modeled based on diffusion Still referring to FIG . 7 , the input image 710 includes an speeds . illumination component 715 and a non - illumination com In this example , the tester 613 may calculate a diffusion 10 ponent . In this example , the illumination component 715 speed based on the input image from the receiver 611 and the refers to a component , from among components constituting diffusion image from the diffuser 612. In one example , the pixel values , that is affected ( e.g. , substantially affected ) by tester 613 may calculate a diffusion speed for each pixel external illumination . The non - illumination component using Equation 4 discussed above . To calculate statistical refers to a component , from among components constituting information related to diffusion speeds , the tester 613 may 15 pixel values , that is substantially unaffected by external extract a small - scale region using Equation 5. The extracted illumination . The image processing apparatus may separate small - scale region may be represented as an SR map . The the illumination component 715 from the input image 710 to tester 613 may then determine the liveness of the object generate an image less susceptible ( e.g. , impervious ) to included in the input image using Equation 6 , 7 , 8 or 9 . changes in illumination . The tester 613 may test the liveness of the object included 20 The image processing apparatus may detect a face region in the input image based on a variety of diffusion speed from an input image . In this example , example embodiments based statistical information . The tester 613 may use a may be applicable to the face region detected from the input distribution of pixels having diffusion speeds greater than or image . Hereinafter , the term “ face image ” refers to an input equal to a given , desired , or alternatively predetermined , image including a face , or a face region extracted from the threshold value . The tester 613 may also use diffusion speed 25 input image . based statistical information without using an SR map . A face image may be expressed based on an illumination When the calculated statistical information corresponds to component and a non - illumination component . The face statistical information related to the medium displaying the image may be based on a Lambertian model , as shown face , the tester 613 may output a signal corresponding to a below in Equation 10 .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='failed test . When the calculated statistical information cor- 30 [ Equation 10 ] responds to statistical information related to the actual face of the user , the tester 613 may output a signal corresponding In Equation 10 , I denotes a face image , w denotes an to a successful test . That is , for example , when the calculated illumination component , and v denotes a non - illumination statistical information is indicative of a medium displaying component . With regard to the example shown in FIG . 7 , I the face , the tester 613 may output a signal indicating a 35 corresponds to the input image 710 , w corresponds to the failed test , whereas when the calculated statistical informa- image 720 related to the illumination component , and v tion is indicative of an actual face of the user , the tester 613 corresponds to the image 730 related to the non - illumination may output a signal indicating a successful test . component . \\n The liveness testing apparatus 600 may test the liveness of The image 720 related to the illumination component may the object based on a single input image . The single input 40 include the illumination component 715 , whereas the image image may correspond to a single picture , a single image , or 730 related to the non - illumination component may not a still image of a single frame . include the illumination component 715. Thus , the image 730 related to the non - illumination may be an image less Image Processing According to Example susceptible ( e.g. , impervious ) to changes in illumination .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='Embodiments 45 The image 730 related to the non - illumination component may also be referred to as a canonical image . FIG . 7 is a flow diagram illustrating image processing The illumination component 715 may have a relatively according to example embodiments . FIG . 8 illustrates high probability of being distributed in a large - scale region example changes in an input image depending on illumina- of the image . Thus , the image 720 related to the illumination tion according to example embodiments 50 component may be an image corresponding to a large - scale Referring to FIG . 7 , an input image 710 includes a face of region . The illumination component 715 may have a rela a user . The face of the user included in the input image 710 tively low probability of being distributed in a small - scale is affected ( e.g. , greatly or substantially affected ) by illumi- region . Thus , the image 730 related to the non - illumination nation . For example , referring to FIG . 8 , although a face of component may be an image corresponding to a small - scale the same user is photographed , different images may be 55 region . generated depending on illumination . When an input image The image processing apparatus may generate the image is vulnerable to changes in illumination , reliability of face 730 related to the non - illumination component based on the recognition and / or user verification may decrease ( e.g. , input image 710. In one example , the image processing considerably or substantially decrease ) , and / or a computa- apparatus may receive the input image 710 , and generate the tional complexity may increase ( e.g. , substantially or con- 60 image 720 related to the illumination component based on siderably increase ) . the input image 710. The image processing apparatus may An image processing method and / or apparatus according calculate the image 730 related to the non - illumination to one or more example embodiments may generate an component based on the input image 710 and the image 720 image less susceptible ( e.g. , impervious ) to changes in related to the illumination component using Equation 10 illumination from an input image . An image processing 65 shown above . method and / or apparatus according to one or more example According to at least one example embodiment , the image embodiments may also generate an image that is substan- processing apparatus may diffuse the input image 710 to a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 24}), Document(page_content='? \\n + \\n a \\n + 1 \\n u a \\n 30 US 11,151,397 B2 \\n 17 18 \\n generate the image 720 related to the illumination compo- In Equation 13 , I denotes a value of a pixel in the input nent . Diffusion speeds of pixels belonging to the small - scale image 710 , Ac denotes a horizontal diffusion matrix , A , region may be greater than diffusion speeds of pixels belong- denotes a vertical diffusion matrix , and t denotes a time step . ing to the large - scale region . The image processing appara- The final iteration count L and the time step t may be given , tus may separate the small - scale region and the large - scale 5 desired , or alternatively predetermined . In general , when the region based on a difference in diffusion speeds . The image time step t is set to be relatively small and the final iteration processing apparatus may diffuse a plurality of pixels count L is set to be relatively large , a reliability of u? , which included in the input image 710 a number of times corre denotes a value of a final diffused pixel , may increase . sponding to a given , desired , or alternatively predetermined , iteration count ( e.g. , about 20 ) to generate the image 720 10 the image processing apparatus to reduce the final iteration Using the AOS scheme to solve Equation 11 may enable \\n related to the illumination component corresponding to the count L. When the AOS scheme is used , the reliability of the large - scale region . According to at least one example embodiment , the image final diffused pixel ut may be sufficiently high although the \\n processing apparatus may iteratively update values of the time step t of a given , desired , or alternatively predeter plurality of pixels using a diffusion equation . In one 15 mined , size is used . Image processing apparatuses according example , the image processing apparatus may diffuse the to one or more example embodiments may increase an plurality of pixels corresponding to the face included in the efficiency of operations for diffusion processes using the input image 710 using Equation 11 shown below . AOS scheme to solve diffusion equations . \\n Ux + 1 = + * + div ( d ( \" Vukl ) Vuk ) [ Equation11 ] The image processing apparatus may generate the image \\n 20 730 related to the non - illumination component based on the In Equation 11 , k denotes an iteration count , u denotes a input image 710 and the image 720 related to the illumina value of a pixel after a k - th iteration , uk + 1 denotes a value of a tion component . In one example , the image processing a pixel after a ( k + 1 ) -th iteration , and uk corresponds to uk ( x , y ) , which is a value of a pixel at coordinates ( x , y ) in an apparatus may generate the image 730 related to the non \\n image after k diffusions . The value ukal corresponds to 25 in Equation 10 corresponds to ué , Equations 14 and 15 may illumination component using Equation 14 or 15. Since \\' w \\' \\n ok + 1 ( x , y ) , which is a value of a pixel at coordinates ( x , y ) in an image after ( k + 1 ) diffusions . In this example , uº denotes be derived from Equation 10 . \\n a value of a pixel in the input image 710. When a final v = irut iteration count corresponds to “ L ” , ut denotes a value of a [ Equation 14 ] \\n pixel in the image 720 related to the illumination compo \\n nent . log v = log 1 - log u [ Equation 15 ] \\n As before , V denotes a gradient operator , div ( ) denotes a In Equations 14 and 15 , I denotes a face image , and may divergence function , and d ( ) denotes a diffusivity function . correspond to , for example , the input image 710. The face The diffusivity function may be given , desired , or alterna tively predetermined . In one example , the image processing 35 ut denotes a large - scale region , and may correspond to , for image I may also correspond to uº . The final diffused pixel', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 25}), Document(page_content=\"apparatus may define the diffusivity function as shown example , the image 720 related to the illumination compo below in Equation 12 . nent . Still referring to Equations 14 and 15 , v denotes a d ( iVul ) = 1 / ( IVul + B ) [ Equation 12 ] small - scale region , and may correspond to , for example , the \\n In Equation 12 , ß denotes a small positive number . When the image 730 related to the non - illumination component . diffusivity function defined in Equation 12 is used , a bound- 40 FIG . 9 illustrates an image processing apparatus 910 ary of a face may be preserved relatively well during the according to an example embodiment . Also shown in FIG . diffusion process . When the diffusivity function corresponds 9 is a face recognition and / or user verification circuit 920 , \\n to a function of pixel gradient Vu as shown in Equation 12 , which will be discussed in more detail later . \\n the diffusion equation is nonlinear . Herein , an image gen Referring to FIG . 9 , the image processing apparatus 910 erated by diffusion is referred to as a diffusion image . When 45 includes : a receiver 911 ; a diffuser 912 ; and a generator 913 . the diffusivity function is nonlinear , an image generated by As with FIG . 6 , although element 912 in FIG.9 is referred \\n diffusion is referred to as a nonlinear diffusion image . to as a diffuser and the example embodiment shown in FIG . Equation 12 is provided as an example of the diffusivity 9 will be described with regard to a diffusion operation , the function , however , example embodiments may utilize other element 912 may be more generally referred to as a filter or diffusivity functions . For example , one of a plurality of 50 filter circuit 912. Moreover , as mentioned above , the filter candidate diffusivity functions may be selected based on an utilize any suitable filtering operation , rather than \\n input image . the diffusion process discussed here . Moreover , although example embodiments are discussed In example operation , the receiver 911 may receive an with regard to diffusivity functions , other filter functions input image . In a more specific example , the receiver 911 \\n may also be used , as mentioned above . 55 may receive an input image generated by an image sensor According to at least some example embodiments , the ( not shown ) . The receiver 911 may be connected to the image processing apparatus may apply an AOS scheme to image sensor using a wire , wirelessly , or via a network . solve Equation 11. In one example , the image processing Alternatively , the receiver 911 may receive the input image apparatus may diffuse the plurality of pixels corresponding from a storage device , such as , a main memory , a cache to the face included in the input image 710 using Equation 60 memory , a hard disk drive ( HDD ) , a solid state drive ( SSD ) , \\n 13 shown below . a flash memory device , a network drive , etc. The diffuser 912 may diffuse a plurality of pixels corre sponding to an object included in the input image . In one \\n [ Equation 13 ] example , the object may correspond to a face of a user . The 4 + 1 litt = ' ' all1 – 2TAz ( ed ) + + ( 1 – 2tAy ( uck ) Pjecte 65 diffuser 912 may iteratively update values of the plurality of pixels corresponding to the object included in the input image based on a diffusion equation . In one example , the 912 may \\n 1 \\n =\", metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 25}), Document(page_content='a \\n 2 US 11,151,397 B2 \\n 19 20 \\n diffuser 912 may diffuse the plurality of pixels correspond- With regard to the liveness testing apparatus 310 shown ing to the object included in the input image according to in FIG . 3 , for example , at operation 1010 , the receiver 311 Equation 11 . receives the input image , and the tester 312 tests a liveness The diffuser 912 may iteratively update the values of the of the object included in the received input image at opera plurality of pixels corresponding to the object included in 5 tion 1020. The tester 312 may test the liveness of the object the input image by applying an AOS scheme to the diffusion included in the input image received at the receiver 311 equation . In one example , diffuser 912 may diffuse the based on whether the object in the input image has one or plurality of pixels corresponding to the object included in more characteristics of a flat surface or a 3D structure . The the input image using Equation 13. The diffuser 912 may details of the operation performed by the tester 312 are output a diffusion image generated when the plurality of 10 discussed above with regard to FIG . 3 , and thus , a detailed pixels are diffused . The diffusion image may correspond to discussion is not repeated here . an image related to an illumination component ( e.g. , 720 in FIG . 7 ) . With regard to the liveness testing apparatus 600 shown \\n Still referring to FIG . 9 , the generator 913 may generate in FIG . 6 , for example , at operation 1010 the receiver 611 an output image based on the input image and the diffusion 15 receives the input image . At operation 1020 , the diffuser 612 image . The generator 913 may generate the output image diffuses a plurality of pixels corresponding to the object \\n using Equation 14 or 15. The output image may correspond included in the input image received at the receiver 611 , and \\n to an image related to a non - illumination component ( e.g. , the tester 613 tests the liveness of the object based on', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='to an image related to a non - illumination component ( e.g. , the tester 613 tests the liveness of the object based on \\n 730 in FIG . 7 ) . The generator 913 may output the output diffusion speeds of the plurality of pixels . The details of the image to the face recognition and / or user verification circuit 20 operation performed by the diffuser 612 and the tester 613 920. The face recognition and / or user verification circuit 920 are discussed above with regard to FIG . 6 , and thus , a may perform any well - known face recognition and / or user detailed discussion is not repeated here . verification circuit 920 operation as will be discussed in FIG . 11 is a flow chart illustrating an image processing some detail later . Alternatively , the generator 913 may method according to an example embodiment . For example output the output image to a memory ( not shown ) . 25 purposes , the image processing method shown in FIG . 11 According to at least some example embodiments , the will be discussed with regard to the image processing image processing apparatus 910 may generate the output apparatus shown in FIG . 9 . image based on a single input image . The single input image Referring to FIG . 11 , at operation 1110 , the image pro may correspond to a single picture , a single image , or a still cessing apparatus receives a first image . At operation 1120 , image of a single frame . Still referring to FIG . 9 , in one 30 the image processing apparatus generates a second image , example the face recognition and / or user verification circuit and at operation 1130 the image processing apparatus gen 920 may recognize a face included in the input image based erates a third image . on the output image from the generator 913. The output The first image may correspond to an input image ( e.g. , image may correspond to an image related to the non- 710 in FIG . 7 ) , the second image may correspond to an illumination component and less susceptible ( e.g. , impervi- 35 image related to an illumination component ( e.g. , 720 in ous ) to changes in illumination . The face recognition and / or FIG . 7 ) , and the third image may correspond to an image user verification circuit 920 may recognize the face included related to a non - illumination component ( e.g. , 730 in FIG . in the input image based on an image less susceptible ( e.g. , 7 ) . \\n impervious ) to changes in illumination . Thus , accuracy In more detail , with regard to FIGS . 9 and 11 , for and / or a reliability of the face recognition may increase . 40 example , at operation 1110 the receiver 911 receives a first When an image less susceptible ( e.g. , impervious ) to ( input ) image . changes in illumination is used , a performance of an align- At operation 1120 , the diffuser 912 generates a second ment operation may increase in a relatively low - luminance image based on the first ( input ) image . In this example , the', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='environment . second image is an image related to an illumination com In another example , the face recognition and / or user 45 ponent ( e.g. , 720 in FIG . 7 ) . verification circuit 920 may verify the user based on the At operation 1130 , the generator generates a third ( output ) output image from the generator 913. The image processing image based on the first ( input ) image and the second image apparatus 910 may verify the user by recognizing the face of generated by the diffuser 912. In this case , the third ( output ) the user based on the output image . The output image may image is an image related to a non - illumination component correspond to an image related to the non - illumination 50 ( e.g. , 730 in FIG . 7 ) . The details of the operations performed component and less susceptible ( e.g. , impervious ) to by the diffuser 912 and the generator 913 are discussed changes in illumination . The image processing apparatus above with regard to FIG . 9 , and thus , a detailed discussion 910 may verify the user based on an image less susceptible is not repeated here . ( e.g. , impervious ) to a change in illumination . Thus , accu- More generally , the descriptions provided with reference racy and / or a reliability of the user verification may increase . 55 to FIGS . 1A through 9 may be applicable to operations of FIGS . 10 and 11 , and thus , more detailed descriptions are Flowchart According to Example Embodiments omitted for conciseness . FIG . 12 illustrates an image processing method according FIG . 10 is a flow chart illustrating a liveness testing to another example embodiment . method according to an example embodiment . In some 60 The example embodiment shown in FIG . 12 combines the cases , the flow chart shown in FIG . 10 will be discussed with liveness testing method of FIG . 10 with the image process regard to the liveness testing apparatus shown in FIGS . 3 ing method of FIG . 11. For example purposes , the method \\n and 6 . shown in FIG . 12 will be described with regard to the image Referring to FIG . 10 , at operation 1010 the liveness processing apparatus shown in FIG . 9. The details of the testing apparatus receives an input image . At operation 65 operations described with regard to FIG . 12 are provided 1020 , the liveness testing apparatus tests a liveness of an above with regard to , for example , FIGS . 3 , 6 , 9 , 10 and 11 , object included in the received input image . and thus , a detailed discussion is not repeated here .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 26}), Document(page_content='10 \\n 15 US 11,151,397 B2 \\n 21 22 \\n Referring to FIG . 12 , at operation 1210 the receiver 911 Referring to FIG . 13 , the electronic system includes , for of the image processing apparatus 910 receives a first image . example : an image sensor 1300 , an image signal processor The first image may correspond to an input image including ( ISP ) 1302 , a display 1304 and a memory 1308. The image a face of a user . The receiver 911 outputs the first image to sensor 1300 , the ISP 1302 , the display 1304 and the memory the diffuser 912 and the generator 913 . 5 1308 communicate with one another via a bus 1306 . At operation 1220 , the diffuser 912 generates a second The image sensor 1300 may be the image sensor 115 image based on the first image received at the receiver 911 . described above with regard to FIGS . 1A and 1B . The image The diffuser 912 generates the second image by diffusing the sensor 1300 is configured to capture an image ( also referred first image from the receiver 911. The second image may be to as image data ) in any well - known manner ( e.g. , by an image related to an illumination component . converting optical images into electrical signals ) . The image At operation 1240 , the generator 913 calculates a diffu- is output to the ISP 1302 . sion speed for each pixel based on the first image and the The ISP 1302 may include one or more of the apparatuses second image . The diffusion speed for each pixel may be and / or may perform one more of the methods discussed calculated based on a difference between a pixel value in the above with regard to discussed above with regard to FIGS . second image and a corresponding pixel value in the first 1A through 12. The ISP 1302 may also include the face image . recognition and / or user verification circuit 920 to perform At operation 1250 , the generator 913 extracts statistical face recognition and / or user verification operations dis information based on diffusion speeds . For example , the cussed above with regard to FIGS . 1A through 12. In a more generator 913 calculates a number of pixels having diffusion 20 specific example , the ISP 1302 may include the liveness speeds greater than a given , desired , or alternatively prede- testing apparatus 310 shown in FIG . 3 , the liveness testing termined , threshold value . apparatus 610 shown in FIG . 6 , the image processing At operation 1270 , the generator 913 performs a liveness apparatus 910 shown in FIG . 9 and / or the face recognition test based on the diffusion speed based statistical informa- and / or user verification circuit 920 shown in FIG . 9. The tion . In one example , the generator 913 determines whether 25 memory 1308 may store images captured by the image the input image corresponds to a real 3D object based on the sensor 1300 and / or generated by the liveness testing appa number of pixels having diffusion speeds greater than the ratus and / or the image processing apparatus . The memory', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='threshold value . 1308 may be any suitable volatile or non - volatile memory . If the generator 913 determines that the input image does The display 1304 may display images captured by the image not correspond to a real 3D object ( the liveness test fails ) , 30 sensor 1300 and / or generated by the liveness testing appa then the face recognition and / or user verification circuit 920 ratus and / or the image processing apparatus . does not perform face recognition and / or user verification at The ISP 1302 may also be configured to execute a operation 1260 , and the process terminates . program and control the electronic system . The program Returning to operation 1270 , if the generator 913 deter- code to be executed by the ISP 1302 may be stored in the mines that the input image does correspond to a live 3D 35 memory 1308 . object ( the liveness test succeeds ) , then face recognition The electronic system shown in FIG . 13 may be con and / or user verification is performed . In this example , an nected to an external device ( e.g. , a personal computer or a image less susceptible ( e.g. , impervious ) to changes in network ) through an input / output device ( not shown ) and illumination may be generated for use in face recognition may exchange data with the external device . and / or user verification operations . The electronic system shown in FIG . 13 may embody Still referring to FIG . 12 , at operation 1230 the generator various electronic systems including : a mobile device , such 913 generates a third image based on the first image and the as a mobile phone , a smartphone , a personal digital assistant second image . In one example , the generator 913 calculates ( PDA ) , a tablet computer , a laptop computer , etc .; a com the third image based on a ratio of the first image to the puting device , such as a personal computer ( PC ) , a tablet PC , second image as discussed above with regard to Equation 14 45 a netbook ; or an electronic product , such as a television ( TV ) or a difference between the first image and the second image or smart TV , a security device for a gate control , etc. in a log domain as discussed above with regard to Equation FIG . 14 is a flowchart illustrating a user recognition 15. The third image may be an image related to a non- method according to an example embodiment . Each opera illumination component and impervious to a change in tion of the user recognition method of FIG . 14 may be illumination . 50 performed by a user recognition apparatus . The user recog At operation 1260 , the face recognition and / or user veri- nition apparatus may be implemented using a software fication circuit 920 performs face recognition and / or user module , a hardware module , or a combination thereof . verification based on the third image . In the example Referring to FIG . 14 , at operation 1410 , the user recog embodiment shown in FIG . 12 , the face recognition and / or nition apparatus extracts a first feature of a first image . For user verification circuit 920 performs the face recognition 55 example , at operation 1410 , the user recognition apparatus and / or user verification operations in operation 1260 only extracts the first feature by diffusing the first image . The first when the input image does correspond to a real 3D object image is an image acquired by capturing a user , and may ( the liveness test succeeds ) . In this example , the face rec- include a face of the user . The first feature of the first image ognition and / or user verification may be performed based on may be a diffusion - based feature such as a diffusion speed , the third image corresponding to the image less susceptible 60 for example . ( e.g. , impervious ) to changes in illumination . At operation 1420 , the user recognition apparatus per Details of the descriptions provided with reference to forms a liveness test based on the first feature of the first FIGS . 1A through 11 may be applicable to operations of', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='recognition apparatus per Details of the descriptions provided with reference to forms a liveness test based on the first feature of the first FIGS . 1A through 11 may be applicable to operations of image . The descriptions provided with reference to FIGS . 1 FIG . 12 , and thus , duplicated descriptions are omitted for through 13 may be applicable to operations 1410 and 1420 , conciseness . 65 and thus more detailed descriptions are omitted for concise FIG . 13 is a block diagram illustrating an electronic ness . For example , operation 1420 may correspond to opera system according to an example embodiment . tion 1270 of FIG . 12 . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='a \\n a \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 27}), Document(page_content='9 \\n a \\n a US 11,151,397 B2 \\n 23 24 \\n If the liveness test succeeds , the user recognition appa- in which payment information is input from the user at the ratus extracts a second feature of the first image , and point in time at which the actual payment is needed , user performs user recognition based on the second feature of the verification may be requested for payment approval . first image at operation 1430. The user recognition may In this example , the terminal may acquire an input image include verification to determine whether the user in the first 5 including a face of the user using a camera in response to image matches a pre - registered user , or identification to reception of a request for an electronic commerce payment determine a user corresponding to the user in the first image , from the user . When a liveness test and / or user verification among a plurality of users . is performed based on the input image , the electronic The second feature of the first image may be determined commerce payment may be performed based on a corre in various ways . For example , the second feature of the first 10 sponding result . image may be a feature extracted by processing the first A security module may be implemented to be separate image to recognize a face of the user , a feature extracted from a payment module . The payment module may request from an image generated by diffusing the first image , or a user verification from the security module at a point in time combination thereof . at which a payment approval is needed . The security module The user recognition of operation 1430 may be performed 15 may be executed in a safety region in an operating system . only when the liveness test of operation 1420 succeeds . In response to reception of the user verification request from Although not shown in FIG . 14 , if the liveness test fails , the the payment module , the security module may receive the user recognition apparatus may receive a new image and input image from the camera via a security channel , and perform a liveness test with respect to the new image . If the perform a liveness test and / or user verification with respect liveness test with respect to the new image succeeds , the 20 to the input image . The security module may return a user recognition apparatus may perform user recognition verification result to the payment module . The payment using the new image . In an example , the first image and the module may approve or reject a payment based on the new image may correspond to different frames in a video . verification result . Although not shown in FIG . 14 , in another example , the In another example , the examples may be applicable to first image may be a video . In this example , the liveness test 25 user verification to unlock a screen , to execute a predeter may be performed based on at least one first frame in the mined application , to execute a predetermined function in an video , and user recognition may be performed based on at application , or to access a predetermined folder or file . In', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='least one second frame in the video based on a result of the response to a request for unlocking the screen , the terminal liveness test . The at least one first frame and the at least one may acquire an input image including a face of a user , and second frame may correspond to consecutive frames in the 30 perform a liveness test and / or user verification to determine video . The at least one frame and the at least one second whether the screen is to be unlocked . In response to a request frame may be the same or different frames . for executing a predetermined application , executing a pre Various criteria may be determined for a liveness test with determined function in an application , or accessing a pre respect to the video based on the at least one first frame . If determined folder or file , the terminal may acquire an input any one of the at least one first frame included in the video 35 image including a face of a user , and perform a liveness test passes the liveness test , the video may be determined to be and / or user verification to determine whether a correspond a live video . If any one of the first frame included in the ing operation is to be allowed . video fails the liveness test , the video may be determined to The examples described with respect to FIGS . 1 through be a fake video . If a predetermined number of consecutive 14 may be applicable to user identification in various fields . first frames , for example , three consecutive first frames , pass 40 For example , the examples may be applied to user identi the liveness test , the video may be determined to be a live fication to sort content associated with a user among a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='video . plurality of items of contents stored in a gallery . In response When the video is determined to be a live video through to a request for an access to the gallery , the terminal may the liveness test performed based on the at least one first acquire an input image including a face of a user using the frame , user recognition may be performed based on the at 45 camera , and identify the user by performing a liveness test least one second frame in the video . In this example , the at and / or user identification based on the input image . When least one second frame may be the same as or different from the user is identified , the terminal may sort content associ the at least one first frame . For example , a portion of frames ated with the identified user among the plurality of items of included in the video may be suitable for a liveness test , but contents stored in the gallery , and provide the sorted content unsuitable for user recognition . Conversely , a portion of the 50 to the user . In another example , rather than automatically frames included in the video may be suitable for user sorting content in response to the access to the gallery , recognition , but unsuitable for a liveness test . Through the content associated with the user may be sorted and provided example provided above , the liveness test may be performed by an explicit request of the user , for example , a user input . based on at least one first frame more suitable for the In the above examples , the terminal may include various liveness test , among the plurality of frames included in the 55 computing devices such as a smartphone , a portable phone , video , and the user recognition may be performed based on a tablet computer , a laptop computer , a desktop computer , a at least one second frame more suitable for the user recog- wearable device , a smart vehicle , and a smart home appli nition . The examples described with respect to FIGS . 1 through One or more example embodiments ( e.g. , liveness testing 14 may be applicable to user verification in various fields . 60 apparatuses , image processing apparatuses electronic sys For example , the examples may be applied to user verifi- tems , etc. ) described herein may be implemented using cation for electronic commerce . Information related to a hardware components and software components . For payment method of a user , for example , a credit card , may example , the hardware components may include micro be registered in a terminal in advance , and a payment may phones , amplifiers , band - pass filters , audio to digital con be easily performed using the pre - registered payment 65 vertors , and processing devices . A processing device may be method through user verification at a point in time at which implemented using one or more special purpose computers , an actual payment is needed . In another example , in a case such as , for example , a processor , a controller and an a \\n a \\n a \\n ance . \\n a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 28}), Document(page_content='a \\n a US 11,151,397 B2 \\n 25 26 \\n arithmetic logic unit , application - specific - integrated - circuit , a compiler , and files containing higher level code that may system - on - chip device , a digital signal processor , a micro- be executed by the computer using an interpreter . The computer , a field programmable array , a programmable logic above - described devices may be configured to act as one or unit , a microprocessor or any other device capable of more software modules in order to perform the operations of responding to and executing instructions in a defined man- 5 the above - described example embodiments , or vice versa . ner . The processing device may run an operating system A number of examples have been described above . Nev ( OS ) and one or more software applications that run on the ertheless , it should be understood that various modifications OS . The processing device also may access , store , manipu- may be made . For example , suitable results may be achieved late , process , and create data in response to execution of the if the described techniques are performed in a different order software . For purpose of simplicity , the description of a 10 and / or if components in a described system , architecture , processing device is used as singular ; however , one skilled device , or circuit are combined in a different manner and / or in the art will appreciated that a processing device may replaced or supplemented by other components or their include multiple processing elements and multiple types of equivalents . Accordingly , other implementations are within processing elements . For example , a processing device may the scope of the following claims . include multiple processors or a processor and a controller . 15 What is claimed is : In addition , different processing configurations are possible , 1. A user recognition method comprising : such a parallel processors . receiving a first image acquired by capturing a user ; Furthermore , example embodiments may be implemented performing a liveness test by extracting a first feature of by hardware , software , firmware , middleware , microcode , the first image ; and hardware description languages , or any combination thereof . 20 recognizing the user by extracting a second feature of the When implemented in software , firmware , middleware or first image in response to a successful result of the microcode , the program code or code segments to perform liveness test , the second feature being different from the the necessary tasks may be stored in a machine or computer first feature , wherein readable medium such as a computer readable storage the liveness test verifies whether an object corresponds medium . When implemented in software , a processor or 25 to a real three - dimensional ( 3D ) object or a fake processors will perform the necessary tasks . two - dimensional ( 2D ) representation of the object , A code segment may represent a procedure , function , and subprogram , program , routine , subroutine , module , software the first feature includes a degree of uniformity in a package , class , or any combination of instructions , data distribution of light energy included in a plurality of structures or program statements . A code segment may be 30 pixels corresponding to an object included in the first coupled to another code segment or a hardware circuit by image .', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 29}), Document(page_content='passing and / or receiving information , data , arguments , 2. The user recognition method of claim 1 , further com parameters or memory co nts . Information , arguments , prising , in response to a failed result of the liveness test : parameters , data , etc. may be passed , forwarded , or trans- receiving a second image ; mitted via any suitable means including memory sharing , 35 performing a liveness test by extracting a first feature of message passing , token passing , network transmission , etc. the second image ; and The software may include a computer program , a piece of recognizing the user by extracting a second feature of the code , an instruction , or some combination thereof , to inde- second image based on a result of the liveness test with pendently or collectively instruct or configure the processing respect to the second image . device to operate as desired . Software and data may be 40 3. The user recognition method of claim 2 , wherein the embodied permanently or temporarily in any type of first image corresponds to a first frame in a video , and the machine , component , physical or virtual equipment , com- second image corresponds to a second frame in the video . puter storage medium or device , or in a propagated signal 4. A user recognition method , comprising wave capable of providing instructions or data to or being receiving a first image acquired by capturing a user ; interpreted by the processing device . The software also may 45 performing a liveness test by extracting a first feature of be distributed over network coupled computer systems so the first image ; that the software is stored and executed in a distributed recognizing the user by extracting a second feature of the fashion . The software and data may be stored by one or more first image based on a result of the liveness test ; and non - transitory computer readable recording mediums . wherein the performing includes Example embodiments described herein may be recorded 50 generating a second image by diffusing a plurality of in non - transitory computer - readable media including pro pixels included in the first image , gram instructions to implement various operations embod calculating diffusion speeds of the plurality of pixels ied by a computer . The media may also include , alone or in based on a difference between the first image and the combination with the program instructions , data files , data second image , and structures , and the like . The program instructions recorded 55 extracting the first feature based on the diffusion speeds on the media may be those specially designed and con of the plurality of pixels . structed for the purposes embodied herein , or they may be 5. The user recognition method of claim 4 , wherein the of the kind well - known and available to those having skill in generating comprises : the computer software arts . Examples of non - transitory iteratively updating values of the plurality of pixels using computer - readable media include magnetic media such as 60 a diffusion equation . hard disks , floppy disks , and magnetic tape ; optical media 6. The user recognition method of claim 4 , wherein the such as CD ROM discs and DVDs ; magneto - optical media extracting the first feature comprises : such as optical discs ; and hardware devices that are specially estimating a surface property related to an object included configured to store and perform program instructions , such in the first image based on the diffusion speeds of the as read - only memory ( ROM ) , random access memory 65 plurality of pixels , ( RAM ) , flash memory , and the like . Examples of program wherein the surface property comprises at least one of a instructions include both machine code , such as produced by light - reflective property of a surface of the object , a a', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 29}), Document(page_content='10 \\n 15 \\n 20 \\n 2 \\n 25 US 11,151,397 B2 \\n 27 28 \\n number of dimensions of the surface of the object , or a determining whether the first feature corresponds to a material of the surface of the object . feature related to a medium that displays a face or a \\n 7. The user recognition method of claim 4 , wherein the feature related to an actual face ; \\n extracting the first feature comprises at least one of : outputting a signal corresponding to a failed result of the \\n calculating a number of pixels corresponding to a diffu- 5 liveness test in a case in which the first feature corre \\n sion speed greater than or equal to a first threshold , sponds to the feature related to the medium that dis plays a face ; or among the diffusion speeds of the plurality of pixels ; outputting a signal corresponding to the successful result calculating a distribution of the pixels corresponding to of the liveness test in a case in which the first feature the diffusion speed greater than or equal to the first corresponds to the feature related to an actual face . threshold , among the diffusion speeds of the plurality of 12. The user recognition method of claim 1 , further pixels ; comprising : calculating at least one of an average or a standard receiving a user verification request for approving an \\n deviation of the diffusion speeds ; or electronic commerce payment ; and \\n calculating a filter response based on the diffusion speeds approving the electronic commerce payment in a case in \\n of the plurality of pixels . which the recognizing the user is successful . \\n 8. The user recognition method of claim 4 , wherein the 13. The user recognition method of claim 1 , further \\n extracting the first feature comprises : comprising : \\n extracting a first - scale region from the first image based receiving a user input which requires user verification ; and on the diffusion speeds of the plurality of pixels ; and calculating an amount of noise components included in performing an operation corresponding to the user input \\n the first - scale region based on a difference between the in a case in which the recognizing the user is success ful . first - scale region and a result of applying median filtering to the first - scale region . 14. The user recognition method of claim 13 , wherein the \\n 9. The user recognition method of claim 1 , wherein the user input which requires user verification comprises at least \\n performing comprises : one of : \\n determining whether the object has a planar property or a a user input to unlock a screen , \\n three - dimensional ( 3D ) structural property , based on a user input to execute an application , \\n the first feature ; a user input to execute a function in an application , or \\n outputting a signal corresponding to a failed result of the a user input to access a folder or file . \\n liveness test in a case in which the object is determined 15. The user recognition method of claim 1 , further \\n to have the planar property ; and comprising : \\n outputting a signal corresponding to the successful result receiving a user input related to a gallery including a \\n of the liveness test in a case in which the object is plurality of items of content ; \\n determined to have the 3D structural property . sorting content corresponding to the user among the \\n 10. A user recognition method comprising : plurality of items of content in the gallery in a case in \\n receiving a first image acquired by capturing a user ; which the recognizing the user is successful ; and \\n performing a liveness test by extracting a first feature of providing the content to the user . \\n the first image ; and 16. A non - transitory computer - readable medium compris \\n recognizing the user by extracting a second feature of the ing a program that , when executed on a computer device , \\n first image in response to a successful result of the causes the computer device to perform the method of claim', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 30}), Document(page_content='first image in response to a successful result of the causes the computer device to perform the method of claim \\n 1 . liveness test , the second feature being different from the first feature , 17. A user recognition apparatus comprising a processor \\n wherein the liveness test verifies whether an object cor configured to : \\n responds to a real three - dimensional ( 3D ) object or a perform a liveness test by extracting a first feature of a \\n fake two - dimensional ( 2D ) representation of the object , first image acquired by capturing a user ; \\n and recognize the user by extracting a second feature of the \\n wherein the performing includes first image based on a result of the liveness test ; and \\n calculating a degree of uniformity in a distribution of wherein the processor is further configured to \\n light energy included in a plurality of pixels corre generate a second image by diffusing a plurality of \\n sponding to an object included in the first image pixels included in the first image , \\n based on the first feature , calculate diffusion speeds of the plurality of pixels \\n outputting a signal corresponding to a failed result of based on a difference between the first image and the \\n the liveness test in a case in which the degree of second image , and \\n uniformity in the distribution of light energy is extract the first feature based on the diffusion speeds of the plurality of pixels . greater than or equal to a threshold , and outputting a signal corresponding to the successful 18. The user recognition method of claim 1 , wherein the \\n result of the liveness test in a case in which the object is a face included in the first image ; and the liveness test verifies whether the face corresponds to degree of uniformity in the distribution of light energy is less than the threshold . a real three - dimensional ( 3D ) face or a fake two \\n 11. The user recognition method of claim 1 , wherein the dimensional ( 2D ) representation of the face . \\n performing further comprises at least one of : 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/b1/98/34/94c48fd1f99eeb/US11151397.pdf', 'page': 30})]\n",
      "[Document(page_content='US009060688B2 \\n (12) United States Patent (10) Patent No.: US 9,060,688 B2 \\n ROWe (45) Date of Patent: *Jun. 23, 2015 \\n (54) BIOMETRICS BASED ON LOCALLY A61B5/6838 (2013.01); A61 B 5/726 (2013.01); CONSISTENT FEATURES A61B 2562/0233 (2013.01); A61B 2562/046 \\n (2013.01); G06K9/00046 (2013.01); G06K (71) Applicant: LUMIDIGM, INC., Albuquerque, NM 9/28 (2013.01); G06K 2009/0006 (2013.01); (US) G06K 2009/00932 (2013.01); G07C 9/00158 (2013.01); A61 B 5/7264 (2013.01) \\n (72) Inventor: Robert K. Rowe, Corrales, NM (US) (58) Field of Classification Search USPC ......... 382/100, 115, 117, 124, 125, 128, 116, \\n (73) Assignee: HID GLOBAL CORPORATION, 382/191, 127, 149, 162, 312 Austin, TX (US) See application file for complete search history. \\n (*) Notice: Subject to any disclaimer, the term of this (56) References Cited \\n patent is extended or adjusted under 35 U.S.C. 154(b) by 0 days. U.S. PATENT DOCUMENTS \\n This patent is Subject to a terminal dis- 5,291.560 A * 3/1994 Daugman ..................... 382,117 \\n claimer. 5,812,252 A * 9/1998 Bowker et al. .................. 356/71 6,005,963 A * 12/1999 Bolle et al. ... ... 382,124 \\n 6,018,586 A * 1/2000 Kamei ....... ... 382,125 (21) Appl. No.: 13/624.361 6,041,410 A * 3/2000 Hsu et al. ... ... 713, 186 \\n 6,052.474. A * 4/2000 Nakayama . ... 382,124 (22) Filed: Sep. 21, 2012 6,175.407 B1 * 1/2001 Sartor ............ 356,71 \\n 6,356,649 B2 * 3/2002 Harkless et al. ... ... 382,115 \\n (65) Prior Publication Data 6.421,453 B1* 7/2002 Kanevsky et al. . ... 382,115 \\n 7,545,963 B2 * 6/2009 Rowe ................. ... 382,124 US 2013/OO22248A1 Jan. 24, 2013 7,809,168 B2 * 10/2010 Abiko et al. .................. 382,115 \\n (Continued) Related U.S. Application Dat e pplication Uata Primary Examiner — Vu Le \\n (63) Continuation of application No. 12/051,173, filed on Assistant Examiner — Aklilu Woldemariam \\n Mar. 19, 2008, now Pat. No. 8,285,010. (74) Attorney, Agent, or Firm — Marsh Fischmann & \\n (60) Provisional application No. 60/896,063, filed on Mar. Breyfogle LLP: Daniel J. Sherwinter \\n 21, 2007. (57) ABSTRACT \\n (51) Int. Cl. Systems, devices, methods, and software are described for \\n G06K 9/00 (2006.01) biometric sensors that permit a reduction in the size of the \\n A6 IB5/00 (2006.01) sensing area without significant reduction in biometric func \\n Ok 5.2. 7 3:08: tionality of the sensor. A skin site of an individual is illumi nated, and light scattered from the skin site is received. An GO7C 9/OO (2006.01) image of a locally consistent feature of the skin site is formed \\n (52) U.S. Cl. from the received light. The locally consistent feature is ana CPC ................ A61B5/0059 (2013.01); G06K 9/00 lyzed to perform a biometric function. \\n (2013.01); A61B5/0062 (2013.01); A61B 5/1172 (2013.01); A61 B 5/6826 (2013.01); \\n 100 N 24 Claims, 16 Drawing Sheets \\n 102 \\n 104', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 0}), Document(page_content='US 9,060.688 B2 \\n Page 2 \\n (56) References Cited 2007/0086624 A1* 4/2007 Breed et al. ................... 382/104 \\n 2007/0206842 A1* 9, 2007 Hamid ......... ... 382,125 \\n U.S. PATENT DOCUMENTS 2007/0242858 A1* 10/2007 Aradhye et al. ... 382,115 \\n 2008.0025579 A1 1/2008 Sidlauskas et al. ........... 382,124 \\n 7,835,554 B2 * 1 1/2010 Rowe ............................ 382,124 2008.0025580 A1 1/2008 Sidlauskas et al. ........... 382,124 \\n 2003/0044051 A1* 3/2003 Fujieda ... 382,124 2008/0273768 A1* 11/2008 Dennis et al. ... ... 382,124 \\n 2003/O16371.0 A1* 8, 2003 Ortiz et al. ..... 713, 186 2008/0298.642 A1* 12/2008 Meenen .......... ... 382,115 \\n ck 2009/0046903 A1 2/2009 Corcoran et al. ... 382,124 2004/OOO3295 A. 1/2004 Elderfield et al. . 713,202 2009, O154792 A1* 6, 2009 Sun et al. ..... ... 382,154 \\n 2004/0208343 A1* 10, 2004 Golden et al. . 382,110 2009,0245591 A1* 10, 2009 R. ck owe et al. .. ... 382,115 2004/02407 12 A1* 12/2004 Rowe et al. ................... 382,124 ck 2010/0177937 A1 7/2010 Zhang et al. . ... 382,115 2006/0173256 A1* 8, 2006 Ridder et al. 600/316 2012/0114251 A1* 5, 2012 Solem et all 382,195 \\n 2006/0210120 A1* 9, 2006 Rowe et al. . 382,115 . . . . . . . . . . . . . . . . . . \\n 2006/0274921 A1* 12/2006 Rowe ............................ 382,124 * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 1}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 1 of 16 US 9,060,688 B2 \\n 102 \\n 104 \\n Fig. 1', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 2}), Document(page_content='US 9,060,688 B2 Sheet 2 of 16 Jun. 23, 2015 U.S. Patent \\n S. \\n C C S \\n C s SS \\n S. KX S. \\n 106', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 3}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 3 of 16 US 9,060,688 B2 \\n Fig. 3', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 4}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 4 of 16 US 9,060,688 B2 \\n Fig. 4', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 5}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 5 of 16 US 9,060,688 B2 \\n Fig. 5', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 6}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 6 of 16 \\n 600 \\n 608 606 \\n 602 604 \\n Fig. 6 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 7}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 7 of 16 US 9,060,688 B2 \\n Fig. 7', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 8}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 8 of 16 US 9,060,688 B2 \\n 719', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 9}), Document(page_content='US 9,060,688 B2 Sheet 9 of 16 \\n Fig. 9A \\n saw A Jun. 23, 2015 U.S. Patent \\n Fig. 9B', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 10}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 10 of 16 US 9,060,688 B2 \\n 1000 1008 \\n N- N1/ 27 C5/777777,777,770 YY \\n 719', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 11}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 11 of 16 US 9,060,688 B2 \\n 25 \\n 2 O \\n 1 5 \\n 10 \\n 350 450 550 650 750 850 950 1050 \\n Wavelength (nm) \\n Fig.11B', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 12}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 12 of 16 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 13}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 13 of 16 US 9,060,688 B2 \\n Fig. 13', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 14}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 14 of 16 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 15}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 15 of 16 US 9,060,688 B2 \\n 1500 y 151Ob \\n Computer \\n Readable Storage \\n Media \\n 1502 1504 1506 1508 \\n 1510a \\n Input Output Storage Real St. e Device(s) Device(s) Device(s) Media E. \\n 526 \\n 1520 \\n Communications Processing Working \\n System Acceleration Memory \\n L \\n Operating \\n Biometric 1518 \\n Sensor Other Code (Programs) \\n Fig. 15', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 16}), Document(page_content='U.S. Patent Jun. 23, 2015 Sheet 16 of 16 \\n illuminate Skin Site \\n Receive Light Scattered From Skin Site \\n Generate Local Feature Profile from the \\n Received Light \\n Analyze local Feature Profile by Comparing \\n Local Feature Profile to Reference Feature \\n Profile \\n Perform Biometric Function based on \\n Analysis of Local Feature Profile \\n Fig. 16 1604 \\n 1608 \\n 1612 \\n 1616 \\n 1620 US 9,060,688 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 17}), Document(page_content=\"US 9,060,688 B2 \\n 1. \\n BOMETRICS BASED ON LOCALLY \\n CONSISTENT FEATURES \\n CROSS REFERENCES \\n This application is a continuation of, and claims the benefit of the filing date of U.S. patent application Ser. No. 12/051, \\n 173, entitled “BIOMETRICS BASED ON LOCALLY CON \\n SISTENT FEATURES.” filed Mar. 19, 2008 by Robert K. Rowe, which is a nonprovisional of, and claims the benefit of \\n the filing date of U.S. Provisional Patent Application No. \\n 60/896,063, filed Mar. 21, 2007, entitled “BIOMETRICS \\n BASED ON MULTISPECTRAL SKINTEXTURE Each of \\n the these applications is hereby incorporated by reference, as \\n if set forth in full in this document, for all purposes. This application is related to each of the following commonly \\n assigned applications, the entire disclosure of which is incor porated herein by reference for all purposes: U.S. patent \\n application Ser. No. 1 1/219,006, entitled “COMPARATIVE \\n TEXTURE ANALYSIS OF TISSUE FOR BIOMETRIC \\n SPOOF DETECTION, filed Sep. 1, 2005 by Robert K. Rowe; and U.S. patent application Ser. No. 1 1/458,619, \\n entitled “TEXTURE-BIOMETRICS SENSOR, filed Jul. \\n 19, 2006 by Robert K. Rowe. \\n BACKGROUND \\n The present invention relates to biometrics in general and, in particular, to biometrics based on multispectral skin tex \\n ture. \\n Fingerprint-based biometric sensors are used across a \\n broad range of applications, from law enforcement and civil \\n identification to commercial access control. They are even \\n used in Some consumer devices such as laptops and cellular telephones. In at least Some of these applications, there is a \\n general need in the art to reduce the overall size of the sensor \\n in order to reduce the area of the device that the sensor \\n occupies, as well as to reduce the overall cost of the sensor. Most fingerprint sensors work by imaging a fingerprint and \\n comparing the image to one or more stored images in a \\n database. As such, when a large area of the fingerprint is imaged, more data may be compared and more discriminating \\n results may be obtained. Conversely, the performance of \\n these types of fingerprint sensors may degrade as their sizes \\n decrease. \\n One way to accommodate this concern may be to produce long, narrow fingerprint sensors that simulate a larger-area \\n sensor by combining a series of narrow images collected \\n while the user Swipes a finger across the sensor Surface. \\n Another, similar way to accommodate this concern may be to piece together a set of Smaller images of a fingerprint to collectively form a larger fingerprint image (called “mosaik \\n ing'). Such configurations may reduce the sensor size, but \\n they may also place additional burdens on the user (e.g., requiring the user to learn how to Swipe a fingerprint or \\n requiring precise locating of the finger on the sensor) and may \\n limit the applications in which Such a sensor may be employed. \\n It may be desirable, therefore, to provide a reduced-size biometric sensor that may be highly usable and employed in a wide variety of applications without a significant degrada \\n tion in functionality or usability. \\n SUMMARY \\n Among other things, methods, systems, and devices are \\n described for biometric sensors that permit a reduction in the 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n size of the sensing area without significant reduction in bio metric functionality of the sensor. \\n Embodiments of the invention achieve a reduction in sens \\n ing area of a biometric sensor by measuring a property of the \\n skin that is locally consistent while still being distinct between different people. Because the property is locally \\n consistent, measurements at an enrollment skin location and at a measurement site may be meaningfully compared even if \\n the enrollment and measurement sites are different. In some \\n embodiments, the property comprises an optical property of \\n the skin. In particular, certain embodiments use a range of spatiospectral imaging techniques with Small-area sensors.\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 18}), Document(page_content=\"embodiments, the property comprises an optical property of \\n the skin. In particular, certain embodiments use a range of spatiospectral imaging techniques with Small-area sensors. \\n Matching may be done using spatial, spectral, and/or textural \\n descriptors of the imaging data combined with a classification \\n methodology that applies characteristics of the data that are locally consistent. \\n In some embodiments, spatial information is detected. For example, different spacings may be provided between light \\n Sources and imagers to look for spatial illumination signa \\n tures (e.g., “roll-off). In other embodiments, spectral infor mation is detected. For example, at a fixed spacing between the light source and the imager, different frequency compo \\n sitions may be obtained by using various illumination wave lengths, polarizations, filters, and other characteristics of illu \\n mination. Further, the spectral information may be detected and/or used in many different ways, including pixel-by-pixel, \\n on average over a certain window of pixels, as Summarized by various frequency decomposition methods, etc. In still other \\n embodiments, textural information is detected. In some con figurations, the textural information may indicate optical fea tures (e.g., lumpiness, ridge spacing, etc.); while in other \\n configurations, the textural information may indicate features of the spatial and/or spectral data (e.g., the distribution of \\n coefficients derived from a Fourier decomposition of one or more multispectral image regions). In yet other embodi \\n ments, multiple types of information are detected simulta neously or in series, and are used in conjunction for a variety \\n of situations. In even other embodiments, the sensor or other components may be operable to “learn' over time. For \\n example, each time a user is identified, the biometric infor mation may be processed, along with some or all of the \\n previous biometric information gathered from that user. The \\n combined information may be used to generate a more dis criminatory biometric profile for that user. \\n One set of embodiments provides a method of performing \\n a biometric function. The method includes illuminating a \\n Small-area purported skin site of an individual with illumina tion light, wherein the Small-area purported skin site is in \\n contact with a surface; receiving light scattered from the \\n Small-area purported skin site, wherein the light is received Substantially in a region that includes the Surface; generating \\n a local feature profile from the received light, wherein the \\n local feature profile identifies a feature of the small-area purported skin site, the feature of the Small-area purported \\n skin site being of a type predetermined to exhibit substantial local consistency; and analyzing the generated local feature \\n profile to perform the biometric function. Analyzing the gen \\n erated local feature profile includes comparing the generated \\n local feature profile with a reference local feature profile, \\n wherein the reference local feature profile was generated \\n from light scattered from a small-area reference skin site, and the small-area purported skin site is substantially different \\n from the reference Small-area skin site. \\n Another set of embodiments provides a biometric sensor. \\n The biometric sensor includes a Surface adapted for contact with a purported skin site of an individual; an illumination Subsystem disposed to illuminate the purported skin site; a\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 18}), Document(page_content='US 9,060,688 B2 \\n 3 \\n detection Subsystem disposed to receive light scattered from \\n the purported skin site, wherein the light is received substan \\n tially in a region that includes the Surface; and a computa \\n tional unit interfaced with the detection subsystem. The com putational unit has instructions for forming an image from the 5 received light; instructions for generating an image-texture \\n measure from the image; and instructions for analyzing the generated image-texture measure to perform the biometric \\n function. \\n 10 \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n A further understanding of the nature and advantages of the \\n present invention may be realized by reference to the follow ing drawings. In the appended figures, similar components or 15 \\n features may have the same reference label. Further, various components of the same type may be distinguished by fol \\n lowing the reference label by a dash and a second label that distinguishes among the similar components. If only the first \\n reference label is used in the specification, the description is 20 applicable to any one of the similar components having the \\n same first reference label irrespective of the second reference \\n label. \\n FIG. 1 shows a simplified perspective view of an embodi \\n ment of an exemplary Small-area sensor, according to various 25 \\n embodiments of the present invention. \\n FIG. 2 shows a simplified cross-sectional view of a sensor head, like the sensor head shown in FIG. 1, according to \\n various embodiments of the invention. \\n FIG.3 shows a simplified top view of a sensor head having 30 \\n a number of light sources arranged to be equidistant from a \\n detector, according to various embodiments of the invention. FIG. 4 shows a simplified top view of a sensor head having \\n a number of light sources arranged around a common detec \\n tor, according to various embodiments of the invention. 35 FIG. 5 shows a simplified top view of a sensor head having a number of light sources arranged with respect to multiple \\n detector elements, according to various embodiments of the \\n invention. \\n FIG. 6 shows a simplified top view of a sensor head having 40 \\n a number of light sources arranged around a detector array, \\n according to various embodiments of the invention. \\n FIG. 7 shows a simplified illustration of an embodiment of a multi-spectral biometric sensor using direct illumination, \\n according to various embodiments of the invention. 45 \\n FIG. 8 shows a simplified illustration of an embodiment of a multi-spectral biometric sensor using TIR imaging, accord \\n ing to various embodiments of the invention. FIG.9A illustrates nine exemplary images captured during \\n a single finger placement using an embodiment of an MSI 50 \\n biometric sensor, according to various embodiments of the \\n invention. \\n FIG.9B shows an exemplary result of applying a compos iting algorithm to two placements of the same finger, accord \\n ing to various embodiments of the invention. 55 FIG. 10A shows a simplified perspective view of an MSI \\n sensor having a number of light sources arranged around a \\n detector array, according to various embodiments of the \\n invention. \\n FIG. 10B shows a simplified side view of an embodiment 60 \\n of an MSI sensor, like the one in FIG. 10A, where the imager is substantially in contact with the skin site, according to \\n various embodiments of the invention. \\n FIG. 10C shows a simplified side view of an embodiment \\n of an MSI sensor, like the one in FIG. 10A, where the imager 65 is displaced from the skin site, according to various embodi \\n ments of the invention. 4 \\n FIG. 11A shows an illustration of an exemplary Bayer \\n color filter array in which filter elements correspond to a set of primary colors and are arranged in a Bayer pattern. \\n FIG. 11B shows an illustrative color response curve for an exemplary Bayer filter, like the one in FIG. 11A. \\n FIG. 12 shows an illustrative datacube form of storing or analyzing a body of spatio-spectral data.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 19}), Document(page_content='FIG. 12 shows an illustrative datacube form of storing or analyzing a body of spatio-spectral data. \\n FIG. 13 shows an embodiment of a personal spectral bio metric system in the configuration of an electronic key fob, \\n according to various embodiments of the invention. FIG. 14 shows an embodiment of a personal spectral bio metric system in the configuration of a watch, according to \\n various embodiments of the invention. \\n FIG. 15 shows an exemplary computational system for \\n implementing biometric sensors and related functionality \\n according to various embodiments of the invention. FIG.16 provides a flow diagram of exemplary methods for \\n using multispectral sensor structures according to various \\n embodiments of the invention. \\n DETAILED DESCRIPTION OF THE INVENTION \\n Systems, devices, methods, and software are described for \\n biometric sensors that permit a reduction in the size of the sensing area without significant reduction in biometric func tionality of the sensor. It may be desirable to provide a small \\n area sensor for a number of reasons. For example, many applications may be limited by size (e.g., by form factor), by power consumption (e.g., by battery life), by cost, or by some \\n other limitation. It will further be appreciated that providing \\n a small, but reliable, sensor may allow the sensor to be used in many applications, including laptops, cell phones, car keys, \\n garage door openers, locks, industrial machine Switches, or \\n any other application where it may be desirable to obtain \\n biometric information or to restrict access. \\n One way to reduce the sensing area while maintaining a \\n simple, single-touch user interface may be to measure a prop \\n erty of the skin that is locally consistent while still being \\n distinct from person to person. In this way, a small-area \\n sensor may be able to perform a biometric match using a skin location never previously enrolled as long as the optical prop \\n erties of the enrolled and tested skin sites were “similar \\n enough.” \\n Embodiments of the invention provide methods and sys tems that allow for the collection and processing of a variety \\n of different types of biometric measurements, including inte \\n grated, multifactor biometric measurements in some embodi ments. These measurements may provide strong assurance of a person’s identity, as well as of the authenticity of the bio \\n metric sample being taken. \\n Skin composition and structure is very distinct, very com plex, and varies from person to person. By performing optical \\n measurements of spatio-spectral properties of skin and under \\n lying tissue, a number of assessments may be made. For example, a biometric-identification function may be per formed to identify or verify whose skin is being measured, a \\n liveness function may be performed to assure that the sample \\n being measured is live and viable skin and not another type of material, estimates may be made of a variety of physiological parameters such as age, gender, ethnicity, and other demo \\n graphic and anthropometric characteristics, and/or measure \\n ments may be made of the concentrations of various analytes and parameters including alcohol, glucose, degrees of blood \\n perfusion and oxygenation, biliruben, cholesterol, urea, and \\n the like. \\n The complex structure of skin may be used in different \\n embodiments to tailor aspects of the methods and systems for', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 19}), Document(page_content='US 9,060,688 B2 \\n 5 \\n particular functions. The outermost layer of skin, the epider mis, is Supported by the underlying dermis and hypodermis. \\n The epidermis itself may have identified sub-layers that \\n include the stratum corneum, the stratum lucidum, the Stra tum granulosum, the stratum spinosum, and the stratum ger \\n minativum. Thus, for example, the skin below the top-most \\n stratum corneum has some characteristics that relate to the \\n Surface topography, as well as Some characteristics that change with depth into the skin. While the blood supply to \\n skin exists in the dermal layer, the dermis has protrusions into the epidermis known as \"dermal papillae, which bring the \\n blood supply close to the surface via capillaries. In the volar \\n surfaces of the fingers, this capillary structure follows the pattern of the friction ridges and Valleys on the Surface. In \\n some other locations on the body, the structure of the capillary \\n bed may be less ordered, but is still characteristic of the particular location and person. \\n As well, the topography of the interface between the dif ferent layers of skin is complex and characteristic of the skin \\n location and the person. While these sources of subsurface structure of skin and underlying tissue may represent a sig \\n nificant noise Source for non-imaging optical measurements \\n of skin for biometric determinations or analyte measure \\n ments, the structural differences may be manifested by spa tiospectral features that can be compared through embodi \\n ments of the invention. \\n In some instances, inks, dyes, and/or other pigmentation may be present in portions of the skin as topical coating or \\n Subsurface tattoos. These forms of artificial pigmentation \\n may or may not be visible to the naked human eye. However, if one or more wavelengths used by the apparatus of the \\n present invention is sensitive to the pigment, the sensor can be used in some embodiments to Verify the presence, quantity, \\n and/or shape of the pigment in addition to other desired mea \\n Surement tasks. \\n In general, embodiments of the present invention provide \\n methods and systems that collect spatio-spectral information \\n that may be represented in a multidimensional data structure that has independent spatial and spectral dimensions. In cer \\n tain instances, the desired information is contained in just a \\n portion of the entire multidimensional data structure. For example, estimation of a uniformly distributed, spectrally \\n active compound may require just the measured spectral char \\n acteristics, which may be extracted from the overall multidi mensional data structure. In Such cases, the overall system design may be simplified to reduce or eliminate the spatial \\n component of the collected data by reducing the number of \\n image pixels, even to a limit of a single pixel. Thus, while the \\n systems and methods disclosed are generally described in the context of spatio-spectral imaging, it will be recognized that \\n the invention encompasses similar measurements in which the degree of imaging is greatly reduced, even to the point \\n where there is a single detector element. Some embodiments of the invention use multispectral \\n imaging (MSI) of skin to provide information about both the surface and subsurface (“multispectral\\') characteristics of \\n the skin tissue. In addition to the potential of MSI revealing \\n significant information from below the surface of the skin, the plurality of wavelengths, illumination angles, and optical \\n polarization conditions may yield additional information beyond that available through simple surface reflectance \\n measurements. These multispectral characteristics may be \\n described as \"textures” and used with a classification meth \\n odology that seeks to find characteristics of the data that are locally consistent (e.g., to determine the identity of an indi vidual). Moreover, the plurality of wavelengths, illumination', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content=\"odology that seeks to find characteristics of the data that are locally consistent (e.g., to determine the identity of an indi vidual). Moreover, the plurality of wavelengths, illumination \\n angles, and optical polarization conditions used in this inves 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n tigation yields additional information beyond that available \\n through simple Surface reflectance measurements More broadly, data collected under a plurality of optical \\n conditions, whether they be collected simultaneously or sequentially, is referred to herein as “multispectral data. A more complete description of aspects of multispectral data is \\n described in co-pending, commonly assigned U.S. patent \\n application Ser. No. 1 1/379,945, entitled “MULTISPEC \\n TRAL BIOMETRIC SENSORS filed Apr. 24, 2006, the entire disclosure of which is incorporated herein by reference \\n for all purposes. The distinct optical conditions may include \\n differences in polarization conditions, differences in illumi nation angle, differences in imaging angle, differences in illumination wavelength, and the like. Spatio-spectral data \\n may thus be considered to be a subset of certain types of multispectral data that includes spatial information, e.g., \\n where the different multispectral illumination conditions are \\n recorded with a detector that provides for at least a pair of measurements at Substantially the same illumination condi tion and different spatial or angular positions. Alternatively, \\n spatio-spectral data may also be derived from a single detec \\n tor that measures the optical response of the sample at 2 or \\n more illumination conditions that differ by their spatial or \\n angular orientation with respect to the detector. Also, as used \\n herein, “small-area' sensors refers invarious embodiments to sensors having an active area less than 1 cm, with certain specific embodiments using sensors having an active area less than 0.75 cm, less than 0.5 cm, less than 0.25 cm, less than 0.1 cm, less than 0.05 cm, or less than 0.01 cm. In one embodiment, the sensor is substantially a point sensor. \\n It will be appreciated that the devices, systems, and meth \\n ods described herein may be applied to many types of bio \\n metric identification. Particularly, locally consistent features \\n may be identified in a number of different regions of the body. \\n For example, various Volar Surfaces of the skin may include \\n locally consistent features which may be identified with Small-area sensors according to the invention, including the Volar Surfaces of the palm, fingers, joints, knuckles, etc. As \\n such, while many embodiments of the invention are described with reference to fingerprints, it will be appreciated that the \\n embodiments may also apply to other biometric sites, and should not be taken as limiting in any way. \\n There are a number of ways of using texture matching of fingerprints. In one example, a local texture analysis using \\n Gabor filters may be applied to tessellated regions around the core point. In another example, a Fourier analysis fingerprint \\n texture may be used as the basis for biometric determinations. In still another example, wavelet analyses may be applied to \\n fingerprint images to distinguish between them. While these \\n examples differ in basis functions and other methodological \\n matters, they are all based on conventional fingerprint images. In other words, the observed textural pattern is \\n extracted from a single image, which may limit the informa tion content and may be adversely affected by artifacts due to \\n effects such as dry skin, poor contact between the skin and sensor, and other operationally important matters. \\n Some embodiments of the invention use multiple images \\n taken with a robust form of imaging. The images contain \\n information about both the surface and subsurface character\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content='Some embodiments of the invention use multiple images \\n taken with a robust form of imaging. The images contain \\n information about both the surface and subsurface character \\n istics of the skin. In certain embodiments, the data plane in the MSI stack that most closely matches conventional optical fingerprinting images is removed from the texture analysis in \\n order to avoid the presence of spurious effects (e.g., dry skin, \\n poor contact between the skin site and the sensor platen, etc.). It will be appreciated that there are a number of ways to perform biometric matching using Small-area fingerprint sen \\n sors. One approach may be to build up a large enrollment', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 20}), Document(page_content='US 9,060,688 B2 \\n 7 \\n image by piecing together a series of small fingerprint images \\n taken over multiple placements of the finger (known as \\n \"mosaiking\\'). Another approach may be to combine minutiae \\n information instead of combining the images themselves. One limitation to these approaches is that they may require \\n precise enrollment of a skin site to take accurate measure ments. For example, to provide accurate results with these types of sensors, a user may have to precisely locate his finger \\n on the sensor, avoid any movements of the skin during mea \\n Surement, etc. \\n AS Such, it may be desirable instead to sense Small-area features of the skin site that are locally consistent over other \\n areas of the skin site. Some embodiments of the invention find \\n characteristics of the skin site that are locally consistent. Using locally consistent characteristics may allow an enroll \\n ment measurement to be made at one skin site and Success \\n fully verified at a different (e.g., nearby) skin site. This may \\n minimize certain types of errors inherent in many Small-area sensors (e.g., slight movements of the skin site during sens ing, etc.) and may improve usability and reliability. \\n To capture information-rich data about the Surface and \\n Subsurface features of a skin site (e.g., the skin of a finger), an MSI sensor may collect multiple images of the skin site under a variety of optical conditions. The raw images may be cap \\n tured using different wavelengths of illumination light, dif \\n ferent polarization conditions, and/or different illumination \\n orientations. Each raw image may then contain somewhat different, but complementary, information about the skin site. The different wavelengths may penetrate the skin to different \\n depths and be absorbed and scattered differently by various \\n chemical components and structures in the skin. The different polarization conditions may change the degree of contribu \\n tion of Surface and Subsurface features to the raw image. \\n Further, different illumination orientations may change the \\n location and degree to which surface features are accentuated. \\n Embodiments of the MSI sensors according to the inven \\n tion are configured to detect textural information from a skin site. “Texture\\' may generally refer to any of a large number of \\n metrics that describe some aspect of a spatial distribution of \\n tonal characteristics of an image, some of which were \\n described above. For example, Some textures, such as those commonly found in fingerprint patterns or wood grain, are \\n flowlike and may be well described by metrics such as an \\n orientation and coherence. For textures that have a spatial regularity (at least locally), certain characteristics of the Fou \\n rier transform and the associated power spectrum are impor tant Such as energy compactness, dominant frequencies and \\n orientations, etc. Certain statistical moments such as mean, \\n variance, skew, and kurtosis may be used to describe texture. \\n Moment invariants may be used, which are combinations of \\n various moments that are invariant to changes in scale, rota tion, and other perturbations. Gray-tone spatial dependence \\n matrices may be generated and analyzed to describe image \\n texture. The entropy over an image region may be calculated \\n as a measure of image texture. Various types of wavelet trans forms may be used to describe aspects of the image texture. \\n As mentioned above, steerable pyramids, Gabor filters, and \\n other mechanisms of using spatially bounded basis functions \\n may be used to describe the image texture. These and other \\n Such measures of texture known to one familiar in the art may be used individually or in combination in embodiments of the \\n invention. \\n Image texture may thus be manifested by variations in \\n pixel intensities across an image, which may be used in \\n embodiments of the invention to perform biometric func', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='invention. \\n Image texture may thus be manifested by variations in \\n pixel intensities across an image, which may be used in \\n embodiments of the invention to perform biometric func \\n tions. In some embodiments, additional information may be extracted when such textural analysis is performed for differ 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n ent spectral images recorded under different illumination \\n wavelengths and/or polarization conditions and extracted \\n from a multispectral data set, producing a chromatic textural \\n description of the skin site. These embodiments may enable biometric functions to be performed by capturing only a \\n Small-area portion of an image of a skin site. The texture \\n characteristics of the skin site are expected to be approxi \\n mately consistent over the skin site, permitting biometric \\n functions to be performed with measurements made at differ \\n ent portions of the image site. In many instances, it may not \\n even be required that the portions of the skin site used in \\n different measurements overlap with each other. \\n Exemplary Sensor Embodiments \\n This ability to use different portions of the skin site pro \\n vides considerable flexibility in the structural designs that \\n may be used. This is, in part, a consequence of the fact that \\n biometric matching may be performed Statistically instead of \\n requiring a match to a deterministic spatial pattern. The sen \\n Sor may be configured in a compact manner because it need \\n not acquire an image over a specified spatial area. The ability \\n to provide a small sensor also permits the sensor to be made more economically than sensors that need to collect complete \\n spatial information to perform a biometric function. In dif \\n ferent embodiments, biometric functions may be performed with purely spectral information, while in other embodi \\n ments, spatio-spectral information is used. FIG. 1 shows a simplified perspective view of an embodi \\n ment of an exemplary Small-area sensor, according to some \\n embodiments of the present invention. The sensor assembly \\n 100 consists of a series or plurality of light sources 104 \\n arranged in a selected manner on a sensor head 102, which also contains one or more detectors 106. The sensor assembly 100 may also include power conditioning electronics (not \\n shown), which Supply power to the light Sources 104 and may \\n also include signal processing electronics (not shown) which amplify the resulting signal from the detector 106. A multi \\n conductor cable 108 may be provided to power the sensor \\n head and to communicate with a system (e.g., a microproces sor or computer) for processing the detected signals. \\n The light sources 104 may be light emitting diodes (LEDs), \\n laser diodes, vertical cavity surface emitting lasers (VC SELS), quartz tungsten halogen incandescent bulbs with or \\n without optical pass-band filters and with or without optical \\n shutters, or a variety of other optical sources known in the art. The light sources 104 may each have the same wavelength \\n characteristics or can be comprised of sources with different center wavelengths in the spectral range from about 350 nm to \\n about 2500 nm. In general, the collection of light sources 104 \\n may include Some sources that have the same wavelengths as \\n others and some sources that are different. In one embodi \\n ment, the light sources 104 include sets of LEDs, laser diodes, VCSELs, or other solid-state optoelectronic devices with dif fering wavelength characteristics that lie within the spectral \\n range from about 350 nm to about 1100 nm. In some cases, the detector array may include an optical filter array to limit \\n the wavelengths of light seen by certain array elements. \\n The detector 106 may be a single element or it may be a \\n one- or two-dimensional array of elements. The detector type \\n and material are chosen to be appropriate to the source wave lengths and the measurement signal and timing requirements.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='one- or two-dimensional array of elements. The detector type \\n and material are chosen to be appropriate to the source wave lengths and the measurement signal and timing requirements. \\n For example, the detectors may include PbS, PbSe, InSb, \\n InGaAs, MCT, bolometers and/or micro-bolometer arrays. In \\n one embodiment where the light sources 104 are solid-state optoelectronic devices operating in the spectral range from \\n about 350 nm to about 1100 nm, the detector material is \\n silicon.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 21}), Document(page_content='US 9,060,688 B2 \\n 9 \\n The light sources 104 may be sequentially illuminated and \\n extinguished to measure the tissue properties for each Source \\n by turning power to each of them on and off. Alternatively, \\n multiple light sources 104 may be electronically modulated \\n using encoding methods that may be known in the art. These \\n encoding patterns include, for example, Fourier intensity \\n modulation, Hadamard modulation, random modulation, and \\n other modulation methods. \\n It is worth noting that the configuration shown in FIG. 1 \\n includes a number of light sources 104 and a single detector 106, effectively providing variable source-detector spacings. \\n This configuration may be applicable, for example, where a \\n small number of light sources 104 with different wavelength \\n characteristics are available. In these cases, providing vari \\n able source-detector spacings may be useful in gathering \\n additional optical information from tissue. \\n FIG. 2 shows a simplified cross-sectional view of a sensor \\n head, like the sensor head 100 shown in FIG. 1, according to \\n some embodiments of the invention. Also shown is the tissue \\n 210 in contact with the face 209 of the sensorhead 102 and the \\n mean optical paths 212, 214, 216, 218, 220, and 222 of the \\n light traveling from each light source 211,213,215, 217, 219, \\n and 221, respectively, to the detector 106. In acquiring tissue \\n spectral data, measurements can be made in at least two different sampling modes. The optical geometry illustrated in \\n FIG. 2 is known as diffuse reflectance sampling geometry \\n where the light sources and detector lie on the same side of the \\n tissue. An alternative method is known as transmission Sam \\n pling, wherein light enters a thin tissue region Such as an \\n earlobe or a fingertip on one side and then is detected by a \\n detector located on the other side of the tissue. Although light \\n in Such regions as the silicon-region can penetrate tissue to \\n significant depths of one centimeter or more, depending upon \\n the wavelength, transmission sampling of the tissue limits the \\n region of the body that can be used. Thus, while either mode of sampling may be applicable to the present invention, and especially to analysis utilizing light in the silicon-region, \\n many embodiments utilize sampling methods based on \\n reflected light. \\n Referring to FIG. 2, when the tissue is illuminated by a \\n particular light source 211, the resulting signal detected by \\n detector 106 contains information about the tissue optical properties along a path between the source 211 and detector 106. The actual path of any given photon may be highly \\n erratic due to effects of optical scattering by the tissue, but the \\n mean optical path 212 may be a more regular and Smooth \\n curve, as shown in the Figure. The mean optical path (e.g., 212) may, in general, be dif \\n ferent for different source-detector separation differences. If \\n another light source 221 is located at the same distance from \\n the detector 106 as light source 211, and the two light sources have the same wavelength characteristics, the resulting sig \\n nals may be combined (e.g., to increase the resulting signal \\n to-noise ratio of the measurement) or may be used as inde pendent spatio-spectral measurements. Iflight source 221 has \\n a different wavelength characteristic from light source 211, the resulting signals may provide information about optical \\n properties of the tissue 210, especially as they relate to bio \\n metric determinations, and should be analyzed as distinct data \\n points. In a similar manner, if two light sources have the same wavelength characteristics and are positioned at different dis \\n tances from the detector 106 (for example light sources 211 \\n and 213), then the resulting information in the two signals is \\n different and the measurements may be recorded and ana lyzed as distinct data points. Differences in both wavelength 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content='different and the measurements may be recorded and ana lyzed as distinct data points. Differences in both wavelength 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n characteristics and source-detector separation may provide \\n additional information about optical characteristics of the \\n tissue 210. \\n In some embodiments, the detector 106 is located either in \\n the center of the sensor head 102 or offset to one side of the \\n sensor head 102 (e.g., to provide for varying source-detector \\n separation distances). The sensor head 102 may be various shapes including oval, square, or rectangular. The sensor head \\n 102 may also have a compound curvature on the optical \\n surface to match the profile of the device in which it is \\n mounted, or to match the profile of the skin site intended to \\n touch the sensor. \\n Light that reflects from the topmost layer of skin may not \\n contain significant information about the deeper tissue prop \\n erties. In fact, reflections from the top surface of tissue (known as “specular or “shunted light) may sometimes be \\n detrimental to optical measurements. For this reason, FIG. 2 \\n illustrates a sensor head 102 geometry wherein the detector 106 is recessed from the sensor surface 209 in optically opaque material 207 that makes up the body of the sensor \\n head 102. The recessed placement of detector 106 minimizes \\n the amount of light that can be detected after reflecting off the first (e.g., epidermal) Surface of the tissue. It can be seen that the same optical blocking effect may be produced by recess \\n ing each of the light sources 211,213,215, 217, 219, and 221, and/or the detector 106. Of course, other optical blocking \\n methods, such as the use of different polarization filters, are possible according to the invention. \\n It will be appreciated that many other configurations of \\n light sources and detectors are possible without departing \\n from the invention. In one embodiment, as shown in FIG.3, a \\n sensor head 102 has a number of light sources 104 arranged to be equidistant from a detector 106. This configuration may be \\n useful, for example, where each light source 104 is a different wavelength and sufficient light sources 104 may be obtained \\n to achieve desired accuracy results from the system. An \\n example of this may occur where individual light Sources result from combining optical filters with one or more broad \\n band (e.g., incandescent) light sources. In this case, many unique wavelength bands may be defined and each of the \\n sources 104 may be placed equidistant from the central detec tor 106. Alternatively, each of the light sources 104 may be \\n Substantially the same and the resulting set of measurements \\n are used as a measure of the spatial differences of the skin at a particular wavelength. \\n In another embodiment, as shown in FIG. 4, a number of light sources 401, 404, 407,410 are arranged around a com \\n mon detector 413. Four different light sources 401, 404, 407, \\n 410 are shown for illustration but fewer or more can be used \\n in a particular embodiment. Each of the light sources 401, 404, 407, 410 is optically coupled to a different optical \\n waveguide 402, 405, 408, 411. Each waveguide 402, 405, \\n 408,411 has individually controllable electronic or mechani cal optical shutters 403, 406, 409, 412. These optical shutters \\n 403, 406, 409, 412 can be individually controlled to encode the light by allowing light to enter the tissue from a waveguide \\n 402,405, 408,411 at a predetermined position or positions. In \\n certain embodiments, the optical shutters 403, 406, 409, 412 include micro-electromechanical systems (“MEMS) struc \\n tures. The light sources 401, 404, 407, 410 may include \\n different LEDs, laser diodes,VCSELs, or other types of illu mination sources. Alternatively, one or more incandescent sources with different optical filters may be used to generate \\n light of different wavelength characteristics to couple into \\n each of the waveguides 402, 405, 408, 411.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content='light of different wavelength characteristics to couple into \\n each of the waveguides 402, 405, 408, 411. \\n In yet another embodiment, multiple source-detector dis \\n tances may be achieved by using more than one detector', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 22}), Document(page_content=\"US 9,060,688 B2 \\n 11 \\n element, as shown in FIG. 5. In the illustrated embodiment, \\n each of three different light sources 502, 504, 506 is posi \\n tioned relative to three detectors 501, 503, 505 such that the \\n spacing between a given light source and each of the detectors is different. For example, the source-detector spacing for a \\n light source 502 is shortest with respect to detector 501 and longest with respect to detector 505. By turning on the light \\n sources 502,504,506 in a sequential or encoded pattern and measuring the response at each of the three detectors 501, \\n 503,505, tissue characteristics for some or all of the available Source-detector separations at Some or all of the wavelengths \\n may be measured. \\n Some embodiments configure multiple detector elements and multiple illumination Sources using a detector array. FIG. \\n 6 illustrates a simplified top view of a sensor 600 using a \\n detector array according to some embodiments of the inven \\n tion. In this embodiment, multiple light sources 602, 604, 606, 608 are placed at the perimeter of a detector array 609. \\n The signal detected at each of the array elements may then represent a different Source-detector separation with respect \\n to the light from a given light Source. Many variants on this \\n configuration may exist, including the use of one dimensional \\n (“1-D), two-dimensional (2-D), or three-dimensional (3- D') arrays, and placing sources within the array as well as on the periphery. \\n Other embodiments are configured differently to detect \\n similar or different optical data. For example, many different types of multi-spectral imaging (“MSI) sensors are possible. \\n One embodiment of an MSI biometric sensor is depicted with the schematic diagram of FIG.7, which shows a front view of \\n the MSI sensor using direct illumination. The MSI biometric sensor 701 comprises an illumination subsystem 721 having \\n one or more light sources 703 and a detection subsystem 723 \\n with an imager 715. The figure depicts an embodiment in which the illumination subsystem 721 comprises a plurality \\n of illumination subsystems 721a and 721b, but the invention \\n is not limited by the number of illumination subsystems 721 or detection subsystems 723. For example, the number of illumination subsystems 721 may conveniently be selected to \\n achieve certain levels of illumination, to meet packaging \\n requirements, and to meet other structural constraints of the \\n MSI biometric sensor 701. \\n Illumination light passes from the source 703 through illu \\n mination optics 705 that shape the illumination to a desired form, such as in the form of flood light, light lines, light \\n points, and the like. The illumination optics 705 are shown for convenience as consisting of a lens but may more generally \\n include any combination of one or more lenses, one or more mirrors, and/or other optical elements. The illumination optics 705 may also comprise a scanner mechanism (not \\n shown) to scan the illumination light in a specified one \\n dimensional, or two-dimensional, or three-dimensional pat tern. The light source 703 may comprise a point source, a line \\n Source, an area source, or may comprise a series of Such \\n Sources in different embodiments. In various embodiments, one or more light sources 703 may be configured to illuminate \\n a skin site 719 (e.g., the skin of a finger) at different wave lengths, different polarization conditions, and/or different \\n illumination orientations. \\n In some embodiments, the light source 703 may comprise one or more quasimonochromatic sources in which the light \\n is provided over a narrow wavelength band. Such quasi \\n monochromatic sources may include Such devices as light emitting diodes, laser diodes, or quantum-dot lasers. Alterna \\n tively, the light source 703 may comprise abroadband source \\n Such as a white-light LED, an incandescent bulb, or a glow \\n bar. In the case of a broadband source, the illumination light 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content=\"Such as a white-light LED, an incandescent bulb, or a glow \\n bar. In the case of a broadband source, the illumination light 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 12 \\n may pass through a bandpass filter 709 to narrow the spectral \\n width of the illumination light. In one embodiment, the band \\n pass filter 709 comprises one or more discrete optical band \\n pass filters. In another embodiment, the bandpass filter 709 \\n comprises a continuously variable filter that moves rotation \\n ally or linearly (or with a combination of rotational and linear \\n movement) to change the wavelength of illumination light. In \\n still another embodiment, the bandpass filter 709 comprises a \\n tunable filter element such as a liquid-crystaltunable filter, an \\n acousto-optical tunable filter, a tunable Fabry-Perot filter, or \\n other filter mechanism known to one knowledgeable in the \\n art. \\n In other embodiments, white light is used. As used herein, “white light” refers to light that has a spectral composition \\n amenable to separation into constituent wavelength bands, \\n which in some cases may comprise primary colors. The usual \\n primary colors used to define white light are red, green, and \\n blue, but other combinations may be used in other instances, \\n as will be known to those of skill in the art. For clarity, it is emphasized that “white light’ as used herein might not appear \\n white to a human observer and might have a distinct tint or \\n color associated with it because of the exact wavelength dis \\n tribution and intensity of the constituent wavelength bands. In \\n other cases, the white light may comprise one or more bands \\n in the ultraviolet or infrared spectral regions. In some cases, \\n the white light might not even be visible at all to a human \\n observer when it consists of wavelength bands in the infrared and/or ultraviolet spectral regions. A portion of the light scat \\n tered by the skin and/or underlying tissue exits the skin and is \\n used to form an image of the structure of the tissue at and \\n below the surface of the skin. Because of the wavelength \\n dependent properties of the skin, the image formed from each \\n wavelength of light comprised by the white light may be \\n different from images formed at other wavelengths. In some \\n embodiments, an optical filter or filter array may be incorpo \\n rated with the detector array to separate the white light into a \\n set constituent wavelengths. For example a color filter array \\n comprised of red, green and blue filter elements arranged in a \\n Bayer pattern may be used to separate the white light, as \\n known to one familiar in the art. \\n Various embodiments of the illumination subsystem 721 and detection subsystem 723 are configured to operate in a variety of optical regimes and at a variety of wavelengths. \\n One embodiment uses light sources 703 that emit light sub stantially in the region of 350-1 100 nanometers. In this \\n embodiment, the detector 715 may be based on silicon detec \\n tor elements or other detector material knownto those of skill \\n in the art as sensitive to light at Such wavelengths. In another \\n embodiment, the light sources 703 may emit radiation at wavelengths that include the near-infrared regime of 1.0-2.5 \\n microns, in which case the detector 715 may include elements \\n made from InGaAs, InSb, PbS, MCT, and other materials \\n known to those of skill in the art as sensitive to light at such wavelengths. \\n In the embodiment shown in FIG. 7, the first illumination subsystem 721a includes a first light emitting diode (“LED') \\n 703a (i.e., the light source) and an illumination polarizer 707. \\n Light from the first LED 703a passes through the illumination polarizer 707 before illuminating a finger 719 (i.e., the skin \\n site) as it rests on a sensor platen 717. The illumination polarizer 707 may include, for example, a linear polarizer or \\n a circular polarizer. Light interacts with the finger 719 and a\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content='site) as it rests on a sensor platen 717. The illumination polarizer 707 may include, for example, a linear polarizer or \\n a circular polarizer. Light interacts with the finger 719 and a \\n portion of the light is directed toward the detection subsystem \\n 723. Other light which does not reflect directly back toward the detection Subsystem 723 may undergo refractions, scat', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 23}), Document(page_content='US 9,060,688 B2 \\n 13 \\n tering, other reflections, and other optical events. Some of this \\n other light may ultimately reflect toward the detection sub \\n system 723. \\n The detection Subsystem 723 includes an imaging polar \\n izer 711. The imaging polarizer 711 is oriented with its optical \\n axis to be orthogonal to the axis of the illumination polarizer \\n 707, such that light with the same polarization as the illumi nation light is Substantially attenuated by the imaging polar \\n izer 711. This may significantly reduce the influence of light \\n reflected from the surface of the skin and emphasize light that has undergone multiple optical scattering events after pen \\n etrating the skin. \\n The second illumination subsystem 721b includes a second \\n LED 703b, but no illumination polarizer 707. When the sec \\n ond LED 703b is illuminated, the illumination light may be \\n randomly polarized. The surface-reflected light and the deeply penetrating light may both able to pass through the \\n imaging polarizer 711 in equal proportions due to the random \\n polarization. As such, the image produced from this unpolar \\n ized second LED 703b may contain stronger influence from \\n surface features of the finger. \\n It is worth noting that the direct-illumination sources, the polarized first LED 703a and the unpolarized second LED \\n 703b, as well as the imaging system, may be arranged to avoid \\n critical-angle phenomena at platen-air interfaces. This may \\n provide certainty that each illuminator will illuminate the \\n finger 719 and that the imager 715 will image the finger 719 \\n (e.g., regardless of whether the skin is dry, dirty, or even in \\n good contact with the sensor). \\n In some embodiments, the sensor layout and components \\n may advantageously be selected to minimize the direct reflec \\n tion of the illumination into the detection optics 713. In one \\n embodiment, such direct reflections are reduced by relatively \\n orienting the illumination subsystem 721 and detection sub \\n system 723 such that the amount of directly reflected light \\n detected is minimized. For instance, optical axes of the illu mination subsystem 721 and the detection subsystem 723 \\n may be placed at angles Such that a mirror placed on the platen \\n 717 does not direct an appreciable amount of illumination \\n light into the detection subsystem 723. In addition, the optical \\n axes of the illumination and detection subsystems 721 and 723 may be placed at angles relative to the platen 717 such that the angular acceptance of both Subsystems is less than the \\n critical angle of the system; such a configuration avoids \\n appreciable effects due to total internal reflectance (“TIR) \\n between the platen 717 and the skin site 719. In other embodiments, the detection subsystem 723 may \\n incorporate detection optics that include lenses, mirrors, and/ \\n or other optical elements that forman image of the region near \\n the platen surface 717 onto the detector 715. The detection optics 713 may also include a scanning mechanism (not shown) to relay portions of the platen region onto the detector \\n 715 in sequence. It will be appreciated that there are many \\n ways to configure the detection subsystem 723 to be sensitive \\n to light that has penetrated the surface of the skin and under gone optical scattering within the skin and/or underlying \\n tissue before exiting the skin. \\n In some embodiments, in addition to or instead of the direct \\n illumination illustrated in FIG. 7, the MSI sensor 701 also integrates a form of TIR imaging. For example, the polariza \\n tion effects discussed with respect to FIG.7 may be effective \\n when non-contact sensors are used, while TIR and other \\n techniques may be more effective with contact sensors. FIG. 8 provides an exemplary illustration of an MSI sensor 801 \\n using TIR imaging. Like the direct-illumination MSI sensor 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 24}), Document(page_content='using TIR imaging. Like the direct-illumination MSI sensor 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n 701 (FIG. 7), the TIR-based MSI sensor 801 includes one or more light sources 803 and a detection subsystem 723 with an imager 715. \\n In the TIR illumination mode, one or more light sources 803 (e.g., LEDs) illuminate the side of the platen 717. A portion of the illumination light propagates through the platen \\n 717 by making multiple TIR reflections at the platen-air inter \\n faces. At points where the TIR is broken by contact with the skin (e.g., 810), light enters the skin and is diffusely reflected. \\n A portion of this diffusely reflected light is directed toward the imaging system and passes through the imaging polarizer \\n (since this light is randomly polarized), forming an image for \\n this illumination state. Unlike all the direct illumination \\n states, the quality of the resulting raw TIR image may be \\n dependent on having skin of Sufficient moisture content and cleanliness making good optical contact with the platen, just \\n as is the case with conventional TIR sensors. \\n In practice, many MSI sensors may contain multiple direct \\n illumination LEDs of different wavelengths. For example, the \\n Lumidigm J1 10 MSI sensor has four direct-illumination wavelength bands (430, 530, and 630 nanometers, as well as a white light), in both polarized and unpolarized configura \\n tions. When a finger is placed on the sensor platen, eight \\n direct-illumination images are captured along with a single \\n T1R image. The raw images are captured on a 640x480 image \\n array with a pixel resolution of 525 ppi. All nine images are captured in approximately 500 mSec. \\n FIG.9A illustrates nine exemplary images captured during \\n a single finger placement using an embodiment of an MSI \\n biometric sensor. The upper row 902 shows raw images for unpolarized illumination wavelengths of blue (430 nanom \\n eters), green (530 nanometers), and red (630 nanometers), as well as white light. The middle row 904 shows images, cor responding to those in the upper row 902, for a cross-polar \\n ized case. The single image on the bottom row 906 shows a TIR image. The grayscale for each of the raw images has been expanded to emphasize the features. \\n It can be seen from FIG. 9 that there are a number of \\n features present in the raw data including the textural charac \\n teristics of the Subsurface skin, which appear as mottling that is particularly pronounced under blue and green illumination \\n wavelengths. As well, the relative intensities of the raw \\n images under each of the illumination conditions is very \\n indicative of the spectral characteristics (e.g., color) of the finger or other sample. It is worth noting that the relative \\n intensities have been obscured in FIG. 9 to better show the \\n comparative details of the raw images. \\n The set of raw images shown in FIG.9 may be combined together to produce a single representation of the fingerprint \\n pattern. In some embodiments, this fingerprint generation \\n relies on a wavelet-based method of image fusion to extract, \\n combine, and enhance those features that are characteristic of a fingerprint. In one embodiment, the wavelet decomposition \\n method that is used is based on the dual-tree complex wavelet transform (“DTCWT). Image fusion may occur by selecting \\n and compiling the coefficients with the maximum absolute magnitude in the image at each position and decomposition \\n level. \\n An inverse wavelet transform may then be performed on the resulting collection of coefficients, yielding a single, com posite image. An example of the result of applying the com positing algorithm to two placements of the same finger is \\n shown in FIG. 9B. The two composite fingerprint images \\n (*0952 and 954) may then be used with conventional finger print matching Software. \\n In some embodiments, the DTCWT process used to gen erate composite fingerprint images is also used to provide the', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 24}), Document(page_content='US 9,060,688 B2 \\n 15 \\n spectral-textural features of the multispectral data. In one \\n embodiment, the coefficients from a 3rd level of the DTCWT decomposition of the multispectral image Stack are used as \\n features for the texture analysis. Since the strength and qual ity of the raw TIR image plane may be highly variable (depen \\n dent on skin moisture, good contact, etc), certain embodi ments omit the TIR image plane from the multispectral \\n texture analysis. \\n In some embodiments, an inter-image product, P. is defined \\n as the conjugate product of coefficients at Some direction, d. and decomposition level, k, generated by any two of the raw multispectral images, iand, in a multispectral image stack at \\n location X.y, as defined by: \\n In this equation, C(x, y, k) is the complex coefficient for image iat decomposition levelk, direction d, and locationX.y. and C (x, y, k) is the conjugate of the corresponding complex value for image j. The conjugate products may represent the \\n fundamental features of the image while remaining insensi \\n tive to translation and some amount of rotation. \\n In one embodiment, all real and imaginary components of all the conjugate products generated from each unique image \\n pair are compiled as a feature vector. For eight raw image \\n planes, this results in a 384-element vector (28 conjugate \\n products per direction, six directions, two Scalar values (i.e., real and imaginary) per product for iz; plus eight conjugate \\n products per direction, six directions, one scalar value (i.e., real only) for it). In addition, the isotropic magnitudes of the \\n coefficients are added to the feature vector, where the isotro pic magnitude is the sum of the absolute magnitudes over the \\n six directional coefficients. Finally, the mean DC values of each of the raw images over the region of analysis are added \\n to the feature vector. Concatenating all of these values results \\n in a 400-element feature vector at each element location. \\n In addition to the direct illumination and TIR sensors of \\n FIGS. 7 and 8, respectively, some embodiments of MSI sen \\n sors use illuminated arrays. FIG. 10A shows an illustration of an exemplary embodiment of an array-based MSI sensor. The \\n sensor 1000 includes a number of light sources 1004 and an imager 1008. In some embodiments, the light sources 1004 include white-light Sources, although in other embodiments, the light sources 1004 include quasi-monochromatic sources. \\n Similarly, the imager 1008 may include a monochromatic or color imager. \\n In one example, the imager 1008 has a Bayer color filter array in which filter elements corresponding to a set of pri mary colors are arranged in a Bayer pattern. An example of \\n Such a pattern is shown in FIG. 11A for an arrangement that \\n uses red 1104, green 1112, and blue 1108 color filter ele ments. In some instances, the imager 1008 may additionally \\n include an infrared filter or other filter, e.g., disposed to \\n reduce the amount of infrared light detected. FIG. 11B shows an illustrative color response curve for an exemplary Bayer filter. As shown, there may generally be \\n Some overlap in the spectral ranges of the red 1124, green \\n 1132, and blue 1128 transmission characteristics of the filter \\n elements. As evident particularly in the curves for the green \\n 1132 and blue 1128 transmission characteristics, the filter \\n array may allow the transmission of infrared light. This may \\n be avoided with the inclusion of an infrared filteras part of the detector subsystem. In other embodiments, the infrared filter \\n may be omitted and one or more light sources 1004 that emit infrared light may be incorporated. In this way, all color filter \\n elements 1104, 1108, and 1112 may allow the light to sub stantially pass through, resulting in an infrared image across \\n the entire imager 1008. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n Returning to FIG. 10A, in some embodiments, the sensor', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 25}), Document(page_content='the entire imager 1008. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n Returning to FIG. 10A, in some embodiments, the sensor \\n 1000 is a “contact sensor, because the image is collected substantially in the region of the skin site 719 being mea \\n sured. It is possible, however, to have different configurations \\n for operating the sensor, some with the imager 1008 substan \\n tially in contact with the skin site 719 and some with the imager 1008 displaced from the region of the skin site 719. \\n In the embodiment of FIG. 10B, the imager 1008 of the sensor 1000-1 is substantially in contact with the skin site \\n 719. Light from the sources 1004 propagates beneath the \\n tissue of the skin site 719. This may permit light scattered \\n from the skin site 719 and in the underlying tissue to be detected by the imager 1008. \\n An alternative embodiment in which the imager 1008 is displaced from the skin site 719 is shown schematically in \\n FIG. 10C. In this drawing the sensor 1000-2 includes an optical arrangement 1012 that translates an image at the \\n region of the skin site 719 to the imager 1008. In one embodi ment, the optical arrangement 1012 includes a number of \\n optical fibers, which translate individual pixels of an image by \\n total internal reflection along the fiber without substantially loss of intensity. In this way, the light pattern detected by the imager 1008 is substantially the same as the light pattern \\n formed at the region of the skin site 719. The sensor 1000-2 may thus operate in Substantially the same fashion as the \\n sensor 1000-1 shown in FIG. 10B. That is, light from the sources 1004 is propagated to the skin site, where it is reflected and scattered by underlying tissue after penetrating \\n the skin site 719. Because information is merely translated Substantially without loss, the image formed by the imager \\n 1008 in such an embodiment may be substantially identical to \\n the image that would beformed with an arrangement like that \\n in FIG. 10A. \\n In embodiments where purely spectral information is used to perform a biometric function, spectral characteristics in the \\n received data may be identified and compared with an enroll \\n ment database of spectra. The resultant tissue spectrum of a particular individual includes unique spectral features and \\n combinations of spectral features that can be used to identify \\n individuals once a device has been trained to extract the \\n relevant spectral features. Extraction of relevant spectral fea tures may be performed with a number of different tech niques, including linear and quadratic discriminant analysis, \\n genetic algorithms, simulated annealing, and other such tech niques. While not readily apparent in visual analysis of a spectral output, such analytical techniques can repeatably \\n extract unique features that can be discriminated to perform a \\n biometric function. Classification may be performed using a variety of methods including Support vector machines, K \\n nearest neighbors, neural networks, and other well known classification methods. Examples of specific techniques are \\n disclosed in commonly assigned U.S. Pat. No. 6.560,352. \\n entitled APPARATUS AND METHOD OF BIOMETRIC \\n IDENTIFICATION OR VERIFICATION OF INDIVIDU \\n ALS USING OPTICAL SPECTROSCOPY: U.S. Pat. No. \\n 6,816,605, entitled “METHODS AND SYSTEMS FOR \\n BIOMETRIC IDENTIFICATION OF INDIVIDUALS \\n USING LINEAR OPTICAL SPECTROSCOPY: U.S. Pat. \\n No. 6,628,809, entitled “APPARATUS AND METHOD FOR \\n IDENTIFICATION OF INDIVIDUALS BY NEAR-INFRA \\n RED SPECTRUM: U.S. Pat. No. 7,203,345, entitled \\n APPARATUS AND METHOD FOR IDENTIFICATION \\n OF INDIVIDUAL BY NEAR-INFRARED SPECTRUM, \\n filed Sep. 12, 2003 by Robert K. Rowe et al.; and U.S. patent \\n application Ser. No. 09/874,740, entitled “APPARATUS \\n AND METHOD OF BIOMETRIC DETERMINATION \\n USING SPECIALIZED OPTICAL SPECTROSCOPYSYS', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 25}), Document(page_content='US 9,060,688 B2 \\n 17 \\n TEM filed Jun. 5, 2001 by Robert K. Rowe et al. The entire disclosure of each of the foregoing patents and patent appli \\n cations is incorporated herein by reference in its entirety. \\n Many of the methods taught by the foregoing disclosures are readily applicable to spatio-spectral data. In particular, all 5 \\n or part of the spatio-spectral data may be analyzed using \\n techniques such as wavelets, Fourier decomposition, Steer \\n able pyramids, Gabor filters, and other decomposition meth \\n ods known in the art. The coefficients derived from these \\n decompositions may then be concatenated together into a \\n vector of values as described elsewhere in this disclosure. The \\n resulting vector may then be analyzed in similar ways as a \\n vector of spectral values can be. \\n The ability to perform biometric functions with image \\n texture information, including biometric identifications, may \\n exploit the fact that a significant portion of the signal from a \\n living body is due to capillary blood. For example, when the \\n skin site 719 comprises a finger, a known physiological char \\n acteristic is that the capillaries in the finger follow the pattern 20 \\n of the external fingerprint ridge structure. Therefore, the con \\n trast of the fingerprint features relative to the illumination wavelength is related to the spectral features of blood. In particular, the contrast of images taken with wavelengths \\n longer than about 580 nanometers may be significantly 25 \\n reduced relative to those images taken with wavelengths less than about 580 nanometers. Fingerprint patterns generated \\n with non-blood pigments and other optical effects (e.g., \\n Fresnel reflectance) may have a different spectral contrast. Light scattered from a skin site 719 may be subjected to 30 variety of different types of comparative texture analyses in \\n different embodiments. Some embodiments make use of a \\n form of moving-window analysis of image data derived from \\n the collected light to generate a figure of merit, and thereby \\n evaluate the measure of texture or figure of merit. In some 35 embodiments, the moving window operation may be replaced \\n with a block-by-block or tiled analysis. In some embodi \\n ments, a single region of the image or the whole image may be \\n analyzed at one time. \\n In one embodiment, fast-Fourier transforms are performed 40 on one or more regions of the image data. An in-band contrast \\n figure of merit C is generated in Such embodiments as the ratio of the average or DC power to in-band power. Specifi \\n cally, for an index i that corresponds to one of a plurality of wavelengths comprised by the white light, the contrast figure 45 \\n of merit is: 10 \\n 15 \\n XX, F6, n) 50 i \\n In this expression, F.(S. m) is the Fourier transform of the image f(x, y) at the wavelength corresponding to index i. \\n where Xandy are spatial coordinates for the image. The range 55 defined by R. and R, represents a limit on spatial fre quencies of interest for fingerprint features. For example, \\n R may be approximately 1.5 fringes/mm in one embodi \\n ment and R, may be 3.0 fringes/mm. In an alternative formulation, the contrast figure of merit may be defined as the 60 ratio of the integrated power in two different spatial frequency \\n bands. The equation shown above is a specific case where one of the bands comprises only the DC spatial frequency. \\n In another embodiment, moving-window means and mov \\n ing-window standard deviations are calculated for the col- 65 lected body of data and used to generate the figure of merit. In this embodiment, for each wavelength corresponding to 18 \\n index i. the moving-window meanu and the moving-window \\n standard deviation O, are calculated from the collected image \\n f(x,y). The moving windows for each calculation may be the \\n same size and may conveniently be chosen to span on the order of 2-3 fingerprint ridges. Preferably, the window size is', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 26}), Document(page_content='f(x,y). The moving windows for each calculation may be the \\n same size and may conveniently be chosen to span on the order of 2-3 fingerprint ridges. Preferably, the window size is \\n sufficiently large to remove the fingerprint features but suffi ciently small to have background variations persist. The fig \\n ure of merit C, in this embodiment is calculated as the ratio of the moving-window standard deviation to the moving-win \\n dow mean: \\n In still another embodiment, a similar process is performed but a moving-window range (i.e., max(image values)-min \\n (image values)) is used instead of a moving-window standard \\n deviation. Thus, similar to the previous embodiment, a mov ing-window mean LL and a moving-window range Ö, are cal \\n culated from the collected image f(x, y) for each wavelength \\n corresponding to index i. The window size for calculation of the moving-window mean is again preferably large enough to \\n remove the fingerprint features but Small enough to maintain \\n background variations. In some instances, the window size \\n for calculation of the moving-window mean is the same as for calculation of the moving-window range, a suitable value in \\n one embodiment spanning on the order of two to three fin gerprint ridges. The figure of merit in this embodiment is \\n calculated as the ratio of the moving-window mean: \\n This embodiment and the preceding one may be consid ered to be specific cases of a more general embodiment in which moving-window calculations are performed on the \\n collected data to calculate a moving-window centrality mea Sure and a moving-window variability measure. The specific \\n embodiments illustrate cases in which the centrality measure comprises an unweighted mean, but may more generally \\n comprise any other type of statistical centrality measure Such \\n as a weighted mean or median in certain embodiments. Simi \\n larly, the specific embodiments illustrate cases in which the variability measure comprises a standard deviation or a range, \\n but may more generally comprise any other type of statistical \\n variability measure Such as a median absolute deviation or \\n standard error of the mean in certain embodiments. \\n In another embodiment that does not use explicit moving window analysis, a wavelet analysis may be performed on \\n each of the spectral images. In some embodiments, the wave let analysis may be performed in a way that the resulting \\n coefficients are approximately spatially invariant. This may be accomplished by performing an undecimated wavelet decomposition, applying a dual-tree complex wavelet \\n method, or other methods of the sort. Gabor filters, steerable pyramids and other decompositions of the sort may also be \\n applied to produce similar coefficients. Whatever method of decomposition is chosen, the result is a collection of coeffi cients that are proportional to the magnitude of the variation corresponding to a particular basis function at a particular \\n position on the image. To perform biometric spoof detection, \\n the wavelet coefficients, or some derived summary thereof, may be compared to the coefficients expected for generated \\n from an appropriate reference dataset samples. In the case', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 26}), Document(page_content=\"US 9,060,688 B2 \\n 19 \\n where the biometric matching function is a determination of a person’s identity, the appropriate reference dataset is one \\n collected from the target person at an earlier time. These \\n enrollment data may be collected from the same skin site or a nearby, locally-consistent skin site and processed in the man \\n ner described. The results of the processing are stored in a \\n database and then retrieved to be used to compare to a similar measurement taken at a later time. If the comparison shows that the results are sufficiently close, the sample is deemed \\n match and identity is established or confirmed. Otherwise, the sample is determined not to match (e.g., to be a spoof). In a \\n similar manner, the coefficients may also be used for biomet ric verification by comparing the currently measured set of \\n coefficients to a previously recorded set from a set of genuine measurements taken on a representative population of people. \\n Various embodiments described above may produce a body of spatio-spectral data, which may be used in various \\n biometrics applications. The invention is not limited to any particular manner of storing or analyzing the body of spatio \\n spectral data. For purposes of illustration, it is shown in the \\n form of a datacube in FIG. 12. \\n The datacube 1201 is shown decomposed along a spectral \\n dimension with a plurality of planes 1203, 1205, 1207, 1209, \\n 1211, each of which corresponds to a different portion of the light spectrum and each of which include spatial information. In some instances, the body of spatio-spectral data may \\n include additional types of information beyond spatial and \\n spectral information. For instance, different illumination con \\n ditions as defined by different illumination structures, differ ent polarizations, and the like may provide additional dimen \\n sions of information. \\n In an embodiment where illumination takes place under white light, the images 1203, 1205, 1207, 1209, and 1211 might correspond, for example, to images generated using \\n light at 450 nm, 500 nm, 550 nm, 600 nm, and 650 nm. In another example, there may be three images that correspond \\n to the amount of light in the red, green, and blue spectral \\n bands at each pixel location. Each image represents the opti cal effects of light of a particular wavelength interacting with \\n skin. Due to the optical properties of skin and skin compo nents that vary by wavelength, each of the multispectral \\n images 1203, 1205, 1207,1209, and 1211 will be, in general, \\n different from the others. The datacube may thus be expressed \\n as RCXs, Ys, X, Y, w) and describes the amount of diffusely reflected light of wavelength seen at each image point X, Y, \\n when illuminated at a source point X, Ys. Different illumi nation configurations (flood, line, etc.) can be summarized by Summing the point response over appropriate source point \\n locations. A conventional non-TIR fingerprint image F(X, Y) can loosely be described as the multispectral data cube for a given wavelength, W, and Summed over all source posi \\n tions: \\n F(X,Y) =XX R(Xs, Ys, X, Y, lo). YS X's \\n Conversely, the spectral biometric dataset S(0) relates the measured light intensity for a given wavelength w to the \\n difference D between the illumination and detection loca \\n tions: \\n The datacube R is thus related to both conventional finger print images and to spectral biometric datasets. The datacube 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n R is a superset of either of the other two data sets and contains \\n correlations and other information that may be lost in either of the two separate modalities. \\n The light that passes into the skin and/or underlying tissue is generally affected by different optical properties of the skin \\n and/or underlying tissue at different wavelengths. Two opti \\n cal effects in the skin and/or underlying tissue that are affected differently at different wavelengths are scatter and\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 27}), Document(page_content='and/or underlying tissue at different wavelengths. Two opti \\n cal effects in the skin and/or underlying tissue that are affected differently at different wavelengths are scatter and \\n absorbance. Optical scatter in skin tissue is generally a Smooth and relatively slowly varying function wavelength. \\n Conversely, absorbance in skin is generally a strong function \\n of wavelength due to particular absorbance features of certain components present in the skin. For example blood, melanin, \\n water, carotene, biliruben, ethanol, and glucose all have sig nificant absorbance properties in the spectral region from 400 \\n nm to 2.5 Lim, which may sometimes be encompassed by the white-light sources. \\n The combined effect of optical absorbance and scatter causes different illumination wavelengths to penetrate the \\n skin to different depths. This effectively causes the different spectral images to have different and complementary infor \\n mation corresponding to different Volumes of illuminated tissue. In particular, the capillary layers close to the Surface of \\n the skin have distinct spatial characteristics that can be imaged at wavelengths where blood is strongly absorbing. Because of the complex wavelength-dependent properties of \\n skin and underlying tissue, the set of spectral values corre sponding to a given image location has spectral characteris \\n tics that are well-defined and distinct. These spectral charac teristics may be used to classify the collected image on a pixel-by-pixel basis. This assessment may be performed by generating typical tissue spectral qualities from a set of quali \\n fied images. For example, the spatio-spectral data shown in \\n FIG. 12 may be reordered as an Nx5 matrix, where N is the number of image pixels that contain data from living tissue, rather than from a surrounding region of air. An eigen-analy \\n sis, Fisher linear discriminant analysis, or other factor analy sis performed on this set matrix produces the representative \\n spectral features of these tissue pixels. The spectra of pixels in a later data set may then be compared to such previously \\n established spectral features using metrics Such as Mahalano \\n bis distance and spectral residuals. If more thana Small num ber of image pixels have spectral qualities that are inconsis \\n tent with living tissue, then the sample is deemed to be non genuine and rejected, thus providing a mechanism for \\n incorporating anti-spoofing methods in the sensor based on \\n determinations of the liveness of the sample. The foregoing analysis framework can also be used to \\n determine identity. In this case, the earlier reference data are taken from a single target individual rather than a represen tative user population. A Subsequent Successful comparison \\n to a particular person’s reference data may then establish both \\n the identity and the genuineness of the measurement (e.g., \\n assuming the earlier reference measurement is ensured to be genuine). \\n Alternatively, textural characteristics of the skin may alone \\n or in conjunction with the spectral characteristics be used to determine the authenticity of the sample. For example, each spectral image may be analyzed in Such a way that the mag \\n nitude of various spatial characteristics may be described. \\n Methods for doing so include wavelet transforms. Fourier \\n transforms, cosine transforms, gray-level co-occurrence, and the like. The resulting coefficients from any such transform \\n described an aspect of the texture of the image from which \\n they were derived. The set of such coefficients derived from a set of spectral images thus results in a description of the \\n chromatic textural characteristics of the multispectral data.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 27}), Document(page_content='US 9,060,688 B2 \\n 21 \\n These characteristics may then be compared to similar char \\n acteristics of known samples to perform a biometric determi \\n nation Such as spoof or liveness determination. Alternatively, \\n the characteristics may be compared to those developed from \\n data taken on reputedly the same person at an earlier time to \\n establish identity. Methods for performing such determina \\n tions are generally similar to the methods described for the \\n spectral characteristics above. Applicable classification tech \\n niques for Such determinations include linear and quadratic \\n discriminant analysis, classification trees, neural networks, \\n and other methods known to those familiar in the art. \\n Similarly, in an embodiment where the sample is a volar \\n Surface of a hand or finger, the image pixels may be classified \\n as “ridge.” “valley, or “other based on their spectral quali \\n ties or their chromatic textural qualities. This classification \\n can be performed using discriminant analysis methods such \\n as linear discriminant analysis, quadratic discriminant analy \\n sis, principal component analysis, neural networks, and oth \\n ers known to those of skill in the art. Since ridge and valley pixels are contiguous on a typical Volar Surface, in some \\n instances, data from the local neighborhood around the image pixel of interest are used to classify the image pixel. In this \\n way, a conventional fingerprint image may be extracted for \\n further processing and biometric assessment. The “other category may indicate image pixels that have spectral quali \\n ties that are different than anticipated in a genuine sample. A \\n threshold on the total number of pixels in an image classified \\n as “other may be set. If this threshold is exceeded, the sample may be determined to be non-genuine and appropriate indi \\n cations made and actions taken. \\n In a similar way, multispectral data collected from regions \\n Such as the Volar Surface of fingers may be analyzed to \\n directly estimate the locations of “minutiae points.” which are \\n defined as the locations at which ridges end, bifurcate, or undergo other Such topographic change. For example, the \\n chromatic textural qualities of the multispectral dataset may \\n be determined in the manner described above. These qualities may then be used to classify each image location as “ridge \\n ending.” “ridge bifurcation, or “other in the manner \\n described previously. In this way, minutiae feature extraction may be accomplished directly from the multispectral data \\n without having to perform computationally laborious calcu \\n lations such as image normalization, image binarization, image thinning, and minutiae filtering, techniques that are \\n known to those familiar in the art. \\n Biometric determinations of identity may be made using the entire body of spatio-spectral data or using particular portions thereof. For example, appropriate spatial filters may \\n be applied to separate out the lower spatial frequency infor mation that is typically representative of deeper spectrally \\n active structures in the tissue. The fingerprint data may be extracted using similar spatial frequency separation and/or \\n the pixel-classification methods disclosed above. The spec \\n tral information can be separated from the active portion of \\n the image in the manner discussed above. These three por tions of the body of spatio-spectral data may then be pro \\n cessed and compared to the corresponding enrollment data \\n using methods known to one familiar in the art to determine the degree of match. Based upon the strength of match of \\n these characteristics, a decision can be made regarding the \\n match of the sample with the enrolled data. Additional details regarding certain types of spatio-spectral analyses that may \\n be performed are provided in U.S. Pat. No. 7,147,153, \\n entitled “MULTISPECTRAL BIOMETRICSENSOR, filed', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content=\"be performed are provided in U.S. Pat. No. 7,147,153, \\n entitled “MULTISPECTRAL BIOMETRICSENSOR, filed \\n Apr. 5, 2004 by Robert K. Rowe et al., the entire disclosure of which is incorporated herein by reference for all purposes. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 22 \\n It will now be appreciated that the various sensor embodi \\n ments may be used to detect locally consistent features of the \\n skin site in a number of ways. Various embodiments may \\n detect spatial, spectral, textural, and/or other information, \\n alone or in combination, in series or in parallel. It will further be appreciated that many configurations of sensors may be \\n used, according to the invention, some of which are described \\n below. \\n Exemplary Application Embodiments \\n Small-area biometric sensors, such as those discussed above, may be embedded in a variety of systems and appli \\n cations according to the invention. In some embodiments, the sensor is configured as a dedicated System that is connected to a PC or a network interface, an ATM, Securing an entryway, or allowing access to a particular piece of electronics Such as a \\n cellular phone. In these embodiments, one or more people may be enrolled in the biometric system and use a particular \\n reader to gain access to a particular function or area. In other embodiments, the sensor is configured as a per \\n sonal biometric system that confirms the identity of the sale \\n person authorized to use the device, and transmits this autho rization to any properly equipped PC, ATM, entryway, or \\n piece of electronics that requires access authorization. In one \\n embodiment, the personal biometric system transmits an \\n identifying code to a requesting unit and then uses a biometric \\n signal to confirm authorization. This may imply that the sys \\n tem needs to perform a verification task rather than a poten \\n tially more difficult identification task. Yet, from the user's perspective, the system may recognize the user without an \\n explicit need to identify himself or herself. Thus, the system \\n may appear to operate in an identification mode, which may \\n be more convenient for the user. \\n An advantage of a personal biometric system may be that, \\n if an unauthorized person is able to defeat the personal bio metric system code for a particular biometric system-person \\n combination, the personal biometric system may be reset or \\n replaced to use a new identifying code and thus re-establish a \\n secure biometric for the authorized person. This capability may be in contrast to multi-person biometric systems that \\n base their authorization solely on a biometric signature (e.g., spatio-spectral information from a fingerprint). In this latter \\n case, if an intruder is able to compromise the system by \\n Somehow imitating the signal from an authorized user, there may be no capability to change the biometric code when it is based solely on a fixed physiological characteristic of a per \\n SO. \\n FIG. 13 shows one embodiment of a personal spectral \\n biometric system 1300 in the configuration of an electronic key fob 1302. While an equidistant sensor configuration is \\n shown, it will be appreciated that any type of sensor according \\n to the invention may be used. In some embodiments, an illumination system 1304 and a detection system 1306 are \\n built into the fob 1302, as is a means to collect and digitize the spectral information (not shown). In one embodiment, short range wireless techniques 1303 (e.g., based upon RF signals) \\n are transmitted to communicate between the fob and a corre \\n sponding reader (not shown), e.g., to allow access to a PC, \\n entryway, etc. In another embodiment, an infrared optical \\n signal is used to transmit information between the fob 1302 \\n and the reader. In yet another embodiment, a direct electrical \\n connection is established between the fob 1302 and the \\n reader. \\n Actual processing of the detected biometric information\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content='and the reader. In yet another embodiment, a direct electrical \\n connection is established between the fob 1302 and the \\n reader. \\n Actual processing of the detected biometric information \\n may be made either within the fob 1302 or at the reader. In the former case, logical operations necessary to perform the com \\n parison may be done within the fob 1302, and then a simple \\n confirmation ordenial signal may be transmitted to the reader.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 28}), Document(page_content=\"US 9,060,688 B2 \\n 23 \\n In the latter case, the detected biometric information may be \\n transmitted from the fob 1302 to the reader (either all at once, serially, or in any other useful way), and the comparison and \\n decision may be performed at the reader or at a host to which \\n the reader is connected. In either case, the communication \\n between the fob 1302 and the reader may be performed in a secure manner, e.g., to avoid interception and unauthorized \\n use of the system. Methods for ensuring secure communica \\n tion between two devices are well known to one of ordinary \\n skill in the art. \\n A second embodiment of a personal spectral biometric \\n system 1400 is depicted in FIG. 14. In this embodiment, the \\n biometric reader 1411 is built into the case of a watch 1412 \\n and operates based upon signals detected from the skin proxi \\n mate to the location of the watch (e.g., around the wrist). In certain embodiments, operation of this system may be iden \\n tical to the operation described with respect to FIG. 13. \\n In addition to the watch or fob, similar biometric capability may be built into other personal electronic devices, including, \\n for example, personal digital assistants (“PDAs) and cellular telephones. In each case, the personal biometric system may \\n provide user authorization to access both the device in which \\n it is installed, as well as authorization for mobile commerce \\n (“M-Commerce') or other wireless transactions that the device may be capable of performing. Small-area sensors may also be put into firearms, commercial equipment, power tools, or other potentially dangerous devices or systems, e.g., \\n to prevent unauthorized or unintended usage. For example, a \\n biometric sensor may be placed in the handgrip of a firearm to sense tissue properties while the gun is being held in a normal \\n a. \\n Further embodiments provide the ability to identify people who are to be explicitly excluded from accessing protected \\n property as well as determining those who are authorized to access the property. This capability may, for example, \\n improve the biometric performance of the system with \\n respect to those unauthorized people who are known to attempt to use the device, which could be particularly impor \\n tant in certain cases (e.g., the case of a personal handgun). In particular, parents who own a biometrically enabled handgun \\n may enroll themselves as authorized users and also can enroll their children as explicitly unauthorized users. In this way, \\n parents may have further insurance that children who are \\n known to be in the same household as a gun will not be able \\n to use it. \\n Even further embodiments use the explicit-denial capabil \\n ity of a biometric system in a fixed installation Such as a home, \\n place of business, oran automobile. For example, a biometric system installed at the-entryway of a place of business can be \\n used to admit authorized employees and temporary workers. If an employee is fired or the term of the temporary employee \\n expires, then their enrollment data can be shifted from the \\n authorized to the unauthorized database, and an explicit check is made to deny access to the former employee if he or she attempts to enter. It will be appreciated that the applications described herein are only examples, and that many other applications may be \\n possible. For example, many spatio-spectral features may be \\n indicative of living tissue, allowing some sensors to be used to \\n detect the “liveness” of a sample. This may deter certain types \\n of circumvention attempts, like the use of latex or wax “sur rogates.” or dead or excised tissue. In some applications. Such \\n as Internet access authorization, it may be useful to be able to Verify the sex and/or age of the person using the spectral \\n biometric System. Because ages and sexes may manifest in different ways in skin structure and composition, the optical spectra may also change in Systematic and indicative ways, 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 29}), Document(page_content='10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 24 \\n Such that age and sex may be estimated using the biometric data. Additional details regarding estimation of certain per \\n Sonal characteristics from biometric measurements are pro \\n vided in U.S. Pat. No. 7,623,313, entitled “METHODS AND \\n SYSTEMS FORESTIMATION OF PERSONAL CHAR \\n ACTERISTICS FROM BIOMETRIC MEASUREMENTS \\n filed Dec. 9, 2004 by Robert K. Rowe, the entire disclosure of which is incorporated herein by reference for all purposes. \\n In various embodiments, a biometric sensor of any of the types described above may be operated by a computational \\n system to implement biometric functionality. FIG.15 broadly \\n illustrates how individual system elements may be imple \\n mented in a separated or more integrated manner. The com putational device 1500 is shown comprised of hardware ele \\n ments that are electrically coupled via bus 1526, which is also \\n coupled with the biometric sensor 1556. The hardware ele ments include a processor 1502, an input device 1504, an output device 1506, a storage device 1508, a computer-read \\n able storage media reader 1510a, a communications system \\n 1514, a processing acceleration unit 1516 such as a DSP or special-purpose processor, and a memory 1518. The com \\n puter-readable storage media reader 1510a is further con nected to a computer-readable storage medium 1510b, the \\n combination comprehensively representing remote, local, \\n fixed, and/or removable storage devices plus storage media for temporarily and/or more permanently containing com \\n puter-readable information. The communications system \\n 1514 may comprise a wired, wireless, modem, and/or other type of interfacing connection and permits data to be \\n exchanged with external devices. The computational device 1500 also comprises software \\n elements, shown as being currently located within working memory 1520, including an operating system 1524 and other \\n code 1522. Such as a program designed to implement methods \\n of the invention. It will be apparent to those skilled in the art \\n that Substantial variations may be used in accordance with specific requirements. For example, customized hardware \\n might also be used and/or particular elements might be imple \\n mented in hardware, Software (including portable software, \\n Such as applets), or both. Further, connection to other com puting devices such as network input/output devices may be employed. \\n It will be appreciated that these units of the device may, individually or collectively, be implemented with one or more Application Specific Integrated Circuits (ASICs) adapted to \\n perform some or all of the applicable functions in hardware. Alternatively, the functions may be performed by one or more \\n other processing units (or cores), on one or more integrated \\n circuits. In other embodiments, other types of integrated cir \\n cuits may be used (e.g., Structured/Platform ASICs, Field Programmable Gate Arrays (FPGAs), and other Semi-Cus \\n tom ICs), which may be programmed in any manner known in \\n the art. The functions of each unit may also be implemented, \\n in whole or in part, with instructions embodied in a memory, \\n formatted to be executed by one or more general or applica tion-specific processors. \\n Exemplary Methods \\n FIG.16 provides a flow diagram of exemplary methods for using multispectral sensor structures according to embodi \\n ments of the invention. While the drawing shows a number of steps performed in a particular order, this is intended to be \\n illustrative rather than limiting. In other embodiments, the steps may be performed in a different order, further steps may \\n be added, and/or some steps identified specifically may be \\n omitted. \\n At block 1604, a skin site of an individual is illuminated. In \\n Some embodiments, the skin site is illuminated under a plu', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 29}), Document(page_content=\"US 9,060,688 B2 \\n 25 \\n rality of distinct optical conditions, e.g., including a plurality \\n of wavelengths, polarizations, and/or source-detector spac \\n ings. Light scattered from the skin site is received at block \\n 1608. An image is then formed of the skin site from the received light (e.g., by generating images of the skin site 5 \\n corresponding to the distinct optical conditions) to generate a \\n local feature profile at block 1612. The local feature profile \\n may characterize a locally consistent feature of the skin site being imaged. \\n At block 1616, the local feature profile may be analyzed. In \\n Some embodiments, this analysis includes comparing the \\n local feature profile to a reference feature profile. In certain \\n embodiments, the reference feature profile includes image \\n data relating to locally consistent features of the individual \\n using the sensor. For example, the data may come from pre \\n vious uses of the sensor and/or from a database of previously \\n collected images. Further, in some embodiments, the sensor may be capable of “learning over time by enhancing its \\n analysis algorithm. For example, genetic algorithms, image 20 \\n averaging, neural networks, and other techniques may be \\n used. \\n Based on the analysis of the local feature profile, a biomet \\n ric function may be performed at block 1620. In some \\n embodiments, the biometric function comprises an identity 25 function. For example, the local feature profile may be com \\n pared with stored reference feature profiles to identify the \\n individual or to confirm the identity of the individual. In other \\n embodiments, the biometric function includes demographic, anthropomorphic, liveness, analyte measurement, and/or 30 \\n other biometric functions. \\n This description provides example embodiments only, and \\n is not intended to limit the scope, applicability or configura \\n tion of the invention. Rather, the ensuing description of the \\n embodiments will provide those skilled in the art with an 35 enabling description for implementing embodiments of the \\n invention. Various changes may be made in the function and arrangement of elements without departing from the spirit \\n and Scope of the invention. Also, it should be emphasized that technology evolves and, thus, many of the elements are 40 examples and should not be interpreted to limit the scope of \\n the invention. \\n Thus, various embodiments may omit, Substitute, or add various procedures or components as appropriate. For \\n instance, it should be appreciated that in alternative embodi- 45 \\n ments, the methods may be performed in an order different \\n from that described, and that various steps may be added, \\n omitted, or combined. Also, features described with respect to \\n certain embodiments may be combined in various other \\n embodiments. Different aspects and elements of the embodi- 50 \\n ments may be combined in a similar manner. \\n Also, it is noted that the embodiments may be described as a process which is depicted as a flow diagram or block dia gram. Although each may describe the operations as a sequential process, many of the operations can be performed 55 \\n in parallel or concurrently. In addition, the order of the opera \\n tions may be rearranged. A process may have additional steps \\n not included in the figure. \\n Moreover, as disclosed herein, the term “memory” or “memory unit may represent one or more devices for storing 60 \\n data, including read-only memory (ROM), random access memory (RAM), magnetic RAM, core memory, magnetic \\n disk storage mediums, optical storage mediums, flash \\n memory devices, or other computer-readable mediums for \\n storing information. The term “computer-readable medium'' 65 \\n includes, but is not limited to, portable or fixed storage \\n devices, optical storage devices, wireless channels, a sim 10 \\n 15 26 \\n card, other Smart cards, and various other mediums capable of storing, containing, or carrying instructions or data. \\n Furthermore, embodiments may be implemented by hard\", metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 30}), Document(page_content='15 26 \\n card, other Smart cards, and various other mediums capable of storing, containing, or carrying instructions or data. \\n Furthermore, embodiments may be implemented by hard \\n ware, Software, firmware, middleware, microcode, hardware \\n description languages, or any combination thereof. When \\n implemented in Software, firmware, middleware, or micro \\n code, the program code or code segments to perform the \\n necessary tasks may be stored in a computer-readable \\n medium Such as a storage medium. Processors may perform \\n the necessary tasks. \\n Having described several embodiments, it will be recog \\n nized by those of skill in the art that various modifications, \\n alternative constructions, and equivalents may be used with \\n out departing from the spirit of the invention. For example, \\n the above elements may merely be a component of a larger \\n system, wherein other rules may take precedence over or \\n otherwise modify the application of the invention. Also, a \\n number of steps may be undertaken before, during, or after \\n the above elements are considered. Accordingly, the above description should not be taken as limiting the scope of the \\n invention, which is defined in the following claims. \\n What is claimed is: \\n 1. A method of performing a biometric function, the method comprising: \\n illuminating a small-area purported skin site of an indi \\n vidual with illumination light; receiving light scattered from the Small-area purported \\n skin site; generating a local feature profile from the received light, \\n wherein the local feature profile identifies a feature of the Small-area purported skin site of a type predeter \\n mined to exhibit Substantial local consistency across non-overlapping skin sites of the individual; and analyzing the generated local feature profile to perform the \\n biometric function, the biometric function selected from a group consisting of an identity function, a demo \\n graphic function, and an anthropometric function, wherein analyzing the generated local feature profile com prises comparing the generated local feature profile with \\n a reference local feature profile generated from light \\n scattered from a small-area reference skin site of the \\n individual that was previously captured during biomet \\n ric enrollment of the individual and does not overlap with the Small-area purported skin site. \\n 2. The method recited in claim 1, wherein generating the local feature profile comprises: \\n forming an image from the received light; generating spatially-distributed multispectral data from the \\n image; and processing the spatially-distributed multispectral data to \\n generate the local feature profile. \\n 3. The method recited in claim 1, wherein generating the local feature profile comprises: \\n forming an image from the received light; \\n generating an image-texture measure from the image; and processing the generated image-texture measure to gener \\n ate the local feature profile. \\n 4. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n determining an identity of the individual from the gen \\n erated local feature profile. \\n 5. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n estimating a demographic characteristic of the indi \\n vidual from the generated local feature profile.', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 30}), Document(page_content='US 9,060,688 B2 \\n 27 \\n 6. The method recited in claim 1 wherein: \\n analyzing the generated local feature profile comprises \\n estimating an anthropometric characteristic of the indi \\n vidual from the generated local feature profile. \\n 7. The method recited in claim 1 wherein illuminating the Small-area purported skin site comprises illuminating the \\n Small-area purported skin site under a plurality of distinct \\n optical conditions. \\n 8. The method recited in claim 7 wherein illuminating the \\n Small-area purported skin site under the plurality of distinct optical conditions comprises illuminating the Small-area pur \\n ported skin site with light under a plurality of distinct polar \\n ization conditions. \\n 9. The method recited in claim 7 wherein illuminating the \\n Small-area purported skin site under the plurality of distinct \\n optical conditions comprises illuminating the Small-area pur \\n ported skin site with light under a plurality of distinct wave lengths. \\n 10. The method recited in claim 7 wherein generating the local feature profile from the received light comprises: \\n generating a plurality of images, each of the plurality of \\n images corresponding to one of the distinct optical con \\n ditions; and applying a compositing algorithm to the plurality of \\n images. \\n 11. A biometric sensor comprising: \\n an illumination Subsystem disposed to illuminate a small \\n area purported skin site of an individual with illumina tion light; \\n a detection Subsystem disposed to receive light scattered \\n from the Small-area purported skin site; and \\n a computational unit interfaced with the detection sub system and having: \\n instructions for generating a local feature profile from \\n the received light, wherein the local feature profile \\n identifies a feature of the small-area purported skin site of a type predetermined to exhibit substantial local consistency across non-overlapping skin sites of \\n the individual; and instructions for analyzing the generated local feature \\n profile to perform a biometric function selected from a group consisting of an identity function, a demo \\n graphic function, and an anthropometric function, wherein analyzing the generated local feature profile comprises comparing the generated local feature pro \\n file with a reference local feature profile generated \\n from light scattered from a small-area reference skin site of the individual that was previously captured \\n during biometric enrollment of the individual and does not overlap with the Small-area purported skin \\n site. \\n 12. The biometric sensor recited in claim 11, wherein the instructions for generating the local feature profile comprise: 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 28 \\n forming an image from the received light; generating spatially-distributed multispectral data from the image; and processing the spatially-distributed multispectral data to \\n generate the local feature profile. \\n 13. The biometric sensor recited in claim 11, wherein the instructions for generating the local feature profile comprise: forming an image from the received light; generating an image-texture measure from the image; and processing the generated image-texture measure to gener \\n ate the local feature profile. \\n 14. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise determining an identity of the individual from the generated local feature profile. \\n 15. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise estimating a demographic characteristic of the \\n individual from the generated local feature profile. \\n 16. The biometric sensor recited in claim 11 wherein: \\n the instructions for generating the local feature profile comprise estimating an anthropometric characteristic of \\n the individual from the generated local feature profile. \\n 17. The biometric sensor recited in claim 11 wherein the', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 31}), Document(page_content='the individual from the generated local feature profile. \\n 17. The biometric sensor recited in claim 11 wherein the \\n illumination Subsystem is disposed to illuminate the Small area purported skin site under a plurality of distinct optical \\n conditions. \\n 18. The biometric sensor recited in claim 17 wherein the \\n plurality of distinct optical conditions comprises a plurality of \\n distinct polarization conditions. \\n 19. The biometric sensor recited in claim 17 wherein the \\n plurality of distinct optical conditions comprises light under a plurality of distinct wavelengths. \\n 20. The biometric sensor recited in claim 17 wherein the \\n instructions for generating the local feature profile from the received light comprise: \\n instructions for generating a plurality of images, each of the plurality of images corresponding to one of the dis \\n tinct optical conditions; and instructions for applying a compositing algorithm to the plurality of images. \\n 21. The method recited in claim 1, wherein the feature of the Small-area purported skin site comprises artificial pig \\n mentation. \\n 22. The method recited in claim 21, wherein analyzing the generated local feature profile comprises verifying a pres \\n ence, quantity, and/or shape of the artificial pigmentation. \\n 23. The biometric sensor recited in claim 11, wherein the \\n feature of the Small-area purported skin site comprises artifi cial pigmentation. \\n 24. The biometric sensor recited in claim 23, wherein the instructions for analyzing the generated local feature profile comprise verifying a presence, quantity, and/or shape of the \\n artificial pigmentation. \\n k k k k k', metadata={'source': 'https://patentimages.storage.googleapis.com/a6/11/c7/5583de7c8fb29d/US9060688.pdf', 'page': 31})]\n",
      "[Document(page_content='US010956732B2 \\n ( 12 ) United States Patent \\n Le Henaff ( 10 ) Patent No .: US 10,956,732 B2 \\n ( 45 ) Date of Patent : Mar. 23 , 2021 \\n ( 56 ) ( 54 ) SYSTEM AND METHOD FOR DETECTING THE AUTHENTICITY OF PRODUCTS References Cited \\n U.S. PATENT DOCUMENTS \\n ( 71 ) Applicant : Guy Le Henaff , Montreal ( CA ) 5,267,332 A 5,673,338 A * 11/1993 Walch et al . 9/1997 Denenberg ( 72 ) Inventor : Guy Le Henaff , Montreal ( CA ) G06K 9/3233 \\n 382/209 \\n ( Continued ) ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 130 days . FOREIGN PATENT DOCUMENTS \\n ( 21 ) Appl . No .: 15 / 527,907 CN CN 102576394 7/2012 103310256 9/2013 \\n ( Continued ) ( 22 ) PCT Filed : Nov. 23 , 2015 \\n PCT / CA2015 / 051216 OTHER PUBLICATIONS ( 86 ) PCT No .: \\n $ 371 ( c ) ( 1 ) , ( 2 ) Date : May 18 , 2017 Secure fragile watermarking - capabilities , Sergio Bravo - Solorio et \\n al . , Elsevier , 2011 , pp . 728-739 ( Year : 2011 ) . * \\n ( Continued ) \\n ( 87 ) PCT Pub . No .: WO2016 / 077934 \\n PCT Pub . Date : May 26 , 2016 Primary Examiner Jayesh A Patel ( 74 ) Attorney , Agent , or Firm - Hamre , Shcumann , Mueller & Larson , P.C. - \\n ( 65 ) Prior Publication Data \\n US 2018/0349695 A1 Dec. 6 , 2018 \\n Related U.S. Application Data \\n ( 60 ) Provisional application No. 62 / 082,939 , filed on Nov. 21 , 2014 . \\n ( 51 ) Int . Ci . GO6K 9/60 ( 2006.01 ) \\n G06K 9/00 ( 2006.01 ) \\n ( Continued ) \\n ( 52 ) U.S. CI . CPC G06K 9/00577 ( 2013.01 ) ; G06F 16/583 \\n ( 2019.01 ) ; G06K 9/00 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search CPC G06K 9/00577 ; G06K 9/00 ; G06K 9/2063 ; GOOK 9/3233 ; G06K 9/6201 ; \\n ( Continued ) ( 57 ) ABSTRACT \\n System and method for detecting the authenticity of prod ucts by detecting a unique chaotic signature . Photos of the products are taken at the plant and stored in a database / server . The server processes the images to detect for each authentic product a unique authentic signature which is the result of a manufacturing process , a process of nature etc. To detect whether the product is genuine or not at the store , the user / buyer may take a picture of the product and send it to the server ( e.g. using an app installed on a portable device or the like ) . Upon receipt of the photo , the server may process the receive image in search for a pre - detected and / or pre - stored chaotic signature associated with an authentic product . The server may return a response to the user indicating the result of the search . A feedback mechanism may be included to guide the user to take a picture at a specific location of the product where the chaotic signature may exist . \\n 26 Claims , 16 Drawing Sheets', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 0}), Document(page_content='US 10,956,732 B2 Page 2 \\n ( 51 ) Int . Ci . \\n GO6F 16/583 ( 2019.01 ) \\n GO6K 9/20 ( 2006.01 ) \\n G06T 5/40 ( 2006.01 ) \\n G06T 7/44 ( 2017.01 ) G06T 5/00 ( 2006.01 ) \\n G06T 7/42 ( 2017.01 ) \\n G06T 7770 ( 2017.01 ) \\n GO6K 9/32 ( 2006.01 ) \\n G06K 9/62 ( 2006.01 ) GO6K 19/06 ( 2006.01 ) \\n G06T 7700 ( 2017.01 ) \\n ( 52 ) U.S. Ci . CPC G06K 9/2063 ( 2013.01 ) ; GOOK 9/3233 ( 2013.01 ) ; G06K 9/6201 ( 2013.01 ) ; G06K 19/06037 ( 2013.01 ) ; GO6T 5/006 ( 2013.01 ) ; GO6T 5/40 ( 2013.01 ) ; G06T 7/0002 ( 2013.01 ) ; G06T 7/42 ( 2017.01 ) ; G06T 7/44 ( 2017.01 ) ; G06T 7/70 ( 2017.01 ) ; G06K 2009/0059 ( 2013.01 ) ; G06K 2209/25 ( 2013.01 ) ; G06T 2207/20061 ( 2013.01 ) ; GOOT 2207/20064 ( 2013.01 ) ; G06T 2207/20076 ( 2013.01 ) ( 58 ) Field of Classification Search CPC ..... G06K 19/06037 ; G06K 2009/0059 ; G06K 2209/25 ; G06T 7/0002 ; G06T \\n 2207/20076 See application file for complete search history . \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 6,101,265 A * 8/2000 Bacus 2006/0013486 A1 * 1/2006 Burns G06T 7/001 \\n 382/195 \\n 2006/0100964 A1 5/2006 Wilde et al . \\n 2006/0290136 A1 * 12/2006 Alasia G07D 7/128 \\n 283/72 \\n 2007/0150829 A1 * 6/2007 Eschbach G06F 40/103 \\n 715/781 \\n 2007/0158434 A1 * 7/2007 Fan H04N 1/32144 \\n 235/487 \\n 2007/0274585 A1 * 11/2007 Zhang G06F 19/321 \\n 382/132 \\n 2007/0292034 A1 * 12/2007 Tabankin HO4N 5/232 \\n 382/232 \\n 2008/0095465 A1 * 4/2008 Mullick G06T 7/35 \\n 382/284 2008/0201305 A1 * 8/2008 Fitzpatrick G06Q 30/02 2008/0219503 A1 * 9/2008 Di Venuto GO6K 9/6857 \\n 382/103 \\n 2008/0310765 A1 * 12/2008 Reichenbach G06K 7/1443 \\n 382/312 \\n 2009/0080695 A1 * 3/2009 Yang GO6K 9/3241 \\n 382/103 \\n 2009/0302101 A1 * 12/2009 Poizat G06K 9/00577 \\n 235/375 \\n 2010/0067691 A1 * 3/2010 Lin HO4L 9/3247 \\n 380/55 \\n 2010/0128964 A1 * 5/2010 Blair G06K 9/2027 \\n 382/135 2010/0195894 A1 * 8/2010 Lohweg G07D 7/2016 \\n 382/135 \\n 2010/0296583 A1 * 11/2010 Li HO4N 19/162 \\n 375 / 240.24 \\n 2011/0007935 Al * 1/2011 Reed H04N 1/32309 \\n 382/100 \\n 2011/0135160 A1 * 6/2011 Sagan G07D 7/12 \\n 382/108 \\n 2011/0142302 A1 * 6/2011 Leung G06T 1/005 \\n 382/128 \\n 2012/0104097 A1 * 5/2012 Moran G06K 19/06037 \\n 235/449 2013/0142440 A1 * 6/2013 Hirayama B42D 25/305 \\n 382/212 \\n 2013/0173383 Al 7/2013 Sharma et al . \\n 2013/0287267 A1 * 10/2013 Varone G06K 19/18 \\n 382/115 \\n 2014/0006101 A1 * 1/2014 Andrade G06Q 30/02 705 / 7.29 \\n 2014/0037129 A1 * 2/2014 Reed GO6K 9/00 \\n 382/100 \\n 2014/0147046 A1 * 5/2014 Massicot G07D 7/128 \\n 382/182 \\n 2014/0185882 A1 * 7/2014 Masuura HO4N 5/23267 \\n 382/107 2014/0201094 Al * 7/2014 Herrington G06Q 30/018 705/317 \\n 2014/0304122 A1 * 10/2014 Rhoads G06T 19/006 \\n 705 / 27.2 \\n 2014/0341374 A1 * 11/2014 Thozhuvanoor HO4L 9/0869 \\n 380/28 2015/0310601 A1 * 10/2015 Rodriguez G07G 1/0072 \\n 348/150 \\n 2016/0055398 A1 * 2/2016 Ishiyama G06K 9/036 \\n 382/190 \\n 2016/0063302 A1 * 3/2016 Doerr G06K 9/0053 \\n 382/154 \\n 2016/0071101 A1 * 3/2016 Winarski G06Q 20/3829 705/71 2016/0140420 A1 * 5/2016 Di Venuto Dayer , V \\n G06K 9/00577 \\n 382/103 \\n 2016/0171744 A1 * 6/2016 Rhoads G06K 9/6255 \\n 345/419 \\n 2016/0259947 A1 * 9/2016 Negrea GO6F 21/31 \\n 2016/0259976 A1 * 9/2016 Halasz GO6K 19/086 6,396,507 B1 * 5/2002 Kaizuka \\n 7,443,508 B1 * 10/2008 Vrhel \\n 7,891,565 B2 * 2/2011 Pinchen \\n 8,027,509 B2 * GOIN 15/1475 \\n 345/665 G06T 3/0025 \\n 345/660 \\n GO1J 3/10 \\n 348/195 GO6K 19/086 \\n 235/385 \\n G06T 1/0042 \\n 382/100 \\n H04L 9/001 \\n 713/176 \\n GO6F 21/64 \\n 726/32 G06K 9/00671 \\n 382/189 \\n HO4N 19/124 \\n 382/233 \\n G06K 9/00577 \\n 382/190 9/2011 Reed \\n 8,090,952 B2 * 1/2012 Harris \\n 8,171,567 B1 * 5/2012 Fraser \\n 8,731,301 B1 * 5/2014 Bushman \\n 8,908,980 B2 * 12/2014 Hong \\n 8,989,500 B2 * 3/2015 Boutant \\n 9,208,394 B2 * 12/2015 Di Venuto Dayer \\n 9,269,022 B2 * 2/2016 Rhoads 9,401,153 B2 * 7/2016 Sharma 9,800,360 B2 * 10/2017 Yamada 9,947,015 B1 * 4/2018 Vildosola 10,482,471 B2 * 11/2019 Herrington 2002/0025084 Al * 2/2002 Yang \\n 2003/0174863 A1 * 9/2003 Brundage G06K 9/6857 \\n G06F 16/583 \\n GIOL 19/018 \\n HO4H 60/80 G06Q 30/0185 G06Q 30/018 HO4N 1/3875', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 1}), Document(page_content='2003/0174863 A1 * 9/2003 Brundage G06K 9/6857 \\n G06F 16/583 \\n GIOL 19/018 \\n HO4H 60/80 G06Q 30/0185 G06Q 30/018 HO4N 1/3875 \\n 382/299 B42D 25/29 \\n 382/100 G07D 7/0032 \\n 382/100 A61B 8/465 \\n 600/407 \\n G07D 7/128 \\n 382/100 \\n G06K 9/36 \\n 382/132 2004/0001604 A1 * 1/2004 Amidror \\n 2004/0068170 A1 * 4/2004 Wang \\n 2004/0258274 A1 * 12/2004 Brundage \\n 2006/0013464 A1 * 1/2006 Ramsay', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 1}), Document(page_content='US 10,956,732 B2 \\n Page 3 \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 2016/0321531 A1 * 11/2016 Lau \\n 2018/0107915 A1 * 4/2018 Toedtli \\n 2020/0027106 A1 * 1/2020 Kendrick G06K 19/06103 \\n G06K 19/06028 \\n G06K 9/6201 \\n FOREIGN PATENT DOCUMENTS Supplementary European Search Report issued in European Appli cation No. 15860424.9 dated Mar. 19 , 2018 ( 19 pages ) . “ Template Matching ” ; Wikipedia , XP055433175 , 2014 , Retrieved from the Internet , https://en.wikipedia.org/w/index.php?title=Template_ matching & oldid = 615694604 . ( 6 pages ) . Wiegand et al .: “ Motion Estimation for Video Coding \" , 2012 , XP055433170 , Retrieved from the Internet , https : //web.stanford . edu / class / ee398a / handouts / lectures / EE398a_MotionEstimation_ 2012. ( 33 pages ) . European Search Report issued in European Application No. 15860424 dated Dec. 19 , 2017 ( 18 pages ) . “ Template Matching ” ; Wikipedia , XP055433175 , 2014 , 6 pages , retrieved from the Internet , https://en.wikipedia.org/w/index.php ? title = Template_matching & oldid = 615694604 . Wiegand et al .: “ Motion Estimation for Video Coding \" ; 2012 , 33 pages , Retrieved from the Internet : https://web.stanford.edu/class/ ee398a / handouts / lectures / EE398a_MotionEstimation_2012.pdf . Qiu Chen et al . , “ A Modified Way of Image Matching Based on Grid Searching , \" Image Processing and Multimedia Technology , 2009 , CN CN \\n GB \\n WO \\n WO \\n WO \\n WO 103702217 4/2014 104123537 10/2014 \\n 2464723 4/2010 \\n 2013173408 11/2013 \\n WO - 2013173408 A1 * 11/2013 \\n 2013191281 12/2013 \\n WO - 2013191281 A1 * 12/2013 G06K 9/3216 \\n HO4N 7/18 \\n OTHER PUBLICATIONS pp . 56-60 . \\n International Search Report issued in International Application No. \\n PCT / CA2015 / 051216 dated Mar. 1 , 2016 ( 5 pages ) . Written Opinion issued in International Application No. PCT / CA2015 / 051216 dated Mar. 1 , 2016 ( 6 pages ) . First Office Action and Search Report issued for Chinese Patent Application No. 201580074242.8 , dated Mar. 19 , 2020 , 38 pages including English translation of the Office Action . \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 2}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 1 of 16 US 10,956,732 B2 \\n ? \\n 0 \\n ? \\n 112', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 3}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 2 of 16 US 10,956,732 B2 \\n 122 \\n 1 \\n 125 \\n 7 \\n FIC 3C', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 4}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 3 of 16 US 10,956,732 B2 \\n FIG 4a \\n 1992 \\n FIG 46 FIG 40', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 5}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 4 of 16 US 10,956,732 B2 \\n 136a \\n 136 \\n KW \\n SLIM FIT 138 \\n 1386 FIG 5a \\n SLIME S_INT ST \\n a \\n } FOLDER 1 \\n FOLDER 2 \\n } \\n FOLDER 3 \\n ? \\n ? . \\n FOLDER \\n FIG 6 6', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 6}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 5 of 16 US 10,956,732 B2 \\n Image 1 Sub - image 1 \\n Image 2 Sub - image 2 \\n 1 I \\n 1 \\n 1 \\n Imagen Sub - imagen \\n FIG 8a', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 7}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 6 of 16 US 10,956,732 B2 \\n FIG 8C \\n 156', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 8}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 7 of 16 US 10,956,732 B2 \\n 162 \\n FIG 9a \\n FIG 9b', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 9}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 8 of 16 US 10,956,732 B2 \\n Take picture \\n 170 App \\n passes minimal requirements ? \\n 174 \\n Transmit picture \\n Determine category ( optional ) \\n Extract set of features from received image \\n Identity ( Search set of features in database ) \\n Declare Unknown \\n 184 \\n Authenticate a ROI \\n Server \\n side Suggest new ROI \\n Aggregated probability beyond a certain \\n treshold ? \\n Deliver results', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 10}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 9 of 16 US 10,956,732 B2 \\n 1 \\n 3000 \\n ODOS \\n ve \\n O \\n Soc Doo \\n 00 \\n FIG 116', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 11}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 10 of 16 US 10,956,732 B2 \\n 1022 \\n Q \\n WWW Won \\n * Y \\n X cy \\n FIG 16 FIG 17', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 12}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 11 of 16 US 10,956,732 B2 \\n1 Best Boots Wars00 \\n Move to Side \\n Please Paracetamol 500mg Tablets XXX \\n 2. W ***** WWWXXX ***** ***** \\n0 \\n FIG 19 \\n Boots Sera 530 \\n close um \\n ***** 20.10.13 22 2.0.X EZEGO & 888888888888 2021 8000H 9090 \\n FIG 21 \\n FIG 23', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 13}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 12 of 16 US 10,956,732 B2 \\n * Us \\n us \\n 3011 \\n FIG 24 \\n FIG 25 \\n Close up \\n 3020 \\n 3021 \\n FIG 26 \\n FIG 27 \\n Even 3060 \\n FIG 29 FIG 28', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 14}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 13 of 16 US 10,956,732 B2 \\n 1 \\n } - 144 14 \\n 3 \\n para \\n 144 \\n FIG 30-1 ? \\n . \\n 2 3 \\n F # . 1 \\n 17 FIG 30-2 \\n } \\n 3 \\n ?? \\n .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 15}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 14 of 16 US 10,956,732 B2 \\n } \\n FIG 30-3 \\n 1 \\n } \\n 2 \\n 3 \\n * Kit FIG 30-4 \\n 1 \\n }', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 16}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 15 of 16 US 10,956,732 B2 \\n 250 \\n using an image capturing device , capturing images of authentic products \\n 252 processing the captured images including detecting , for each authentic product , a unique chaotic signature in \\n one or more images associated with that authentic product \\n receiving , from a remote computing device , an authenticity request for a given product , the authenticity request comprising an image of the given product \\n performing a search for a pre - detected chaotic signature 256 \\n associated with one of the authentic products , within the received image of the given product \\n determining the authenticity of the given product based \\n FIG 31', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 17}), Document(page_content='U.S. Patent Mar. 23 , 2021 Sheet 16 of 16 US 10,956,732 B2 \\n 260 \\n receiving captured images of authentic products \\n 262 processing the captured images including detecting , for \\n each authentic product , a unique chaotic signature in \\n one or more images associated with that authentic product \\n 264 receiving , from a remote computing device , an authenticity request for a given product , the authenticity request comprising an image of the given product \\n performing a search for a pre - detected chaotic signature 266 associated with one of the authentic products searching within the received image of the given product ; \\n 268 determining the authenticity of the given product based \\n FIG 32', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 18}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 1 2 \\n SYSTEM AND METHOD FOR DETECTING in fact requires adding this lens to the capturing device and \\n THE AUTHENTICITY OF PRODUCTS also requires the user to know where to check for the signature part of the product ) . However , natural chaos of \\n RELATED APPLICATIONS matter already exists in most of the product categories that 5 need authentication , which is the fundamental concept This application claims priority from U.S. Provisional behind human fingerprint . Accordingly , the embodiments Application No. 62 / 082,939 filed on Nov. 21 , 2014 , the aim at capturing the existing natural chaos and dealing with specification of which is incorporated herein by reference in the difficulties that exist to generalize the process when the its entirety . end user is either not expecting to see any addition of material ( Art , luxury ) or increase in cost ( Drug manufac BACKGROUND turing ) . \\n Accordingly , a method is described which allows for ( a ) Field detecting the natural chaos and aggregating analysis from \\n The subject matter disclosed generally relates to systems various areas of the product ( hard or soft ( flexible ) ) using \\n and methods for detecting the authenticity of products . In only photos captured by portable computing devices . In \\n particular , the subject matter relates to a system and method more simplified words , the identification process acts as a \\n for uniquely identifying items so as to be able to distinguish method for detecting a unique and chaotic virtual serial \\n genuine items from counterfeit items . number , as well as a way for confirming existence or 20 non - existence of the virtual serial number .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='genuine items from counterfeit items . number , as well as a way for confirming existence or 20 non - existence of the virtual serial number . \\n ( b ) Related Prior Art In a non - limiting example of implementation , a user guided method is described which allows for a powerful Counterfeiting is a hugely lucrative business in which algorithm to be use , which algorithm , allows for a guided criminals rely on the continued high demand for cheap progressive elimination / decimation of suspicion to enable a goods coupled with low production and distribution costs . 25 vision - based authentication system that serves the purpose . Counterfeiting of items such as luxury goods , food , In other words , a fingerprint authentication system is exem alcoholic beverages , materials , art work ( paintings , sculp- plified which is generalized to cover natural chaos in prod ture ) , drugs and documents defrauds consumers and tar- ucts and goods . The system does not always allow for a nishes the brand names of legitimate manufacturers and simultaneous identification but may require first to know providers of the genuine items . Additionally , the counterfeit 30 where to search for the fingerprint . items can often endanger the public health ( for example , In an aspect , there is provided a method for determining when adulterated foods and drugs are passed off as genuine ) . the authenticity of products , the method comprising : using The Organization for Economic Cooperation and Devel- an image capturing device , capturing images of authentic opment ( OECD ) estimated the value of counterfeiting to be products ; processing the captured images including detect in the region of $ 800 billion per year worldwide , including 35 ing , for each authentic product , a unique chaotic signature in 250 billion in drugs and medical . This imposes a real burden one or more images associated with that authentic product ; on world trade estimated to 2 % of world trade for 2007 . receiving , from a remote computing device , an authenticity Anti - counterfeiting measures have included serial num- request for a given product , the authenticity request com bers , machine readable identifiers ( e.g. , barcodes and two- prising an image of the given product ; performing a search dimensional codes ) , “ tamper - proof ” security labels ( e.g. , 40 for a pre - detected chaotic signature associated with one of holograms and labels that change state or partly or com- the authentic products , within the received image of the pletely self - destruct on removal ) , and remotely detectable given product ; and determining the authenticity of the given tags ( e.g. , radio - frequency identification tags ) applied to product based on a result of the search . items directly or to tags , labels , and / or packaging for such In another aspect , there is provided a method for deter items . The principle behind the most common approach is to 45 mining the authenticity of products , the method comprising try to increase the difficulties to reproduce a specific item / receiving captured images of authentic products ; processing tag / object affixed on the goods being purchased . the captured images including detecting , for each authentic However , such measures have themselves been counter- product , a unique chaotic signature in one or more images', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='feited . associated with that authentic product ; receiving , from a Some methods exist which affix an additional label that 50 remote computing device , an authenticity request for a given contains some sort of signature of a non - reproducible mate- product , the authenticity request comprising an image of the rial , in the form of a hologram or in the form of grains of given product ; performing a search for a pre - detected cha certain colors ( like Stealth Mark ) . Some other methods exist otic signature associated with one of the authentic products which heat up a polymer to create a bubble to purposely searching within the received image of the given product ; introduce a chaotic pattern . 55 and determining the authenticity of the given product based \\n Therefore , there remains a need in the market for a more on a result of the search . secure system and method for detecting the authenticity of In a further aspect , there is provided a memory device the products being purchased . having recorded thereon non - transitory computer readable instructions for determining the authenticity of products ; the \\n SUMMARY 60 instructions when executed by a processor cause the pro cessor to process images of authentic products including The present embodiments describe such system and detecting , for each authentic product , a unique chaotic \\n method . signature in one or more images associated with that authen As discussed above , existing approaches aim at affixing a tic product ; receive , from a remote computing device , an controlled material of a certain aspect onto some hard 65 authenticity request for a given product , the authenticity surface of the product to allow for a simple signature request comprising an image of the given product ; search for detection when looked at through a magnifying lens ( which a pre - detected chaotic signature associated with one of the', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 19}), Document(page_content='US 10,956,732 B2 \\n 3 4 \\n authentic products within the received image ; and determine FIG . 3b is dose up view of an ROI region in FIG . 3a ; the authenticity of the given product based on a result of the FIG . 3c is an image transform of the image of FIG . 3b \\n search . using a gradient over a low pass filtered image , in accor In yet a further aspect , there is provided a computing dance with a non - limiting example of implementation ; device having access to a memory having recorded thereon 5 FIG . 3d show a high pass filtering of the image of FIG . 36 computer readable code for determining the authenticity of once luminance had been neutralized ; products , the code when executed by the processor of the FIGS . 4a to 4c illustrate images of a different product ; computing device causes the computing device to process FIGS . 5a to 5c illustrate images of yet a further product images of authentic products including detecting , for each and show how the chaotic aspect may be produced as a result authentic product , a unique chaotic signature in one or more 10 of the manufacturing process ; images associated with that authentic product ; receive , from FIG . 6 illustrates an example of a database including a a remote computing device , an authenticity request for a plurality of folders 1 - n , each folder containing images of given product , the authenticity request comprising an image genuine products of the same product series ; of the given product ; search for a pre - detected chaotic signature associated with one of the authentic products 15 FIG . 7 illustrates an exemplary configuration of a folder \\n within the received image ; and determine the authenticity of associated with a given product series ; \\n the given product based on a result of the search . FIGS . 8a - 8e illustrate a non - limiting example of imple \\n In yet another aspect , there is provided a memory device mentation of the guiding process ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content='In yet another aspect , there is provided a memory device mentation of the guiding process ; \\n having recorded thereon non - transitory computer readable FIG . 9a illustrates an example of a QR data matrix instructions for installing on a portable computing device 20 surrounding a chaotic signature on a cork of a bottle ; comprising an image capturing device , the instructions when FIG . 9b illustrates an example of a package including a executed by a processor causes the computing device to : QR data matrix in accordance with an embodiment ; capture an image of a given product ; send the image of the FIG . 10 is a general flowchart of the workflow between given product to a remote server for verification ; receive the app and the server , in accordance with an embodiment ; from the remote server a request to take a close - up image of 25 FIG . 11a illustrates a chaotic aspect of paper of a label the given product , and location information identifying a within vicinity and under a bar code provided on a bottle ; region of interest ( ROI ) for the close - up image ; identify the FIG . 11b is a gradient image of FIG . 11a over a small location of the ROI on the given product ; display a visual amount of pixels in which luminance is neutralized to indicator of the ROI on a display device associated with the enhance high frequencies ; computing device to allow a user to zoom over the ROI and 30 FIGS . 12 to 29 illustrate different methods for determin take the close - up picture . ing the ROI on different products ; In yet another aspect , there is provided a memory device FIGS . 30-1 to 30-4 illustrate an example of a method for having recorded thereon non - transitory computer readable determining the authenticity of a product by progressively instructions for installing on a computing device , the com- detecting a lattice defining a web comprising a plurality of puter readable instructions comprising : images of authentic 35 paths in an image ; products , each image comprising a chaotic signature which FIG . 31 is flowchart of a method for determining the is specific to an authentic product , and / or data representing authenticity of products , in accordance with an embodiment ; the chaotic signatures associated with the authentic prod- FIG . 32 is flowchart of a method for determining the ucts ; and executable instructions which when executed by authenticity of products , in accordance with another the computing device cause the computing device to : cap- 40 embodiment . ture or receive a first image of a first product ; process the It will be noted that throughout the appended drawings , first image to determine an authenticity of the first product , like features are identified by like reference numerals . wherein processing of the first image comprises detecting a presence or lack of presence of a pre - recorded chaotic DETAILED DESCRIPTION', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content='signature in the first image . Features and advantages of the subject matter hereof will Embodiments of the invention describe a system and become more apparent in light of the following detailed method for detecting the authenticity of products by detect description of selected embodiments , as illustrated in the ing a unique chaotic signature that should be intimately part accompanying figures . As will be realized , the subject matter of the product itself . As a non - limiting example , leather disclosed and claimed is capable of modifications in various 50 pore , fiber in Xray of metal goods , wood , fiber in paper . respects , all without departing from the scope of the claims . Photos of the products are taken at the plant and stored in a Accordingly , the drawings and the description are to be database / server which is accessible via a telecommunica regarded as illustrative in nature , and not as restrictive and tions network . The server processes the images to detect for the full scope of the subject matter is set forth in the claims . each authentic product a unique authentic signature which is 55 the result of a manufacturing process or a process of nature BRIEF DESCRIPTION OF THE DRAWINGS or any other process that may leave a unique signature that can be used as a unique identification of the product . To Further features and advantages of the present disclosure detect whether the product is genuine or not at the store , the will become apparent from the following detailed descrip- user may take a picture of the product and send it to the tion , taken in combination with the appended drawings , in 60 server ( e.g. using an app installed on a portable device or the which : like ) . Upon receipt of the photo , the server may process the FIG . 1 describes an example of the production registration receive image in search for a pre - detected and / or pre - stored phase in accordance with an embodiment ; chaotic signature associated with an authentic product . The FIG . 2 illustrates an example of the verification request server may return a response to the user indicating the result phase , in accordance with an embodiment ; 65 of the search . A feedback mechanism may be included to FIG . 3a illustrates an example of an image taken at the guide the user to take a picture at a specific location of the authentication phase on a Smartphone ; product where the chaotic signature may exist . 45', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 20}), Document(page_content=\"US 10,956,732 B2 \\n 5 6 \\n The authentication method may include three main turing device 102 is operably installed to take photos of the phases : 1 ) a product registration phase which usually occurs bags 104 as the bags 104 pass down the production chain . at the plant where the genuine products are manufactured , Pictures of the bags are stored on a database / server 106 whereby , images of the genuine products are taken and which is accessible via a communications network 108 . stored . 2 ) The second phase is the verification request phase 5 The image capturing device 102 can , without limitation , which occurs at the store where the product is being sold , be a smartphone equipped with a good quality camera and whereby , a user who wants to check the authenticity of a a direct telecommunication system a WiFi , Bluetooth or given product may take a picture of the product and send the internet enabled camera or any camera with wired or wire picture to the server for verification . This phase can involve less remote transmission capabilities . an Augmented Reality experience in order to guide the user 10 FIG . 2 illustrates an example of the verification request toward an area of the product that are best suited for phase , in accordance with an embodiment . As shown in FIG . authentication ( and which include one or more ROIs as 2 , when the user wants to verify the authenticity of a given described below ) . 3 ) The third phase is the comparison product 110 , the user may take a picture of the product 110 phase whereby the server compares the received image with using a portable computing device 112 and send the picture pre - stored images of genuine products and outputs the 15 of the telecommunications network 108 to the server 106 for comparison results for viewing on the user's computing comparison . \\n device . In an embodiment , an application may be provided for Phase 2 ) may involve three sub - phases : Phase 2.1 ) per- installing on the portable computing device 112 for taking a forms an identification of the product , phase 2.2 ) is a picture of the product 110 and sending the picture to the dialogue between the system and the user based on phase 2.1 20 server 106. The application may include the IP address of the to guide user toward a Zone Of Authentication ( aka Region server and may be configured to transmit the picture to the of Interest ROI ) as ( 122 ) , ( 128 ) , ( 154 ) ( 156 ) , ( 1041 ) , ( 2012 ) server once the picture is taken . The application may also be for dose range capture for capturing an image with sufficient configured to perform local image processing and compress resolution for the identification process . Phase 2.3 ) com- ing before sending the image to the server 106. In another prises capturing of the dose range image if determined to be 25 embodiment , the application may include an interactive wide enough within acceptable tolerance for slant and aspect to interact with the user and ask for additional rotation ( a purified Current Transformation Matrix ( x ' = ax pictures in specific areas of the product to improve the x + cxy + e y ' = bxx + dxy + f , where a , b , c , and d express a analysis . This will be described in further detail herein traditional rotation matrix with a zoom coefficient and e , f below . translation ) with “ a ” and “ d ” coefficient equal and “ e and “ b ” 30 Image Processing and Search at the Server coefficient dose to zero ) “ e ” and f ” coefficient being as Once the image is locally processed and / or compressed it expected for targeting the ROI . Sub Phase 2.3 may be is sent to the server for further processing and comparison performed automatically by the app when the portable with pictures of genuine products .\", metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 21}), Document(page_content='device detects that a sufficient resolution of the ROI has been In an embodiment , the server 106 may be configured to reached ( sufficient increase in size and / or number of pixels ) . 35 locally process the image of a genuine product taken in the The third phase includes reporting to the user the result of product registration phase to determine one or a plurality of the search . This phase performs the authentication , working region of interests ( ROI ) within the image these ROI on high precision analysis . The result can be done by includes a potential chaotic signature . The embodiments aggregation of probability of analysis either of various describe a non - limiting example of an image processing methods applied to same image or of various attempts to 40 method for the sake of illustration and understanding . How authenticate from different views of same object as it can be ever , the embodiments are not limited to this method and seen in FIG . 13. For example three probabilities of 50 % for may be implemented with other image processing methods three different ROI can result in a pretty high combined or a combination of the method described herein and other probability that a positive match has occurred confirming the methods . The region of interest may be chosen based on an authenticity of the product . A Neyman - Pearson lemma algo- 45 increased frequency in a given area as compared to the rest rithm allows to calculate the final probability using all of the image . In an embodiment , three histograms of fre probabilities and their associated qualities . The main quencies may be used : a global 2D , a vertical 1D , and a 1D embodiment may report the final probability as a bar graph . horizontal histogram of frequencies . A search is done on If the report delivers a probability that is beyond doubt then peak and valley of frequencies to find all relative peaks in the display is an OK message , otherwise the result takes the 50 the frequency range 1/500 of image dimension to 1/2000 of form of a bar from red to green allowing the user to evaluate image dimension . Search uses a Kalman filtering of the if they need another method of authentication or to restart histogram then extracts differences in the range of an the analysis in better environmental condition ( generally due adjusted percentage of the difference between peaks and to lighting ) valleys , for example 30 % is an acceptable range . Accord It must be noted that the 2nd phase may be done with a 55 ingly a signal that drops less than 30 % between two relative repetition of image pickup and analysis if a doubtful con- maximums will disqualify the two relative maximums as dition is determined by the machine or if the product does being peaks . This determines the coordinates of area of not have sufficient chaotic aspect on a single place ( single interest where a certain regularity exists ( high pass filter ) but ROI ) . This is exemplified on the case of a bag shown in where the higher frequencies exhibits irregularities without FIGS . 8B and 8D which show two different requests for a 60 being in the range of the noise of the image or the surface close range pickup allowing top aggregate probability out of of the good / product at registration time ( typically in a 2 the authentication process . pixels range ) . Referring now to the drawings , FIG . 1 describes an When more than one product is available , the apparatus example of the production registration phase in accordance may use a succession of operation to find commonality with an embodiment . In the present example , the product 65 between images . This is done by looking for a medium registration phase is exemplified as being a plant for manu- frequency . The server may use a Hough transform to adjust facturing luxury bags . As shown in FIG . 1 an image cap- orientation of the object and adjust scale . For example ,', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 21}), Document(page_content='US 10,956,732 B2 \\n 7 8 \\n without limitation it is possible to focus on the label of the signature . As the Mask contains local information about product , the stitches , and other areas where a unique signa- frequency contrast in an area or interest it is of interest to ture may be present . Without being mandatory , this helps the consider an histogram of the Mask itself as well as an asserting recognition of chaos in ROI which give a safer histogram of Mask frequencies allowing to characterize a determination of the identity of chaos in respective Area of 5 factor based on ratio of medium frequencies versus all interests ( ROI ) as well as giving a first set of clue as the others , that allow evaluation of the capabilities the apparatus image processing parameters to use for the signing part have to determine signature using only one area or need to analysis as it allows the analysis of chaos to work closer to aggregate over a couple of others . The more narrow is the a confirmation mode than an estimation mode . In particular frequency histogram , the more a specific area is good place the relative positioning to anchor point . This allows for a 10 to find signing chaotic pattern . simpler identification of relation between sub areas of the FIG . 3a illustrates an example of an image taken at the image that exhibit at least some signing capabilities . The registration phase . The exemplary image 120 represents a other interest of such rough common analysis is to determine hand bag . The server 106 may process the image 120 as a minimal feature set that should appear . This is crucial to discussed above to determine a ROI 122 which includes the allow a better determination of cases where the system will 15 chaotic signature . FIG . 3b is close up view of the ROI region assert that it cannot be said that the object is a genuine one , 122 in FIG . 3a . As shown in FIG . 3b , the region 122 includes but without reporting it by negation as being not genuine . It a plurality of irregularities that can be used as anchor points is then said as Undetectable . 124 as compared to the rest of the areas . The other irregu The single or multiple ROIs are characterized by a higher larities shown are identified as 125 and these constitute the frequency than the Anchor points , a noise estimation is done 20 features set . The difference between anchor points and first to determine the noise dispersion and the signal / noise features set is that the anchor points exist in all the products ratio . This allows to determine a cut off frequency for in a given line of products while the features set 125 frequencies of interest . The interesting areas for ROI are constitute the chaotic signature and may be the result of a characterized by a high contrast of frequency on relatively natural process , a manufacturing process , or a natural evo wide area ( 4 to 8 time the pixel distance of the low 25 lution once a manufacturing process is done , like a fermen frequency ) . All areas that exhibit such pattern of frequencies tation process of mildew that will have its own chaotic are grouped by connectivity and create a Mask of the ROI . structure and can worn out a copy of this structure as a worn The values used in the mask reveal the contrast for the out on a material too smooth to be analyzed , or any surrounding neighbor . This allow for weighting the quality combination of the above .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content=\"of the authentication that can be done on the ROI at a later 30 FIG . 3c Is an image transform of the image 122 using a stage . In an embodiment , the Mask is computed on every low pass filtered image using a pixels level adjustment using object individually then an intersection of every Mask or a local luminance around each pixels weighting an image ROI is done . The intersection of the Mask takes the form of gradient over this pixels . On such image a Hough transform an addition of contrast . Additionally the first Mask for a first is applied that allow to search for major line in Hough space object can profitably be enriched from experience learned 35 that intersect . Such crossing is considered as anchor point from computation of other new objects as they arrive and the quality of such crossing of Hough line is increased without requiring to re - compute it for all others . This Mask when regularly found over other sampled of production has the higher value where the contrast is not only rather goods when they exhibit same crossing in nearly same high but also very common . This creates a mix of common- vicinities . FIG . 3d show a high pass filtering of the image ality of the ROI with also the characteristics of it . This Mask 40 122 once luminance had been neutralized , another Hough is a key component to know where to consider ROIs in scan transform allow to search for commonality of lines but in that will occur during the image analysis in real time . The this case it is the complementary that are taken in account as apparatus focuses its search on areas where the Mask or ROI the feature set of chaotic points that will be metered between is high , gradually suggesting areas where the mask have them , or more generally to an anchor point . This only lower and lower values . Each degree or recognition found 45 exemplify a method for analyzing irregularities over goods for each ROI are considered in a final result , each can be that qualify a signature , But other method can be used either weighted with different factor to aggregate their similarity in statistical analysis and classification or non - parametric clas a consistent manner . Under a certain threshold value ( typi- sification of pattern , that allow at later time a simpler search . cally Number of Object * 3 % of contrast swing ) the Mask Other Examples are Provided Below . areas of no interest are nullified to avoid a scan of ROI on 50 For example , FIG . 4a illustrates an image of another areas that definitively have no reason to show similarity . So product . There is at least 3 directions of view that allow to that when the object is not genuine and all previous ROI take a picture allowing authentication . Direction 440 is scan failed individually or by cumulated weighted aggrega- typically used during the sale process and exemplified in tion to deliver an acceptable combined probability of simi- FIGS . 4b and 4c . Direction 441 is aimed toward another area larity , the system can determine that the product is not 55 of the product where sufficient chaotic aspect exists in a genuine . place that is expected to be worn out less easily during the In some cases , the location of the ROI may be substan- life of a product . This can even be used for analyzing tially the same for all products in a given product line . For passage of person either with a camera looking upward example , if the highest chaotic signature is around the ( provided that focus plane and image pre - processing location of button on the bag due the fact that the button is 60 respects people's privacy ) . It can even be used sideway installed manually , then the location of the ROI may be using the grain of the leather as a signature . The image 126 substantially the same for all the products in this series , and shown in FIG . 4b includes an area 128 which is rich with these cases are faster to search since the server does not have irregularities . When processing the image 126 , the\", metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content='in this series , and shown in FIG . 4b includes an area 128 which is rich with these cases are faster to search since the server does not have irregularities . When processing the image 126 , the server to search in other locations when the image is received from would identify the area 128 as a ROI which includes the the user . However , in most cases the location of the ROI of 65 chaotic signature . FIG . 4b is a close up view of the ROI 128 choice or set or ROIs are different for each product of the of FIG . 4a . As shown in FIG . 4b , the ROI 128 includes same series depending on where the server finds the chaotic Anchor points 131. These Anchor points are determined as', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 22}), Document(page_content='US 10,956,732 B2 \\n 9 10', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='features which exist across production ( de facto if not fore , if the received image does not have a sufficient quality existing it should warn a trigger that product is defective ) , that allows for searching for the chaotic signature , the server the other set involving more chaotic pattern is made of at may send a notification back to the portable device asking least a few of the leather pores 132 that are amongst several for another picture . The notification may be accompanied chaotic aspects including the cut 130 made in the fabric . 5 with a certain message to display to the user on the portable Combined , these irregularities define a chaotic signature device such as for example : not sufficient lighting , or too which is unique to the product associated with that picture . much lighting , or change position etc. Such chaotic signature constitutes a unique identification In an embodiment , and in order to expedite the identifi aspect which is impossible to forge reproduce , let alone to cation process and make it acceptable to reduce the waiting detect where the chaotic signature exists on the product . The 10 time for the client ( user ) who is waiting for a response , the signature may be strengthened further by creating relation- identification process only delivers one to many ( < 100 ) ships between the different chaotic aspects such as but not plausible candidates to a final processing stage for final limited to : number of chaotic aspects , distance between a comparison with the received image . In other words , if there certain chaotic aspect and another one etc. are 100,000 registered products in the database , hence at A further example is shown in FIGS . 5a and 56 which 15 least 100,000 images ( each image representing one authentic shows how the chaotic aspect may be produced as a result product ) , the identification process may chose a smaller of the manufacturing process . FIG . 5a is a top view of a subset of images e.g. 1/1000 to 1/10000 of the entire set of product . FIG . 5b is dose up view of the ROI of the product images for comparing them with the received image . The shown in FIG . 5a and FIG . 5c is dose up view of the ROI selection of this subset is based on the features set . In in an image associated with a different product from the 20 particular , the server may chose the images having features same series of the product of FIG . 5a . set that are the closest to the received image to then compare In particular , FIG . 5a illustrates a picture 136 of a shirt by each one of those images to the received image using an the brand Perry Ellis® . The picture 136 includes a ROI 138 . authentication process that is more thorough and subse FIG . 5b is close up view of the ROI 138a of the shirt of FIG . quently more CPU intensive . In an example , the server may 5a , and FIG . 5b is a close up view of the ROI 138b of 25 only chose 100 images maximum for this final comparison another shirt but in the same location . As shown at 140a the step . Therefore , the final decision by the server may be area beside the sticker “ SLIM FIT ” in FIG . 5b includes three rendered in about 10 sec not counting communication delays squares that are spaced apart from the sticker . By contrast , between the user and the server . FIG . 5c shows two squares that are partially provided under The purpose of this stage is to certify beyond a reasonable the sticker and another square that is just in touch with the 30 doubt that one or none of the identified sets offers sufficient sticker . This is an example of another type of chaotic similarity to the received image so that the outcome prob signatures that the server may detect in the pictures and look ability that the object is genuine can be asserted . The reason for in the images received from the user . of such 2 strokes action is due to the long processing time In an embodiment , the server may , based on the shape , that can require an Authentication process for all', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='images received from the user . of such 2 strokes action is due to the long processing time In an embodiment , the server may , based on the shape , that can require an Authentication process for all the recoded color and other aspects of the product , classify the products 35 products in the database which depending on the type of belonging to the same series ( aka line of products ) in a given product may reach millions . The analysis of chaotic area can folder for facilitating the search when receiving images from be either completely pattern dependent , looking for a pattern the user . For example , as shown in FIG . 6 , the database may at any place using an algorithm like a GIST , SURF or a BOF include a plurality of folders 1 - n each folder containing ( Bag Of Features ) to generate descriptors working on a images of genuine products of the same product series e.g. 40 pre - processed image . The preprocessing may be done in the PERRY FLUS® slim fit shirts , or the Monogram Delight- portable device but the GIST analysis is preferably done at ful® series by Louis Vuitton® etc. Analysis of such a the server . characteristic image can be done using either an imbedded Then the server side analyzer enters the authentication solution using a GIST kind of method or a ORBS or SURF process working on the very limited subset but using a fine method to extract descriptors feeding a LSH ( Locally Sen- 45 pattern analysis mode . The identification process uses an sitive Hashing ) tree , but can also be done using online algorithm that deliver a plausible CTM ( coordinates trans solution like the QUALCOMM Vuforia SURF implemen- formation matrix ) allowing to transform the perspective tation or MoodStock search engine implementation which distorted image that also very frequently exhibits rotation or offer solutions that are essentially able to give a rough even slant . In one embodiment the authentication process identification of the product for Phase 2.1 . 50 can use algorithm methodologies derived from fingerprint In a further embodiment , the server may associate with authentication processing taking the smears and curves out each image of a genuine product a sub - image representing of the oriented gradient processing . The fingerprint algo one of the ROI of that image as exemplified in FIG . 7. FIG . rithm delivers reasonable success and mainly very low level 7 illustrates an exemplary configuration of a folder associ- of mistakes . The penalty is that it frequently delivers a ated with a given product series . The sub - images may be 55 higher level of undetermined situations ( neither a yes nor a used for expediting the comparison process whereby the no ) . This may be reported to user who in turn should be server may compare the pixels in the received image with guided to take another snapshot . In another embodiment , it each sub - image representing each stored chaotic signature of is possible to use a specific algorithm made of speculative a genuine product until a match occurs or until the entire pattern matching in a wavelet converted image . This gives a folder is searched without any match . 60 certain flexibility to compensate for orientation beyond the Accordingly , when receiving the image from the useriapp , CTM delivered by the identification phase . the identification process begins . The server may first deter- However , an enhanced embodiment exists whereby the mine the product line , and when the product line is deter- identification process is advantageously helped by making a mined the server may start searching in the folder associated certain number of position within the image available as with that product line in order to find whether or not a 65 anchor points that can be used to determine the position previously photographed genuine product exists which has where to search for chaotic salient point . This is exemplified the same chaotic signature as in the received picture . There- in FIG . 3b at 124', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='photographed genuine product exists which has where to search for chaotic salient point . This is exemplified the same chaotic signature as in the received picture . There- in FIG . 3b at 124 , and FIG . 4b at 131 where the anchor point', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 23}), Document(page_content='US 10,956,732 B2 \\n 11 12', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='( or beacon ) are determined from points that exist and are scan to a decent minimum . The interest of combining the AR common in nearly all manufactured goods for each product , with Authentication method is to allow this high precision line as part of the design and that have no or very few chaotic position to be determined with the help of the AR system and comportment . generally asking the user to focus on some very specific area Anchor points are considered as points of interest , for 5 at very close range , This allows to find safer anchor points example constellation of peaks out of the gradient image , that allow for a simpler scanning of the image to find if a which have a common geographical location . An example of decent amount of the expected patterns from the original a criteria that can be used for the tolerance of commonality image exist in the submitted sample . of geographical location evaluated during the Anchor point The benefit of the AR recognition phase is that it can be determination can be the Chi - square of position dispersion . 10 repeated at various scales over the same object to gradually This dispersion of position is kept as a parameter that will qualify the object at closer and closer range still keeping characterize the anchor point . Anchor points sets are used to previous probability of being genuine for a progressive characterize more easily the object and optionally identify a aggregation until it reach a sufficient level or all ROI are batch or a manufacturing method . When anchor points are scanned and it failed to deliver a conform probability or available ( determined by processing the batch of images for 15 being genuine . To exemplify this , FIG . 12 shows an object a class of same product line ) then the analysis of chaotic area ( Art ) as FIG . 14 at long range recognized by the AR system . or interest can occur more easily . The relative position of The white spot like 1021 represents anchor points as found chaotic salient point of chaotic area ( ROI ) is then metered to by the recognition system . They allow to estimate a decently a reasonable number of all surrounding anchor points . The precise CTM ( current transformation matrix ) and extract value kept is the distance , the angle versus a relative horizon 20 from the AR database the Asset that must be associated ) with and the quality of dispersion of the anchor point . This this candidate to be displayed atop the Art , this suggests to information is stored in the central database . The number of the user to close up on areas of interest for Authentication Anchor points within a radius of a chaotic point determines ( ROI ) , like the area 1031 shown in FIG . 17. FIG . 15 shows a statistical minimum and maximum allowing to decode the the same object but with the closer range pickup . It had also constellation during reading and asserts a quality of recog- 25 been recognized by the AR identifier and anchor points nition based on the number of points found versus all the determined as shown by bright white spots . It quickly point that had been stored during the registration . appears that the signature of the artist is a structure that are It is also possible sometimes that Anchor points them- of a medium entropy , means are regular enough to allow a selves exhibit sufficient chaotic dispersion so that it should good chance that feature points ( descriptors ) like ORBS can be very unlikely that the positions of anchors point between 30 be found and efficiently used ( the bright white spots alike themselves are sufficient for a unique discrimination of the 1021 ) , as well as giving enough of these feature points with goods . While being true on some goods . It did appear that a rather high precision so that a pattern matching can start this direct method is setting constraints on the algorithms with a decent load on CPU because the number of tries that can be used to search for these signatures made of the needed is', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='can start this direct method is setting constraints on the algorithms with a decent load on CPU because the number of tries that can be used to search for these signatures made of the needed is thankfully limited by the high precision of posi differences of distances . While using anchor points and 35 tioning which decrease the need to try various shift and scale additionally using a pattern that should be at a certain and even rotation . bearing of the anchor points increase the flexibility offered This capability to guide user is of high interest as it is for these fuzzy searches . Too many parameters are uncon- shown on FIGS . 14 to 15. The device is started , the AR trolled while being involved in the whole chain of analysis . system performs a first recognition which allows to know For example , the moisture in the atmosphere where the 40 that at least this artwork is known by the system , then it goods are stored and also the way the goods are stored or suggests a predefined area of interest in the form a certain stacked may distorted it slightly within acceptable tolerance number of areas where the apparatus would like the user to for the end user , while it definitively changes the geometries focus on . It then displays a bullseye and an arrow to guide of the galaxy of the features set . Therefore this 2 levels the user to take a closer picture of the area that is indicated approach allows the pattern detection of chaotic features to 45 on the screen display . When a recognition is established , this be done using a smaller area for the ROI , which is better for in itself is first good sign of existence of similar products in dealing with a distorted final product , and simultaneously the database ( the initial probability of the feature set is based the presence of anchor points allows this smaller pattern area for example on the number of goods which have similar in the ROI to still be easy enough to position and then descriptors the sets for goods that are different ) . Subse correlate with the content of the central image database . 50 quently , the purpose of the next action is to qualify a certain The difficulties of the art lies in the tradeoff necessary pattern and / or intersection of patterns such as for example between the low precision position delivered by the capture the line of the signature with the texture of the paper , this system with rotation , scaling and even natural perspective natural alliance of chaos cannot be reproduced at least using distortions , compared to the high precision position needed known technics as of today . by a safe authentication system . These distortions are a 55 Once the user takes a closer picture of area 1021 then the consequence of the coarse nature of the image pickup process starts the image transform and pattern analysis . FIG . device , generally hand held device which is held without 16 shows a first level of image transform done through a great concern by consumer at time of pickup . A minimum gradient localized in a direction , this make the structure of guidance is needed and offered , as it can be seen on FIG . 12 the paper appear . The AR system will allow to help user and FIG . 13 where the bracket like 1022 does guide the user 60 focusing on this area with their own tracking mechanism and to have a picture as compatible with a 90 degrees modulus the assets here are the arrow and the bullseye ( see FIG . 13 ) . as possible while decreasing shear , slant and maximizing Then either in real time over the video feed or trough action image size . However without additional guidance either by of the camera , a picture of higher resolution will be taken on the computer using computer vision CV to establish correc- a more critical area ROI , in this particular example the ROI tion factor , or more efficiently using capabilities offered by 65 is shown as being area 1041 . the Augmented Reality Paradigm and tools , this precision of Another example is depicted by', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='the ROI tion factor , or more efficiently using capabilities offered by 65 is shown as being area 1041 . the Augmented Reality Paradigm and tools , this precision of Another example is depicted by FIGS . 18 to 23 where the pickup is largely insufficient to narrow down the samples to full process is explained . FIG . 18 shows the view from the', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 24}), Document(page_content='US 10,956,732 B2 \\n 13 14', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='smartphone side before being identified , then FIG . 19 shows and creates the authentication area that uniquely identifies the response once the package has been identified . As it is the product or gives a strong indication that the product is known to have better ROI on a side ( this will be FIG . 22 ) , authentic . the user is driven to take another view of the package as in Such numerical analysis to matching can be expected in FIG . 26. Once identified the AR embarks again and suggests 5 a simple and 1/1 relationship on some products that have a to have a close view of a ROI in FIG . 21. This allows a high serial number . Analyzing the chaotic pattern naturally or resolution image to be taken as shown in FIG . 22. FIG . 23 purposely added onto the bank note allows for an absolute depicts the image once processed and shows the anchor identification of genuine banknote in a 1/1 mode . points ( rectangles ) and the area of pattern search ( ellipse ) . The concept of anchor point allows to reduce the com The anchor points had been determined as a constant pattern 10 putational problem to avoid analysis using a topography from image to image during take - up and as maximizing the method such as the triangulation method , close to astro usage of Vertical and horizontal shape that are less likely to nomical position analysis , in which the apparatus first uses be chaotic , therefore , they can be expected to be present in a detector to identify a first object and its weighted center , almost every image of the same product line . This is also then establishes the evolution of the first object based on the exemplified on FIG . 11b where the printed barcode itself 15 metrics determined by the position acquired by triangulation naturally offers anchor points . As well as on FIG . 4c with of two other objects known to exist and which form a label and stitches . triangle qualified by angles with the first object . However , Hence the interest there is to focus on barcode , not for the many differences between the embodiments and the angular code itself but also for the alignment it provides which can based triangulation method . For example , if the other two be used as a reference for metering the pattern . In FIG . 22 20 objects are not detected the triangulation method determines it can also be seen that the manufacturer had to affix that the first object also does not exist or does not qualify . production information that allows a drastic reduction in the However , in the present embodiments very many relevant number of pattern to check against . It can be seen that objects may exist and very any fake irrelevant / fake objects patterns are expected at some specific place so that it is not may also exist at the relevant place ( ROI ) . Therefore , there of interest to scan every place for every pattern but rather use 25 exists a necessity to assemble a multitude of signature sets a pattern matching and displacement analysis . and compare them to a reference database . Accordingly , a For example , in the pharmaceutical industry , lot numbers statistical analysis is used to measure the distance between are usually in the ranges of 1000 to 100,000 while the the anchor points and the ROIs which may then create a product manufactured can include millions of units . In other plurality of probabilities associated with the different sets of words , 1000-100,000 units can have the same lot number on 30 ROIs , each set being associated with a different product . The them . Therefore , it is possible to narrow down the search plurality of probabilities may then be combine into a final from millions to 100,000 sets of patterns which can be probability . reasonably explored even with a crude enumerating process For example , a product may have 100 different patterns at ( pattern size is typically 16x16 and a CPU can compare 1 some specific ROI , and another product may have another Millions / second so that 10 second over 100 000 set allow', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='100 different patterns at ( pattern size is typically 16x16 and a CPU can compare 1 some specific ROI , and another product may have another Millions / second so that 10 second over 100 000 set allow to 35 set of patterns at a different ROI . During authentication have 10 patterns / set check at 10 position , which appears to phase , the first result of pattern matching may deliver be more than sufficient as generally there is always at least patterns coming from the first set and may very well 6 patterns that differs between products made with a sub- determine patterns coming from the second set . Therefore , strate like leather , paper , wood , clothe , embossed plastics , the misplacement of pattern should be resolved using the and in general all high chaotic substrate . It can be noted that 40 statistical analysis to authenticate the most relevant set of a tree organization of patterns allow to drastically speed up pattern as being the one having the highest number of even such enumeration process . patterns in the given ROI and the best match between pattern Another scenario is to look for the serial number ( if any ) and candidate image in each ROI found to be at the proper and search for the chaotic signature in the ROI for this distance between anchor points and pattern . product . However , not all products have serial numbers , 45 Other attempts have been made to perform authentication which necessitates the methods discussed above for narrow- using pattern matching . One of these methods uses analysis ing down the search . Other cases of unique products include of a cross section of pattern ( as if the pattern is like a fuzzy art work such as original paintings which are by default barcode ) using a classification based on weak coincidence of unique . code which is strengthened with the number of decently Another exemplification is depicted in FIGS . 24 to 29 50 close match of code . Other methods use classification meth which deal with the blister of the drug . FIGS . 24 and 26 ods based on the number of patterns that have a decent represent two different blisters and the Identification suc- image match while being simultaneously in immediate cessfully embarks on both pictures asking for a close up of vicinity of a relative other pattern matching in same condi a ROI ( 3010 ) and ( 3020 ) . The full size gradient shown on tion . However , the present embodiments do not consider the FIGS . 19 and 27 are for the sake of general understanding 55 amount of relationship between patterns in immediate vicin but the apparatus may focus on ROIs only . The information ity , but rather use classification through analysis of distance on the side of the blister ( 3011 ) and ( 3021 ) are of interest to to at least one reference point i.e. the anchor points . be decoded with a glyph recognition ( Support Vector Other methods exists which are color / intensity based Machine ( SVM ) which is a typical first stage of an OCR ) to approaches which look at a substrate to identify colors and determine if some alpha - numerical information would allow 60 their intensities in different areas on a document . These a reduction in the number of pattern to cross analyze . The method do not use the pattern identification approach per se , ROI of FIG . 28 and FIG . 29 show anchor points ( rectangles ) but rather a merged pattern intensity in which the loci of and pattern area ( ellipse ) . It must be noted that 3050 and analysis must be precise to avoid influence of nearby print 3060 are emphasizing the difference that exists between the ing . The side effect here is to decrease the requirement of two blisters . An extreme difference exists here but it is an 65 precision of location . Typically these methods are mainly exemplification of the natural manufacturing chaos occur- used with very regular products like banknotes , but do not ring during the processing that , which chaos is rare in nature properly work on productions that have wide', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='manufacturing chaos occur- used with very regular products like banknotes , but do not ring during the processing that , which chaos is rare in nature properly work on productions that have wide varieties of', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 25}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 15 16 \\n aspects like paint or variations during production , like drugs Accordingly , a decrease of constraints for qualification blisters . Some additional approaches exist which are keeps the operational quality of the authentication above the labelled as being pattern based but which in fact look at needed requirement . patterns as shapes that must exist , and not as pixels sets . Pattern searching and identification of pattern from a These shapes are looked for during authentication without 5 database is difficult to put in a generalized tree like method performing a reconstruction which requires a high quality that would allow a scan through very many pattern sets . This image pickup . The main problem with these approaches is is due to the amount of combination of patterns that create that they decrease the probability to successfully authenti- a set and the similarity between the patterns of different sets . cate if the image intake is of bad quality or not - normed . Yet In some case the recognition of the object allow to focus on an additional approach exists which is based on pattern only one reference image like in example on FIG . 14 for an analysis and which requires high magnification which inher- artwork which is unique , or as exemplified here of a lithog ently allow seizing more of chaotic nature of substrate which raphy in limited quantity , and numbered ( as exemplified by greatly simplify the issue because the variety of pattern is area 1011 which explicit the sample printed ) these cases are high enough to look for a one / one relation ( a pattern have near perfect cases as the set of pattern to check against is very small to none chance to belong to another product ) . The de - facto limited in size . But in some other case like the drugs identification of the pattern area become crucial and apply box on FIG . 18 or on the blister on FIG . 24 , the number of mainly in cases where the product itself can be checked reference images can be huge ( size of a batch lot ) , so that using a method that allows a physical analysis of the even if the apparatus narrows down the number of refer position . This approach optimizes the case of false positive 20 ences to scan through various mechanism , like the batch because the severity of the process is too high . In other number ( 2010 ) or ( 3010 ) on the blister , there is still a need words , the number of cases where a product is identified as to scan many reference images . being genuine when in fact it is not are extremely dimin- While the LSH tree allows for some organization of ished , but the penalty is the increase in the number of cases pattern for fuzzy search , another approach using a reverse where genuine products are not identified as such , which in 25 principle where for each pattern the associated data are the fact is very bad from the standpoint of the manufacturer who list of products that may contain it , is more efficient in the is generally paying for the service . main embodiment . When the amount of reference image is In a non - limiting example of implementation , the prin very high the sortation approach described above for finding ciple may accumulate the distance to anchor points , sub tracted from expected distance , thus establishing a score 30 tions ) can be of a cardinality sized to have a geometrical the best pattern ( 10 pattern / set tried for match at 10 posi \\n based on the sum of delta of distances . When a distance is beyond reasonable values , it is completely discarded . The progression with samples and requires minutes of CPU for \\n Sum Of Square differences ( SSD ) or alike norma of match processing time . Then the pattern search can benefit to create \\n ing of difference between pattern and target , are com and “ a posterior ” principle because using square of 16x16 pounded in this analysis . As an example for values used in 35 this deliver enough case where nearly each combination of', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 26}), Document(page_content='a main embodiment . If the SSD is 30 % of maximum match 16x16 square may be used , either within the same reference \\n for the analysis session then the tolerance on distance before image at various locations , or across all the reference \\n being discarded is 20 % if the pattern matching SSD is higher images . It is then of interest to use a memory based indexing \\n than 66 % then the tolerance on distance is lower in the range mechanism as this 16 * 16 square can be coded as 16 + 16 bits of 5 % . The counter - intuitive principle being that a strong 40 which would lead to 32 bits to index every combination of matching with a crude method like SSD is rather an oddity patterns . This allows to create for each of those 23224 Billion and should not be bonified . While the embodiments use the patterns a list of reference image index and their ROI SSD method , it must be noted that the embodiments are not locations which allows to speed up the search by quickly limited to this method and that other pattern matching decimating patterns that are not existing and then filter the methods are available for example in CV library like 45 commonality of position and index of the reference . The OpenCV and can be used with such approach . The maxi- position to an anchor point is a key component of the final mum for the analysis session being determined with the determination . The embodiments can properly operate using analysis of the dispersion of SSD , the regularity of high 1K square patterns to analyses ( 64x16 ) patterns , each pattern score being a strong indicator of the quality of the match set being 16 * 16 pixels over an image of 2K * 1K pixels once analysis . While a dispersion that may even include some 50 areas of no interest are eliminated . The distance of pattern to very strong matching is a negative indication of a matching anchor point is in itself a sufficient factor for classification authentication . This information may be compounded using because the aggregation of found distances subtracted from a Neural Network with a training set done using various the expected distance deliver a final score that involves 1000 substrates . It must be noted that preliminary to or during the distances which if coded over 4 bits deliver a 4K bits capturing of images of the reference images , there generally 55 precision for the index . The implementation of the main exists the possibility to inform the system about the nature embodiment does not allow a high precision because it had of the substrate in the object , which allows a user to match been evaluated from the rules in industry that 128 bits are it to a predefined category ( Wood , Leather , Cardboard , sufficient to qualify the uniqueness of an object . Accord paper , plastics , skin , woven fabrics , agglomerated grains , ingly , the embodiments only consider patterns that appear in etc ... ) so that the Trained Set used at classification for final 60 the ROI at a distance of maximum 4 pixels from the anchor aggregation of all scores , can be chosen based on this point . specificity allowing greater discrimination which allows to However , for the case where we have millions of products increase tolerance to change of shape of the object versus the to search against , even optimized pattern matching such as reference image . This is typically a more critical problem the methods discussed above , can be expensive in term of when analyzing object made of leather or cloth , while being 65 CPU bandwidth / usage to allow for a decent response time . a less critical problem because this kind of objects substrates And knowing a - priori precisely ( pixel precision ) where to naturally exhibits more randomness of the natural chaos . search for a match of pattern of a set will dramatically speed', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 26}), Document(page_content='10 \\n 15 US 10,956,732 B2 \\n 17 18 \\n up the pattern matching . The main embodiment uses a where path intersects the more path lead to a place the more specific method accelerator in this case to reduce the interesting this intersection is and should be consider as an response time . entry point for the search . The principle of this accelerator relies on a subpart of the The radius of search is typically equal to 1 square i.e. H264 compression encoding , by deriving the pattern match- 5 covers 32x32 pixels . ing from the motion estimator used in the H264 compression The computation is organized by first doing the image algorithm ( http://web.stanford.edu/class/ee398a/handouts/ processing needed to extract edges of gradient , then con lectures / EE398a_MotionEstimation2012.pdf ) . This allows verted to a skeleton of single pixels applying a kernel to use hardware assisted systems and feeding a deeply convolution to erode the paths . When the skeleton convo modified version of the encoder , keeping only the analyzer , lution is done , a process maybe started to join “ interrupted ” and where the supplied image from the Device is the lattice paths , based on a tangential approach , ( at last 3 pixels \" moving ” image and where the reference image act as a aligned allow to bridge another lattice sub path if same 3 \" previous ” frame . This allows to aggregate the vector and pixels tangent exist on the other side and the bridge doesn\\'t direction of vector to determine the accuracy of the match- need to cover more than a couple of pixels . This compen ing . sates for disappearance of continuity in the sample image This method dramatically speeds up the search and is also mainly due to poor lighting condition . Incidentally this improved by reducing the amount of tile to check focusing allows a certain analysis of the quality of the image as the on a subset determined at registration time . This set is based number of bridges needed gives an indication of the differ on a histogram of frequencies and the tile which exhibit the 20 ence of quality with the image pickup done on the reference best homogenous histogram ( modeled after a search for a image . “ flat line ” ) is the most interesting . Then path too close ( 1 pixels apart ) are joined and eroded A4Kx4K encoder working on a subset made of 1K square again . of 16x16 , allow to compute ( 4000/1000 ) * ( 4000/16 ) * 30 This complex processing is justified by the interest there Comparison / seconds which is in the range of 30000 vector 25 is to feed a massively multithreaded architecture like the one of displacement of pattern / sec , the organization of search available for CPU assistance by GPU like the NVidia GPU can also be improved with tree organized search prequali- architecture in CUDA . So that these massive amounts of fying certain aspects of pattern using a method as explained processors can all work in parallel doing the path analysis as above , this gives a factor 100 to the efficiency toward a brute well as reconsider node numbering as described above . force scan , so that a machine can compare an average of 10 30 The image processing part can also be done in the GPU . Million references images in a bit over 3 seconds . As it is This approach allows for a strong confirmation of authen more interesting to try slight changes in position of the ticity while preserving a flexibility on the aspect of the sample image ( 4 different shift and 2 scale ) this decreases image which is compatible with the poor pickup image . the throughput to an average of change 1.1 Millions / sec The performance is improved by an organization of node', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 27}), Document(page_content='which is a good high mark limits of the number of reference 35 between reference images which allow to create a tree of to scan once the AR experience allows to decrease the field similar locality of node across reference images using a very of product to explore ( a drug batch size of a common drugs similar approach as the one described above for similarity of are in the 100,000 sample range for blisters . ) square 16x16 when many reference exists . If the pattern matching method fails to deliver a suffi- An example is described in FIGS . 30-1 to 30-4 . FIGS . ciently high score at proper positions of anchors point , or in 40 30-1 to 30-4 illustrate an example of a method for deter general if a confirmation is requested using another mining the authenticity of a product by progressively detect approach the later decision can become a need due to ing a lattice defining a web comprising a plurality of paths lighting conditions , then another confirmation method in an image . The lattice may be taken out of a reference embarks using as much as possible of the information from image and FIG . 30-2 represents a lattice taken out of a the previous one in particular about positioning of the 45 candidate image . As clearly shown in FIG . 4-2 a disconti sample image . nuity exists in the line joining the nodes 1 and 3 , when This method uses the fact that most chaotic aspect out of compared to FIG . 30-1 . Also as shown in FIGS . 30-1 and the image transformation using frequency gives way to a 30-2 , three nodes exists which are numbered 1-3 between lattice like pattern . Otherwise described as a labyrinths . The the three different lines that define the lattice . The embodi search will take some lattice node and dive in the lattice of 50 ments may chose the nodes as an entry point in order to the reference image at some specific entry point . This in turn circulate the lattice to determine the authenticity of FIG . allow to aggregate a score made of the total of length of path 30-2 with respect to FIG . 30-1 . Assuming that the system that do match between the 2 lattices . chooses node 3 as entry point and mistakenly considers node The interest of this method is to simplify the search of the 3 as being node 2 due to some similarity of the pixels around best insertion point , If the insertion point for the search is 55 the two nodes . In the present case , the system may proceed badly chosen then rapidly the amount of length covered by with the processing of the lattice by choosing a path that the similarity of path become small however at each inter- starts with node 3 expecting to end up at node 1 as secting within a limited distance , the search may restart exemplified in FIG . 30-3 . However , after an expected dis using already computed path but considering them as part of tance has been travelled on the line , the system may detect the same reference node but inserted at different place in the 60 that it does not find node 1. In which case , the system may sample image . This allow to rapidly allow a re - adjustment of try a different direction as exemplified in FIG . 30-4 until it the analysis . finds node 2 and re - apply the same strategy to find node 1 , At each major crossing points the computation can be which when accomplished can make the system re - asset its done starting another thread that will re - evaluate the perti- initial travel path with a great benefit of being able to use all nence of this point as a better entry point . 65 the computations that are already pre - done by requalifying The insertion point can be such as putting grid of squares the end points and re - using the expensive computations of 16x16 then within each squares sort places if the lattice the path .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 27}), Document(page_content='US 10,956,732 B2 \\n 19 20', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='Processing at the App server to identify the potential location of the ROI may As discussed above , since the location of the chaotic include one or more of : dimensions of the ROI , surface ratio signature is not always the same for all the genuine products , between the ROI and the surface of the product on which the the server may have to search different areas within the ROI is located , and location information identifying the received image to detect the presence of a previously 5 location of the ROI within the product . The app may be recorded / photographed chaotic signature . configured to perform basic image processing to isolate the In one embodiment , the server may apply a progressive product from its background to locate the identified ROI targeting to find the area of interest in the received image . using the received information to then guide the user to take This also allows pinpointing known specific location of closer pictures thereof . interest where the frequency dispersion is optimal ( e.g. using 10 In an embodiment , in addition to the visual indicator 154 the same criteria used to detect the chaotic signature in the the app may be adapted to produce one or more audible images of the authentic products ) . Using such guidance sounds , including voice , that help guiding the user in the allows going from coarse analysis that delivers probability right orientation and / or direction until a decent picture of the amounts to target a final very precise area that exhibit a ROI is available at the screen . unique identity . This process can be done either within the 15 In an embodiment , the app may be configured to display transmitted images by looking at subsets of the image one or more visual indicators on the screen to guide the user establishing a pattern matching of a rather generic area until take a closer picture of the identification location that may a subset exhibits sufficient similarity in the sense of a Norma include a potential ROI . A non - limiting example of imple like sum of square of difference , applied either on the image mentation of the guiding process is provided in FIGS . 8a - 8e . or a transformation thereof , or can be done by requiring the 20 As shown in FIG . 8a , the user takes a picture 150 of a device 152 to adjust the pickup to a specific area . product 151 using a smartphone 152. The picture 150 may Typically the suggested area used to assist the pattern then be sent to the server for processing . If the server does analyzer use the value of the Mask of ROI computed as not find any match to a pre - recorded chaotic signature , the explained above . The aspect of such Mask exhibits an server may select based on the statistics associated with that irregular shape with peaks and valleys with a noise back- 25 line of products another location for the ROI and send the ground . Peaks are directly used as suggestion for the ROI to info to the app . The app may use the information to detect consider . This human guided method is applied very simi- the location of the ROI on the photographed product . larly within the server to consider a succession of ROI . For example , as shown in FIG . 8b , the app may highlight However , because it is executed in the server by an automat an area 154 on that need to be photographed closer . The user going from an ROI to another ROI this occurs in a period of 30 may then bring the device 152 closer to take a closer picture milliseconds for aggregating results of ROI analysis , while of the identified area as exemplified in FIG . 8c . When the using manual operation asking the user to manually take a app detects a resolution level which is equal to or above a better snapshot of the ROI , but this time it will occur the given threshold the app may then allow the user to take a rate in which the user will supply the image . However the second picture ( e.g. by activating the picture taking button ) speed in this case is not the issue as the user', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='allow the user to take a rate in which the user will supply the image . However the second picture ( e.g. by activating the picture taking button ) speed in this case is not the issue as the user understands 35 and send it to the server . As shown in FIG . 8c the surface naturally that the process requires more areas for certifica- area of the visual indicator may enlarge as the device 152 tion and at the contrary the server is not expected to approaches the ROI . The device 152 may send the close up consider multiple ROIs within its own analysis . Selection of picture 155 to the server . If no match exists , the server may a User guided focus on ROI or on a server automatic guided request another picture in a different location 156 as exem approach on ROI is based on the quality of picture taken for 40 plified in FIGS . 8d and 8e . The process may continue until analysis . Below a certain level of quality the system must a final determination is made . compensate by asking user for a closer look but on various The final determination may be one of several cases areas of the object . This also could be needed to compensate including : for poor quality picture due to poor lighting conditions Genuine : this is the case where an image was found in the In another embodiment , the server may also generate 45 database that matches the received image . statistics related to the ROIs in a given folder of pictures of Unknown : Not genuine but not certain as being faked . genuine products . Such statistics may relate to the dimen Meaning no images in the database match the received sions and locations of the ROIs for a given product series . image ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='Therefore , and as explained above , if the server could not Undetectable : the received image does not meet the detect a potential ROI in the image received from the user , 50 minimal criteria for image content , typically the the server may send information to the app installed on the expected frequency distribution . In other words , the phone to guide the user to take a closer picture of a potential received image cannot be used to make proper deter location where the ROI may exist . Examples of the type of mination ; information that the server may send to the app include : Not Genuine : there is a certain number of cases where the size / dimensions of the ROI relative to the image of the 55 good can be either replicated for same serial number product , the location of the ROI within the image , shape of including enough of the chaotic aspect , or the good is the ROI , boundaries of the ROI etc. known from other sources as being a non - registered In addition to the functions discussed above such as one . It can come from external sources and can be taking the picture and sending it to the server etc. , the app ( without limitation ) detected at 2 different places simul may be adapted to determine whether or not the image meets 60 taneously , or being sold for a first time at a place that the basic requirements of clarity and resolution e.g. avoid is not an authorized dealer , or being explicitly regis obvious cases where picture is blurry or dark etc. tered as faked if destruction of good is not the appro In addition , the app may be equipped with the intelligence priate method , or registered but not yet offered . that allows it to use the information received from the server If the image is qualified as “ Unknown ” another embodi to identify / detect the identified location ( potential ROI loca- 65 ment can submit the picture as it was taken over the web to tion ) on the product being photographed . In a non - limiting a processing center where human specialist may act as example of implementation , the information sent by the referee and do a human interpretation of the uncertainties', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 28}), Document(page_content='5 US 10,956,732 B2 \\n Šll????tch?éh? . > É??ch?ETzú?', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='either in an absolute manner or using help for comparison or not because any dislocation of the features defining the with the set of images suspected to be the goods analyzed chaotic signature would result in a negative match . The same but that cannot be confirmed . principle apply to other goods like those delivered in pre The server may then send the final determination back to filled syringe . the portable device for display in the app . The QR data embodiment facilitates the search process at In another embodiment , a local version limited to a certain the server because the size in pixels and the location of the number of digest of the signed image , like a GIST data set ROI would be known . Furthermore , the QR data allows for and pattern set , may be embedded for some specialized associating meta - information with the product . For example , check within the app used for the authentication as discussed the label if can very conveniently include or display a serial below . In the latter case , the portable device may be con- 10 number . This number will go through an OCR and consid figured to perform the entire analysis locally without send- erably eases the recognition process by asking for an image ing the image to the remote server for processing . In a based authentication , rather than image based identification . non - limiting example of implementation , the app may have The QR code can also contains additional focus informa access to the features set of a subset of the registered tion aimed at easing the camera pickup , in particular the products . The subset may be specific to a certain manufac- 15 focal plane of the object 164 can be at a very different depth turer or line of products . The subset may also be dependent than the QR code . Typically the camera will first auto focus on geographical constraints in the sense that users in a given on the easiest part of the picture , generally the high contrast . country will not need access to registered products that are Then the QR code can be read and determined the necessary shipped or scheduled to be sold or distributed in a different de - focus that would allow the camera to enforce the maxi country or different continent etc. 20 mum quality of a small part of the field of view , the aperture The latter embodiment is of particular interest when the 162. So that the quality of the signing chaotic camera pickup objects to authenticate are in limited range . This include is maximized . auctions where offered goods have pretention as for the The presence of a QR data also eases considerably the origin , inventories of luxury goods in warehouse or at analysis of horizontality of the picture at the verification borders , the transiting product between two places , e.g. 25 phase , so that pattern authentication can use algorithm of a servicing of plane engine parts etc. In these cases the amount more speculative nature . of products that are genuine candidates are limited ( 100 to Often , the product is provided in a package , and thus , it 10000 ) then the handheld device may be downloaded in would be difficult to open the package to test the authenticity advance with all the features set and authentication pattern of the product before purchasing it . In an embodiment , the necessary for authentication so that a totally offline experi- 30 QR data matrix 160 may be provided on the package and a ence can be conducted using handheld CPU for the whole visual access may be provided through the aperture 162 to processing . Methods applied for authenticating many not an area of the product defining the chaotic signature as allow to use gigantic amount of memory nor powerful GPU exemplified in FIG . 9b . FIG . 9b illustrates an example of a but on the other side the amount of pattern to check against package including a QR data matrix in accordance with an is very limited ( 1 Million squares 16x16 average ) so that a 35 embodiment . As shown in FIG . 9b , the QR data', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='of pattern to check against package including a QR data matrix in accordance with an is very limited ( 1 Million squares 16x16 average ) so that a 35 embodiment . As shown in FIG . 9b , the QR data matrix 162 good result can be achieved within a reasonable amount of provides visual access to the product included in the package time . Eventually removing reference that are check to speed 166 , thus , providing access to the chaotic signature of the later usage as the check session occurs . product without having to open the package before purchas In an embodiment , the features set may be provided in a library that the app can access for verification purposes . The 40 As exemplified in FIG . 11a the structure of the signing features set in the library may be updated periodically as pattern can be found in an area of choice like a barcode . In new products are registered and / or have reason to be made this case the structure of the paper contains sufficient dif part of the library . For example , if the product was manu- ferences from label to label so that it can be used as a signing factured and registered but not yet released . factor . The richness of such structure can be understood In an embodiment , the app may be specific to a certain 45 from FIG . 11b which is an image process that neutralizes manufacturer or line of products , or even a certain badge of luminance and enhances high frequencies trough a gradient products . In other cases , the app may be used for determin- process over a small amount of pixels . The Anchor point can ing authenticity of different and unrelated products e.g. be taken out of the edge of the Barcode , through a Hough luxury bags , and vaccination syringes . transform looking at crossing of lines , while the chaotic QR Code and Chaotic Seal 50 features are taken out of high frequencies that also show a In an embodiment , it is possible to use a QR data matrix dispersion of characteristics in other word taking a fre in addition to the chaotic seal . For example , a QR data quency range just below the highest ray found in the matrix may be provided which defines an empty space ! frequency histogram ) . aperture to place over the chaotic seal such that the chaotic The usage of barcode as a help to guide correction of seal would be surrounded by the QR data matrix . The 55 horizon is exemplified on FIG . 21 where the user is first chaotic seal may be provided in the form of a piece of wood guided with AR to find one place of interest for authentica or another object having a unique shape / signature . The QR tion , here the bar code area . It is also shown on the example data matrix may be provided on a label to be placed over the that if the system fails to authenticate the drug pack with the chaotic signature . An example is provided in FIG . 9 . barcode , before reporting to user a low probability of FIG . 9a illustrates an example of a QR data matrix 60 genuine origin , the apparatus may guide the user toward surrounding a chaotic signature on a cork of a bottle . As another are of interest like 2021 where the batch code shown in FIG.9a , AQR data matrix 160 defining an aperture appears . This can greatly help reducing the amount of 162 is provided on a bottle 164. The aperture 162 allows a reference image to compare to . visual access to a chaotic signature defined by the shape and In the case of bar code area , the bar code is an additional lines of cork provided in the bottle . Such embodiment not 65 source of information as it can identify the product uniquely only allows for identifying the authenticity of the bottle 164 reducing dramatically the search for identification in the but also to detect whether the bottle was previously opened database , simultaneously the orientation and aspect of bar up ing it .', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 29}), Document(page_content='any kind . US 10,956,732 B2 \\n 23 24 \\n code allow to correct the image hence readjust the distribu- FIG . 32 is flowchart of a method for determining the tion of frequencies on an image that are distorted like the authenticity of products , in accordance with another barrel effect shown on a bottle . This is helpful to readjust embodiment . The method comprises receiving captured image pickup perspective and field taken image pickup . It images of authentic products at step 260. Step 262 comprises can be noted on FIG . 11b that in 1126 the chaos that are part 5 processing the captured images including detecting , for each of the glass molding irregularities are of high interest as they authentic product , a unique chaotic signature in one or more \\n can be part of the chaotic feature set hence certify that not images associated with that authentic product . Step 264 \\n only the label is genuine but belonging to the proper content . comprises receiving , from a remote computing device , an \\n The picture in FIG . 11b is taken with a smartphone Samsung authenticity request for a given product , the authenticity \\n Galaxy SIV Mini® with a standard camera and no add - on of 10 request comprising an image of the given product . Step 266 comprises performing a search for a pre - detected chaotic', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='Authorized Dealer Adjustments signature associated with one of the authentic products searching within the received image of the given product . In an embodiment , it is possible to grant licensed and Step 268 comprises determining the authenticity of the given trusted dealer the authority to test the products before 15 product based on the result of the search . displaying them for sale and the authority to add pictures of While preferred embodiments have been described above the product should the server return a negative response or and illustrated in the accompanying drawings , it will be a positive response which is inconsistent . evident to those skilled in the art that modifications may be The re - registration can then be organized locally under made without departing from this disclosure . Such modifi surveillance from manufacturer , otherwise the goods may be 20 cations are considered as possible variants comprised in the returned to the plant for verification and / or registration . scope of the disclosure . FIG . 10 is a general flowchart of the workflow between For example , the process described above is not limited to the app and the server , in accordance with an embodiment . chaotic signatures and can be applied to characterize human Steps 170-174 take place at the app ( portable computing applied signatures that can be of a non - chaotic nature but device ) . Step 170 comprises taking a picture of the product 25 rather extremely difficult to reproduce , example of such can that need to be verified . At step 172 the app assesses the be inclusion of metallic structures buried within another minimal requirements of the picture taken at 170. If they are metallic object , where X ray may show the pattern . In which ok the picture is transmitted to the server at step 174 case , the manufacturing process may be complexified to a otherwise a new picture is taken ( with or without guidance ) . level where it becomes deterrent to produce a fake product . Step 176 and up take place at the server side . At step 176 30 When the chaotic nature is difficult to assert then it can be the server determines the category ( product line ) of the created on purpose and even contain a method that allow an product shown in the received picture . The received may identification of the product , an example of a technology that then be processed to extract a set of features . In an embodi- can be combined is offered by “ Stealth Mark ” which delivers ment , the set of features may be extracted using the same a product where the dispersion of grain is chaotic , whereas rules and algorithms used at the product registration step 35 the detection of the product does not involve an authenti when the images of the authentic products are processed to cation but rather an identification . extract / find the chaotic signature . At step 180 the extracted feature set is searched in the database to find a match . At step The invention claimed is : 182 the results of searching the different features are iden- 1. A computer - implemented method for determining the tified through their features set ( image descriptors like GIST 40 authenticity of products , the method comprising : this is Phase 2.1 . Then user pickup , or server focus itself on a registration phase that includes : a ROI for authentication at step 184 . receiving images of authentic products , at a server ; If , at step 186 , the combined probability is beyond a detecting anchor points present in one or more of the certain threshold the results are delivered at step 190 . images of every one of the authentic products , the Otherwise , the server may request a new picture at step 188 45 anchor points existing at common geographical loca and suggest a different location for the potential ROI and tions between the images of the authentic products in send the same back to the app for taking the new picture at a given category of the authentic products ; the specified location . The', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='ROI and tions between the images of the authentic products in send the same back to the app for taking the new picture at a given category of the authentic products ; the specified location . The experience is repeated if neces- processing the images at the server including searching sary at step 188 , result of step 184 being kept for aggregation for and detecting , for each authentic product , a until all location of the set of known place of ROI for this 50 unique chaotic signature in one or more of the identified product are scanned , or earlier if the aggregated images associated with each authentic product , the probability reached the required threshold of accuracy . server configured to analyze the images of the FIG . 31 is flowchart of a method for determining the authentic products in search for the unique chaotic authenticity of products , in accordance with an embodiment . signature that uniquely identifies the authentic prod The method begins at step 250 by capturing images of 55 uct with respect to the other authentic products , the authentic products using an image capturing device . Step unique chaotic signature naturally existing in the 252 comprises processing the captured images including authentic product without adding or affixing material detecting , for each authentic product , a unique chaotic onto the authentic product ; signature in one or more images associated with that authen- for each authentic product , measuring relative positions tic product . Step 254 comprises receiving , from a remote 60 between the unique chaotic signature and one or computing device , an authenticity request for a given prod more of the anchor points , the relative positions uct , the authenticity request comprising an image of the including a distance and an angle between the unique given product . Step 256 comprises performing a search for chaotic signature and each anchor point ; a pre - detected chaotic signature associated with one of the storing the anchor points and the unique chaotic sig authentic products , within the received image of the given 65 natures associated with the authentic products and product . Step 258 comprises determining the authenticity of the relative positions associated with each unique the given product based on the result of the search . chaotic signature in a database ; and', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 30}), Document(page_content='5 \\n 10 \\n 15 US 10,956,732 B2 \\n 25 26 \\n an authentication phase that includes : searching , within the image of the given product , for the receiving at the server , an authenticity request for a unique chaotic signature associated with authentic given product from a remote computing device , the products pertaining only to the detected product - line . authenticity request comprising an image of the 11. The method of claim 1 , further comprising : given product ; providing a QPR matrix defining an aperture on the given performing a search within the image of the given product ; and product for one or more of the anchor points and the providing visual access to the unique chaotic signature of unique chaotic signature within the relative positions the given product through the aperture of the QPR of the one or more anchor points ; matrix such that the QPR matrix surrounds the unique determining that the given product is authentic if the chaotic signature of the given product . unique chaotic signature found in the image of the 12. The method of claim 1 , further comprising : given product matches one of the chaotic signatures providing a QPR matrix defining an aperture on a package \\n stored in the database . containing the given product , wherein the package 2. The method of claim 1 , wherein determining that the contains a visual access to an area of the given product given product is authentic comprises comparing the unique containing the unique chaotic signature of the given chaotic signature of the given product to the unique chaotic product ; and signatures of a sub - group of the authentic products stored in providing visual access to the unique chaotic signature of \\n the database . the given product through the aperture of the QPR 3. The method of claim 1 , wherein performing the search 20 matrix such that the QPR matrix surrounds the unique within the image of the given product comprises using a chaotic signature of the given product . \\n hardware accelerator built with a pattern analyzer embedded 13. The method of claim 1 , wherein the unique chaotic with a hardware video encoder . signatures are the result of a manufacturing process , a 4. The method of claim 3 , further comprising aggregating process of nature , or both thereof . motion estimation vectors to create a final score to determine 25 14. The method of claim 1 , wherein the registration phase the authenticity of the given product based on a sum of occurs at a plant where the authentic products are manufac normals of the vectors of the motion estimator . tured , or at a licensed dealer . 5. The method of claim 1 , further comprising finding , in 15. The method of claim 1 , wherein detecting the unique a frequency transformation of a given image of an authentic chaotic signature of each authentic product comprises per product designated nodes of a first lattice found in the given 30 forming a progressive targeting of at least one of the images image of the authentic product ; to find an area of interest where a frequency dispersion is finding an entry point in a second lattice found in the optimal . given image , travelling a given path of the second lattice for a given 16. A computer - implemented method for determining the \\n distance ; authenticity of products , the method comprising : \\n if a node is not found within the given distance , a direction a registration phase that includes : \\n of travel is changed to explore other parts of the second receiving , at a server , captured images of authentic \\n lattice until a number of nodes is found in the given products ; \\n image which match with the designated nodes of the detecting anchor points present in one or more of the \\n given image of the authentic product in position and 40 captured images of every one of the authentic prod', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 31}), Document(page_content='given image of the authentic product in position and 40 captured images of every one of the authentic prod \\n length of path between nodes . ucts , the anchor points existing at common geo 6. The method of claim 1 , further comprising requesting graphical locations between the images of the another image of the given product from the remote com authentic products in a given category of the authen puting device . tic products ; \\n 7. The method of claim 1 , further comprising : storing the detected anchor points ; identifying at least one region of interest ( ROI ) within the processing the captured images at the server including image of the given product ; detecting and storing in a database , for each authen requesting a close - up image of the ROI from the remote tic product , a unique chaotic signature in one or more computing device ; and of the images associated with each authentic product , searching within the close - up image for the unique cha- 50 the server configured to analyze the captured images otic signature associated with the given product . of the authentic products in search for the unique 8. The method of claim 7 , wherein identifying the ROI chaotic signature that uniquely identifies the authen comprises : tic product with respect to the other authentic prod processing the image of the given product using a set of ucts , the unique chaotic signature naturally existing rules used to find the unique chaotic signature in the 55 in the authentic product without adding or affixing images stored in the database ; and material onto the authentic products ; estimating a potential location of the unique chaotic for each authentic product , measuring relative posi signature of the given product , the potential location tions between the unique chaotic signature and representing the ROI . two or more of the anchor points , the relative 9. The method of claim 7 , wherein identifying the ROI is 60 positions including at least one of a distance and done based on statistical information relating to locations of an angle between the unique chaotic signature and the unique chaotic signatures in a given line of products . each anchor point ; and 10. The method of claim 1 , further comprising : an authentication phase that includes : classifying the images of the authentic products into one receiving , at the server , an authenticity request for a or more product - lines ; given product from a remote computing device , detecting a product - line associated with the given product the authenticity request comprising an image of shown in the image of the given product ; and the given product ; 35 \\n 45 \\n 65', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 31}), Document(page_content='US 10,956,732 B2 \\n 27 28 \\n performing a search within the image of the given unique chaotic signature in one or more images product for the anchor points and the unique associated with that authentic product , the comput chaotic signature within the relative positions of ing device configured to analyze the images of the the anchor points ; authentic products in search for the unique chaotic determining that the given product is authentic if the 5 signature that uniquely identifies the authentic prod unique chaotic signature found in the image of the uct with respect to the other authentic products , the given product matches one of the unique chaotic unique chaotic signature naturally existing in the signatures stored in the database . authentic product without adding or affixing material 17. The method of claim 16 , wherein detecting the unique onto the authentic product ; chaotic signature of each authentic product comprises per- 10 measure relative positions between the unique chaotic forming a progressive targeting of at least one of the images to find an area of interest where a frequency dispersion is signature and two or more anchor points , the relative \\n optimal . positions including at least one of a distance and an \\n 18. A memory device having recorded thereon non angle between the unique chaotic signature and each \\n transitory computer readable instructions for determining 15 anchor point ; \\n the authenticity of products , the instructions when executed store the anchor points , the unique chaotic signatures of \\n by a computer cause the computer to : the authentic products and the relative positions perform a registration phase that includes : associated with each unique chaotic signature and \\n receive and process images of authentic products the anchor points in a database ; and including detecting , for each authentic product , 20 perform an authentication phase that includes : anchor points present in one or more of the images receive , from a remote computing device , an authen of every one of the authentic products , the anchor ticity request for a given product , the authenticity points existing at common geographical locations request comprising an image of the given product ; between the images of the authentic products in a search within the image of the given product for the given category of the authentic products , and a 25 anchor points and unique chaotic signatures within unique chaotic signature in one or more of the the relative positions of the anchor points ; and images associated with each authentic product , the determine that the given product is authentic if the computer configured to analyze the images of the unique chaotic signature found in the image of the authentic products in search for the unique chaotic given product matches one of the unique chaotic signature that uniquely identifies the authentic prod- 30 signatures stored in the database . uct with respect to the other authentic products , the unique chaotic signature naturally existing in the 20. The computing device of claim 19 , wherein the \\n authentic product without adding or affixing material computing device is adapted to request another image of the \\n onto the authentic product ; given product from the remote computing device . \\n measure relative positions between the unique chaotic 35 21. The computing device of claim 19 , wherein the \\n signature and two or more anchor points , the relative computing device is adapted to : \\n positions including at least one of a distance and an identify a region of interest ( ROI ) within the image of the \\n angle between the unique chaotic signature and each given product ;', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 32}), Document(page_content='positions including at least one of a distance and an identify a region of interest ( ROI ) within the image of the \\n angle between the unique chaotic signature and each given product ; \\n anchor point ; request a close - up image of the ROI from the remote store the anchor points , the unique chaotic signatures of 40 computing device ; the authentic products and the relative positions search within the close - up image for the unique chaotic associated with each unique chaotic signature and signature associated with the given product . the anchor points in a database ; and 22. The computing device of claim 21 , wherein the perform an authentication phase that includes : computing device is adapted to process the image of the receive , from a remote computing device , an authen- 45 given product using a set of rules used to find the unique ticity request for a given product , the authenticity chaotic signature in the images stored in the database ; and request comprising an image of the given product ; estimate a potential location of the unique chaotic signature search within the image of the given product for the of the given product , the potential location representing the anchor points and unique chaotic signatures within ROI . the relative positions of the anchor points ; and 23. The computing device of claim 21 , wherein the ROI determine that the given product is authentic if the is identified based on statistical information relating to unique chaotic signature found in the image of the locations of the unique chaotic signatures in a given line of given product matches one of the unique chaotic products . signatures stored in the database . 24. The computing device of claim 19 , wherein the 19. A computing device having access to a memory 55 computing device is adapted to : having recorded thereon computer readable code for deter- classify the images of the authentic products into one or mining the authenticity of products , the code when executed more product - lines ; by a processor of the computing device causes the comput- detect a product - line associated with the given product ing device to : shown in the image of the given product ; and perform a registration phase that includes : search , within the image of the given product , for the receive and process images of authentic products unique chaotic signature associated with authentic including detecting , for each authentic product , products pertaining only to the detected product - line . anchor points present in one or more of the images 25. The computing device of claim 19 , wherein the of every one of the authentic products , the anchor computing device is adapted to search for the unique chaotic points existing at common geographical locations 65 signature of the given product in a QPR matrix defining an between the images of the authentic products in a aperture providing visual access to the unique chaotic sig given category of the authentic products , and a nature of the given product . 50 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 32}), Document(page_content='30 US 10,956,732 B2 \\n 29 \\n 26. The computing device of claim 19 , wherein detecting the unique chaotic signature of each authentic product comprises performing a progressive targeting of at least one of the images to find an area of interest where a frequency dispersion is optimal . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/9c/e5/fc/eb90d460e518de/US10956732.pdf', 'page': 33})]\n",
      "[Document(page_content=\"US010504073B2 \\n United States Patent \\n Atsmon et al . ( 10 ) Patent No .: US 10,504,073 B2 \\n ( 45 ) Date of Patent : Dec. 10 , 2019 \\n ( 56 ) References Cited ( 54 ) SYSTEM AND PROCESS FOR AUTOMATICALLY ANALYZING CURRENCY \\n OBJECTS U.S. PATENT DOCUMENTS \\n ( 76 ) Inventors : Alon Atsmon , Qiryat Ono ( IL ) ; Dan Atsmon , Rehovot ( IL ) 7,522,768 B2 7,526,117 B2 2004/02 10529 Al 2005/0156942 A1 \\n 2007/0255662 Al \\n 2008/0046410 A1 \\n 2008/0219543 A1 * 4/2009 Bhatti et al . 4/2009 Foth \\n 10/2004 Wu \\n 7/2005 Jones \\n 11/2007 Tumminaro \\n 2/2008 Lieb \\n 9/2008 Csulits ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 0 days . G06K 9/033 \\n 382/135 \\n 2009/0252371 Al ( 21 ) Appl . No .: 13 / 353,791 10/2009 Rao \\n ( Continued ) \\n ( 22 ) Filed : Jan. 19 , 2012 FOREIGN PATENT DOCUMENTS \\n ( 65 ) Prior Publication Data WO \\n US 2012/0185393 A1 WO Jul . 19 , 2012 WO 2004/027695 4/2004 \\n WO / 2006 / 022513 3/2006 \\n ( Continued ) \\n Related U.S. Application Data \\n ( 60 ) Provisional application No. 61 / 438,993 , filed on Feb. 3 , 2011 , provisional application No. 61 / 433,995 , filed on Jan. 19 , 2011 , provisional application No. \\n 61 / 548,267 , filed on Oct. 18 , 2011 . Primary Examiner \\n Assistant Examiner - Jason Borlinghaus \\n Ambreen A. Alladin \\n ( 57 ) ABSTRACT \\n ( 51 ) Int . Cl . G06Q 20/10 ( 2012.01 ) G06Q 20/32 ( 2012.01 ) GO6Q 20/38 ( 2012.01 ) G06Q 20/40 ( 2012.01 ) \\n G06K 9/00 ( 2006.01 ) GO7D 7700 ( 2016.01 ) GO7D 11/30 ( 2019.01 ) ( 52 ) U.S. CI . CPC G06Q 20/10 ( 2013.01 ) ; G06K 9/00 ( 2013.01 ) ; G06Q 20/3276 ( 2013.01 ) ; G06Q 20/389 ( 2013.01 ) ; G06Q 20/40 ( 2013.01 ) ; GO7D 7700 ( 2013.01 ) ; GO7D 11/30 ( 2019.01 ) ( 58 ) Field of Classification Search USPC 705/44 See application file for complete search history . A method , system , and computer program product for ana lyzing images of visual objects , such as currency and / or payment cards , captured on a mobile device . The analysis allows determining the authenticity and / or total amount of value of the currency and / or payment cards . The system may be used to verify the authenticity of hard currency , to count the total amount of the currency captured in one or more images , and to convert the currency using real time mon etary exchange rates . The mobile device may be used to verify the identity of a credit card user by analyzing one or more images of the card holder's face and / or card holder's signature , card holder's name on the card , card number , and / or card security code . \\n 23 Claims , 4 Drawing Sheets \\n Currency 05 \\n Cortina \\n Visus : 3G \\n Analyze captured image \\n Serious 6 \\n Serve ) 206 \\n Server Oraivis \\n Capro \\n axitar ? March found \\n N 216 \\n END Coxnvort to wesot \\n Cur761107 210 \\n Gwerte woon \\n and aos\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 0}), Document(page_content='US 10,504,073 B2 \\n Page 2 \\n ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n 2010/0008535 A1 * 1/2010 Abulafia G06K 9/2054 \\n 382/100 2010/0082470 A1 * 4/2010 Walach G06Q 20/0425 705/35 \\n 2010/0113091 A1 * 5/2010 Sharma G06K 9/4642 \\n 455 / 556.1 2010/0331043 A1 * 12/2010 Chapman G01C 21/20 \\n 455 / 556.1 2011/0091092 A1 * 4/2011 Nepomniachtchi .. G06K 9/3275 \\n 382/139 \\n 2011/0099107 Al 4/2011 Saxena 2011/0119141 A1 5/2011 Hoyos \\n 2013/0022264 A1 1/2013 Atsmon et al . \\n FOREIGN PATENT DOCUMENTS \\n WO \\n WO \\n WO \\n WO \\n WO WO / 2008 / 126937 \\n WO / 2008 / 147896 \\n WO 2009/137830 WO / 2011 / 032263 \\n WO / 2011 / 047034 10/2008 \\n 12/2008 \\n 11/2009 \\n 3/2011 \\n 4/2011 \\n * cited by examiner', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 1}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 1 of 4 US 10,504,073 B2 \\n 128 \\n 132 \\n Internet \\n www \\n 2003 \\n Currency inventory \\n Note Comments \\n 14.00 USD \\n 100 USD \\n 50 Pilasters 0.10 USD \\n 114.10 USD \\n Figure 1', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 2}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 2 of 4 US 10,504,073 B2 \\n Currency DB \\n Analyze captured \\n Server Analysis \\n Capture \\n Convert to preset currency \\n Generate report \\n Display report \\n Fiqure 2', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 3}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 3 of 4 US 10,504,073 B2 \\n Bank temporary deposit \\n Verify SN \\n Deposit \\n Figure 3', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 4}), Document(page_content='U.S. Patent Dec. 10 , 2019 Sheet 4 of 4 US 10,504,073 B2 \\n Capture 402 \\n Symbol analysis \\n Charge Card \\n Show Charge \\n Figure 4', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 5}), Document(page_content='US 10,504,073 B2 \\n 2 \\n SYSTEM AND PROCESS FOR may further comprise converting the currency in each image AUTOMATICALLY ANALYZING CURRENCY to another monetary currency based on real time world \\n OBJECTS exchange rates . The conversion is preset by the terminal user , the mobile device provider , and / or automatically by the \\n CROSS REFERENCE TO RELATED 5 device based upon location based analysis . \\n APPLICATION The method comprises : capturing a visual object image on a terminal , wherein each image is associated with a particu The present application claims priority benefit under 35 lar object of known authenticity ; conducting a content U.S.C. $ 119 ( e ) to U.S. Provisional Patent Application No. analysis on the captured image ; determining the quantity and 61 / 433,995 filed Jan. 19 , 2011 by Alon Atsmon , entitled 10 authenticity of the captured object based on the content \" System and Process for Automatically Analyzing Currency match , wherein a match exists if the content analysis is Objects ” , No. 61 / 438,993 filed Feb. 3 , 2011 by Alon Ats above a designated threshold for authenticity ; and transmit mon , entitled “ System and Process for Automatically Ana ting an electronic report to the terminal indicating the lyzing Currency Objects ” , and No. 61 / 548,267 filed Oct. 18 , content match . 2011 by Alon Atsmon , entitled “ Automatic Method and 15 Content analysis is conducted using keypoint descriptors System for Visual Analysis of Object Against Preset ” . The as defined herein , and further comprises comparing the present application incorporates the foregoing disclosures image\\'s captured text , visual and symbol data , and option \\n herein by reference . ally other data such as GPS data , the history of the sender and a database of known fake visual objects . Additionally , \\n BACKGROUND 20 the digital images are captured with an electronic commu nications device ( i.e. terminal ) ; to include using terminals 1. Technical Field with a predefined array of cameras to construct a three The present invention relates to systems and processes for dimensional ( 3D ) representation of the object . automatically analyzing currency objects . In a preferred embodiment of the present invention , a user', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content=\"2. Discussion of the Related Art 25 makes a pending deposit to their financial institution by The prior art discloses methods for utilizing mobile transmitting an image of cash or checks captured on their communications devices to process a credit card or debit terminal to their institution account . They subsequently visit card payment . For example , Square , Inc. offers credit card the institution to make the actual deposit , wherein the teller readers that are connected to a mobile device . ( See WIPO will validate the amount and authenticity of the deposit Patent Application Wo / 2011 / 047034 ) . The card reader is 30 before converting the pending deposit to a fully credited configured to read data encoded in a magnetic strip of a deposit credit card and provide a signal that corresponds to the data In another preferred embodiment of the present invention , read to the mobile device , which then decodes the incoming the authenticity of a card , such as a credit or debit card , is signal from the card reader and acts as a point - of - sale device determined by capturing an image of the card on a User's to complete the financial transaction . 35 terminal . An analysis of the card validity is based upon an The prior art also discloses the use of mobile devices to image comprising the name on the card , an embossed credit verify the identity of customers . For example , Siccolla , Inc. card number , expiration date and CVV number and signature offers a mobile device with an identity verification tool on the card . And in addition to capturing an image of the built - in ( See United States Patent Application card , the User's electronic communications device can cap 20110119141 ) . The wireless phone has a specialized built - in 40 ture an image of a card holder's signature executed on the fingerprint sensor , camera ( s ) , and blood sensor to acquire device's screen and a card holder's face and compare them images of biometrics to perform identity verification in order to comparable images stored in the system database . to prevent identity theft and financial fraud during commer Other aspects of the invention may include a system cial transactions . arranged to execute the aforementioned methods and a Both of these products require hardware modification of 45 computer readable program to include a mobile application the mobile device . The prior art also fails to provide a configured to execute the aforementioned methods . These , computer program product and system for use with a mobile additional , and / or other aspects and / or advantages of the device that does not require a hardware modification of the embodiments of the present invention are set forth in the device in order to : 1 ) verify the user of a payment card for detailed description which follows ; possibly inferable from the purpose of preventing identify theft , and 2 ) verify that 50 the detailed description ; and / or learnable by practice of the cash payments are not used with counterfeit currency . Nor embodiments of the present invention . does the prior art provide a mobile device that combines fraud prevention using instantaneous imaging processing of BRIEF DESCRIPTION OF THE DRAWINGS\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content=\"hard currency captured on a device's camera or web - cam , with the ability to count the amount of the currency in an 55 The present invention will now be described in the image , and to convert the amount to another currency using following detailed description of exemplary embodiments of monetary exchange rates in real time . the invention and with reference to the attached drawings , in which dimensions of components and features shown are BRIEF SUMMARY chosen for convenience and clarity of presentation and are 60 not necessarily shown to scale . Generally , only structures , The present invention provides a method , system , and elements or parts that are germane to the discussion are computer program product for analyzing images of an object shown in the figure . ( money , credit cards , etc. ) captured on an electronic com FIG . 1 is a scheme describing the system and process in munications device ( terminal ) , such as a mobile phone accordance with an exemplary embodiment of the invention . camera or laptop web - cam , to quantify their face value , and 65 FIG . 2 is a flowchart of acts performed in capturing and optionally to determine if they are authenticate — not coun matching a visual object , in accordance with an exemplary terfeit or stolen cards . The method of the present invention embodiment of the invention .\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 6}), Document(page_content='15 \\n 30 US 10,504,073 B2 \\n 3 4 \\n FIG . 3 is a flowchart of acts performed in accordance with each descriptor contains an array of 4 histograms around the an exemplary embodiment of the invention to make a band keypoint . This leads to a SIFT feature vector with ( 4x4x deposit . 8 = 128 elements ) . FIG . 4 is a flowchart of acts performed in accordance with The term “ Visual content item ” as used herein in this an exemplary embodiment of the invention to charge a credit 5 application , is defined as an object with visual characteris \\n card . tics such as an image file like BMP , JPG , JPEG , GIF , TIFF , and PNG files ; a screenshot ; a video file like AVI , MPG , DETAILED DESCRIPTION MPEG , MOV , WMV , and FLV files or a one or more frame \\n of a video . Provided herein is a detailed description of this invention . 10 The term “ visual object \" as used herein in this application , It is to be understood , however , that this invention may be is defined as a content that includes visual information such embodied in various forms , and that the suggested ( or as visual content item , images , photos , videos , IR image , proposed ) embodiments are only possible implementations magnified image , an image sequence or TV broadcast . ( or examples for a feasible embodiments , or materializa The term \" currency object ” as used herein in this appli tions ) of this invention . Therefore , specific details disclosed cation , is defined as a physical object having monetary value herein are not to be interpreted as limiting , but rather as a such as paper money , coin , medal , share certificate and basis and / or principle for the claims , and / or as a represen bonds . \\n tative basis for teaching one skilled in the art to employ this The term \" camera ” as used herein in this application is invention in virtually any appropriately detailed system , 20 defined as means of capturing a visual object .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content='structure or manner . The term “ terminal ” as used herein in this application is defined as an apparatus adapted to show visual content such Glossary of Terms as a computer , a laptop computer , mobile phone TV . The term “ visual similarity ” as used herein in this appli To facilitate understanding the present invention , the 25 cation , is defined as the measure of resemblances between following glossary of terms is provided . It is to be noted that two visual objects that can be comprised of : terms used in the specification but not included in this The fit between their color distributions such as the glossary are considered as defined according the normal correlation between their HSV color histograms usage of the computer science art , or alternatively according The fit between their texture to normal dictionary usage . The fit between their shapes The term “ image ” as used herein in this application is The correlation between their edge histograms defined as visual representation that can be presented on two Face similarity dimensional or three dimensional surfaces . Images can be Methods that include local descriptors ( such as keypoint taken in any part of the electromagnetic spectrum such as descriptors ) and such as SIFT , ASIFT , SURF and MSR visible light , infrared , ultraviolet , X - rays , Terahertz , Micro- 35 The term “ Visual analysis ” as used herein in this appli waves , and Radio frequency waves . cation , is defined as the analysis of the characteristics of The term “ photo \" as used herein in this application is visual objects such , as visual similarity , coherence , hierar defined as image in the visible light . chical organization , concept load or density , feature extrac The term \" GPS ” as used herein in this application , is 40 tion and noise removal . defined as a system based on satellites that allows a user with The term “ Text similarity ” as used herein in this appli a receiver to determine precise coordinates for their location cation , is defined as a measure of the pair - wise similarity of on the earth\\'s surface . strings . Text similarity can score the overlaps found between The term “ GPU ” as used herein in this application , is two strings based on text matching . Identical strings will defined as an apparatus adapted to reduce the time it takes 45 have a score of 100 % while “ car ” and “ dogs ” will have close to produce images on the computer screen by incorporating to zero score . “ Nike Air max blue ” and “ Nike Air max red ” its own processor and memory , having more than 16 CPU will have a score which is between the two .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content=\"cores , such as GeForce 8800 . The term “ Regular expression ” as used herein in this The term “ Keypoint ” as used herein in this application , is application , is defined as a string that provides a concise and defined as interest points in an object . For example , in the 50 flexible means for identifying strings of text of interest , such SIFT framework , the image is convolved with Gaussian as particular characters , words , or patterns of characters . filters at different scales , and then the difference of succes The term “ Text analysis ” as used herein in this applica sive Gaussian - blurred images are taken . Keypoints are then tion , is defined as the analysis of the structural characteris taken as maxima / minima of the Difference of Gaussians . tics of text , as text similarity , coherence , hierarchical orga Such keypoint can be calculated for the original image or for 55 nization , concept load or density . Text analysis can use a transformation of the original image such as an affine regular expressions . transform of the original images . The term “ OCR ” as used herein in this application , is The term “ Keypoint descriptor ” as used herein in this defined is the process by which a computer attempts to application , is defined as a descriptor of a keypoint . For match up parts of an electronic image , with characters , such example , in the SIFT framework the feature descriptor is 60 as letters , to produce text . computed as a set of orientation histograms on neighbor The term “ Symbol analysis ” as used herein in this appli hoods . The orientation histograms are relative to the key cation , is defined as analysis of symbolic data such as : OCR , point orientation and the orientation data comes from the hand write recognition , barcode recognition , and QR code Gaussian image closest in scale to the keypoint's scale . Just recognition . like before , the contribution of each pixel is weighted by the 65 The term “ Capturing data analysis ” as used herein in this gradient magnitude , and by a Gaussian with o 1.5 times the application , is defined as the analysis of capturing data such scale of the keypoint . Histograms contain 8 bins each , and as :\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 7}), Document(page_content='5 \\n 10 \\n 15 US 10,504,073 B2 \\n 5 6 \\n X - Y - Z coordinates System for Analyzing Images 3 angles FIG . 1 is a scheme describing the system 100 and process \\n Manufacturer in accordance with an exemplary embodiment of the present Model invention for use in verifying that a form of payment ( i.e. Orientation ( rotation ) top - left currency and payment card ) is valid and to count the amount \\n Software in an automated manner . \\n Date and Time Terminal 101 , such as a mobile phone with camera 102 , YCbCr Positioning centered captures a visual object , of object set 120 comprising paper Compression money bills 122 , 124 and coin 128 . \\n X - Resolution Optionally , object set 120 includes payment card 130 Y - Resolution having visual details 132 such as embossed credit card Resolution Unit number , expiration date and CVV number and card holder Exposure Time name . Optionally , the owner of 132 also signs its name on FNumber terminal 101 or upon deal confirmation . Exposure Program The Capturing can be performed in several ways : 1 ) \\n Exif Version taking a photograph ; 2 ) recording a video ; and 3 ) Continu Date and Time ( original ) ously capturing an image while local or remote processing Date and Time ( digitized ) provides real time feedback such \" currency not fake ” or “ a Components Configuration Y Cb Cr 20 problem was found ” . The continuous capturing can be done Compressed Bits per Pixel while moving the camera such as moving in the directions Exposure Bias shown in 103 . Max Aperture Value The visual object can be captured from a static camera Metering Mode Pattern placed in the marketplace or from a camera held by person Flash fired or not 25 112. Person 112 can be a crowd of people that were Focal Length incentivized to capture the currency object . The visual \\n MakerNote objects 120 comprise recognized world currencies , such as \\n FlashPix Version a Euro Note 122 and U.S. bill 124 , and coins 128. The paper Color Space currency 122 , 124 may also comprise SN 121 , which are PixelXDimension 30 unique identifiers — letters , numbers , threads and symbols Pixel Y Dimension used to authenticate the validity of a currency , and deter', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 8}), Document(page_content=\"File Source counterfeits . Interoperability Index The visual object can be processed locally using the Interoperability Version User's terminal 101 , or it can be sent to a remote server 108 , Derivates of the above such as acceleration in the X - axis 35 as described in step 206 in FIG . 2 , over a network 106 such The term “ Service location ” as used herein in this appli as the internet . cation , is defined as a physical place where objects can be Server 108 or device 101 calculates a currency inventory serviced and / or fixed such as a mobile carrier service center . report 140 that is sent over the internet or created locally . The term “ Location based analysis ” as used herein in this Report 140 shows the currency object identity found , their application , is defined as analysis of local data such as GPS 40 value in a preset conversion currency and the total value location , triangulation data , RFID data , and street address . found in the capturing session . Location data can for example identify the service location A usage case would be that a person takes a photo of 100 or even the specific part of the service location in which the euro note 122 with his mobile device 101. The photo is sent visual object was captured . to a remote server 108 that uses a currency objects database The term “ Content analysis ” as used herein in this appli- 45 to match the objects photographed . Then a currency inven cation , is defined as the combination of text analysis , visual tory report 140 is displayed on device 101 . analysis , symbol analysis , location based analysis , capturing Terminal 101 can also capture a visual object 120 com data analysis and / or analysis of other data such as numerical prising a payment card 130 having visual details 132 , such fields ( price range ) , date fields , logical fields ( Female / male ) , as embossed credit card number , expiration date and CVV arrays and structures , and analysis history . 50 number and card holder name . Optionally , in addition to The term “ Content Match ” as used herein in this appli capturing an image of the card , terminal 101 can also capture cation , is defined as a numerical value that describes the the card holder's signature executed on the terminal 101's results of the content analysis that computes the similarity screen ( graphical interface ) and optionally an image of the between one or more visual objects , or a logical value that card holder's face and submit them to the server to verify the is true in case said similarity is above a certain threshold . 55 card holder is the card owner of record . The term “ marketplace ” as used herein in this application , Mobile Application is defined as a physical place where objects can be bought The present invention further comprises a software appli such as a bank , a change point , a supermarket , a convenience cation loaded onto the User's terminal 101 ( e.g. a mobile store and a grocery store . communications device , such as a smartphone ) configured The term “ Bank ” as used herein in this application , is 60 to communicate with the system server 108 , such as over a defined as a financial institution that accepts deposits . wireless communications network . The application may be The term “ Payment Card ” as used herein in this applica native or web based . The User's device enables the User to tion , is defined as a Card used to make payments such as a instantly transmit an image of the visual object 120 to the debit card , a credit card or a loyalty card . system server 108 , and to receive notifications from the The term “ SN ” as used herein in this application , is 65 system server 108 with the report of the image analysis . The defined as a collection of letters , numbers and symbols terminal 101 of the present invention may further comprise printed on a currency object in order to identify it uniquely . image capture and processing modules that enable the User\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 8}), Document(page_content='US 10,504,073 B2 \\n 7 8 \\n to locally analyze the image and produce a report without In case no match is found in step 208 , a check is done having to electronically communicate with the system server whether another capturing 214 should be performed . The', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"108 . check can be with the User of terminal 101 using his / her Method of Capturing and Matching a Visual Object device's camera , or check against a timer which allows FIG . 2 is a flowchart of acts performed in capturing and 5 taking images for up to a predefined time . In case check matching a visual object , in accordance with an exemplary results are positive , then step 202 is performed again , if not embodiment of the invention . The flowchart describes a then the process ends 216 . process and system 200 to capture and match visual objects . System and Method of Validating Cash Deposits to Banks A Currency object database 201 is loaded , which includes FIG . 3 is a flowchart of acts performed in capturing and photos of a plurality of currency objects from one or more 10 matching a visual object , in accordance with an exemplary sides , such as both sides of hard currency ( i.e. paper cur embodiment of the invention for validating the authenticity rency and coins ) . The image of a visual object 120 is then of cash deposits to banks , either to a bank teller or to an ATM captured in step 202 with the terminal device 101. Option machine . System 300 performs the process described here ally , the object is captured by two or more cameras , thus inafter : constructing a three dimensional ( 3D ) representation of the 15 The Currency objects 120 are authenticated in step 302 object . using methods such as checking their SN comprising : expos Captured object image as mentioned in step 202 is option ing them to an adequate lighting source such as sunlight in ally analyzed locally in step 204 , as further described in step order to enable good capturing of a paper money watermark ; 207 , to get a match using content analysis or to reduce the using an Ultraviolet or Infrared light in order to read UV / IR size of the data to be sent to local or remote servers in step 20 signs ; and using a magnetic sensor in order to conduct a 206 . magnetic reading of the currency object . Optionally the image itself or a processed part of it is sent The total sum of the currency object is deposited in step in step 206 to a remote server 108 or locally processed on a 306 in a bank account temporarily before the currency server at device 101. The server performs server content objects are physically handed to the bank . The bank can analysis 207 to generate a report 140. Such analysis option- 25 credit the account holder in all or a part of the sum ( for ally uses the visual object size estimation of the coin and / or example 99 % as past cases indicate 1 % of the currency bank note based on its 3D representation , and optionally object are fake or will not be physically deposited ) . other data such as GPS data , the history of the sender and a The currency objects are later deposited in step 308 database of known fake visual objects . Object's actual physically in the bank . Optionally the SN’s of the visual volume ( e.g. size , area ) is estimated by performing an 30 object are compared in step 310 to those received in step 306 interline calculation between two registered images and using the SN's in the original visual object so as to verify the using a known distance between camera lens in camera original deposit . Subsequent to that , part or all of the array . The estimated volume is used to aid in the database temporary deposit turns into a permanent deposit 312 . currency search process , and to validate object's authentic FIG . 4 is a flowchart of acts performed in capturing and ity . 35 matching a visual object , in accordance with an exemplary In case predefined criteria are met , such as a match to embodiment of the invention . System 400 performs the predefined database currency object is found 208 , then step process described hereinafter : 210 is performed to convert the currency object to a preset The visual object of payment card 130 is captured in step currency . Preset currency can be set manually by the user , by 402\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"hereinafter : 210 is performed to convert the currency object to a preset The visual object of payment card 130 is captured in step currency . Preset currency can be set manually by the user , by 402 using an electronic communications device 101 with its carrier , or by an operation location based analysis on 40 camera 102 and / or video capacity . Symbol analysis is then capturing location data such as geo - tagging . A report such as performed in step 404 on the visual object to produce a text 140 is then generated 211 and displayed 212 using an such as the visual details of the credit card 132. The text is electronics communication device 101 , such as a smart used to charge the payment card in step 406 with an amount phone . Optionally commercial ads are displayed 212 on 408 that is entered into the electronics communications device 101 , such as ads for a nearby currency exchange 45 device 101. The transaction can further be authenticated place or a bank . using an image of the card owner's signature , his face , or Optionally steps 204 and / or 207 further comprise recog letting him type his PIN number on device 101. Optionally nition of the SN 121 , such as OCR and further measures a charge report 410 is displayed on device 101 . against fake SN’s or fake SN / currency value combinations . Optionally execution is passed to the step 301 further 50 EXEMPLIFICATIONS\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content=\"described in FIG . 3 . In a particular exemplification , report 140 provides the Example 1 — Merchant Receiving Payment in Paper amount of the currency in the original image , the amount of Money of a Foreign Currency the currency after being converted to the designated type , and whether the currency is authenticate . The system can 55 An exemplification of the present invention , especially also sum the amounts of currency in multiple images as that as disclosed in FIG . 1 and FIG . 2 , comprises a merchant selected by the User . The images may be sequential or who receives payment for goods or services that they have non - sequential wherein the User selects which image files to provided to a customer who is paying in paper money and analyze together in one report . See Table 140 in FIG . 1 for coins . an example of a report 212 generated for three images , 60 The merchant can capture an image of the currency on wherein each image is of a different type of currency that are their electronic communications device at the time of pay all converted to the same type of currency ( i.e. US ) . The ment . For example , a store cashier with a computer such as report also provides a “ Comments ” section that details the a laptop , netbook , etc. , or a waitress in a restaurant with a authenticity of the currency in each image . Lastly , the report mobile communications device can capture an image of the will provide a sum of the total amount of all the currencies 65 customer's currency . After the merchant captures the image , successfully analyzed in the different images , wherein the the system will indicate if the image is a match with images sum is in the converted currency . of major world currencies stored in the system database . If\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 9}), Document(page_content='10 US 10,504,073 B2 \\n 9 10 \\n the image is not a match , it is because the image may not be restaurant with a mobile communications device can capture clear enough to make the match , and / or it may indicate the an image of the customer\\'s card , front and back , wherein the currency is a counterfeit . The merchant can elect to re - verify image comprises the card holder\\'s name , the card number , by capturing the image again , and repeating the match . The the signature , and the three digit security code CSC ( also merchant may then instantly convert it to another currency 5 known as a CCID or Credit Card ID or Card Verification if required , and / or sum the total amount of payment by the Value ( CVV or CVV2 ) ) . The merchant can also take an customer . The match and currency conversion analysis is image of the customer\\'s face using his electronic device , accomplished instantaneously through either the software wherein he transmits the images to a local or remote server installed on the merchant\\'s terminal ( i.e. electronic device ) for comparison and analysis to stored images of the card and / or by the merchant transmitting the images via an holder\\'s signature and / or photograph . The merchant will Internet connection to a local or remote system server and then receive a report on his electronic communications then electronically receiving a report of the analysis . The device verifying that the customer is the true owner of the merchant then views the report of the analysis on their card , thus enabling him to process the payment . electronic communications device and processes the cus tomer\\'s payment in accordance with the report . For 15 Computer Program As will be appreciated by one skilled in the art , aspects of example , the merchant may decline the payment and request additional payment if the currency is found to be counterfeit . the present invention may be embodied as a system , method \\n Or the merchant may request additional payment if the or computer program product . Accordingly , aspects of the \\n current exchange rate indicates that the amount of payment present invention may take the form of an entirely hardware \\n is not enough . 20 embodiment , an entirely software embodiment ( including firmware , resident software , micro - code , etc. ) or an embodi Example 2 – Making a Cash Bank Deposit ment combining software and hardware aspects that may all \\n generally be referred to herein as a \" circuit , \" \" module ” or The present invention may also be used in making a cash \" system . ” Furthermore , aspects of the present invention may deposit to a financial institution as illustrated in FIG . 3 , 25 take the form of a computer program product embodied in wherein the bank customer temporarily credits their account one or more computer readable medium ( s ) having computer ( i.e. pending ) by electronically transmitting an image of the readable program code embodied thereon . deposit before visiting the bank and making the actual Any combination of one or more computer readable deposit . The customer would capture an image of the bills medium ( s ) may be utilized . The computer readable medium s / he is depositing using their mobile device camera . The 30 may be a computer readable signal medium or a computer mobile device would have software , or access to an Internet readable storage medium . A computer readable storage connection to the system server for completing the image medium may be , for example , but not limited to , an elec capture , analysis , and reporting of the present invention . tronic , magnetic , optical , electromagnetic , infrared , or semi They would then log into their bank account via the Internet conductor system , apparatus , or device , or any suitable and be authenticated by the bank\\'s system ( 302 ) ; and upload 35 the image of their deposit into their online account . The combination of the foregoing . More specific examples ( a', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 10}), Document(page_content=\"institution will indicate in its electronic records that a non - exhaustive list ) of the computer readable storage \\n temporary deposit 306 ( i.e. pending credit ) has been made at medium would include the following : an electrical connec \\n the time that the image is received . The bank customer will tion having one or more wires , a portable computer diskette , subsequently visit the bank within a set time period as 40 a hard disk , a random access memory ( RAM ) , a read - only determined by bank rules for time limitations to process memory ( ROM ) , an erasable programmable read - only \\n pending deposits . When the customer makes the actual memory ( EPROM or Flash memory ) , an optical fiber , a deposit at the bank , the institution will then indicate that an portable compact disc read - only memory ( CD - ROM ) , an actual deposit 308 was made . Additionally , the bank may optical storage device , a magnetic storage device , or any utilize the software / system of the present invention to deter- 45 suitable combination of the foregoing . In the context of this mine if the actual deposit 308 is counterfeit . If the authen document , a computer readable storage medium may be any ticity of the hard currency is verified by analyzing the tangible medium that can contain , or store a program for use captured image of the currency 310 or by other means by or in connection with an instruction execution system , known in the banking industry ( i.e. teller physically check apparatus , or device . ing ) , then the deposit is designated as permanent in the 50 Program code embodied on a computer readable medium customer's account 312. If the currency is found not to be may be transmitted using any appropriate medium , includ authentic or the customer does not make the actual physical ing but not limited to wireless , wire line , optical fiber cable , deposit at the bank ( 308 ) , then the customer is notified of RF , etc. , or any suitable combination of the foregoing . such , and the pending state of the deposit is dropped from Computer program code for carrying out operations for the customer's account so that no credit is given for the 55 aspects of the present invention may be written in any deposit . combination of one or more programming languages , including an object oriented programming language such as Example 3 — Merchant Processing a Payment Card Java , Smalltalk , C ++ or the like and conventional procedural programming languages , such as the “ C ” programming A merchant receives payment for goods or services that 60 language or similar programming languages . The program they have provided to a customer who is paying using a code may execute entirely on the user's computer , partly on credit or a debit or a loyalty card . The merchant can capture the user's computer , as a stand - alone software package , an image of the card on their electronic communications partly on the user's computer and partly on a remote device at the time of payment and extra the text ( i.e. card computer or entirely on the remote computer or server . In the number ) for electronically submitting a charge to the card 65 latter scenario , the remote computer may be connected to the from the mobile device . For example , a store cashier with a user's computer through any type of network , including a computer such as a laptop , netbook , etc. , or a waitress in a local area network ( LAN ) or a wide area network ( WAN ) , or\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 10}), Document(page_content='10 US 10,504,073 B2 \\n 11 12 \\n the connection may be made to an external computer ( for Reference in the specification to “ some embodiments ” , example , through the Internet using an Internet Service \" an embodiment ” , “ one embodiment ” or “ other embodi \\n Provider ) . ments ” means that a particular feature , structure , or charac Aspects of the present invention are described above with teristic described in connection with the embodiments is reference to flowchart illustrations and / or block diagrams of 5 included in at least some embodiments , but not necessarily methods , apparatus ( systems ) and computer program prod all embodiments , of the inventions . ucts according to embodiments of the invention . It will be It is to be understood that the phraseology and terminol understood that each block of the flowchart illustrations ogy employed herein is not to be construed as limiting and and / or block diagrams , and combinations of blocks in the are for descriptive purpose only . \\n flowchart illustrations and / or block diagrams , can be imple It is to be understood that the details set forth herein do not mented by computer program instructions . These computer construe a limitation to an application of the invention . program instructions may be provided to a processor of a Furthermore , it is to be understood that the invention can general purpose computer , special purpose computer , or be carried out or practiced in various ways and that the other programmable data processing apparatus to produce a 15 invention can be implemented in embodiments other than machine , such that the instructions , which execute via the the ones outlined in the description above . processor of the computer or other programmable data It is to be understood that the terms “ including ” , “ com processing apparatus , create means for implementing the prising ” , “ consisting ” and grammatical variants thereof do functions / acts specified in the flowchart and / or block dia not preclude the addition of one or more components , gram block or blocks . 20 features , steps , or integers or groups thereof and that the These computer program instructions may also be stored terms are to be construed as specifying components , fea in a computer readable medium that can direct a computer , tures , steps or integers . other programmable data processing apparatus , or other If the specification or claims refer to “ an additional ” devices to function in particular manner , such that the element , that does not preclude there being more than one of instructions stored in the computer readable medium pro- 25 the additional element . duce an article of manufacture including instructions which It is to be understood that where the claims or specifica implement the function / act specified in the flowchart and / or tion refer to \" a \" or \" an \" element , such reference is not be block diagram block or blocks . construed that there is only one of that element . The computer program instructions may also be loaded It is to be understood that where the specification states onto a computer , other programmable data processing appa- 30 that a component , feature , structure , or characteristic “ may ” , ratus , or other devices to cause a series of operational steps \" might ” , “ can ” or “ could ” be included , that particular com to be performed on the computer , other programmable apparatus or other devices to produce a computer imple ponent , feature , structure , or characteristic is not required to \\n be included . mented process such that the instructions which execute on the computer or other programmable apparatus provide 35 Where applicable , although state diagrams , flow diagrams \\n processes for implementing the functions / acts specified in or both may be used to describe embodiments , the invention \\n the flowchart and / or block diagram block or blocks . is not limited to those diagrams or to the corresponding', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 11}), Document(page_content='the flowchart and / or block diagram block or blocks . is not limited to those diagrams or to the corresponding \\n The aforementioned flowchart and diagrams illustrate the descriptions . For example , flow need not move through each \\n architecture , functionality , and operation of possible imple illustrated box or state , or in exactly the same order as mentations of systems , methods and computer program 40 illustrated and described . products according to various embodiments of the present Methods of the present invention may be implemented by invention . In this regard , each block in the flowchart or block performing or completing manually , automatically , or a diagrams may represent a module , segment , or portion of combination thereof , selected steps or tasks . code , which comprises one or more executable instructions The descriptions , examples , methods and materials pre for implementing the specified logical function ( s ) . It should 45 sented in the claims and the specification are not to be also be noted that , in some alternative implementations , the construed as limiting but rather as illustrative only . functions noted in the block may occur out of the order noted Meanings of technical and scientific terms used herein are in the figures . For example , two blocks shown in succession to be commonly understood as by one of ordinary skill in the may , in fact , be executed substantially concurrently , or the art to which the invention belongs , unless otherwise defined . blocks may sometimes be executed in the reverse order , 50 The present invention may be implemented in the testing depending upon the functionality involved . It will also be or practice with methods and materials equivalent or similar noted that each block of the block diagrams and / or flowchart to those described herein . illustration , and combinations of blocks in the block dia Any publications , including patents , patent applications grams and / or flowchart illustration , can be implemented by and articles , referenced or mentioned in this specification are special purpose hardware - based systems that perform the 55 herein incorporated in their entirety into the specification , to specified functions or acts , or combinations of special pur the same extent as if each individual publication was spe pose hardware and computer instructions . cifically and individually indicated to be incorporated In the above description , an embodiment is an example or herein . In addition , citation or identification of any reference implementation of the inventions . The various appearances in the description of some embodiments of the invention of “ one embodiment , \" \" an embodiment ” or “ some embodi- 60 shall not be construed as an admission that such reference is ments ” do not necessarily all refer to the same embodiments . available as prior art to the present invention . Although various features of the invention may be While the invention has been described with respect to a described in the context of a single embodiment , the features limited number of embodiments , these should not be con may also be provided separately or in any suitable combi strued as limitations on the scope of the invention , but rather nation . Conversely , although the invention may be described 65 as exemplifications of some of the preferred embodiments . herein in the context of separate embodiments for clarity , the Other possible variations , modifications , and applications invention may also be implemented in a single embodiment . are also within the scope of the invention . Accordingly , the', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 11}), Document(page_content='10 US 10,504,073 B2 \\n 13 14 \\n scope of the invention should not be limited by what has thus 5. The system of claim 1 , wherein said content analysis far been described , but by the appended claims and their comprises performing an authentication of each currency legal equivalents . object of said plurality of types of currency objects . 6. A computer implemented method for utilizing a mobile What claimed is : 5 device to analyze the authenticity and monetary value of 1. A system for conducting a content analysis of an image currency deposited into a bank account , the method per depicting a plurality of currency objects , the system com formed by a server in network communication with the prising a server in network communication with a mobile mobile device , comprising : device , the server comprising : a ) receiving by the server over a network , a single image a hardware processor ; captured by at least one camera of the mobile device , a non - transitory computer readable medium having the single image depicting at least one currency object embodied thereon code instructions that in response to of each of a plurality of types of currency objects execution by a hardware processor of the server , cause denoting respective denominations of major world cur \\n the server to : rencies , and location data indicative of location of the receive over a network , a single image captured by at least 15 mobile device computed by a location based analysis of one camera of the mobile device , the single image output of a location data sensor ; depicting at least one currency object of each of a b ) conducting by the server a content analysis of said plurality of types of currency objects denoting respec single image , wherein said analysis comprises : tive denominations of major world currencies , and identifying each of the plurality of types of currency location data indicative of location of the mobile device 20 objects , computed by a location based analysis of output of a computing a plurality of keypoint descriptors as a set of', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content=\"location data sensor ; orientation histograms on neighboring pixels of said conduct a content analysis of the single image , wherein plurality of types of currency objects ; said content analysis comprises : convoluting the captured image with Gaussian filters at identifying each of the plurality of types of currency 25 different scales to create a plurality of successive objects , Gaussian - blurred images ; computing a plurality of keypoint descriptors as a set of wherein the orientation histograms are relative to the orientation histograms on neighboring pixels of said orientation of the plurality of keypoints , wherein the plurality of types of currency objects ; orientation data for the orientation histograms is convoluting the captured image with Gaussian filters at 30 derived from the Gaussian image closest in scale to different scales to create a plurality of successive the scale of each respective keypoint of the plurality Gaussian - blurred images ; of keypoints ; wherein the orientation histograms are relative to the computing a quantity of each currency object of the orientation of the plurality of keypoints , wherein the plurality of types of currency objects ; orientation data for the orientation histograms is 35 c ) converting each of the plurality of identified types of derived from the Gaussian image closest in scale to currency objects to a common monetary currency in the scale of each respective keypoint of the plurality use at the location of the mobile device ; of keypoints ; d ) calculating a total monetary value of the plurality of computing a quantity of each currency object of the types of currency objects when converted to the com plurality of types of currency objects ; mon monetary currency ; convert each of the plurality of identified types of cur e ) temporarily depositing the total monetary value into a rency objects to a common monetary currency in use at user's bank account according to the common mon the location of the mobile device ; etary currency ; and create a currency inventory report comprising a total f ) authenticating the plurality of currency objects after monetary value of the plurality of types of currency 45 receiving an indication of a physical deposit of the objects when converted to the common monetary cur plurality of currency objects in the user's bank to convert the temporarily deposited monetary value to a transmit from the server over the network to said mobile permanent deposit . device , the currency inventory report for presentation 7. The method of claim 6 , wherein said verifying is on a display of said mobile device . 50 performed by utilizing light to check for watermarks and UV 2. The system of claim 1 , wherein said content analysis or signs . further comprises an analysis of a member selected from a 8. The method of claim 6 , wherein said content analysis group consisting of the image's captured text , visual and further comprises an analysis of a member selected from a symbol data from said captured image , a history of images group consisting of the image's captured text , currency and captured using said mobile device and at least one currency 55 symbol data from said captured image , a history of images object from a database of known fake currency objects . captured using said mobile device and at least one currency 3. The system of claim 1 , wherein said content analysis object from a database of known fake currency objects . further comprises : 9. The method of claim 6 , wherein said content analysis calculating a face value of the plurality of types of further comprises calculating the value of the plurality of currency objects , and 60 types of currency objects in said image by summing the calculating a total value by summing the value of each of value of each of said plurality of types of currency objects said plurality of types of currency objects . in said image . 4. The system of claim 1 , wherein said at least one camera 10. The method of claim 6\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='each of said plurality of types of currency objects said plurality of types of currency objects . in said image . 4. The system of claim 1 , wherein said at least one camera 10. The method of claim 6 , wherein said at least one comprises an array of cameras of said mobile device and camera comprises an array of cameras of said mobile device wherein said capturing is conducted to construct a three 65 and wherein said capturing is conducted to construct a three dimensional ( 3D ) representation of said plurality of types of dimensional ( 3D ) representation of said plurality of types of currency objects . currency objects . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='rency ; and', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 12}), Document(page_content='5 \\n 15 \\n 20 US 10,504,073 B2 \\n 15 16 \\n 11. A non - transitory computer readable medium having 17. An internet based system for quantifying and analyz embodied thereon code instructions that , when executed by ing the authenticity of a plurality of types of currency one or more hardware processors of a server in network objects , the system comprising : communication with at least one mobile device , cause the a mobile device comprising : one or more hardware processors of the server to : at least one hardware processor ; a ) receive a single image captured using at least one a non - transitory computer readable medium having camera of the mobile device , the single image com embodied thereon code instructions that in response prising at least one currency object of each of a to execution by the at least one hardware processor plurality of types of currency objects denoting respec of the mobile device , cause the mobile device to : tive denominations of major world currencies , and 10 capture a single image that images at least one location data indicative of location of the mobile device currency object of each of a plurality of types of computed by a location based analysis of output of a \\n location data sensor ; currency objects denoting respective denomina \\n b ) conduct a content analysis of the single image , wherein tions of major world currencies , using a camera or \\n said content analysis comprises : a video recorder of a respective said mobile \\n identifying each of the plurality of types of currency device , and provide location data indicative of \\n objects ; location of the mobile device computed by a \\n computing a plurality of keypoint descriptors as a set of location based analysis of output of a location data orientation histograms on neighboring pixels of said sensor ; plurality of types of currency objects ; a database storing a plurality of reference denominations convoluting the captured image with Gaussian filters at of major world currencies ; \\n different scales to create a plurality of successive a processor ; Gaussian - blurred images ; non - transitory storage coupled to the processor and stor wherein the orientation histograms are relative to the ing code that , when executed by the processor , cause orientation of the plurality of keypoints , wherein the 25 the processor to : orientation data for the orientation histograms is apply a decision function to the plurality of types of derived from the Gaussian image closest in scale to currency objects to yield an analysis of authenticity of the scale of each respective keypoint of the plurality each member of said type of currency object according of keypoints ; to a match with said plurality of denominations of computing a quantity of each currency object of the 30 major world currencies , wherein a match exists when plurality of types of currency objects ; the decision function is above a designated threshold c ) convert each of the plurality of identified types of for authenticity ; currency objects to a common monetary currency in wherein the decision function comprises : use at the location of the mobile device ; identifying each of the plurality of types of currency d ) create a currency inventory report comprising a total 35 objects , monetary value of the plurality of types of currency computing a plurality of keypoint descriptors as a set of objects when converted to the common monetary cur orientation histograms on neighboring pixels of said plurality of types of currency objects ; e ) transmit to said mobile device , the currency inventory convoluting the captured image with Gaussian filters at report for presentation on a display of said mobile 40 different scales to create a plurality of successive', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 13}), Document(page_content=\"device . Gaussian - blurred images ; 12. The non - transitory computer readable medium of wherein the orientation histograms are relative to the claim 11 , wherein said content analysis further comprises an orientation of the plurality of keypoints , wherein the analysis of a member selected from a group consisting of the orientation data for the orientation histograms is image's captured text , visual and symbol data from said 45 derived from the Gaussian image closest in scale to captured image , a history of images captured using said the scale of each respective keypoint of the plurality mobile device and at least one currency object from a of keypoints ; database of known fake currency objects . computing a quantity of each currency object of the 13. The non - transitory computer readable medium of plurality of types of currency objects ; claim 11 , wherein said conducting further comprises : convert each of the plurality of identified types of cur calculating a face value of the plurality of types of rency objects to a common monetary currency in use at currency objects , and the location of the mobile device ; calculating a total value by summing the value of each of create a currency inventory report comprising a total said plurality of types of currency objects . monetary value of the plurality of types of currency 14. The non - transitory computer readable medium of 55 objects when converted to the common monetary cur claim 11 , wherein said at least one camera comprises an array of cameras of said mobile device and wherein said generate a currency inventory report and transmit said capturing is conducted to construct a three dimensional ( 3D ) currency inventory report to said user mobile device for representation of said plurality of types of currency objects . presentation on a display of said mobile device . 15. The non - transitory computer readable medium of 60 18. The system of claim 17 , wherein said content analysis claim 11 , wherein said content analysis comprises perform further includes determining the authenticity of each cur ing an authentication of each currency object of said plu rency object of said plurality of types of currency objects rality of types of currency objects . based on a content match , wherein said match exists when 16. The non - transitory computer readable medium of the content analysis is above a designated threshold for claim 11 , wherein said content analysis further comprises 65 authenticity . comparing differences and similarities between at least one 19. The system of claim 17 wherein said plurality of types reference image object of a reference image and said image . of currency objects comprises a payment card . rency ; and \\n 50 \\n rency ; and\", metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 13}), Document(page_content='18 US 10,504,073 B2 \\n 17 \\n 20. The system of claim 1 , wherein said plurality of types currency objects include a plurality of types of coins , and said content analysis identifies said plurality of types of coins . 21. The system of claim 1 , wherein computing the plu- 5 rality of keypoints comprises computing the plurality of keypoints as maxima and minima of the differences of the successive Gaussian - blurred images . 22. The system of claim 1 , wherein said currency inven tory report is presented on said display of said mobile device 10 at the same time as said single image . 23. The system of claim 1 , wherein the currency inventory report in the common monetary currency is generated in real time , according to a real time location of the mobile device computed based on said output of the location data sensor 15 and based on the single image captured in real time by the \\n camera of the mobile device .', metadata={'source': 'https://patentimages.storage.googleapis.com/da/c5/41/ea81c81c813087/US10504073.pdf', 'page': 14})]\n",
      "[Document(page_content=\"(12) United States Patent \\n Tadayon et al. USOO8873813B2 \\n (10) Patent No.: US 8,873,813 B2 \\n (45) Date of Patent: Oct. 28, 2014 \\n (54) APPLICATION OF Z-WEBS AND Z-FACTORS \\n TO ANALYTICS, SEARCH ENGINE, \\n LEARNING, RECOGNITION, NATURAL \\n LANGUAGE, AND OTHERUTILITIES \\n (71) Applicants: Saied Tadayon, Potomac, MD (US); \\n Bijan Tadayon, Potomac, MD (US) \\n (72) Inventors: Saied Tadayon, Potomac, MD (US); Bijan Tadayon, Potomac, MD (US) \\n (73) Assignee: Z. Advanced Computing, Inc., Potomac, \\n MD (US) \\n (*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 140 days. \\n (21) \\n (22) Appl. No.: 13/781,303 \\n Filed: Feb. 28, 2013 \\n (65) Prior Publication Data \\n US 2014/OO79297 A1 Mar. 20, 2014 \\n Related U.S. Application Data \\n Provisional application No. 61/701,789, filed on Sep. \\n 17, 2012. (60) \\n Int. C. \\n G06K 9/00 \\n G06K9/40 \\n U.S. C. \\n CPC ................................. G06K9/00288 (2013.01); \\n G06K 9/00 (2013.01) \\n USPC .............. 382/118:382/181: 382/263; 706/52 \\n Field of Classification Search \\n CPC ..... G06K9/00; G06K9/00288: G06N 7/005; \\n GO6N 7/OO \\n USPC ......... 382/115, 118,305, 224, 278, 103, 176, \\n 382/190, 195, 209, 218, 219, 282,307, 275, \\n 382/226, 227, 254, 181, 263: 340/5.81, \\n 340/5.83; 707/E17.022, E17.026, E17.023, (51) \\n (2006.01) (2006.01) \\n (52) \\n (58) \\n input module Pre-processor \\n Face recognizer \\n module 707/736, 758,999.107; 706/52; 715/825; \\n 902/3: 348/239,370 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 5,295.228 A 3, 1994 Koda et al. \\n 5,329,611 A 7, 1994 Pechanek et al. \\n (Continued) \\n OTHER PUBLICATIONS \\n Ali Sanayei, titled “Towards a complexity theory: Theoratical foun dations and practical applications'. Submitted to Satellite Meeting Unravelling and Controlling Discrete Dynamical Systems, on Jun. \\n 17, 2011. No page number, volume number, or publisher's location mentioned. (Paper dedicated to Professor Lotfi A. Zadeh, et al., by the \\n author.). \\n (Continued) \\n Primary Examiner — Sheela Chawan (74) Attorney, Agent, or Firm — Saied Tadayon; Bijan Tadayon \\n (57) ABSTRACT \\n Here, we introduce Z-webs, including Z-factors and Z-nodes, for the understanding of relationships between objects, sub jects, abstract ideas, concepts, or the like, including face, car, images, people, emotions, mood, text, natural language, Voice, music, video, locations, formulas, facts, historical data, landmarks, personalities, ownership, family, friends, love, \\n happiness, Social behavior, Voting behavior, and the like, to be used for many applications in our life, including on the search engine, analytics, Big Data processing, natural language pro cessing, economy forecasting, face recognition, dealing with reliability and certainty, medical diagnosis, pattern recogni tion, object recognition, biometrics, security analysis, risk analysis, fraud detection, satellite image analysis, machine generated data analysis, machine learning, training samples, \\n extracting data or patterns (from the video, images, and the like), editing video or images, and the like. Z-factors include reliability factor, confidence factor, expertise factor, bias fac \\n tor, and the like, which is associated with each Z-node in the \\n Z-web. \\n 20 Claims, 81 Drawing Sheets \\n 2-web \\n Output module \\n eigenface \\n module \\n eigenface Component recognition \\n module Edge \\n detector Eigenvector \\n on face components \\n module library \\n wavelet \\n module \\n Anchor points or \\n features \\n module \\n Relationships \\n between \\n features or \\n face components \\n rules or \\n database \\n Z-web \\n Z-factors Training samples \\n input module Face components library wavelet library Input \\n module for \\n libraries \\n Training \\n module for \\n librarias\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 0}), Document(page_content=\"US 8,873,813 B2 \\n Page 2 \\n (56) References Cited 8, 108,207 B1 1/2012 Harvey et al. 8, 108.324 B2 1/2012 Krupka et al. \\n U.S. PATENT DOCUMENTS 8, 116,534 B2 2/2012 Nishiyama et al. 8, 150,109 B2 4/2012 Sung et al. \\n 5,517,596 A 5, 1996 Pechanek et al. 8, 165,354 B1 4/2012 Zhao 6,157,921. A 12/2000 Barnhill 8, 199,203 B2 6/2012 Sugimoto 7,542,947 B2 6/2009 Guyon et al. 8, 199,242 B2 6/2012 Sugihara 7,664,962 B2 * 2/2010 Kuhlman ...................... T13, 186 8,199.979 B2 6/2012 Steinberg et al. 7,689,529 B2 3/2010 Fung et al. 8,204,310 B2 6, 2012 Zou et al. T.697.761 B2 4/2010 Napper 8,208,764 B2 6/2012 Guckenberger 7698.236 B2 4/2010 Coxetal. 8,209,179 B2 6/2012 Aoyama et al. 7,721,336 B1 5/2010 Adjaoute 8,213,737 B2* 7/2012 Steinberg et al. ............. 382,275 7,734.400 B2 6/2010 Gayme et al. 8,224,040 B2 7/2012 Li 7,734.451 B2 6/2010 MacArthur et al. 8.224,042 B2 7/2012 Wang 7,739,337 B1 6, 2010 Jensen 8,233,676 B2 7/2012 Ngan et al. \\n 7,742,103 B1 6/2010 He et al. 8,244,040 B2 8.2012 Imagawa 7,761,742 B2 7/2010 Di Palma et al. 8,249,313 B2 8/2012 Yanagi \\n 7,769,512 B2 8, 2010 Norris et al. 8,254,691 B2 8/2012 Kaneda et al. \\n 7,783,580 B2 8/2010 Huang et al. 8,259,168 B2 9/2012 Wu et al. \\n 7,784,295 B2 8/2010 McCormicket al. 8,260,009 B2 * 9/2012 Du et al. ....................... 382,117 \\n 7,792,746 B2 9, 2010 Del Callar et al. 8,265,399 B2 9/2012 Steinberg et al. \\n 7,792,750 B2 9/2010 Moeller 8,265,474 B2 9/2012 Kanayama \\n 7,797.268 B2 9/2010 Bigus et al. 8,275,175 B2 9/2012 Baltatu et al. 7,801,840 B2 9/2010 Repasi et al. 8,285,006 B2 10/2012 Tang 7,805.396 B2 9/2010 Wagner et al. 8,289.546 B2 10/2012 Hayasaki \\n 7,805,397 B2 9, 2010 Kurian et al. 8,295,558 B2 10/2012 Su et al. \\n 7,805,984 B2 10/2010 McLain et al. 8,300,898 B2 10/2012 Baket al. 7,817,854 B2 10/2010 Taylor 8,300,900 B2 10/2012 Lai et al. 7,832,511 B2 11/2010 Syed et al. 8,306.279 B2 11/2012 Hanna 7,836,496 B2 11/2010 Chesla et al. 8,316,436 B2 11/2012 Shirai et al. 7,840,500 B2 11/2010 Khanbaghi 8,320,682 B2 11/2012 Froeba et al. 7,844,564 B2 11/2010 Donohue et al. 8.325,999 B2 12/2012 Kapoor et al. 7,853,538 B2 12/2010 Hildebrand 8,326,001 B2 12/2012 Free 7.856.356 B2 12/2010 Chung et al. 8.330,831 B2 * 12/2012 Steinberg et al. .......... 348,231.3 \\n 7,857.976 B2 12/2010 Bissler et al. 8.331,632 B1 12/2012 Mohanty et al. 7,864,552 B2 1/2011 Heber et al. 8.332,422 B2 12/2012 Chang et al. 7,869,989 B1 1/2011 Harvey et al. 8,340,366 B2 12/2012 Masuda et al. \\n 7,895,135 B2 2/2011 Norris et al. 8,352,467 B1 1/2013 Guha. \\n 7,921,068 B2 4/2011 Guyon et al. 8,359,611 B2 1/2013 Johnson et al. \\n 7,925,874 B1 4/2011 Zaitsev 8,370,352 B2 2/2013 Lita et al. \\n 7,929,771 B2 4/2011 Koet al. 8,374,405 B2 2/2013 Lee et al. \\n 7,930,265 B2 4/2011 Akelbein et al. 8,379,074 B2 2/2013 Currivan et al. \\n 7,934,499 B2 5/2011 Berthon-Jones 8,379,920 B2 2/2013 Yang et al. 7,936,906 B2 5, 2011 Hua et al. 8,379,940 B2 2/2013 Wechsler et al. \\n 7,941,350 B2 5/2011 Ginsburg et al. 8,386.446 B1 2/2013 Pasupathy et al. 7,966,061 B2 6, 2011 Al-Abed et al. 8,503,800 B2 * 8/2013 Blonk et al. .................. 382,226 \\n 7.974.455 B2 7, 2011 Peters et al. 8,593,542 B2 * 1 1/2013 Steinberg et al. ... 348,239 \\n 7.991,754 B2 8, 2011 Maizel et al. 8,670.597 B2* 3/2014 Petrou et al. ... ... 382,116 \\n 7.999,857 B2 8, 2011 Bunn et al. 8,682,097 B2 * 3/2014 Steinberg et al. ... 382.275 \\n 8,004,544 B1 8/2011 Zhang et al. 2010/0172550 A1* 7/2010 Gilley et al. .................. 382,118 \\n 8,015, 196 B2 9/2011 Taranenko et al. 8,016,319 B2 9, 2011 Winkler et al. OTHER PUBLICATIONS \\n 8,023,974 B1 9, 2011 Diao et al. Ronald Yager, titled “On Z-valuations using Zadeh's Z-numbers'. 8,054,592 B2 11/2011 Rivers, Jr. 8,060,456 B2 11/2011 Gao et al. International Journal of Intelligent Systems, vol. 27. Issue 3, pp. \\n 8,063,889 B2 11/2011 Anderson 259-278, Mar. 2012, Wiley Periodicals, Inc. The online version first \\n 8,077.983 B2 12/2011 Qiu et al. published on Jan 20, 2012. No publisher's location mentioned.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 1}), Document(page_content=\"8,063,889 B2 11/2011 Anderson 259-278, Mar. 2012, Wiley Periodicals, Inc. The online version first \\n 8,077.983 B2 12/2011 Qiu et al. published on Jan 20, 2012. No publisher's location mentioned. \\n 8,081,844 B2 12/2011 Steinberg et al. \\n 8,095,483 B2 1/2012 Weston et al. * cited by examiner\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 1}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 1 of 81 US 8,873,813 B2 \\n FIG 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 2}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 2 of 81 US 8,873,813 B2 \\n f-mark R \\n Approximately 3 O) \\n FIG2(a) \\n e - as o ... 2 Na. v w \\n . \\n : \\n . V M. \\n \"...Y. 1 . . . . . . . . . ... sle 1. ...\\' s - s o * . . . . . . ... 2 NN. v V. \\n : \\n . V M. ... W. W ...SS-...\" \\n A B Y N- 1 FIG2(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 3}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 3 of 81 US 8,873,813 B2 \\n Bandwidth (Ab) \\n o Support \\n FIG 3', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 4}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 4 of 81 US 8,873,813 B2 \\n Probability Membership Test SCOre Models Distribution Function CalculatOr database database database \\n Zn - umber USer interface estimatOr \\n Parameters Integration for prob. Certainty \\n module Distribution E. \\n Functions 33O3S6 \\n database Sup \\n function \\n mOdule Set functions \\n module DOmains \\n database \\n GeOmetrical \\n Shape \\n analyzer 3D rendering \\n images \\n Fig. 4', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 5}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 5 of 81 US 8,873,813 B2 \\n Context \\n determination \\n module large Dissecting & Analyzing \\n p parsing module device \\n Default Context 3 Context 2 Context 1 \\n analyZer analyZer analyzer analyzer \\n Membership \\n values COrrelation \\n module \\n aggregator \\n Fig. 5', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 6}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 6 of 81 US 8,873,813 B2 \\n Natural \\n language \\n processing \\n module Ouestion/ \\n Answering \\n system Search engine \\n Analyzer/ Forecasting Rules engine processor engine \\n -> crisis COnflict Applications E. analyzing and For Analyzer resolving \\n module \\n Fig. 6', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 7}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 7 of 81 US 8,873,813 B2 \\n Intensity (or other image \\n Y parameters) mapping \\n 45 degree X \\n diagonal line \\n Within specific range, for \\n each Section, part, or region of image \\n Fig. 7', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 8}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 8 of 81 US 8,873,813 B2 \\n Dissecting & Task Classification Input Segmentation assignment module device mOdule module \\n Recognizer for type Recognizer for type Recognizer for type \\n 3 module 2 module 1 module \\n analyzer \\n Recognizer for sub-type 3.1 Membership \\n values module COrrelation \\n module \\n Recognizer for aggregator sub-type 3.2 or \\n module \\n Output \\n module \\n Recognizer for \\n Sub-Sub-type \\n 3.2.1 module \\n And type hierarchy Fig. 8 \\n COntinues', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 9}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 9 of 81 US 8,873,813 B2 \\n Input Search engine classifier \\n device \\n Sub Classifier 3 Sub Classifier 2 Sub Classifier 1 \\n Expert module \\n 1 \\n Expert module Z factors \\n 2 module Z-node \\n module \\n Analyzer and \\n Expert module aggregator \\n 3 \\n Output \\n module \\n Sub Expert \\n module 3.1 \\n And expert \\n hierarchy \\n COntinues Fig. 9', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 10}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 10 of 81 US 8,873,813 B2 \\n Cbntext 1 \\n Z-factors parameters? Z-factors for \\n asSOCiated With node 1 \\n each node \\n Z-factorS for \\n node 2 \\n t2to3 (thickness of branch), e.g. for strength of \\n Correspondence or \\n COrrelation, as \\n proportional to thickness Context 2 \\n datos (length of \\n branch), e.g. for \\n CloSeneSS Of \\n COncepts, as \\n inverse of length Z-factorS for \\n nOde 3 \\n One Arrow, indicating one way or asymmetric relationship, e.g. ownership between human and car ( \\n Fig 1 O Z-web with 2 Contexts, or 2 Sub-regions \\n of Z-Web, associated with NOce 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 11}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 11 of 81 US 8,873,813 B2 \\n (2) made processor PD dimensions \\n brackets Classifier for - \\n NP Clusters More age \\n tvOe of head Kid library y face 5-7 years \\n Pre-teen A \\n 8-12 library ge Gender progression module Teenager mOdel \\n 13-16 \\n years old \\n library \\n M Training Emotions \\n Ore age module model brackets \\n Emotions \\n library Training samples \\n Input module \\n Fig.11 Emotions \\n classifier \\n Output \\n module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 12}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 12 of 81 US 8,873,813 B2 \\n Input OCGSSO? Head or face \\n module O library \\n More age brackets Classifier for Classifier for \\n type of head tit Or Kid library Orface rotation 5-7 years \\n Gender Tilt or \\n Pre-teen A model rOtation 8-12 library ge library progression \\n Teenager model \\n 13-16 Analytical \\n years old model library \\n Training Morphing U \\n module model Rotational \\n Operators, to \\n rotate the \\n edges or \\n lines Translational \\n operators, to \\n Training samples move the \\n Input module edges or lines \\n Fig. 12', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 13}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 13 of 81 US 8,873,813 B2 \\n Input Search engine classifier \\n device \\n Wire mesh model Face recognizer training \\n of face library module module \\n Geometrical Analyzer or \\n model of face proCeSSOr \\n library Z factorS \\n module Z-node ContOur model module of face library \\n model of face Crisp rules Output module module library \\n Fuzzy rules \\n engine Fuzzy \\n descriptor for \\n face library, \\n e.g. Small lips Semantic \\n Fuzzy rules \\n library \\n Rules and Fig. 1 3 descriptors \\n Input module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 14}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 14 of 81 US 8,873,813 B2 \\n Front view Storage or Input Pre-processor database module \\n MOdified faces MOdification Translation \\n library module module \\n Eigenface \\n generator Tilt module \\n module \\n Emotions \\n y mOdule \\n Output Training \\n module for module libraries \\n Input \\n module for \\n Training samples libraries \\n Input module \\n Fig. 14', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 15}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 15 of 81 US 8,873,813 B2 \\n Pre-OrOCessor Storage or O database \\n Modified faces \\n Storage Modification fuZZification \\n module module \\n Eicenf FuZZification CenaCe E. averaging Haveraging rules or library \\n module module 2 module 1 \\n Loop Condition cloudifying module \\n Averaging \\n matrix Or \\n filter library Output \\n module Special \\n effect filter \\n module for \\n libraries \\n Training Training samples module for Input module libraries \\n Fig. 15', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 16}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 16 of 81 US 8,873,813 B2 \\n Pre-processor Z-Web \\n Output module \\n Face \\n recognizer \\n input module module \\n Component recognition Eigenvector \\n module Edge On face \\n detector Components \\n mOdule Wavelet \\n module eigenface \\n module \\n eigenface \\n library \\n Anchor \\n points or \\n features \\n module FaCe \\n Components \\n library Wavelet \\n library Input \\n Relationships module for \\n between libraries \\n features Or \\n face Trainind Samoles Training Components n E. module for \\n rules Or O libraries \\n Catabase \\n Z-factors * Fig. 16', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 17}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 17 of 81 US 8,873,813 B2 \\n NOde 6 \\n NOde 8 Jeff's brother's \\n Happiness Birthday party \\n (emotions) \\n No arrow, just indicating a \\n relationship \\n between 2 Z \\n nodes Node 4 NOde 2 y \\n Jeff Smith Jeff's dad \\n TWO arrOWS \\n - - indicate bi \\n z-factors NOde 3 directional \\n for node 3 Toyota sedan relationship, e.g. \\n - - - (Jeff's car) two objects being close to \\n each Other \\n One Arrow, indicating One Way or asymmetric relationship, e.g. \\n Ownership between human and Car \\n Fig. 17\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 18}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 18 of 81 US 8,873,813 B2 \\n (2) face library Classifier for emotions \\n Classifier \\n mitor E. Classifier for for age \\n faces OCUe face \\n Without \\n glasses Classifier for \\n mOdel tilt of the \\n face \\n Geometrical (2) model Analytical model \\n And/or face library \\n Analytical models Computer for glasses generated \\n Simulation geometrical models \\n glasses for glasses \\n Input \\n module for E. face With \\n faces with OCUC acCesory Classifier for \\n glasses \\n Library of glasses \\n Aggregator (styles and models) \\n module \\n face library \\n Fig. 18', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 19}), Document(page_content='U.S. Patent Oct. 28, 2014 \\n A (an ) sky \\n Sky \\n images \\n -O- \\n degree \\n Fig. 19 Continuity \\n analysis \\n module Sheet 19 of 81 US 8,873,813 B2 \\n Tilt angle w.r.t. 90 degree angle \\n -D \\n image input Storage \\n module module \\n Histogram \\n CUVSS \\n analyzer \\n Frequency \\n analyzer \\n Training processor module Color Or \\n pattern \\n recognizer \\n Training samples \\n Input module \\n Tilt \\n COrrection \\n module \\n Output \\n module \\n -CH', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 20}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 20 of 81 US 8,873,813 B2 \\n Image data Video data OCR/ Other data text formats \\n Input module Storage module \\n Context object Context Selected Objects library analyzer analyzer \\n trainer Storage Output \\n module module module Z-factors \\n Trainer samples \\n input module \\n Fig. 20', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 21}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 21 of 81 US 8,873,813 B2 \\n To recognize \\n an object (B), | partially or \\n hidden \\n image input Storage \\n module module \\n Histogram \\n CUWGS \\n Color Or analyzer \\n pattern \\n recognizer Frequency \\n analyzer BAB \\n Region \\n Continuity \\n analysis edge \\n detecting \\n module module \\n Training samples Training processor Input module module \\n Z-Web COntext Expected \\n analyzer objects Obiect library jec recognition \\n Other data module \\n Output Redrawing or \\n mOdule filler line or region \\n or extrapolation \\n module B Storage \\n module \\n Fig. 21', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 22}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 22 of 81 US 8,873,813 B2 \\n Z-Web, including Z-factors Related objects or expected objects \\n White \\n board On \\n Desktop the Wall \\n Computer \\n Inheritance of properties on The child class generally has more \\n Subclass (Or Sub-type Or Sub- Connecting nodes, as shown by \\n Category or specific example) dashed line boxes \\n X-ra Dental y cleaning machine equipment \\n H - - \\n board On \\n Desktop the Wall \\n Computer', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 23}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 23 of 81 US 8,873,813 B2 \\n Z-Web On \\n locations White board On based On - the Wall \\n COOrdinates of \\n - Objects, as proportional to \\n length dato2 Radius Of \\n Search, to find \\n related objects \\n in the COntext Or \\n environment dito2 (length of \\n branch), e.g. for physical -/ Cistance \\n between 2 Relative to \\n objects Z-node, - Center node \\n Node 1, (Node 1) Computer \\n - - \\n Expected or \\n / Node N, OUSG printer \\n -/ Can be expressed by relative average Or \\n median distance \\n distance, perCentage, \\n absolute number, or fuzzy parameter, e.g. \"far\" \\n Search input \\n module \\n Search for objects (in \\n Found Obiect Space Or imade Or Search Criteria Or Ound OOjectS Sp 9 SCope or rules module Storage video frame) module \\n Search module -/ Fig. 23 Related objects \\n Z-web input \\n module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 24}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 24 of 81 US 8,873,813 B2 \\n Rectangle shape in \\n perspective view or \\n COOrdinate System \\n POint at () infinity () \\n ( enlarged \\n PP \\n ( Projections on new \\n COOrdinates \\n Fig. 24', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 25}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 25 of 81 US 8,873,813 B2 \\n Reconstructing the \\n D past events NOce 6 \\n NOde 8 Jeff's brother's \\n Happiness Birthday party \\n (emotions) \\n NOde 4 NOde 1 y Jeff Smith Jeff's dad \\n Node N, 1N Smiling \\n face ReCollection \\n through Z-web, \\n starting from Node \\n N, and coming back \\n to Node 1, original \\n node symbol \\n Fig. 25\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 26}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 26 of 81 US 8,873,813 B2 \\n Arrow indicating the Means that table is Rules & y Conditions\\\\ operator \"OVER\" or over the legs \"ON-TOP-OF\" for \\n positional or \\n location description \\n COndition \\n Means that table is \\n upside down, with \\n legs Over the table \\n Earthquake \\n O War ZO6 \\n COndition \\n Rules storing \\n module or storage \\n Condition input \\n module Context analysis or \\n environmental \\n determination Context \\n Output', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 27}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 27 of 81 US 8,873,813 B2 \\n picture A () () \\n Eiffe John Joe Sun \\n TOWer \\n input \\n preprocessing \\n Image object \\n recognizer \\n mOnument faCe Natural object \\n recognizer recognizer recognizer \\n Eiffe \\n TOWer: \\n recognized \\n Z-Web Relation: Z-Web Z-Web Friend\\'Or \\n \"Family\\' G LOCation: y time: \"day \\n \"Paris\\' Fig 27 time\" Joe: John: Sun: \\n recognized recognized recognized', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 28}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 28 of 81 US 8,873,813 B2 \\n Multiple Input from History or \\n people Trained factual \\n VOting aS Computer- Catabase \\n input Input \\n module module \\n Training Samples Training \\n Input module module \\n Rules \\n builder \\n module Verified input samples \\n module \\n processor \\n Relationship \\n builder \\n module \\n Relationship \\n library \\n Fig. 28', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 29}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 29 Of 81 US 8,873,813 B2 \\n Still image input \\n Voice input module \\n module \\n video input \\n module \\n recognizer \\n Text Music Voice \\n recognizer recognizer recognizer bias factor \\n library \\n Reliability \\n factor \\n library Aggregator \\n analyzer \\n Confidence \\n factor \\n library Z-factors \\n truthfulness \\n factor \\n library \\n expertise \\n factOr \\n Fig. 29 library', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 30}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 30 of 81 US 8,873,813 B2 \\n ? y ? yy \"Jim\" July\" on the VaCation recognized \\n in photo \\n User input Z-web input \\n module \\n Z-factors Trainer trainer \\n Samples input module Z-Web \\n module \\n 2\" or other tag for photo in Comment \\n library module \\n Input from \\n fact fact module \\n library module \\n Conclusion module, e.g.: Output \\n \"Jim, On vacation, in module \\n Southern Hemisphere\" \\n Z-factors Input to Z \\n Web \\n module \\n Fig. 30', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 31}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 31 of 81 US 8,873,813 B2 \\n Library of Library of KnOWn Photo input FaCe faces input faces in the module recognizer SOCial module \\n network \\n y \"Jim \"Jason\" \\n recognized recognized \\n Photo or frame List of \\n matching with people Voice recognition eople, Or peop recognized module \\n tagging, module -record Or \\n Storage \\n PhOtO Or PhOtO Or n & Email \\n frame frame list \\n SelectOr attachment Construction \\n module module Input Email \\n module \\n module \\n Scheduling Email \\n module SeVer \\n Video Conferencing \\n module \\n Calendar \\n mOdule \\n Initiating \\n videO COnf. Fig. 31 module', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 32}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 32 of 81 US 8,873,813 B2 \\n Picture Or Video frame \\n O \\n generic \\n Eiffe John Beer Can \\n TOWer \\n input \\n Client: beer Company: \\n module \\n Input from \\n Local beer can image - \"g t : storage or library C OOUGe \\n Normalize size & adjust Find object (a beer can) in the \\n Color - Correction module image-search module \\n Replace module- replacing first beer can image with local beer can \\n Output to picture or frame exchange \\n Picture Or Video frame \\n Fig. 32', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 33}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 33 of 81 US 8,873,813 B2 \\n Relative, \\n Geometrical OCation \\n descriptor: \\n \"U\" shape Fuzzy Mathematical descriptor: \\n Curved shape descriptor, \\n W.r.t. the \\n rest OffaCe \\n Trained data \\n User input Z-web input module Tagged Samples \\n Z-factorS Trainer trainer \\n samples input module Z-Web \\n module \\n Other \\n data- input \\n Rule Rule module \\n library module \\n Context analyzer input \\n fact fact module, e.g. \\n library module \"at a party\" \\n Emotion determination Output \\n module, e.g.: \"Smiling module \\n face\" formulations \\n Z-factorS Input to Z \\n Web \\n module \\n Fig. 33', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 34}), Document(page_content=\"U.S. Patent Oct. 28, 2014 Sheet 34 of 81 US 8,873,813 B2 \\n Medical \\n Food allergy Drug Medical diagnosis interactions knowledge library library history library base \\n library \\n DOCtor's notes \\n library \\n User input Z-web input Nutritional module utritiona data library \\n Z-factors Trainer trainer \\n samples input module Z-Web \\n module \\n Image of \\n food-input \\n Rule Rule module \\n library module \\n Z-web FOOC \\n indredient fact fact analyzer E. \\n module library (determinator) \\n GOals Set Food proportion optimizer FOOC library \\n Output module Mobile \\n (including device Food proportion Calculator recommendations or PC, for \\n and Warnings) display \\n Fig. 34\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 35}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 35 of 81 US 8,873,813 B2 \\n Z-web input module \\n Rule database \\n task management \\n module (for agendas) \\n processing module \\n (Controller) \\n resolution module \\n Subgoals \\n Goal analyzing module KnOWledge database \\n Rule execution \\n mOdule', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 36}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 36 of 81 US 8,873,813 B2 \\n Z-web input module \\n Policy (with all rules listed) database \\n (e.g. Rules 1, 2, ..., N, or (R1,R2,..., RN)) \\n (e.g. Containing (if...Then ....) rules) \\n Involving parameters P1, P2, ..... PM \\n List all rules Storage for Rules involving goal \\n Output listed P3 \\n Original goal \\n (P3) value \\n Extract all parameters \\n involved in the \\n LOOp backward, to get all Selected rules above corresponding rules in the chain \\n activated, as we get all the \\n parameters figured out to activate \\n the rules Parameters \\n P1 & P8 \\n Storage for \\n value of \\n parameters Loop, until for one of \\n the Sub-goals, We have \\n a known value, which \\n fires Or activates the Sub-goal: P8 \\n Corresponding rule \\n Fig. 36', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 37}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 37 Of 81 US 8,873,813 B2 \\n Z-web input module \\n Rule database Knowledge database \\n task management \\n module (for agendas) \\n processing module Rule execution \\n (controller) module \\n Pattern \\n matching \\n Module (e.g. for \\n RETE) interpreter module \\n Goal analyzing module \\n exit Fig. 37', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 38}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 38 of 81 US 8,873,813 B2 \\n Fuzzy set \\n Z-web input module library \\n Rule database Knowledge database \\n task management \\n module (for agendas) \\n processing module \\n (controller & execution \\n resolver) \\n aggregator module \\n Fig. 38', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 39}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 39 Of 81 US 8,873,813 B2 \\n /N \\n And the tree \\n Structure \\n COntinues in this \\n nanner \\n C21 C22 \\n C12 \\n C13 C11 \\n Fig. 39', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 40}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 40 of 81 US 8,873,813 B2 \\n Z-web input module Fuzzy Rule database \\n Fuzzy inference \\n Fuzzy engine \\n COntroller \\n DefuZZification module \\n FuZZification (e.g. taking Center of \\n mOdule mass Or average or \\n weighted average for \\n area under the Curve) \\n COnditions actions \\n process \\n User manual input module \\n Fig. 40', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 41}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 41 of 81 US 8,873,813 B2 \\n Another expert \\n System \\n CaSCaded Or in \\n Series \\n Knowledge \\n acquisition module \\n Database interface or library \\n Inference engine Knowledge base library module \\n Meta-knowledge base library \\n Expert system \\n Fig. 41', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 42}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 42 of 81 US 8,873,813 B2 \\n Picture or video frame \\n (distances) 2 people away \\n far away (fuzzy term) \\n input \\n Image library \\n from history, \\n E. Image Object Input from Z pictures in recognizer Web module the photo \\n album, Or \\n Other frames Face recognition module \\n in video \\n Finding distances or positions of faces with respect to each other- for \\n one or more images- values or statistics- position module \\n Input from Z-web module Finding relationships \\n between people Fig. 42', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 43}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 43 of 81 US 8,873,813 B2 \\n Memory unit 1 Memory unit 2 Memory unit 3 \\n $104,322.34 About 100K LOW 6 figures \\n -CHD \\n More crisp More fuzzy \\n Larger Smaller \\n requirement requirement for \\n for storage Storage \\n Slower acCeSS Faster acCeSS \\n Use for Short term Use for long term \\n memory memory \\n -> time \\n In one embodiment, gradually fuzzify & move \\n the data toward memory unit 3, as time passes \\n Fig. 43', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 44}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 44 of 81 US 8,873,813 B2 \\n Vocabulary \\n Or \\n alphabets for pattern N- O \\n recognition \\n Store the encoded \\n patterns \\n Compare the encoded \\n patterns input \\n patterns \\n module Encode the patterns \\n with the alphabets \\n Pattern Fig 44 N- recognizer', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 45}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 45 of 81 US 8,873,813 B2 \\n Picture or Video frame \\n Object recognizer \\n Skeleton operator or thinning filter \\n basic shape \\n Geometrical \\n library Recognizer & \\n matching module \\n Inverted \"Y\" shape alphabets \\n library \\n Stored as \\n GeOmetrical textual descriptor library \\n descriptor (natural language or fuzzy \\n library descriptorS or parameters) \\n Fig. 45', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 46}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 46 of 81 US 8,873,813 B2 \\n /Variations on the right eye \\n O (6. and eyebrow \\n Operator \\n SL7 \\n Shear StreSS Or \\n deformation \\n Elastic model Operator, e.g. where \\n One side of the \\n object moves, but \\n the parallel side \\n stays as-is \\n A0 \\n Eyebrow \\n displacement O O N / \\n value COOrdinates Eve AY y displacement \\n Fig. 46 AX', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 47}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 47 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 48}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 48 of 81 US 8,873,813 B2 \\n - Image of an eye \\n Mimic the \\n shape of an \\n eye (the object) \\n Or, use a more approximated \\n or simpler Use it as a \\n version basis object 17 (in library) \\n Fig. 48', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 49}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 49 of 81 US 8,873,813 B2 \\n Input preprocessing Segmentation Classification \\n device \\n st Or Knowledge base Recognition \\n nory database Module \\n unitS interface & \\n task \\n assignment \\n Video recognizer mOdule face \\n recognizer \\n Recognition Output \\n Natural locations module module \\n and geographical \\n recognizer Image \\n recognition \\n Monuments module \\n and building \\n recognizer Voice \\n recognizer \\n Medical image OCR for text \\n analyzer recognition \\n module \\n Fig. 49', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 50}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 50 Of 81 US 8,873,813 B2 \\n Still image \\n music Voice text \\n Z-web input \\n module \\n table \\n dictionaries \\n encyclopediaS \\n Computer \\n generated \\n data Medical Public forms Images reCOrds \\n Fig. 50', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 51}), Document(page_content='U.S. Patent \\n \"Behind\" \\n operator \\n \"position\" \\n operator \\n Trainer \\n Samples input \\n mOdule \\n Super-template \\n Creating module \\n Rule COnclusion \\n mOdule \\n Fig. 51 Oct. 28, 2014 Sheet 51 of 81 \\n \"more\" \\n operator \"Over\" \\n operator \"ti me\" \\n operator \\n \"less\" rator rul Operator rules Operator and logic \\n library \\n New rules \\n Input Operator mOdule \\n rules \\n applying \\n module trainer \\n module \\n Z-Web \\n Input from \\n analyzer Z-web \\n module \\n Relationship Output \\n establishing module \\n Or extraction \\n module \\n Z-factors \\n Z-web US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 52}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 52 of 81 US 8,873,813 B2 \\n Computer \\n Carea SenSO detector generated \\n data \\n Big Data \\n Trainer trainer \\n Samples input module Z-web \\n mOdule \\n Z-factors \\n Calculator \\n Rule Rule \\n library module \\n Fuzzy an. rules fact fact y engine \\n library module \\n Mobile device or PC, for 1 Math processor function display Or Output library \\n Kernel Library DCT FOUrier \\n FFT, Wavelet, and CustOmized \\n Markov model, Bayesian Other basis basis functions \\n model, and other models functions Or module Pattern Or User input object \\n eigenvectors \\n library Fig. 52', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 53}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 53 Of 81 US 8,873,813 B2 \\n Z-web (for analysis, search, Z-factors (e.g. \\n relationships, and extracted data) reliability factor, bias factor, and \\n truthfulness factor) \\n URL Qstore (storage sh plus \\n Catabase \\n plus \\n (Wsite) \\n Direct 3CCCSS (e.g text, \\n image, and \\n numbers) \\n Internet \\n Search, \\n Query \\n User (U) \\n (User interface, browser \\n Computer or Optional plug-in \\n mobile device) \\n Fig. 53', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 54}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 54 of 81 US 8,873,813 B2 \\n Dissecting & Task Classification Input Segmentation assignment module device module module \\n Category or type 3 Category or type 2 Category or type 1 \\n Transformation Transformation Transformation \\n module module module \\n analyzer \\n SubCategory M \\n 3.1 Membership \\n transformation C lati values \\n mOCdule O63.On \\n module \\n Subcategory \\n 3.2 \\n transformation \\n module aggregator \\n mOdule Sub \\n SubCategory \\n 3.2.1 \\n transformation \\n module \\n And type hierarchy Fig. 54 \\n COntinues', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 55}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 55 Of 81 US 8,873,813 B2 \\n 1N model templates \\n in library-2 \\n methods \\n Na \\n 2 \\n N- Grid model \\n Region model with Kregions & M \\n relationships \\n Fig. 55', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 56}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 56 of 81 US 8,873,813 B2 \\n W \\n B 45 \\n degree () (2 W \\n And similar \\n OCS ......', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 57}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 57 Of 81 US 8,873,813 B2 \\n Or use the \\n tilted One \\n (e.g. at 45 \\n degrees) \\n (e.g. at 45 degrees) \\n Fig. 57', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 58}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 58 Of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 59}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 59 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 60}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 60 of 81 US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 61}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 61 of 81 US 8,873,813 B2 \\n (1)T W \\n (2)T W \\n (3)T W \\n W(8) \\n W(2) \\n w) \\n data \\n FIG. 62', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 62}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 62 of 81 US 8,873,813 B2 \\n Labels Data (e.g., image) \\n FIG. 63 \\n Degree of \\n Correlation Or Features/ \\n Conformity Classification \\n NetWork/Classifier/Feature Detector \\n Labels Data (e.g., image) \\n FIG. 64', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 63}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 63 of 81 US 8,873,813 B2 \\n w) \\n Labels Data (e.g., image) \\n Model Parameters \\n Parameters Ranges/Constraints || Model Function(s) \\n Sample generator \\n FIG. 65', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 64}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 64 of 81 US 8,873,813 B2 \\n For Model M For Model M2 \\n FIG. 66', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 65}), Document(page_content='US 8,873,813 B2 Sheet 65 of 81 Oct. 28, 2014 U.S. Patent \\n FIG. 67', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 66}), Document(page_content='US 8,873,813 B2 Sheet 66 of 81 Oct. 28, 2014 U.S. Patent \\n ., image pixels \\n FIG. 68', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 67}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 67 of 81 US 8,873,813 B2 \\n Features/Classifications/Recognition \\n Expert Mixer \\n Recognition/Classifier Chooser/Scheduler \\nrt \\n Preprocessing Preprocessing \\n Data (e.g., image) \\n FIG. 69', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 68}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 68 of 81 US 8,873,813 B2 \\n FIG. 70(a) \\n FIG.70(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 69}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 69 of 81 \\nA) E1. xy \\n FIG. 71 (b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 70}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 70 of 81 \\n FIG. 72(a) \\n FIG.72(b) US 8,873,813 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 71}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 71 of 81 US 8,873,813 B2 \\n FIG. 73(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 72}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 72 of 81 US 8,873,813 B2 \\n SSSSNSNSSSSSSNSCY SSSSSSSSR3NNSSSC \\n ow \\n a 1 SS (SSNSSS) 1 1 - asSNSSNSSP a \\n Data, e.g., image pixels \\n FIG. 74', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 73}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 73 Of 81 US 8,873,813 B2 \\n Data, e.g., Sparse units approximating low res. thumbnail \\n FIG. 75(a) \\n Thumbnail \\n pixel Data, e.g., sparse V \\n units fed from thumbnail \\n Data, e.g., thumbnail wide pixels on V units \\n FIG. 75(c)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 74}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 74 of 81 US 8,873,813 B2 \\n Data (e.g., image) \\n Object detector/classifier \\n Object/Concept \\n Correlating \\n Objects/Concepts Correlation Weights, \\n Context Clusters \\n A 7 \\n Object detectors/classifiers \\n Additional Objects/Concepts \\n FIG. 76', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 75}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 75 Of 81 US 8,873,813 B2 \\n Pixel Size 41 In-Pixel Size 4N PN NarroWest MYAEEEN IN 24 \\n WindoW Of EEEEE A - Pixel Size \\n FOCUS \\n NarroWer \\n WindoW Of \\n FOCUS \\n FIG. 77(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 76}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 76 of 81 US 8,873,813 B2 \\n L1 Li Li LN \\n E.g.: PerSOn Car \\n Labels Data (e.g., image) \\n Annotation: Jim \\n David \\n Eiffel tower', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 77}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 77 of 81 US 8,873,813 B2 \\n Feature detector/classifier \\n (e.g., with correlation) \\n Data (e.g., image) \\n Jim \\n David \\n Date? Time \\n GPS \\n Annotation and metadata \\n FIG. 79', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 78}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 78 of 81 US 8,873,813 B2 \\n Correlator/Analyzer \\n AL Features of Features of \\n Image 1 Image 2 \\n Without \\n Feature f \\n Database \\n FIG. 80', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 79}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 79 of 81 US 8,873,813 B2 \\n Network Or Sources \\n DOmain/NetWOrk Of Information \\n Traffic Data/Statistics Fetch \\n S Bots or bkgnd processes \\n Content, metadata, tags URL \\n Updates \\n Cache Analytics Bkgnd \\n processes \\n Bkgnd (me) \\n processes Category \\n N \\n N \\n N \\\\ \\n requests and Fetching Content/ \\n Selections Info/URL \\n Ranking Query Content/ \\n Engine Selection Summary/ \\n Ranked URL \\n Result \\n FIG. 81 S', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 80}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 80 of 81 US 8,873,813 B2 \\n CRBM \\n Frame-2 Frame- Frameo \\n FIG. 82(b)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 81}), Document(page_content='U.S. Patent Oct. 28, 2014 Sheet 81 of 81 US 8,873,813 B2 \\n Linear COmbination \\n based on mapping \\n (i.e., resolution reduction) \\n Frame att Wo State att \\n FIG. 83(b) \\n Contribution to dynamic \\n mean based on the mapping \\n Frame att Wo State att \\n FIG. 83(c)', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 82}), Document(page_content=\"US 8,873,813 B2 \\n 1. \\n APPLICATION OF Z-WEBS AND Z-FACTORS \\n TO ANALYTICS, SEARCH ENGINE, \\n LEARNING, RECOGNITION, NATURAL \\n LANGUAGE, AND OTHERUTILITIES \\n RELATED APPLICATIONS \\n This application claims the benefit of the following appli \\n cation, with the same inventors: The U.S. provisional appli cation No. 61/701,789, filed Sep. 17, 2012, by Tadayonet.al, \\n titled “Method and system for approximate Z-number evalu ation based on categorical sets of probability distributions'. \\n The current application incorporates by reference all of the teachings of the provisional application, including all of its \\n appendices and attachments. It also claims benefits of the earlier (provisional) application. \\n BACKGROUND OF THE INVENTION \\n There are a lot of research going on today, focusing on the search engine, analytics, Big Data processing, natural lan guage processing, economy forecasting, dealing with reli \\n ability and certainty, medical diagnosis, pattern recognition, \\n object recognition, biometrics, security analysis, risk analy \\n sis, fraud detection, satellite image analysis, machine gener ated data, machine learning, training samples, and the like. \\n For example, see the article by Technology Review, pub \\n lished by MIT, “Digging deeper in search web, Jan. 29. \\n 2009, by Kate Greene, or search engine by GOOGLE(R), \\n MICROSOFTR) (BINGO), or YAHOOR, or APPLERSIRI, or WOLFRAMR ALPHA computational knowledge engine, \\n or AMAZON engine, or FACEBOOKR engine, or \\n ORACLER database, orYANDEXOR search engine in Russia, \\n or PICASAR (GOOGLE(R) web albums, or YOUTUBER (GOOGLE(R) engine, or ALIBABA (Chinese supplier con \\n nection), or SPLUNKR (for Big Data), or MICROSTRAT EGYR (for business intelligence), or QUID (or KAGGLE, \\n ZESTFINANCE, APIXIO, DATAMEER, BLUEKAI, GNIP, \\n RETAILNEXT, or RECOMMIND) (for Big Data), or paper by Viola-Jones, Viola et al., at Conference on Computer Vision and Pattern Recognition, 2001, titled “Rapid object \\n detection using a boosted cascade of simple features, from \\n Mitsubishi and Compaq research labs, or paper by Alex Pent \\n land et al., February 2000, at Computer, IEEE, titled “Face \\n recognition for smart environments', or GOOGLE(R) official blog publication, May 16, 2012, titled “Introducing the \\n knowledge graph: things, not strings’, or the article by Tech \\n nology Review, published by MIT, “The future of search”. \\n Jul. 16, 2007, by Kate Greene, or the article by Technology \\n Review, published by MIT, “Microsoft searches for group \\n advantage'. Jan. 30, 2009, by Robert Lemos, or the article by Technology Review, published by MIT, “WOLFRAM \\n ALPHA and GOOGLE face off, May 5, 2009, by David Talbot, or the paper by Devarakonda et al., at International \\n Journal of Software Engineering (IJSE), Vol. 2, Issue 1, 2011, titled “Next generation search engines for information \\n retrieval’, or paper by Nair-Hinton, titled “Implicit mixtures \\n of restricted Boltzmann machines, NIPS, pp. 1145-1152, 2009, or paper by Nair, V. and Hinton, G. E., titled “3-D Object recognition with deep belief nets', published in \\n Advances in Neural Information Processing Systems 22. (Y. \\n Bengio, D. Schuurmans, J. lafferty, C. K. I. Williams, and A. \\n Culotta (Eds.)), pp 1339-1347. \\n One of such research and recent advances is done by Prof Lotfi Zadeh, of UC Berkeley, “the Father of Fuzzy Logic', who recently came up with the concept of Z-numbers, plus \\n related topics and related technologies. In the following sec 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n tion, we discuss the Z-numbers, taught by the U.S. Pat. No. 8.311,973, by Zadeh (issued recently). \\n Z-Numbers: \\n This section about Z-numbers is obtained from the patent by Zadeh, namely, the U.S. Pat. No. 8,311,973, which \\n addresses Z-numbers and its applications, as well as other concepts.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 83}), Document(page_content=\"Z-Numbers: \\n This section about Z-numbers is obtained from the patent by Zadeh, namely, the U.S. Pat. No. 8,311,973, which \\n addresses Z-numbers and its applications, as well as other concepts. \\n A Z-number is an ordered pair of fuzzy numbers, (A,B). For simplicity, in one embodiment, A and B areassumed to be trapezoidal fuzzy numbers. A Z-number is associated with a \\n real-valued uncertain variable, X, with the first component, A, playing the role of a fuzzy restriction, RCX), on the values \\n which X can take, written as X is A, where A is a fuzzy set. What should be noted is that, strictly speaking, the concept of \\n a restriction has greater generality than the concept of a con \\n straint. A probability distribution is a restriction but is not a \\n constraint (see L.A. Zadeh, Calculus of fuZZy restrictions, In: \\n L. A. Zadeh, K. S. Fu, K. Tanaka, and M. Shimura (Eds.), Fuzzy sets and Their Applications to Cognitive and Decision \\n Processes, Academic Press, New York, 1975, pp. 1-39). A restriction may be viewed as a generalized constraint (see L. A. Zadeh, Generalized theory of uncertainty (GTU) princi pal concepts and ideas, Computational Statistics & Data \\n Analysis 51, (2006) 15-46). In this embodiment only, the \\n terms restriction and constraint are used interchangeably. \\n The restriction \\n is referred to as a possibilistic restriction (constraint), with A playing the role of the possibility distribution of X. More specifically, \\n where L is the membership function of A, and u is a \\n generic value of X. L. may be viewed as a constraint which is associated with RCX), meaning that u(u) is the degree to \\n which u satisfies the constraint. \\n When X is a random variable, the probability distribution of X plays the role of a probabilistic restriction on X. A probabilistic restriction is expressed as: \\n where p is the probability density function of X. In this \\n Case, \\n Note. Generally, the term “restriction' applies to X is R. \\n Occasionally, “restriction' applies to R. Context serves to disambiguate the meaning of “restriction.” \\n The ordered triple (XA,B) is referred to as a Z-valuation. A Z-valuation is equivalent to an assignment statement, X is \\n (A,B). X is an uncertain variable if A is not a singleton. In a related way, uncertain computation is a system of computa \\n tion in which the objects of computation are not values of \\n variables but restrictions on values of variables. In this \\n embodiment/section, unless stated to the contrary, X is \\n assumed to be a random variable. For convenience, A is \\n referred to as a value of X, with the understanding that, strictly speaking, A is not a value of X but a restriction on the \\n values which X can take. The second component, B, is referred to as certainty. Certainty concept is related to other \\n concepts, such as Sureness, confidence, reliability, strength of \\n belief, probability, possibility, etc. However, there are some \\n differences between these concepts. \\n In one embodiment, when X is a random variable, certainty may be equated to probability. Informally, B may be inter \\n preted as a response to the question: How Sure are you that X\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 83}), Document(page_content='US 8,873,813 B2 \\n 3 \\n is A2 Typically, A and B are perception-based and are \\n described in a natural language. Example: (about 45 minutes, \\n usually.) A collection of Z-valuations is referred to as Z-in \\n formation. It should be noted that much of everyday reason ing and decision-making is based, ineffect, on Z-information. \\n For purposes of computation, when A and B are described in a natural language, the meaning of A and B is precisiated \\n (graduated) through association with membership functions, \\n LL and LL, respectively, FIG. 1. \\n The membership function of A. L. may be elicited by \\n asking a Succession of questions of the form: To what degree does the number, a, fit your perception of A2 Example: To \\n what degree does 50 minutes fit your perception of about 45 \\n minutes? The same applies to B. The fuzzy set, A, may be interpreted as the possibility distribution of X. The concept of a Z-number may be generalized in various ways. In particular, \\n X may be assumed to take values in R\", in which case A is a Cartesian product of fuzzy numbers. Simple examples of \\n Z-valuations are: \\n (anticipated budget deficit, close to 2 million dollars, very likely) \\n (population of Spain, about 45 million, quite Sure) (degree of Robert\\'s honesty, very high, absolutely) \\n (degree of Robert\\'s honesty, high, not sure) \\n (travel time by car from Berkeley to San Francisco, about 30 minutes, usually) \\n (price of oil in the near future, significantly over 100 dol lars/barrel, very likely) \\n It is important to note that many propositions in a natural language are expressible as Z-valuations. Example: The proposition, p. \\n p: Usually, it takes Robert about an hour to get home from \\n work, is expressible as a Z-valuation: \\n (Robert\\'s travel time from office to home, about one hour, usually) \\n If X is a random variable, then X is A represents a fuzzy event in R, the real line. The probability of this event, p, may \\n be expressed as (see L. A. Zadeh, Probability measures of fuzzy events, Journal of Mathematical Analysis and Applica \\n tions 23 (2), (1968) 421-427.): \\n where p is the underlying (hidden) probability density of \\n X. In effect, the Z-valuation (XA.B) may be viewed as a restriction (generalized constraint) on X defined by: \\n Prob(X is A) is B. \\n What should be underscored is that in a Z-number, (A,B), the underlying probability distribution, p. is not known. \\n What is known is a restriction on p which may be expressed \\n aS \\n Note: In this embodiment only, the term “probability dis \\n tribution\\' is not used in its strict technical sense. \\n In effect, a Z-number may be viewed as a Summary of p. It is important to note that in everyday decision-making, most \\n decisions are based on Summaries of information. Viewing a Z-number as a Summary is consistent with this reality. In 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n applications to decision analysis, a basic problem which \\n arises relates to ranking of Z-numbers. Example: Is (approxi \\n mately 100, likely) greater than (approximately 90, very \\n likely)? Is this a meaningful question? We are going to \\n address these questions below. \\n An immediate consequence of the relation between p and \\n B is the following. If Z=(A,B) then Z=(A\\'.1-B), where A\\' is the complement of A and Z plays the role of the complement \\n of Z. 1-B is the antonym of B (see, e.g., E.Trillas, C. Moraga, \\n S. Guadarrama, S. Cubillo and E. Castifieira, Computing with \\n Antonyms, In: M. Nikravesh, J. Kacprzyk and L. A. Zadeh \\n (Eds.), Forging New Frontiers: Fuzzy Pioneers I, Studies in \\n Fuzziness and Soft Computing Vol 217, Springer-Verlag, \\n Berlin Heidelberg 2007, pp. 133-153.). \\n An important qualitative attribute of a Z-number is infor \\n mativeness. Generally, but not always, a Z-number is infor \\n mative if its value has high specificity, that is, is tightly con \\n strained (see, for example, R. R. Yager. On measures of', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 84}), Document(page_content=\"mativeness. Generally, but not always, a Z-number is infor \\n mative if its value has high specificity, that is, is tightly con \\n strained (see, for example, R. R. Yager. On measures of \\n specificity, In: O. Kaynak, L. A. Zadeh, B. Turksen, I. J. Rudas (Eds.), Computational Intelligence: Soft Computing \\n and FuZZy-Neuro Integration with Applications, Springer \\n Verlag, Berlin, 1998, pp. 94-113.), and its certainty is high. \\n Informativeness is a desideratum when a Z-number is a basis \\n for a decision. It is important to know that if the informative \\n ness of a Z-number is sufficient to serve as a basis for an \\n intelligent decision. \\n The concept of a Z-number is after the concept of a fuzzy \\n granule (see, for example, L.A. Zadeh, Fuzzy sets and infor mation granularity, In: M. Gupta, R. Ragade, R. Yager (Eds.), \\n Advances in Fuzzy Set Theory and Applications, North-Hol \\n land Publishing Co., Amsterdam, 1979, pp. 3-18. Also, see L. A. Zadeh, Possibility theory and soft data analysis, In: L. \\n Cobb, R. M. Thrall (Eds.), Mathematical Frontiers of the Social and Policy Sciences, Westview Press, Boulder, Colo., \\n 1981, pp. 69-129. Also, see L. A. Zadeh, Generalized theory of uncertainty (GTU) principal concepts and ideas, Com \\n putational Statistics & Data Analysis 51, (2006) 15-46.). It \\n should be noted that the concept of a Z-number is much more general than the concept of confidence interval in probability \\n theory. There are some links between the concept of a Z-num ber, the concept of a fuZZy random number and the concept of \\n a fuZZy random variable (see, e.g., J. J. Buckley, J. J. Leonard, \\n Chapter 4: Random fuzzy numbers and vectors, In: Monte Carlo Methods in Fuzzy Optimization, Studies in Fuzziness and Soft Computing 222, Springer-Verlag, Heidelberg, Ger \\n many, 2008. Also, see A. Kaufman, M. M. Gupta, Introduc tion to Fuzzy Arithmetic: Theory and Applications, Van Nos \\n trand Reinhold Company, New York, 1985. Also, see C. V. Negoita, D. A. Ralescu, Applications of Fuzzy Sets to Sys \\n tems Analysis, Wiley, New York, 1975.). A concept which is closely related to the concept of a \\n Z-number is the concept of a Z-number. Basically, a \\n Z'-number, Z is a combination of a fuZZy number, A, and a random number, R, written as an ordered pair Z=(AR). In this pair. A plays the same role as it does in a Z-number, and R is the probability distribution of a random number. Equiva lently, R may be viewed as the underlying probability distri \\n bution of X in the Z-valuation (X, A,B). Alternatively, a Z'-number may be expressed as (Ap) or (Lp), where LL \\n is the membership function of A. A Z-valuation is expressed as (X.A.p.) or, equivalently, as (XLL.p.), where p is the \\n probability distribution (density) of X. A Z-number is asso \\n ciated with what is referred to as a bimodal distribution, that \\n is, a distribution which combines the possibility and probabil \\n ity distributions of X. Informally, these distributions are com \\n patible if the centroids of L and pare coincident, that is,\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 84}), Document(page_content='US 8,873,813 B2 \\n 5 \\n I-4 (a) du R - JAA (u) du \\n The scalar product of L and p, up is the probability \\n measure, P., of A. More concretely, \\n It is this relation that links the concept of a Z-number to that of a Z-number. More concretely, \\n What should be underscored is that in the case of a Z-num \\n ber what is known is not p but a restriction on p. expressed \\n as: LLp is B. By definition, a Z-number carries more \\n information than a Z-number. This is the reason why it is labeled a Z-number. Computation with Z\\'-numbers is a portal to computation with Z-numbers. \\n The concept of a bimodal distribution is of interest in its own right. Let X be a real-valued variable taking values in U. \\n For our purposes, it is convenient to assume that U is a finite set, U-u. . . . , u}. We can associate with X a possibility distribution, L, and a probability distribution, p, expressed as: \\n in which u?u, means that Li, i=1,... n, is the possibility that Xu. Similarly, p,\\\\u, means that p, is the probability that \\n X=u. The possibility distribution, L, may be combined with the probability distribution, p, through what is referred to as \\n confluence. More concretely, \\n AS was noted earlier, the Scalar product, expressed as up, \\n is the probability measure of A. In terms of the bimodal \\n distribution, the Z-valuation and the Z-valuation associated with X may be expressed as: \\n (X, Apx) \\n (X, A,B). Lpx is B, \\n respectively, with the understanding that B is a possibilistic \\n restriction on up. \\n Both Zand Z\\' may be viewed as restrictions on the values which X may take, written as: X is Zand X is Z\", respectively. Viewing Z and Z as restrictions on X adds important con \\n cepts to representation of information and characterization of \\n dependencies. In this connection, what should be noted is that the concept of a fuZZy if-then rule plays a pivotal role in most applications of fuzzy logic. What follows is a very brief \\n discussion of what are referred to as Z-rules—if-then rules in \\n which the antecedents and/or consequents involve Z-numbers \\n or Z-numbers. \\n A basic fuzzy if-then rule may be expressed as: if X is A \\n then Y is B, where A and B are fuzzy numbers. The meaning \\n of such a rule is defined as: \\n if X is Athen Y is B->(X,Y) is AxB \\n where AxB is the Cartesian product of A and B. It is convenient to express a generalization of the basic if-then rule \\n to Z-numbers in terms of Z-valuations. More concretely, \\n if (X, A, B) then (YA,B) 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n Examples \\n if (anticipated budget deficit, about two million dollars, \\n very likely) then (reduction in staff, about ten percent, very likely) \\n if (degree of Robert\\'s honesty, high, not sure) then (offer a \\n position, not, Sure) \\n if (X, Small) then (Y large, usually.) \\n An important question relates to the meaning of Z-rules \\n and Z\\'-rules. The meaning of a Z-rule may be expressed as: \\n if (X, Axspx) then (YApy)- (X,Y) is (AxxApxpy) \\n where AXA is the Cartesian product A and A. Z-rules have the important applications in decision analy \\n sis and modeling of complex systems, especially in the realm \\n of economics (for example, Stock market and specific Stocks) \\n and medicine (e.g. diagnosis and analysis). \\n A problem which plays a key role in many applications of \\n fuZZylogic, especially in the realm of fuZZy control, is that of \\n interpolation. More concretely, the problem of interpolation \\n may be formulated as follows. Consider a collection of fuzzy \\n if-then rules of the form: \\n if X is A, then Y is B. i=1,..., 2 \\n where the A, and B, are fuzzy sets with specified member \\n ship functions. If X is A, where A is not one of the A, then \\n what is the restriction on Y? \\n The problem of interpolation may be generalized in various \\n ways. A generalization to Z-numbers may be described as \\n follows. Consider a collection Z-rules of the form: \\n if X is A, then usually (Y is B.), i=1,..., 2 \\n where the A, and B, are fuzzy sets. Let A be a fuzzy set', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 85}), Document(page_content=\"follows. Consider a collection Z-rules of the form: \\n if X is A, then usually (Y is B.), i=1,..., 2 \\n where the A, and B, are fuzzy sets. Let A be a fuzzy set \\n which is not one of the A. What is the restriction on Y \\n expressed as a Z-number? An answer to this question would \\n add a useful formalism to the analysis of complex systems \\n and decision processes. \\n Representation of Z-numbers can be facilitated through the \\n use of what is called a Z-mouse. Basically, a Z-mouse is a \\n visual means of entry and retrieval of fuZZy data. \\n The cursor of a Z-mouse is a circular fuZZy mark, called an f-mark, with a trapezoidal distribution of light intensity. This distribution is interpreted as a trapezoidal membership func \\n tion of a fuzzy set. The parameters of the trapezoid are con \\n trolled by the user. A fuzzy number such as “approximately 3” \\n is represented as an f-mark on a scale, with 3 being the \\n centroid of the f-mark (FIG. 2a). The size of the f-mark is a measure of the user's uncertainty about the value of the num ber. As was noted already, the Z-mouse interprets an f-markas the membership function of a trapezoidal fuzzy set. This membership function serves as an object of computation. A \\n Z-mouse can be used to draw curves and plot functions. A key idea which underlies the concept of a Z-mouse is that visual interpretation of uncertainty is much more natural than its description in natural language or as a membership func \\n tion of a fuzzy set. This idea is closely related to the remark able human capability to precisiate (graduate) perceptions, \\n that is, to associate perceptions with degrees. As an illustra \\n tion, if I am asked “What is the probability that Obama will be \\n reelected?' I would find it easy to put an f-mark on a scale \\n from 0 to 1. Similarly, I could put an f-mark on a scale from \\n 0 to 1 if I were asked to indicate the degree to which I like my \\n job. It is of interest to note that a Z-mouse could be used as an informative means of polling, making it possible to indicate\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 85}), Document(page_content=\"US 8,873,813 B2 \\n 7 \\n one's strength offeeling about an issue. Conventional polling \\n techniques do not assess strength of feeling. \\n Using a Z-mouse, a Z-number is represented as two \\n f-marks on two different scales (FIG. 2b). The trapezoidal \\n fuzzy sets which are associated with the f-marks serve as objects of computation. \\n Computation with Z-Numbers: \\n What is meant by computation with Z-numbers? Here is a simple example. Suppose that I intend to drive from Berkeley \\n to San Jose via Palo Alto. The perception-based information \\n which I have may be expressed as Z-valuations: (travel time \\n from Berkeley to Palo Alto, about an hour, usually) and (travel \\n time from Palo Alto to San Jose, about twenty-five minutes, usually.) How long will it take me to drive from Berkeley to \\n San Jose? In this case, we are dealing with the sum of two Z-numbers (about an hour, usually) and (about twenty-five minutes, usually.) Another example: What is the square root \\n of (A,B)? Computation with Z-numbers falls within the prov \\n ince of Computing with Words (CW or CWW). Example: \\n What is the square root of a Z-number? Computation with Z-numbers is much simpler than com putation with Z-numbers. Assume that * is a binary operation \\n whose operands are Z-numbers, Z (AR) and Z (A \\n R.) By definition, \\n with the understanding that the meaning of * in R*R is \\n not the same as the meaning of in AA. In this expression, the operands of in AA are fuzzy numbers; the operands \\n of in R*R are probability distributions. \\n Example: Assume that * is sum. In this case, A+A is \\n defined by: \\n Similarly, assuming that R and R are independent, the \\n probability density function of R*R is the convolution, O, of the probability density functions of R and R. Denoting these probability density functions as p and per respec tively, we have: \\n Thus, \\n It should be noted that the assumption that R and R are independent implies worst case analysis. \\n More generally, to compute ZZ what is needed is the \\n extension principle of fuzzy logic (see, e.g., L. A. Zadeh, \\n Probability measures of fuzzy events, Journal of Mathemati \\n cal Analysis and Applications 23 (2), (1968) 421-427.). Basi cally, the extension principle is a rule for evaluating a function \\n when what are known are not the values of arguments but \\n restrictions on the values of arguments. In other words, the \\n rule involves evaluation of the value of a function under less \\n than complete information about the values of arguments. Note. Originally, the term “extension principle” was \\n employed to describe a rule which serves to extend the \\n domain of definition of a function from numbers to fuzzy \\n numbers. In this disclosure, the term “extension principle” \\n has a more general meaning which is stated in terms of restric \\n tions. What should be noted is that, more generally, incom pleteness of information about the values of arguments 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n applies also to incompleteness of information about func \\n tions, in particular, about functions which are described as \\n collections of if-then rules. \\n There are many versions of the extension principle. A basic \\n version was given in the article: (L. A. Zadeh, Fuzzy sets, \\n Information and Control 8, (1965) 338-353.). In this version, the extension principle may be described as: \\n R(X): X is A (constraint on u is a A (it)) \\n R(Y): uy (v) = suppa A (u) (f(A) = R(Y)) \\n Subject to \\n where A is a fuzzy set, L is the membership function of A, \\n L is the membership function of Y, and u and V are generic \\n values of X and Y, respectively. \\n A discrete version of this rule is: \\n R(Y): uy (v) = Sup., H: \\n Subject to \\n V = f(iii) \\n In a more general version, we have \\n R(X): g(X) is A (constraint on u is uta (g (it))) \\n R(Y): uty (V) = Supt A (g(u)) \\n Subject to \\n For a function with two arguments, the extension principle \\n reads: \\n Z=f(X, Y) \\n R(X): g(X) is A (constraint on u is ul A (g (it)))\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 86}), Document(page_content='R(Y): uty (V) = Supt A (g(u)) \\n Subject to \\n For a function with two arguments, the extension principle \\n reads: \\n Z=f(X, Y) \\n R(X): g(X) is A (constraint on u is ul A (g (it))) \\n R(Y): h(Y) is B (constraint on u is up (h(u))) \\n R(Z): uz(w) = Sup, (4 x (g(u)) A ply (h(v))), \\n Wherein: A = min \\n Subject to \\n In application to probabilistic restrictions, the extension \\n principle leads to results which coincide with standard results \\n which relate to functions of probability distributions. Specifi \\n cally, for discrete probability distributions, we have:', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 86}), Document(page_content=\"US 8,873,813 B2 \\n subject to \\n V = f(iii) \\n For functions with two arguments, we have: \\n subject to \\n For the case where the restrictions are Z-numbers, the \\n extension principle reads: \\n Z=f(X, Y) \\n R(X): X is (Aix, px) \\n R(Y): Y is (Ay, py) \\n R(Z): Z is (f(AY, Ay), f(p x, py)) \\n It is this version of the extension principle that is the basis \\n for computation with Z-numbers. Now, one may want to \\n know if f(pp) is compatible with f(AA). \\n Turning to computation with Z-numbers, assume for sim \\n plicity that *=Sum. Assume that Z (AB) and Z (A B). Our problem is to compute the sum Z=X-Y. Assume that \\n the associated Z-valuations are (X, A, B), (Y, A, B) and \\n (Z. A2 B2). \\n The first step involves computation of p2. To begin with, let \\n us assume that p and pare known, and let us proceed as we did in computing the sum of Z'-numbers. Then \\n or more concretely, \\n In the case of Z-numbers what we know are not pand p but restrictions on p and p \\n ?ey (u)py (ii)d it is By \\n R \\n In terms of the membership functions of B and B, these restrictions may be expressed as: 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 10 \\n He has a produ) R \\n Additional restrictions on p and pare: \\n (compatibility) J. Hay (u)du \\n up A (u)du \\n upy (u)du = R - (compatibility) ? y J. Hay (u)du \\n Applying the extension principle, the membership func \\n tion of p may be expressed as: \\n Subject to \\n J. Hay (u) du \\n up A (u) du \\n upy (u) du = R - ? y J. Hay (u) du \\n In this case, the combined restriction on the arguments is \\n expressed as a conjunction of their restrictions, with m inter \\n preted as min. In effect, application of the extension principle \\n reduces computation of p2 to a problem in functional optimi \\n zation. What is important to note is that the solution is not a \\n value of plbutarestriction on the values of p, consistent with \\n the restrictions on p and p. \\n At this point it is helpful to pause and Summarize where we \\n stand. Proceeding as if we are dealing with Z'-numbers, we \\n arrive at an expression for pas a function of pand p. Using \\n this expression and applying the extension principle we can \\n compute the restriction on pe which is induced by the restric \\n tions on p and p. The allowed values of p consist of those values of p2, which are consistent with the given information, with the understanding that consistency is a matter of degree. \\n The second step involves computation of the probability of \\n the fuzzy event, Z is A2, given pe. As was noted earlier, in fuzzy logic the probability measure of the fuzzy event X is A. \\n where A is a fuzzy set and X is a random variable with probability density p is defined as:\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 87}), Document(page_content='US 8,873,813 B2 \\n 11 \\n 4 (a)p(a)du R \\n Using this expression, the probability measure of AZ may \\n be expressed as: \\n B = A, op.(a)du, 10 R \\n where \\n H.A. (u) = Sup, (pt A (v) A play (u - V)) \\n 15 \\n It should be noted that B2 is a number when pe is a known probability density function. Since what we know about p is its possibility distribution, u(p2), B2 is a fuzzy set with membership function le. Applying the extension principle, we arrive at an expression for ua More specifically, 2O \\n HBZ (w) = supplpz (Pz) \\n subject to 25 \\n where u(p2) is the result of the first step. In principle, this 30 completes computation of the Sum of Z-numbers, Z and Z. \\n In a similar way, we can compute various functions of \\n Z-numbers. The basic idea which underlies these computa tions may be summarized as follows. Suppose that our prob \\n lem is that of computing f(ZZ), where Z and Z are \\n Z-numbers, Z (AB) and Z (AB), respectively, and f(ZZ)-(A2B2). We begin by assuming that the underlying \\n probability distributions p and pare known. This assump tion reduces the computation of f(ZZ) to computation of \\n f(Z.Z), which can be carried out through the use of the version of the extension principle which applies to restric tions which are Z-numbers. At this point, we recognize that \\n what we know are not p and pbut restrictions on p and p. Applying the version of the extension principle which relates \\n to probabilistic restrictions, we are led to f(ZZ). We can compute the restriction, B, of the scalar product of f(AA) and f(pp.). Since A2-f(AA), computation of B2 com pletes the computation of f(ZZ). \\n It is helpful to express the Summary as a version of the \\n extension principle. More concretely, we can write: 35 \\n 40 \\n 45 \\n 50 \\n Z=f(X, Y) \\n X is (Ax, Bx) (restriction on X) \\n Y is (Ay, By) (restriction on Y) 55 \\n Z is (AZ, BZ) (induced restriction on Z) \\n AZ = f(Ax, Ay) (application of \\n extension principle for fuzzy numbers) \\n where p and pare constrained by: 12 \\n -continued \\n ?us, a produ is By R \\n In terms of the membership functions of Band B, these restrictions may be expressed as: \\n He (Ius opx(adu) \\n He (Ius (a) produ) \\n Additional restrictions on p and pare: \\n Consequently, in agreement with earlier results we can \\n write: (compatibility) \\n tlet Ay (u)dit \\n R - - - - - - (compatibility) J. Hay (u)du \\n sup (es (?tay upstadt) up (?tay up adu) \\n subject to \\n up A (u)du R - J. H.A. (u)du \\n What is important to keep in mind is that A and B are, for the most part, perception-based and hence intrinsically \\n imprecise. Imprecision of A and B may be exploited by mak \\n ing simplifying assumptions about A and B assumptions \\n that are aimed at reduction of complexity of computation with \\n Z-numbers and increasing the informativeness of results of computation. Two examples of Such assumptions are \\n sketched in the following. Briefly, a realistic simplifying assumption is that p and p \\n are parametric distributions, in particular, Gaussian distribu tions with parameters m, O, and my O., respectively. \\n Compatibility conditions fix the values of m and m. Con \\n sequently, if b and b are numerical measures of certainty,', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 88}), Document(page_content=\"US 8,873,813 B2 \\n 13 \\n then b and by determine p and p, respectively. Thus, the assumption that we know b and b is equivalent to the assumption that we know p and p. Employing the rules \\n governing computation of functions of Z'-numbers, we can \\n compute B2 as a function of b and by. At this point, we recognize that Band Bare restrictions on bandby, respec tively. Employment of a general version of the extension principle leads to B2 and completes the process of computa \\n tion. This may well be a very effective way of computing with \\n Z-numbers. It should be noted that a Gaussian distribution \\n may be viewed as a very special version of a Z-number. Another effective way of exploiting the imprecision of A and B involves approximation of the trapezoidal membership function of Abyan interval-valued membership function, A. where A is the bandwidth of A (FIG.3). Since A is a crisp set, \\n we can write: \\n where BxB is the product of the fuzzy numbers Band B. Validity of this expression depends on how well an inter Val-Valued membership function approximates to a trapezoi \\n dal membership function. \\n Clearly, the issue of reliability of information is of pivotal importance in planning, decision-making, formulation of \\n algorithms and management of information. There are many important directions which are explored, especially in the \\n realm of calculi of Z-rules and their application to decision analysis and modeling of complex systems. \\n Computation with Z-numbers may be viewed as a gener \\n alization of computation with numbers, intervals, fuzzy num \\n bers and random numbers. More concretely, the levels of generality are: computation with numbers (ground level 1); computation with intervals (level 1); computation with fuzzy \\n numbers (level 2); computation with random numbers (level 2); and computation with Z-numbers (level 3). The higher the level of generality, the greater is the capability to construct \\n realistic models of real-world Systems, especially in the \\n realms of economics, decision analysis, risk assessment, planning, analysis of causality and biomedicine. \\n It should be noted that many numbers, especially in fields \\n Such as economics and decision analysis are in reality Z-num bers, but they are not currently treated as such. Basically, the \\n concept of a Z-number is a step toward formalization of the \\n remarkable human capability to make rational decisions in an environment of imprecision and uncertainty. \\n Now, in the next section, we discuss our inventions and embodiments, extending the concepts above, as well as other applications and examples, incorporating various other tech \\n nologies, including new concepts, methods, systems, devices, processes, and technologies. \\n SUMMARY OF THE INVENTION \\n Here, we introduce Z-webs, including Z-factors and Z-nodes, for the understanding of relationships between \\n objects, Subjects, abstract ideas, concepts, or the like, includ \\n ing face, car, images, people, emotions, mood, text, natural \\n language, Voice, music, video, locations, formulas, facts, his torical data, landmarks, personalities, ownership, family, \\n friends, love, happiness, social behavior, voting behavior, and the like, to be used for many applications in our life, including \\n on the search engine, analytics, Big Data processing, natural language processing, economy forecasting, face recognition, \\n dealing with reliability and certainty, medical diagnosis, pat \\n tern recognition, object recognition, biometrics, security \\n analysis, risk analysis, fraud detection, satellite image analy \\n sis, machine generated data analysis, machine learning, train 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 14 \\n ing samples, extracting data or patterns (from the video, \\n images, text, or music, and the like), editing video or images, \\n and the like. Z-factors include reliability factor, confidence\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 89}), Document(page_content=\"65 14 \\n ing samples, extracting data or patterns (from the video, \\n images, text, or music, and the like), editing video or images, \\n and the like. Z-factors include reliability factor, confidence \\n factor, expertise factor, bias factor, truth factor, trust factor, validity factor, “trustworthiness of speaker”, “sureness of ”, “statement helpfulness”, “expertise of speaker, speaker. \\n “speaker's truthfulness”, “perception of speaker (or source of \\n information)”, “apparent confidence of speaker”, “broadness \\n of statement, and the like, which is associated with each \\n Z-node in the Z-web. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1 shows a membership function and the probability \\n density function of X, as an example. \\n FIGS. 2a and 2b show various examples off-mark. \\n FIG.3 shows the structure of a membership function, as an example. \\n FIG. 4 shows one embodiment for the Z-number estimator \\n or calculator device or system. FIG.5 shows one embodiment for contextanalyzer system. \\n FIG. 6 shows one embodiment for analyzer system, with multiple applications. \\n FIG. 7 shows one embodiment for intensity correction, editing, or mapping. \\n FIG. 8 shows one embodiment for multiple recognizers. \\n FIG.9 shows one embodiment for multiple sub-classifiers and experts. \\n FIG. 10 shows one embodiment for Z-web, its compo nents, and multiple contexts associated with it. \\n FIG. 11 shows one embodiment for classifier for head, \\n face, and emotions. \\n FIG. 12 shows one embodiment for classifier for head or \\n face, with age and rotation parameters. \\n FIG. 13 shows one embodiment for face recognizer. \\n FIG. 14 shows one embodiment for modification module \\n for faces and eigenface generator module. \\n FIG. 15 shows one embodiment for modification module \\n for faces and eigenface generator module. \\n FIG. 16 shows one embodiment for face recognizer. \\n FIG. 17 shows one embodiment for Z-web. \\n FIG. 18 shows one embodiment for classifier for accesso \\n 1S. \\n FIG. 19 shows one embodiment for tilt correction. \\n FIG. 20 shows one embodiment for context analyzer. FIG.21 shows one embodiment for recognizer for partially hidden objects. \\n FIG.22 shows one embodiment for Z-web. \\n FIG. 23 shows one embodiment for Z-web. \\n FIG. 24 shows one embodiment for perspective analysis. \\n FIG. 25 shows one embodiment for Z-web, for recollec \\n tion. \\n FIG. 26 shows one embodiment for Z-web and context analysis. \\n FIG.27 shows one embodiment for feature and data extrac \\n tion. \\n FIG. 28 shows one embodiment for Z-web processing. \\n FIG. 29 shows one embodiment for Z-web and Z-factors. \\n FIG. 30 shows one embodiment for Z-web analysis. \\n FIG. 31 shows one embodiment for face recognition inte grated with email and video conferencing systems. \\n FIG. 32 shows one embodiment for editing image for advertising. \\n FIG. 33 shows one embodiment for Z-web and emotion \\n determination. \\n FIG. 34 shows one embodiment for Z-web and food or \\n health analyzer.\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 89}), Document(page_content='US 8,873,813 B2 \\n 15 \\n FIG. 35 shows one embodiment for a backward chaining inference engine. \\n FIG. 36 shows one embodiment for a backward chaining \\n flow chart. \\n FIG. 37 shows one embodiment for a forward chaining \\n inference engine. \\n FIG. 38 shows one embodiment for a fuzzy reasoning \\n inference engine. \\n FIG. 39 shows one embodiment for a decision tree method \\n or system. \\n FIG. 40 shows one embodiment for a fuzzy controller. \\n FIG. 41 shows one embodiment for an expert system. \\n FIG. 42 shows one embodiment for determining relation \\n ship and distances in images. \\n FIG. 43 shows one embodiment for multiple memory unit \\n Storage. \\n FIG. 44 shows one embodiment for pattern recognition. \\n FIG. 45 shows one embodiment for recognition and stor \\n age. \\n FIG. 46 shows one embodiment for elastic model. \\n FIG. 47 shows one embodiment for set of basis functions or \\n filters or eigenvectors. \\n FIG. 48 shows one embodiment for an eye model for basis object. \\n FIG. 49 shows one embodiment for a recognition system. \\n FIG.50 shows one embodiment for a Z-web. \\n FIG. 51 shows one embodiment for a Z-web analysis. \\n FIG. 52 shows one embodiment for a Z-web analysis. \\n FIG. 53 shows one embodiment for a search engine. \\n FIG. 54 shows one embodiment for multiple type transfor \\n mation. \\n FIG. 55 shows one embodiment for 2 face models for \\n analysis or storage. \\n FIG. 56 shows one embodiment for set of basis functions. \\n FIG. 57 shows one embodiment for windows for calcula \\n tion of “integral image, for sum of pixels, for any given \\n initial image, as an intermediate step for our process. \\n FIG. 58 shows one embodiment for an illustration of \\n restricted Boltzmann machine. \\n FIG. 59 shows one embodiment for three-level RBM. \\n FIG. 60 shows one embodiment for stacked RBMs. \\n FIG. 61 shows one embodiment for added weights between \\n visible units in an RBM. \\n FIG. 62 shows one embodiment for a deep auto-encoder. \\n FIG. 63 shows one embodiment for correlation of labels \\n with learned features. \\n FIG. 64 shows one embodiment for degree of correlation or conformity from a network. \\n FIG. 65 shows one embodiment for sample/label generator \\n from model, used for training \\n FIG. 66 shows one embodiment for classifier with multiple \\n label layers for different models. \\n FIG. 67 shows one embodiment for correlation of position \\n with features detected by the network. FIG. 68 shows one embodiment for inter-layer fan-out \\n links. \\n FIG. 69 shows one embodiment for selecting and mixing \\n expert classifiers/feature detectors. \\n FIGS. 70 a-b show one embodiment for non-uniform seg \\n mentation of data. \\n FIGS.71 a-b show one embodiment for non-uniform radial \\n segmentation of data. \\n FIGS. 72 a-b show one embodiment for non-uniform seg \\n mentation in Vertical and horizontal directions. \\n FIGS. 73 a-b show one embodiment for non-uniform trans \\n formed segmentation of data. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 16 \\n FIG. 74 shows one embodiment for clamping mask data to \\n a network. \\n FIGS. 75 a, b, c show one embodiment for clamping \\n thumbnail size data to network. \\n FIG. 76 shows one embodiment for search for correlating objects and concepts. \\n FIGS. 77 a-b show one embodiment for variable field of \\n focus, with varying resolution. FIG.78 shows one embodiment for learning via partially or \\n mixed labeled training sets. \\n FIG. 79 shows one embodiment for learning correlations \\n between labels for auto-annotation. \\n FIG. 80 shows one embodiment for correlation between \\n blocking and blocked features, using labels. \\n FIG. 81 shows one embodiment for indexing on search system. \\n FIGS. 82 a-b show one embodiment for (a) factored weights in higher order Boltzmann machine, and (b) CRBM \\n for detection and learning from data series. \\n FIGS. 83 a, b, c show one embodiment for (a) variable frame size with CRBM, (b) mapping to a previous frame, and (c) mapping from a previous frame to a dynamic mean. \\n DETAILED DESCRIPTION OF THE \\n EMBODIMENTS', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 90}), Document(page_content='DETAILED DESCRIPTION OF THE \\n EMBODIMENTS \\n Now, we discuss the various embodiments of our current \\n invention: \\n Approximate Z-Number Evaluation: In this section, we present a method for approximate evalu \\n ation of Z-Numbers, using category sets of probability distri butions corresponding to similar certainty measures. All the figures are displayed in Appendix 1, as color images. This is \\n also (partially) the subject of a paper (pages 476-483 of the conf. proceedings) and presentation given at an international Fuzzy confin Baku, Azerbaijan, on Dec. 3-5, 2012 (“The 2\" \\n World Conference on Soft Computing), by the inventors. Appendix 1 is a copy of the paper at the Baku Conf. Appendix \\n 3 is a copy of the VU graph PowerPoint presentation at the \\n Baku Conf. Appendix 2 is a copy of the handwritten notes, in addition to the teachings of Appendices 1 and 3. All the \\n Appendices 1-3 are the teachings of the current inventors, in \\n Support of the current disclosure, and are incorporated herein. \\n A Z-Number is denoted as an ordered pair (A,B), where A and B are fuzzy numbers (typically perception-based and \\n described in natural language), in order to describe the level of certainty or reliability of a fuzzy restriction of a real-valued \\n uncertain variable X in Z-valuation (X, A,B). (See L. A. Zadeh, \\'A note on Z-numbers.” Inform. Sciences, Vol 181, pp. 2923-2932, March 2011.) For example, the proposition “the price of ticket is usually high”, may be expressed as a Z-valu \\n ation (price or ticket, high, usually). In Z-valuation, the cer tainty component B describes the reliability of the possibilis \\n tic restriction, R, for the random variable X, where \\n R(X):X is A (1) \\n with the reliability restriction given by \\n Prob(X is A) is B (2) \\n In another words, the certainty component B, restricts the probability measure of A, denoted by V. \\n v-Prob(X is A) L(x)p(x) dix (3) \\n where L(X) is the membership function of X in fuZZy set A \\n on X domain, and p is the probability distribution of X. Therefore, the certainty component B indirectly restricts the', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 90}), Document(page_content=\"US 8,873,813 B2 \\n 17 \\n possibilities of various (candidate) hidden probability distri \\n butions of X by: (eq. 4 below) \\n where u(v) is the membership function of the probability \\n measure V in fuzzy set B. \\n Here, we show a method to approximate Z-valuation, based on categories (sets) of p's with similar probability \\n measures (or resulting in similar certainty measure), as an approach to reuse predetermined calculations of probability \\n measures. First, we demonstrate an example of Z-valuation without Such approach, and then, we present an approximate approach to Z-valuation via categorical sets of probability \\n distributions. \\n A. Z-Valuation: Basics \\n The Z-valuation uses the mapping of the test scores given \\n by (4) to each of hidden probability distribution candidates of \\n X (See L. A. Zadeh, 'A note on Z-numbers.” Inform. Sci \\n ences, vol 181, pp. 2923-2932, March 2011. See also R. Yager. “On Z-valuations using Zadeh's Z-numbers.” Int. J. \\n Intell. Syst., Vol. 27, Issue 3, pp. 259-278, March 2012.), collectively referred to as 10 \\n 15 \\n 25 \\n Prob. Distrib. Candidates={p,3, (5) \\n where inumerates different candidates. Fig. 1 of Appendix 1 conceptually illustrates the mapping, where each p, is first mapped to a probability measure of A. V., and then mapped to \\n a test score determined by B, where \\n and 35 \\n ts, le(v). (7) \\n Note that the dot symbol in (Lp) in (6) is used as short hand for the probability measure. Fig. 1 of Appendix 1 shows \\n the test score mapping to hidden probability distribution can \\n didates p, in X, for Z-valuation (XA,B). Via the extension principle, the application of the restric tion (test scores) on p(x) (i.e., probability distribution can didates in X domain) to other entities is illustrated. For example, the restriction on p(x) can be extended to the possibilistic restriction on the corresponding probability dis \\n tributions, p(y), in Y domain, where \\n In such a case, the restrictions can further be extended to the probability measures, w, of a fuzzy set A, in Y domain, based on p(y). The aggregation of the best test scores for w, would determine the certainty component B in Z-valuation \\n (YAB), based on the original Z-valuation (X.A.B.), as indicated in Fig. 2 of Appendix 1, which illustrates the exten sion of test scores to Y domain. Fig. 2 of Appendix 1 is a test score mapping from X domain to Y domain and aggregation \\n of test scores on probability measures, w, for Z-valuation \\n (YAB). For simplicity, as shown in Fig. 2 of Appendix 1, three \\n probability distribution candidates in X domain, p, p, and ps, are assigned test scorests and ts, via certainty restric tion on probability measures V, and V (with p, and ps. having the same probability measure V for A). By applying \\n f(X) to each probability distribution candidate in X domain, 65 we can obtain a corresponding probability distribution in Y \\n domain, denoted as p, which can be used to compute the 40 \\n 45 \\n 50 \\n 55 \\n 60 18 \\n corresponding probability measure of A (assume given), denoted as w, . In this example, p, and pa (mapped from p, and p,) result in the same probability measure wa (or aggregated w bin), while pis (mapped from ps) maps into w. In this simple example, the aggregation of the best test \\n scores for p, denoted as ts(p), in W domain (e.g., in eachw bin) would result in the following membership function for \\n B: \\n In other words, in this scenario, \\n Subject to \\n Will Api. \\n In case of single variable dependency Y=f(X), the prob \\n ability measure w can be evaluated by unpacking the prob \\n ability distribution in Y as illustrated by (9) and transforming \\n the integration over X domain as shown in (10), without explicitly evaluating p, \\n (9) wi = {t Ay py. \\n ?us (y) pity) dy \\n = u(y) X. y i Psi(xi). f(x) dy\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 91}), Document(page_content='the integration over X domain as shown in (10), without explicitly evaluating p, \\n (9) wi = {t Ay py. \\n ?us (y) pity) dy \\n = u(y) X. y i Psi(xi). f(x) dy \\n where denotes the consecutive monotonic ranges of f(X) in X domain, and X, is the solution for f(y), if any, within the monotonic range j, for a given y. This takes into account that the probability (p, dy) for an event within the infinitesimal interval of y, y+dy in Y domain, is the Summation of the infinitesimal probabilities from various infinitesimal inter vals x+dx (if applicable) in X domain, where for each j: \\n dy f(x) dy, \\n Therefore, with repacking the integration (9) in X domain \\n over the consecutive monotonic ranges of f(X), we obtain: \\n wikila (f(x))p(x) dix \\n Furthermore, if f(X) is monotonic (i.e., f(y) has only one \\n solution in X, if any) AND LL is obtained from LL via the extension principle by applying f(X) to A, then w, is guar \\n anteed to be equal to V, for all candidate probability distribu tions p, because LL(y) L(x) for Wy=f(x) in such a case. This also means that in such a case, B becomes equal to B. and no additional computation would be necessary. \\n B. Z-Valuation: Example \\n To illustrate an example of Z-valuation, assume the follow ing is given: (10) \\n X(AB), \\n Y f(X)=(X+2), and \\n Ay. \\n The goal is to determine the certainty value B for the \\n proposition that (Y is A), i.e., the Z-valuation (Y. A, B). For purpose of this example, assume Figs. 3, 4, and 5 of \\n Appendix 1 depict the membership functions for A, B, and \\n A respectively. The function f(X) is also depicted in Fig. 6 of Appendix 1. Fig. 3 of Appendix 1 is the membership function', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 91}), Document(page_content='US 8,873,813 B2 \\n 19 \\n of A, e.g., \"X is around Zero”. Fig. 4 of Appendix 1 is the membership function of B, e.g., “Likely. Fig. 5 of Appen \\n dix 1 is the membership function of A. e.g., “Y is about \\n nine\\'. Fig. 6 of Appendix 1 is a diagram depicting f(X). \\n In this example, the set of candidate probability distribu \\n tion for X was constructed using Normal distributions with \\n mean (m) ranging from -2 to 2 and standard deviation (O.) \\n ranging from O\\' (close to Dirac delta function) to 1.2. Figs. 7 \\n and 8 of Appendix 1 depict the probability measure of A, \\n denoted as V, based on (3) and each of these probability \\n distribution candidates represented by a point on (m, O.) \\n plane. These also illustrate the contour maps of constant probability measures. Figs. 9 and 10 of Appendix 1 depict the \\n test scores (denoted as ts) for each probability distribution candidate, based on the application of certainty component \\n B to each probability measure, V, via (4). Given that B \\n imposes a test score on each V, the probability distribution \\n candidates that form a contour (on (m, O.) plane) for constant \\n V, also form a contour for the corresponding test score. How \\n ever, given that a range of V values may result in the same test \\n score (e.g., for v less than 0.5 or above 0.75, in this example), some test score contours on (m, O.) plane collapse to flat ranges (e.g., for test scores 0 and 1, in this example), as depicted on Figs. 9 and 10 of Appendix 1. \\n By applying (10), we can then determine the probability \\n measure of A (in Y domain), denoted as w, based on the probability distribution candidates in X domain (i.e., bypass ing the direct calculation of the corresponding probability \\n distributions in Y domain). The probability measure w is depicted in Figs. 11 and 12 of Appendix 1 for each probability \\n distribution candidate in (m, O,) plane. Given that each probability distribution candidate is asso \\n ciated with a possibility restriction test score (as shown for example in Fig. 10 of Appendix 1). Such test score can be \\n applied and correlated with the probability measure w (shown for example in Fig. 12 of Appendix 1). A given w (or a w bin) \\n may be associated with multiple test scores as indicated by \\n contours of constant w or regions of very close or similar win Fig. 12 of Appendix 1. \\n Therefore, to assign a final test score to a given w (or w bin) \\n based on (8), we can determine the maximum test score for all w’s associated with the given wbin. The result of an intermediate step for determining the \\n maximum test score for correlated w\\'s (i.e., falling in the same w bin) is illustrated in Fig. 13 of Appendix 1, on the (m, O.) plane (for illustrative comparison with Fig. 11 of Appen \\n dix 1). The resulting maximum test score associated with a given \\n w bin defines the membership function of w (or a value of w representing the w bin) in B, as depicted for this example in \\n Fig. 14 of Appendix 1. As shown in Figs. 11 and 13 of \\n Appendix 1, where w is high, the maximum associated test score is low, resulting in B which represents “significantly \\n less than 25% for this example. Fig. 7 of Appendix 1 is the probability measure of A. V., per each (Normal) probability \\n distribution candidate represented by (m, O.). Fig. 8 of \\n Appendix 1 is the contours of the probability measure of A, \\n V, per each (Normal) probability distribution candidate rep \\n resented by (m, O.). Fig. 9 of Appendix 1 is the test score based on certainty measure B for each (Normal) probability distribution candidate represented by (m, O.). Fig. 10 of Appendix 1 is the test score based oncertainty measure B for each (Normal) probability distribution candidate represented by (m, O.). Fig. 11 of Appendix 1 is the probability measure \\n of A, w, per each probability distribution (Normal) candidate represented by (m, O). 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n Fig. 12 of Appendix 1 is the contours of the probability', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 92}), Document(page_content=\"15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 20 \\n Fig. 12 of Appendix 1 is the contours of the probability \\n measure of A, w, per each probability distribution (Normal) candidate represented by (m, O.). Fig. 13 of Appendix 1 is \\n the maximum test score for a w-bin associated with each \\n probability distribution (Normal) candidate represented by \\n (m O). Fig. 14 of Appendix 1 is the maximum test scores \\n for w-bins defining the membership function of win fuzzy set \\n B, e.g., “significantly less than 25%'. \\n II. Z-Valuation Using Granular Category Sets \\n A. Predetermined Category Sets: Test Scores, Probability \\n Measures, and Probability Distributions \\n The probability measure of A denoted as V, may be pre determined and reused, given that the integration in (3) may \\n be normalized based on the general shape of the membership \\n function of A and the class/parameters of probability distri \\n bution candidates. In normalized form, for example, a cat egory of normalized membership function may be defined as symmetric trapezoid with its Support at interval -1.1 with a \\n single parameter, B, indicating the ratio of its core to its \\n Support (as shown in Fig. 15 of Appendix 1). Examples of \\n classes of probability distribution are Normal distribution and Poisson distribution, with their corresponding parameters \\n normalized with respect to normalized A. For example, for \\n Normal distribution, the parameters (m, O, ) may be normal ized with respect to halfwidth of the support having the origin \\n of the normalized coordinate translated to cross Zero at the \\n center of the Support. \\n Furthermore, we may reduce the level and complexity of computation in approximating the Z-valuation by using a \\n granular approach. For example, for a category of normalized \\n A (e.g., symmetric trapezoid with B of about 0.5, as shown in Fig. 15 of Appendix 1), we may predetermine relations/map \\n ping (or a set of inference rules) between (fuzzy or crisp) \\n subset of probability distribution candidates (of a given class \\n such as Normal or Poisson distribution) and (fuzzy or crisp) Subsets of probability measures, vs (as for example shown in Fig. 16 of Appendix 1). \\n Let V, denote a category/set of probability measures of A. (e.g., probability measure “High”), where numerates Such categories in V domain. Each V, corresponds to a range or (fuzzy or crisp) subset of probability distribution candidates, \\n denoted by C, whose p, members are defined via the following membership function: (eq. 11, below) \\n Therefore according to (11), we may predetermine C, via a similar method of applying test scores to the probability dis \\n tribution candidates, p, (as for example shown in Fig. 9 of Appendix 1), by replacing B, with V. For example, the cat egories of probability measure V, and V., (shown in Figs. 17 and 18 of Appendix 1, respectively), correspond to \\n the (category) fuzzy sets of probability distribution candi \\n dates, denotes as C, and C, (with labels used in place of j), with a membership function depicted in Figs. 19 and 20 of Appendix 1, respectively. \\n Furthermore, the certainty levels (test scores) may also be made into granular (fuzzy or crisp) sets TS, e.g., in order to reduce the complexity of calculation during the aggregation \\n process of Z-valuation. Index k numerates these test score category sets. Fig. 16 of Appendix 1 may also serve as an \\n example of such categorization (with test score replacing V). In one approach, the certainty component B is granularly \\n decomposed or mapped (or approximately expressed) via\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 92}), Document(page_content=\"US 8,873,813 B2 \\n 21 \\n pairs of probability measure and test score category sets, i.e., (VTS)'s, as for example demonstrated in Fig. 21 of Appen dix 1. In one approach, each relation pair may be further associated with a weight that indicates the degree of map ping of B among the pairs (e.g., when TS is a predefined \\n set). For example: \\n weight = SE (hy, (v) a urs (upy (v)). \\n In one scenario, the decomposition of B may be expressed as series of tuples in the form (VTS, weight) or simply as a matrix with weight, as its elements. Given the correspon dence between C, and V, the granular test score sets TS's are also associated with granular probability distribution candi date sets, C.’s (with the same weight). In another approach, a non-categorical test score (e.g., a \\n fuzzy or crisp set)TS, is determined for each V, (and C), e.g., by using extension principle, based on mapping via B. \\n |lts (ts) supy-roll (li,(v), (12) \\n subject to: ts-up(v). \\n Fig. 15 of Appendix 1 is a membership function parameter \\n B (ratio of core to Support), which adjusts the symmetric \\n trapezoid shape from triangular with (B=0) to crisp with \\n (B=1). Fig. 16 of Appendix 1 shows examples of various granular (fuzzy) sets of probability measures. Fig. 17 of \\n Appendix 1 is membership function of V in V, Fig. 18 of Appendix 1 is membership function of v in V. Fig. 19 of Appendix 1 is membership function of p, in C (with p, represented by its parameters (m, O)). Fig. 20 of Appendix 1 is membership function of p, in C, (withp, represented by its parameters (m, O)). Fig. 21 of Appendix 1 is an example of granularizing/mapping of B via (VTS) pairs. B. Computation and Aggregation Via Normalized Catego \\n 1S \\n One advantage of reusing the predetermined normalized \\n categories is the reduction in number of calculations, such as the integration or Summation in determining probability mea \\n sures per individual probability distribution candidates in X domain or their corresponding probability distributions in Y \\n domain, per (4) and (8). In addition, instead of propagating \\n the test scores via an individual probability distribution can \\n didate, the extension of the test scores may be done at a more granular level of the probability distribution candidate sub \\n sets, C, which are typically far fewer in number than the individual probability distribution candidates. However, the \\n aggregation of test scores for Z-valuation, e.g., for (YAB), \\n will involve additional overlap determination involving vari \\n ous normalized category sets, as described below. The normalization of symmetrical trapezoid membership \\n function A, e.g., “Y is about nine as shown in Fig. 5 of Appendix 1, involves shifting the origin by-9 and Scaling the \\n width by 0.5 (in Y domain) in order to match the position and width of the support to the normalized template depicted in \\n Fig.15 of Appendix 1 (with B=0 determined as the ratio of the \\n core to Support). Note that such normalization (translation and Scaling) also impacts the location and Scaling of associ \\n ated p’s (e.g., mean and standard deviation) in order to pre serve the probability measure of A per (8). Note that the predetermined categorical subset of probabil \\n ity distributions in Y domain, denoted as C, that is associ ated with V may be distinct from the corresponding one in X domain, denoted as C. e.g., due to parameters such as f (or the class of the membership. Such as trapezoid or ramp). For 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 22 \\n example, Fig. 22 of Appendix 1 illustrates the membership \\n function of C. for normalized A (3-0), for comparison with C. depicted in Fig. 20 of Appendix 1, for the same values of normalized probability distribution parameters. Fig. 22 of Appendix 1 is membership function of p, in C, (with p, represented by its parameters (m. O)). i) Mapping in X Domain\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 93}), Document(page_content='In one approach to estimate (10), we may determine (or approximate) u(f(x)) in X domain as for example depicted in Fig. 23 of Appendix 1, labeled u(x). Then, we may proceed with mapping and normalization of the membership \\n function to one or more normalized categories of membership functions (e.g., a symmetric trapezoid shape with (3-0)). Fig. \\n 23 of Appendix 1 is membership function u(x). In such an approach, the normalization effects on AX and A are combined into a transformation operation, T. (e.g., translation \\n and Scaling) used to also transform the normalized probabil \\n ity distribution parameters (e.g., mean and standard devia \\n tion). Thus, T also transforms the predetermined subsets of \\n probability distribution candidates, C., to C.?, e.g., via the extension principle, as follows: \\n \\'...) = (13) Hey (Pk) spect (P.) \\n Subject to \\n p = T(p xi), \\n where px.\" represents the transformed probability distri \\n bution candidate (in X domain) from px. Since in our example, LL (depicted in Fig. 3 of Appendix \\n 1) is already in a normalized form, we focus on the transfor mation due normalization of u(x). Note that in Fig. 11 of Appendix 1, the outline of probability measure w for (O-0+) \\n is the same as the membership function u(x) prior to the normalization, as depicted in Fig. 23 of Appendix 1. To nor \\n malize LL(x), the membership function must be scaled by factor of about 3, denoted by s, and translated by the amount of-3 (or -1 before scaling), denoted by t. The ordered trans lation and Scaling operations, denoted by T, and T respec \\n tively, define the transformation operation which also trans forms a probability distribution (13) by scaling and translating its parameters, for example: \\n with \\n T. px; T (mx.Ox)–(Smx, SOx), \\n Once normalized, u(x) is associated with a predeter mined subset(s) of normalized probability distributions, C’s (e.g., as shown in Figs. 22.24 and 25 of Appendix 1 for j as “High.” “Med,” and “Med-Low” (or “ML\\'), respec tively). To associate C, with the test score value(s) (e.g., TSy) assigned to C (shown for example in Fig. 20 of Appendix 1 with n as “High”), the relative position and scal ing of C, and Cx are adjusted by transforming Cy to Cx, per (13), to determine the intersection between Cx.\" and C. for example by: \\n (15) \\n where I, describes a grade for overlap between Cx, and C. Fig. 26 of Appendix 1 schematically illustrates the (fuzzy) intersection of Cx, and C with n being \"High”', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 93}), Document(page_content=\"US 8,873,813 B2 \\n 23 \\n and being “ML, based on the predetermined category sets C., and C from Figs. 20 and 25 of Appendix 1, respec tively. Fig. 24 of Appendix 1 is membership function C. Fig. 25 of Appendix 1 is membership function C. Fig. 26 of Appendix 1 is illustrating the fuzzy intersection of C, and C., where C., is transformed from C, via scaling and translation. For the predetermined category sets C, and C. Cz and Cyr, are used from Figs. 25 and 20 of Appendix \\n 1. \\n For example, as shown in Fig. 26 of Appendix 1, C., Overlaps C (to a degree), while it may not intersect C, (which is depicted in Fig. 24 of Appendix 1). If I, exceeds an (optional) overlap threshold value, then we may apply the category test score TS associated with C. to C. Note that the association with TS was determined based on B, e.g., through mapping of Lla to the relation pairs (V. TS). This means that the category set of probability measures V, associated with C may get associated with category test score TSr. as well. In general, Va., and V may be sets of probability measures belonging to the same family of sets (i.e., without X or Y dependencies). The steps from B to approximating B is conceptually summarized as: \\n inap Bx - (Vx, TS xk) \\n By - Cx C. - (Vy, TSX.) By. lin Ayl, Ayx - Cy. \\n The determination of the test scores for V may be imple mented via a set of fuzzy rules linking C., and C. For example, the antecedent of each rule is triggered if the corre sponding L, is above an overlap threshold, and the conse quent of the rule assigns TS's (or an aggregate of TS's based on weight, for a given n) to a variable SC. A simpler test score assignment rule may use a non-categorical test \\n score TSr., which is determined for each Vy, e.g., via (12), based on the mapping through B: \\n Rule, if (I) then (SCy is TSx.) (16) \\n However, in correlation/aggregation of assigned (fuZZy) \\n test scores to variable SC, we must consider the maximiza tion of test score required by (8). For example, in aggregating \\n the rules for SC, we may use C-cuts to determine an aggre gated (fuzzy) result, denoted as AGSC as follows: (Eq. 17 below) \\n AGSCy = MAX(Correl(I, TSy)) \\n where Correl(ITS) modifies the membership function of TS by correlating it with the factor I, e.g., via scaling or truncation. Membership function of B is then approxi \\n mated by a series of fuzzy relations (V. AGSC). For a given w (probability measure of A), L(w) may be \\n approximated as a fuZZy number (or a defuZZified value), by further aggregation using fuzzy relations (V. AGSC), e.g.: (Eq. 18 below) \\n Hey (w, ts) = supply (w) a u AGSC (ts). f \\n ii) Overlap Approximation \\n An approach to approximate or render the overlap (15) \\n between the category sets, such as C. may use C-cuts to 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 24 \\n present each crisp C-cuts of predetermined category set as a \\n set of points in (m,O) space. These sets of points may be \\n modeled efficiently, e.g., based on graphical models, opti \\n mized for fast transformation and intersection operations. For example, the models that use peripheral description for the \\n C-cuts allow robust and efficient determination of intersec \\n tion and avoid the need to transform all the points within the set individually, in order to reduce the computation involved \\n in (13). iii) Estimation Using Contour Approach \\n In addition to predetermining C. based on V for a normalized set A, we can predetermine various C-cuts of \\n probability measures (e.g., depicted as contours of constant V \\n in Figs. 7 and 8 of Appendix 1) or various C-cuts of associated \\n test scores (e.g., depicted as contours of constant test scores, \\n ts, in Figs. 9 and 10 of Appendix 1) for a set of predefined \\n (e.g., most frequently used) B components. These C-cuts \\n that represent sets of probability distribution candidates in \\n (m.O) space (already associated with specific test scores) may\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 94}), Document(page_content='(e.g., most frequently used) B components. These C-cuts \\n that represent sets of probability distribution candidates in \\n (m.O) space (already associated with specific test scores) may \\n be transformed per (13) and intersected with C in extend ing their test scores to V. In essence, this is similar to the previous analysis except Va., and TSr., become singleton, and C, becomes a crisp set, while C, and V are predeter mined (crisp or fuZZy) set. \\n Another approach uses (e.g., piecewise) representation of \\n B (not predefined) where based on inspection or description, \\n key values of V associated with key values of test scores may \\n readily be ascertained (e.g., based on C-cuts), resulting in a \\n set of (vts,) pairs. Then, the predetermine C-cuts of prob \\n ability measures (e.g., depicted as contours of constant V in \\n Figs. 7 and 8 of Appendix 1) are used to interpolate the \\n contours of constant ts, sin (m.O) space, based on the corre \\n sponding V, values. Again, these crisp contours of constant \\n (crisp) ts,’s, may be transformed and intersected with C to extend the test scores to V, for estimating B. For quick estimation of B in an alternate approach, the \\n predetermined C-cuts (i.e., ws) of probability measures for \\n normalized A may be used (similar to those shown in Figs. 7 \\n and 8 of Appendix 1 based on A), in essence, turning V to a singleton and C to a crisp set (contour) for carrying out the intersect determination. The estimates for u(w) may be determined via interpolation between the aggregated test \\n score results obtained those w values associated with the \\n C-CutS. \\n In one embodiment, for Z-number analysis, for probability \\n distributions analysis, the predetermined categories of hidden \\n probability distribution candidates and normalized Fuzzy \\n membership functions facilitate the pre-calculation of prob \\n ability measures and their associated reliability measures in Z \\n evaluation or as Z-factors, for fast determination of the reli \\n ability levels of new propositions or conclusions. This \\n approach opens the door to the extension of the reliability \\n measures (e.g., via extension principle) to new propositions, \\n based on graphical analysis of contours (C-cuts) of similar \\n probability measures in the domain of parameters represent \\n ing the probability distribution candidates. Basically, we will \\n use the transformation and mapping of categorical set of the \\n probability distribution candidates (represented as regions or \\n C-cut contours) for extension of the reliability measures. This way, as we pre-calculate and store the shapes and results in \\n our library or database for future use (as templates), the new \\n analysis on any new data can be much faster, because we can readily match it with one of the templates, whose results are \\n already calculated and stored, for immediate use.', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 94}), Document(page_content='US 8,873,813 B2 \\n 25 \\n Now, let\\'s look at Appendix 2. In one embodiment, refer ring to the top Fig. and derivation on page 1 of Appendix 2, we \\n have different values of V, based on various C-cuts (with (ts-C)). Then, we match against category (singleton) V. (see \\n the bottom Fig. on page 1 of Appendix 2). Then, on Fig. and 5 \\n derivation on page 2 of our Appendix 2, we get a series of the curves. We use the predetermined contours C of probabil ity measures V. Note that (V, p, LL\"-e). Note that p,’s define the contour(s) for V., (or regions of p,’s) defining region(s) for V., (such as 0 or 1), to interpolate and determine 10 contours (or regions) of constant denoted by C. These C.\\'s are associated with test scores set by a, i.e. (ts-C) for C ona Then, on Fig. and derivation on page 3 of our Appendix 2. \\n we transform or do other manipulations, according to exten- 15 \\n sion rules (e.g. on normalized) for L: \\n C.\" -T(C., m) \\n While maintaining the test score for C.,\" (as C.). Based on categories of w (similar to V., except for w). 20 probability measure of AinY-domain, where we are single tons (predefined), have corresponding contours (or regions) \\n C (see the figure on the bottom of page 3 of our Appendix 2). Then, we find the intercepts between Co., and Cifany, i.e. \\n I Crai f 25 Then, on Fig. and derivation on page 4 of our Appendix 2. \\n based on the intercepts, we find the best test score for a given C extended from C.\", e.g.: \\n tiss Supw.C.\\' 30 \\n where I exists. (i.e., the best test score from intercept points to a given \\n C.) Now, we associate ts, to w, to construct (Lloy (w)), and interpolate for other w (see the figure on the bottom of page 4 35 \\n of our Appendix 2). Since ts,’s source is C, ts\\'s appear as C-cuts in L, as well. Then, on derivation on page 5 of our Appendix 2, we have: \\n Where the scenario involves e.g. Z f(x,y), instead of y=f(x) \\n (where the solution may be worked out in the X-domain), we 40 can still use contours (or regions) of specific test scores (e.g. \\n based on C-cuts), and contours determined by interpolation of predefined or predetermined probability measure contours or regions. The manipulation, e.g. (pp.Op.), can be imple \\n mented based on contours or regions of constant test scores 45 \\n (for X or Y), instead of individual p, and p, to reduce the number of combinations and calculation. The test scores can \\n be extracted from X, Y domains to Z domain (in this example) and maximized based on the intercept points in p domain with predetermined contours of probability measures of(nor- 50 \\n malized) A2, to again calculate L2. \\n FIG. 4 is a system for Z-number estimation and calcula \\n tion, with all related modules and components shown in the Figure, with a processor or computing unit in the middle for \\n controlling all the operations and commands (Z-number esti- 55 \\n mator). Thus, in Summary, the above section provides the methods for approximation or calculation or manipulation of Z-num \\n bers, and related concepts. Now, we explain other compo \\n nents of our inventions, below. 60 \\n Thumbnail Transformation \\n In one embodiment, the input data (e.g., image) is prepro \\n cessed. For example, the image is transformed into a smaller \\n thumbnail that preserve the high level nature of the image content, while not necessarily preserving its unique charac- 65 \\n teristics. This may be achieved, for example, by down Sam pling or aggregation of neighboring pixels. Other methods 26 \\n may include reduction of the variable space by consolidating \\n the colors into intensity (e.g., gray Scale) and/or reducing the \\n number of bits representing color or intensity. Such a trans \\n formation is denoted as thumbnail. \\n A thumbnail includes less resolution and data, and hence, it \\n contains less overall detailed features. The purpose is to sim plify the task of dealing with many pixels while still manag \\n ing to detect the high level features associated with the images', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 95}), Document(page_content='contains less overall detailed features. The purpose is to sim plify the task of dealing with many pixels while still manag \\n ing to detect the high level features associated with the images \\n (or other type of data). For example, using a thumbnail, a recognition module quickly identifies the presence of a head \\n or face (while not intended to necessarily determine the iden tity of the person or object). \\n One embodiment uses a preliminary search to detect main features in a thumbnail data/image for fast computation. In \\n one embodiment, the limitation may be on the number of pixels on the visual layer (via preprocessing). In one embodi \\n ment, the limitation is imposed on the detection/classifier network (e.g., on hidden layers) itself. For example, the main \\n features are learned and isolated (e.g., by units or neurons of higher hidden layers) or learned by targeted attempt (e.g., by \\n keeping all other weights and letting the weight on certain \\n units change when learning a certain feature.) \\n Feature Detection and Learning In one embodiment, for example where labeled training samples may be difficult to prepare or scarce, the training is \\n done with unlabeled samples to learn the features from the sample details. For example, a restricted Boltzmann machine \\n (RBM) may be used to successively learn the features one layer at a time. \\n A Boltzmann machine refers to a type of stochastic recur rent neural network, where the probability of the state is based on an energy function defined based on the weights/biases \\n associated with the units and the state of Such units. In a \\n Boltzmann machine, some units are denoted visible where the state may be set/clamped or observed and others may be \\n hidden (e.g., those used for determining features). In the \\n Restricted Boltzmann machine (RBM), the weights between hidden units within the same layer are eliminated to simplify the learning process. The learning process tends modifies the \\n weights and biases so that the energy state associated with the samples learned are lowered and the probability of such states \\n is increased. In one embodiment, the state of hidden layers are presented by a stochastic binary variable (e.g., in 0, 1 range) \\n based on a sigmoid Such as logistic function. In one embodi ment, the energy function is given as \\n i \\n where V, and h, denote the state of the i\\' visible unit and the j\" hidden unit (as for example depicted in FIG. 58), respec \\n tively, and b, and c, are bias or threshold associated to such units, respectively. W, is an undirected weight or connection strength linking Such units. Per Boltzmann machine, the \\n probability of the state C. (for a given set of H and V states of the units) depends on the weights (including bias values) and \\n the state of H and V:', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 95}), Document(page_content=\"US 8,873,813 B2 \\n 27 \\n where E, is the energy associated with state a; T denotes the “Temperature' of the system; the denominator denotes \\n the “partition function'. Z; and B denotes any state of the system. Since the energy of a state is proportional to negative \\n log probability of the state, the probability that a binary sto \\n chastic unit is at state 1 (or ON) in such RBM becomes the following logistic function: \\n 1 \\n -AEi 1 + eT Pi is ON \\n where T controls relative width of the above logistic func tion, and AE, (for example for a hidden unit) is given by: \\n Note that in an embodiment with T is set to zero, the \\n stochastic nature of the binary units becomes deterministic, i.e., taking the value sigmoid function (Zero or one), as in Hopfield Network. \\n In one embodiment, the training attempts to reduce the \\n Kullback-Leibler divergence, G, between the distributions of \\n V states based on the training sets and based on thermal equilibrium of the Boltzmann machine, by modifying \\n weights and biases, e.g., via a gradient decent over G with respect to a given weight or bias. The aim of training is to determine weights/biases such that the training samples have high probability. In maximizing the average probability of a \\n state V. P(V), with respect to weights, we have \\n 0 will l. = (vihi) - Kvihi), \\n where the average over the data means average over the training data (i.e., when V units sample from the training sets \\n and are clamped to a training sample while hidden units are \\n updated repeatedly to reach equilibrium distribution), and the \\n average over model means the average from Boltzmann \\n machine sampling from its equilibrium distribution (at a givenT). In one embodiment, learning algorithm uses a small \\n learning rate with the above to perform gradient decent. Simi larly, the following can be used in learning bias c, \\n 0ci . F (hi) (hi), it \\n In one embodiment, where the weights are absent between the hidden units, the updating of the hidden states, H, is done in parallel as the hidden units are conditionally independent \\n for a given set of visible states, V. In one embodiment, Sam pling from model involves one or more iterations alternating between updating (in parallel) hidden and visible layers based \\n on each other. In one embodiment, Sampling for the model is \\n Substituted with sampling from reconstruction, which updates the hidden units (for example, in parallel) using the \\n visible units clamped to a training set, then updates the visible \\n units (e.g., in parallel) to get a reconstruction from the fea tures in the hidden layers, followed by updating the hidden \\n units based on the reconstruction. This approach approxi 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 28 \\n mates the gradient decent of contrastive divergence in an \\n efficient and fast manner. In RBM learning, contrastive diver \\n gence can be used instead of maximum likelihood learning \\n which is expensive. In one embodiment, T is lowered from a higher initial value to make low cost (energy) states more \\n probable than high cost states, while the higher initial value of T allows for reaching and sampling equilibrium states \\n quicker. In one embodiment, the stochastic nature of binary \\n units allows escaping from local minima. In one embodiment, \\n during the reconstruction, a Subset of visible units are clamped to input data to reconstruct other visible units from the features including those affected or derived (e.g., Stochas tically) from the input data. The training in Such a conditional \\n Boltzmann machine tends to maximize the log probability of \\n the observed visual units (now taken as output in reconstruc tion), given the input data. \\n In one embodiment, other non-binary discrete stochastic \\n units may be used. In one embodiment, continuous value units may be used. In one embodiment, mean filed units are used having their state (in the range of 0, 1) determined by the total input (e.g., a logistic function) and a noise (e.g., as a\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 96}), Document(page_content='Gaussian). In one embodiment, other stochastic functions/ distributions (e.g., binomial and Poisson) are used for the \\n units. In one embodiment, where continuous data (including \\n semi-continuous data with many levels as opposed to few \\n discrete levels) is used for state of the visible units, the sam pling from a probability distribution (e.g., Gaussian with a \\n given variance, with the mean determined by the other signal and weights) keeps the stochastic nature, while making the \\n signal in visible unit continuous (as opposed to discrete). The hidden layers may stay binary (stochastic). In one embodi \\n ment, stochastic visible units use continuous signal (e.g., in 0, 1 range) based on other signals and weights and a prob \\n ability distribution (e.g., logistic function) for sampling or updating its signal. \\n In one embodiment, following the training of one RBM, \\n another hidden layer is added on top which employs the lower RBMs hidden layer as input to determine higher level fea \\n tures, and the training is done one layer at the time. For example, FIG.59 illustrates 3 level RBM with 3 hidden layers H\\'\\', H\\'), and H\\'. In one embodiment, in training the weights (w) for additional hidden layer (H), the weights \\n for the trained lower layers are fixed. The fixed weights are used to pass data from bottom up to higher layer and to \\n reconstruct from top down based on higher order features. In one embodiment, as for example depicted in FIG. 60, RBMs are stack on top of each other and training is done one layer at \\n the time from bottom up. In one embodiment, the visible units have continuous value state (e.g., logistic units). In one embodiment, in training a higher level RBM (e.g., RBM), signals in its corresponding visible units (e.g., V) are set to \\n the probability values associated with the corresponding hid den units (e.g., H\\') of the previous RBM, while the hidden units (H) themselves are binary stochastic units. In one embodiment, the top hidden layer (e.g., H) has continuous stochastic value, e.g., based on Gaussian probability distribu \\n tion (e.g., with unit variance) having a mean based on the weights (e.g., w\") and signals from its corresponding visible units, V (e.g., logistic units). In one embodiment, the top \\n hidden layer includes a relatively low number of units (e.g., \\n for representing the high level features as low dimensional \\n codes). In one embodiment, hidden units use continuous vari ables for to represent their features/dimensions, e.g., to facili \\n tate classification based on high level features from the top hidden level (e.g., via training one or more correlation layers, \\n or other methods such as SVM). In one embodiment, layer by layer training creates proper features detection in the hidden', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 96}), Document(page_content=\"US 8,873,813 B2 \\n 29 \\n layers to enhance the back-propagation in discrimination. \\n This allows for fine tuning by local search, e.g., via contras tive wake-sleep approach for better generation. In one \\n embodiment, few labeled samples are used to fine tune the \\n classification boundaries after the features have already been \\n determined primarily based on the unlabeled data features. In one embodiment, weights (y) are introduced in the visible layer while training the weights (w) between the visible layer and the hidden layer (e.g., as depicted in FIG. \\n 61). In one embodiment, this approach is also used for higher level RBMs by introducing weights between hidden units of the lower RBM while training the weights for the higher \\n RBM. In this sense, the RBM becomes a semi-restricted \\n Boltzmann machine. In one embodiment, a gradient decent approach for modifying the weights follows the following \\n update contrastive divergence method: \\n Awe ( vh) o_( vh) 1) \\n where superscript 0 indicates the correlation after the ini tial update of hidden layer after clamping the training sample \\n to the visual units, and SuperScript 1 indicates the correlation \\n after the hidden layer is updated next time by the reconstruc \\n tion at the visual layer. In one embodiment, to get to the \\n reconstruction in the visible layer, the visible units are updated one or more times (e.g., iteratively in parallel) based \\n on the current weights, the updated hidden units, and the state \\n of the visible units (from the initial or prioriteration). In one embodiment, the update activity involves stochastic sampling \\n from the probability distribution (e.g., logistic function). \\n Note that e and e' correspond to the learning rate. In one \\n embodiment, the hidden units are updated multiple times \\n before the correlations are used to determine changes in weight. In one embodiment, visible units with continuous value state (e.g., mean field units) are updated in parallel \\n based on the total input to the unit (e.g., based on a logistic \\n function). In one embodiment, intra-layerweights are introduced dur ing the training of a higher hidden layer in order to establish tighter relationships among inter-layer units (e.g., neighbor \\n ing visible units corresponding to neighboring pixels in an \\n image/data). This enforces constraint during generation. In an \\n embodiment, this facilitates the generation of the parts of a larger recognized object that would not fit each other due to \\n loose relationships between corresponding Sub-features. In \\n one embodiment, more features (e.g., redundant) are used to tighten the relationships. In one embodiment, the interrela \\n tions between the features (e.g., constraints or rules) are used to limit the choices (i.e., placement of parts), and the place \\n ment of one feature helps determine the placement of the other features based on the interrelationship between those \\n features. \\n In one embodiment, as for example depicted in FIG. 62, an autoencoder, e.g., a deep autoencoder, is provided by stacking \\n further hidden layers, in reverse order with respect to the lower layer, having the same size and the same corresponding interlayer weights as their corresponding lower layers. While the lower half layers (including the coding layer H) act as a \\n decoder, the added top layers act as encoder to produce simi lar data in V (output) based on the features learned/captured \\n at the coding layer. The added weights in FIG. 62 are depicted with superscript T to indicate that these weights (initially) are represented by the transpose matrix representing the corre \\n sponding weights in the lower layers. In one embodiment, the \\n weights of the autoencoder is fine tuned, e.g., by using a back \\n propagation method based on gradient decent. Since the ini 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 30 \\n tial weights of autoencoder were determined by a greedy \\n pre-training of lower RBMs, the back propagation will be\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 97}), Document(page_content='15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 30 \\n tial weights of autoencoder were determined by a greedy \\n pre-training of lower RBMs, the back propagation will be \\n efficient. In one embodiment, during the back propagation \\n fine tuning, the stochastic binary units are assumed to be \\n deterministic continuous value units adopting the probability \\n value as their state value, to carry out the back propagation. In \\n one embodiment, the objective function (error function) to optimize in back propagation, is the cross entropy error, E. \\n between the data (e.g., image pixel intensity in V layer) and \\n the reconstruction (e.g., the corresponding pixel intensities in V\" output), for a given sample: \\n E = -X (v; logy, + (1 - vi) log(1 - v)) \\n where V, and v, are the state of the i\\' units (or intensity of the image at given pixel corresponding to unit i) associated \\n with V and V\\', respectively. In one embodiment, for the same number of parameters, deep autoencoders tend to produce \\n less generalization errors compared to shallow ones. \\n In one embodiment, the dimensionality of the data is reduced via the coding presentation at the coding layer (e.g., H\\') having few units compared to the number of units in V. \\n In one embodiment, a noise signal is introduced in the top hidden layer units (e.g., H) during training (but the same for the corresponding training data sample used in V layer) to \\n adjust the weights resulting in more bimodal probabilities in \\n order to make the system more resilient against the noise in \\n the data. \\n In one embodiment, the features of the training samples are learned, e.g., via an unsupervised learning algorithm (e.g., by \\n greedy learning by RBMs). Then, the features are correlated \\n or associated with labels from a Subset of training sample, as for example depicted in FIG. 63. Labels are clamped to a set \\n of units (in L. layer) during the training, while data (e.g., \\n image pixels) are clamped to the Vunits. An RBM is added on \\n top to learn the correlation or association between the data features and the labels. During the training, Llayer and one or more hidden layers (e.g., H’) provide data to Clayer (which \\n may bean RBM, as well). Labels may be binary, multi-valued discrete, or continuous. Similarly the weights (e.g., WP) and \\n biases related to the added layer are learned by feeding labels and corresponding Data at L and V layers, respectively. \\n Once the association between the labels and Data is \\n learned, in one embodiment, data is input to V layer, and its corresponding label is ascertained at L layer, by having the \\n units in Clayer drive the units in Llayer. In one embodiment, data samples corresponding to a label may be constructed by \\n clamping unit(s) in L. layer to derive units in C Layer, and \\n followed by a top-down reconstruction in V layer. In one \\n embodiment, a Subset of units in V layer are clamped to input (e.g., to input a partial image or a portion of image) and the \\n state of one or more labels are set in L. layer by clamping to \\n environment. Then, the other unclamped V units are used to \\n determine the state of the other V units (given the clamped visible and label units), deterministically or stochastically \\n (e.g., through iteration). In one embodiment, a larger image may be recovered from partial data (e.g., partial image) \\n through reconstruction. Reliability Measure \\n In one embodiment, the strength of the correlation between data and label or conformity of data to the system (e.g., a \\n trained system) may be determined based on the energy of \\n states given the clamped data (and label). In one embodiment, \\n the strength of correlation or conformity is based on relative', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 97}), Document(page_content='US 8,873,813 B2 \\n 31 \\n probability of various states. For example, the energy differ \\n ence of two states in Boltzmann machine (in equilibrium) is proportional to the log of the ratio of their probabilities. In one \\n embodiment, the relative strength of the correlation or con formity is based on the relative probability of two states. In \\n one embodiment, a baseline for the probability of training samples is established during and/or after training. In one \\n embodiment, the strength of correlation or conformity indi \\n cates how well the state(s) representing the data (and label) fit into the energy landscape of the system. In one embodiment, \\n as depicted in FIG. 64, the strength of correlation or confor mity of a dataset (including any associated label) is used to \\n determine Z-factor associated with the associated features \\n and/or classification of the data from the network. \\n In one embodiment, the quality of the search is evaluated based one or more approaches including for example, the \\n probability, e.g., the total energy of RBM, or the difference between the regenerated data/image and the input, the fre quency the recognized labels change while anchoring the \\n visible units/neurons to the input/image. Learning Based on Models \\n In one embodiment, the learning is achieved through simu lation using a data (and label) sample generation based on one \\n or more models. In one embodiment, a network trained based \\n on model(s) is used to recognize and classify actual data \\n which may not have been seen before. In one embodiment, the system is trained to infer the potential model(s) itself by recognizing the (e.g., observed) data conforming to a particu \\n lar model and its associated labels/parameters. \\n In one embodiment, as for example depicted in FIG. 65, a sample generator is used to provide data (e.g., images) for \\n training. A rendering unit renders the data according to one or \\n more models (e.g., functional, tabular, and/or heuristic) and the corresponding model parameters governing the instantia \\n tion of the model by the rendering unit. In one embodiment, at least a Subset of model parameters are generated stochasti cally (or via a deterministic sequential algorithm) by a ran domizer unit, which for example, uses applicable probability \\n model(s) and/or model rules to generate the subset of model parameters within given ranges or constraints. In one embodi \\n ment, the training of the network (e.g., a deep belief network based on Boltzmann machines) is done repeatedly generating training data samples via the sample generator to feed to the \\n V layer of a network being trained. In one embodiment, the training is done one hidden layer at the time (e.g., until H). \\n In one embodiment, the training of hidden layers is done unsupervised (i.e., without Supplying labeled training \\n samples). In one embodiment, an autoencoder is setup (e.g., \\n as shown in FIG. 65) and fine tuned using back propagation. \\n In one embodiment, a correlation or associative layer is added \\n to learn the correlation between the features of the data and \\n the labels (L), where the labels are supplied by the sample \\n generator (along with the rendered data). In one embodiment, for example as depicted in FIG. 66, multiple L, layers (e.g., \\n in parallel) are used to represent various classes of (e.g., \\n independent) models. In one embodiment, the relevant \\n weights between C layer and an L. layer are fixed for one \\n class of model(s) while training another class of model(s) through the same C layer. In one embodiment, the cross \\n correlation between two models is determined, via cross cor relation (e.g., through layer C) between the labels associates \\n with both models. For example, by a subset of labels from L layer is clamped and sampled generated from top-down \\n reconstruction from layer C to layer LM2 are used to deter \\n mine Such cross correlation. In one embodiment, states on layer Care stochastically run to derive the reconstruction in', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content=\"reconstruction from layer C to layer LM2 are used to deter \\n mine Such cross correlation. In one embodiment, states on layer Care stochastically run to derive the reconstruction in \\n both L and La layers for determining a correlation 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 32 \\n between the reconstructions samples. In one embodiment, the units in layer Care derived (e.g., through inference) from V \\n layer (by inputting data), and labels are reconstructed in lay \\n ers L and L. In one embodiment, the levels of conformity \\n or correlation of data supplied to V units (or a subset of V \\n units) with models(s) are obtained for each model based on relative probabilities and energy of States. In comparing on \\n model to another, the weights associated with one model are not used in determining energy or probability associated with \\n the other model (for Such comparison). \\n In one embodiment, noise is incorporated into the render \\n ing in order to make the network more resilient to noise. In \\n one embodiment, a stochastic noise (e.g., Gaussian) is applied to the rendering, e.g., in illumination, intensity, tex \\n ture, color, contrast, Saturation, edges, Scale, angles, perspec \\n tive, projection, skew, rotation, or twist, across or for por \\n tion(s) of the image. In one embodiment, noise is added to a hidden layer in a reproducible manner, i.e., for a given data sample (or for a given model parameters), in order to adjust \\n the weight to result in a more modal range of activities to \\n increase tolerance for noise. \\n In one embodiment, elastic distortions (as well as affine transformations) are used to expand the size and variety of the training set, e.g., when the training set is produced from a \\n model (such as a rendered data/image) or when the data/ image is provided separately as part of a training set. In one \\n embodiment, Such a distortion is parameterized and rendered by the rendering unit. One embodiment used both affine (e.g., \\n translation, Scaling, reflection, rotation, homothety, shear mapping, and Squeeze mapping) and distorting type transfor \\n mations. In one embodiment, various transformations are rendered to generate training dataset to let the system learn \\n features that are transformation invariant. In one embodi \\n ment, a shape model is generated with various parameters, \\n Such as various textures, colors, sizes and orientations, to let \\n the system learn the invariant features such as the relative positions of the sub features of the modeled shape. In one embodiment, orthogonal matrixes, for example, are used to \\n perform rotation and reflection transformation for rendering the image or on the provided data sample. \\n In one embodiment, the features of a high level model (with parameters) are learned by a system (such as RBM) through training (e.g., unsupervised). For example, in one embodi \\n ment, a 3D model generates various 2D images at different poses (including position, orientation, and Scale) and expres \\n sions/emotions (or illumination), and the system would learn correlation between the images and their features (derived from the model). Then, the model parameters (and their prob abilities) may be obtained for an image. In one embodiment, various samples are generated/ren \\n dered from a 3D model, by varying relative location and angle of the viewer and the model object (e.g., polar coordinates (r. \\n 0, (p)). These variation span various poses (based on 0 and (p) and Scaling (based on r), using other perspective parameters \\n (e.g., derived from camera/viewer's view span). In one embodiment, a 3D model rendering mapped to 2D \\n images is based on the normal vectors at a given point of the \\n 3D model, illumination parameters (e.g., location of light(s) \\n and intensity), and reflectivity and texture model of the sur \\n face. In one embodiment, the location/presence of rigid points from the model improves the accuracy. In one embodi\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content='and intensity), and reflectivity and texture model of the sur \\n face. In one embodiment, the location/presence of rigid points from the model improves the accuracy. In one embodi \\n ment, PIE (pose, illumination, expression) variations are used to generate training data/images (e.g., by rendering in 2D). \\n In one embodiment, multiple models can be learned in combination. E.g., the model for generating of texture of \\n surfaces or colors can be learned in conjunction with a 3D \\n model of head or body. In rendering a 3D model, the texture', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 98}), Document(page_content='US 8,873,813 B2 \\n 33 \\n model may be incorporated to provide textures and colors for \\n the rendered images used for training. The correlation \\n between the model parameters and the rendered images is \\n learned via training. In one embodiment, noise is added to prevent over fitting and regularize the weights to better gen \\n eralize when used with out of sample data/images. \\n In one embodiment, getting a low level of conformity of a data/image (for example based in a conformity measure Such as energy error or probabilities) with a trained system (e.g., \\n based on a model) causes the data to be marked/tagged or included in a set of data to be recognized/classified by other expert Systems/networks. \\n In one embodiment, the model comprises of rules govern ing the parameters, structure, and relationships between vari \\n ous components and Sub-components of the model. In one \\n embodiment, the rules engine is iteratively executed to gen erate sample data for training, by using a rules engine. \\n In one embodiment, the model includes a databases of background and foreground objects (with parameters) or \\n images. In one embodiment, various data samples are created with various background and foreground models to train the system recognize high level features of foreground and back \\n ground (e.g., wide uniform horizontal bands or regions of \\n color/intensity). In one embodiment, generic labels are used \\n to train the correlation between the labels and the features of \\n the background or foreground Scenes. \\n Correlating of Features and Locations of Interest within the Data (e.g., Image) \\n In one embodiment, a location within the image is specified by a continuous value (e.g., in range of 0, 1 to indicate/ identify the location or pixel along a direction (e.g., X or y \\n direction) in the data/image) or a multi-discrete value (e.g., indicating?identifying a range of locations or pixels along a \\n direction in the date/image). In one embodiment, as for example depicted in FIG. 67, a position L in the data (e.g., a \\n pixel map), is represented by its (x, y) coordinate. In one \\n embodiment, X or y may be fuzzy numbers (e.g., with mem bership functions such as triangular, trapezoidal, rectangular, \\n or singular). In one embodiment, the state of a unit (e.g., neurons) is represented by fuZZy values. In one embodiment, \\n information Such as coordinates, width, height, orientation, type of shape, are presented by units in a parameter layer P. In \\n one embodiment, Mlayer(s) are used to provide?approximate \\n the membership function value of a parameter, Such as coor \\n dinate of allocation. The units in M represent the values (or range of values) that a parameter may take. In one embodi ment, a unit in Mlayer corresponds to a pixel (or a range of \\n pixels) along a direction (e.g., X axis) within the image. In one \\n embodiment, one or more units (e.g., continuous valued) in M layer are set to represent the membership function over the pixels (or range of pixels), for example in X axis, correspond ing to the corresponding fuZZy parameter in Player that, for \\n example, represents the X coordinate of L. In one embodi \\n ment, units in M layer are used to train association of for example, a location on the image and the features of the image. In one embodiment, weighted link are made from Por \\n Munits to a correlation layer C for training the association. In \\n one embodiment, weighted links from Mlayer are made to hidden layers to associate parameters to features of the image. \\n In one embodiment, Mlayer(s) includes a unit for every pixel (or a range of pixels) on the image, e.g., full coverage to \\n specify any shape (or blob) in Mlayer for association with the image. \\n In one embodiment, where inter-layer links between units \\n are not fully connected, the connection from Mlayers to units in lower hidden layer(s) are Substantially arranged to spatially \\n resemble or correspond to Munits corresponding pixels (or 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99}), Document(page_content=\"resemble or correspond to Munits corresponding pixels (or 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 34 \\n range of pixels) in the image viewed via V layer. In Such a \\n case, the links from V layer to higher hidden layers are also \\n limited in number of connectivity, and for example, the few \\n links follow a fan out pattern from a 2D layout of V layer to \\n next hidden layer. \\n In one embodiment, blobs (of fuzzy blobs) are provided on Mlayer for association with the image during training. FuZZy \\n blob, for example, may have fractional membership function \\n value at the blob’s edge. In an embodiment, the membership function value in range of 0, 1 is represented by a logistic \\n function in a unit. \\n In one embodiment, the location, area, or focus of interest is provided on M layer with the corresponding training \\n sample in Vlayer, to train the correlation. In one embodiment, the representation of the focus of interest may be a (fuzzy or crisp) border or a region specified parametrically or per pixel. \\n In one embodiment, with a training sample having multiple \\n focuses of interest, the training may be performed by Submit \\n ting the same data (image) with individual focus of interests \\n during the training. In one embodiment, the stochastic nature \\n of C layer will cause reconstruction of focus of interest in M or Players, given an input image (or a portion of image) in V layer. For example, in training face recognition, images \\n including one or more faces are supplied to Vlayer while their \\n corresponding focuses of interest (e.g., the location/size of \\n the face) are supplied to Mor Players, to train the correlation. \\n In one embodiment, the various focuses of interest are itera tively constructed in M or Player by clamping data (e.g., an \\n image) in V to, for example, derive stochastically the corre \\n sponding focuses of interest from C layer. In one embodi ment, the reconstructed parameters are output in Mor Players based on their corresponding probability. \\n In one embodiment, the correlation of image/data to its locations of interest is performed during training by imple \\n menting a representation of such locations on a layer of units laid out to correspond to the image/data (e.g., by linking Such \\n units to a hidden layer above Vlayer). In one embodiment, the position parameters (e.g., location, width/height, type, orien \\n tation) and the coverage parameters (border type, fill type, \\n fuZZy/crisp) are used to render representation of the loca \\n tion(s) of interest on the representation units, e.g., by using a \\n value in range of 0, 1). In one embodiment, the fuZZy type rendering helps avoid making false correlations with other \\n irrelevant features in the image/data, by representing the fea \\n tures of the location of interest as coarse. Fill type rendering \\n identifies a blob where the location of interest is in the image, \\n so that if the features of the interest are in the middle of the \\n location, the training would catch the correlation. Limiting Number of Weights Based on 2D Fan Out Layout \\n In one embodiment, as for example depicted in FIG. 68, the \\n extent of the inter-layer connections are limited for the lower layers (e.g., H' and/or H). In one embodiment, the number \\n of inter-layer connections between the lower layers is sub stantially less than that of fully connected ones. For example, \\n if the (average) number of fan out links per unit, f, is signifi \\n cantly smaller than the number of units in the higher layer, the number of inter-layer connections (or weights) are signifi \\n cantly reduced compared to the fully connected scheme. This \\n scheme helps reduce the complexity of the structure, reduces the over fitting, and enhances generalization. Conversely, the number of fan out links (top-down, e.g., from H' to Vunits) are also limiting a until in the higher layer to relatively few\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99}), Document(page_content=\"units at the lower unit. Therefore, in one embodiment, for example, the number of fan out links from a unit in H' to V \\n units may be about 3 to 10 pixel wide. \\n In one embodiment, there are multiple type of units in a hidden layer (e.g., H'), with each type corresponding to\", metadata={'source': 'https://patentimages.storage.googleapis.com/7a/fb/91/5ff09b40c62ec6/US8873813.pdf', 'page': 99})]\n",
      "[Document(page_content=\"(12) United States Patent \\n Ryabov et al. US009095285B2 \\n (10) Patent No.: US 9,095,285 B2 \\n (45) Date of Patent: Aug. 4, 2015 \\n (54) PORTABLE BIOMETRIC IDENTIFICATION \\n DEVICE USINGADORSAL HAND VEN \\n PATTERN \\n (71) Applicants:Yaroslav Ryabov, Rockville, MD (US); \\n Denis Broydo, Rockville, MD (US) \\n (72) Inventors: Yaroslav Ryabov, Rockville, MD (US); Denis Broydo, Rockville, MD (US) \\n (*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 \\n U.S.C. 154(b) by 320 days. \\n (21) Appl. No.: 13/860,669 \\n (22) Filed: Apr. 11, 2013 \\n (65) Prior Publication Data \\n US 2014/0307074 A1 Oct. 16, 2014 \\n (51) Int. Cl. \\n A6B 5/17 (2006.01) \\n H04N 5/33 (2006.01) \\n A61B5/OO (2006.01) \\n GO6K 9/OO (2006.01) \\n (52) U.S. Cl. CPC A61B5/117 (2013.01); H04N 5/33 (2013.01); \\n A61B5/0077 (2013.01); A61 B 5/489 (2013.01); A61 B 5/6825 (2013.01); A61 B 5/6898 \\n (2013.01); G06K 9/00 (2013.01) \\n age : : \\n Acquisition & \\n s&as x \\n 8 \\n Registration \\n to the \\n datatase (58) Field of Classification Search \\n CPC ............. A61B5/117; H04N 5/33; G06K9/00 \\n USPC ........ 348/77, 61,78, 180, 189; 382/117, 115, \\n 382/116, 124, 118; 726/2 See application file for complete search history. \\n (56) References Cited \\n U.S. PATENT DOCUMENTS \\n 8,811,681 B2 * 8/2014 Watanabe ..................... 382,115 \\n 2012fO281890 A1* 11/2012 Kamakura et al. ............ 382,126 \\n * cited by examiner \\n Primary Examiner — Jefferey Harold \\n Assistant Examiner — Jean W. Desir \\n (74) Attorney, Agent, or Firm — Nadya Reingand \\n (57) ABSTRACT \\n A portable device for personal identification using a dorsal hand vein-pattern in preferable configuration is disclosed. \\n The mobile device utilizes an on-board camera operating in both visible and near infrared range, a memory unit, a pro cessor and Speeded-Up Robust Features algorithm for image acquisition, processing and comparison against the existing \\n database of hand vein-pattern images. The matching criterion \\n between the images to be used for the person's authentication. Device can optionally use wireless connection for image transferring and processing. \\n 20 Claims, 7 Drawing Sheets \\n image \\n identification \\n Analysis\", metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 0}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 1 of 7 US 9,095.285 B2 \\n PRIOR ART \\n Figure 1', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 1}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 2 of 7 US 9,095.285 B2 \\n Figure 2', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 2}), Document(page_content='U.S. Patent \\n image Acquisition \\n Image Enhancing \\n Image \\n Processing \\n Registration \\n to the \\n database \\n 7s 9-10 - 1 - 12 - is \\n 88: 88888 g33 ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: g::::::: Aug. 4, 2015 \\n 8x88.88% ... \\n ... \\n s \\n I \\n Figure 3 Sheet 3 of 7 \\n Image \\n identification US 9,095.285 B2 \\n image \\n Analysis', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 3}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 4 of 7 US 9,095.285 B2 \\n Figure 4', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 4}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 5 Of 7 US 9,095.285 B2 \\n irrors Loop for each interest point of database image \\n Set up current values of smallest distance, so, and next smallest distance, nsD, to some global MaxValue. \\n ... Loop for each interest point of input image \\n Calculate distance D for 64 dimensional descriptors of the pair of current point of interest for database and input image \\n Yes \\n ns as D. s. :D and mark the current pair of points \\n of interest as possible match \\n No \\n Yes \\n SD ::D \\n and mark the current pair of points of interest as possible matching pair \\n ir Loop for each interest point of input image \\n No s) - Treshold A \\n and \\n siris) K Threshold B \\n Yes \\n Add the current possible matching pair to the list of matching pairs \\n im. Loop for each interest point of database image \\n Figure 5', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 5}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 6 of 7 US 9,095.285 B2 \\n Figure 6', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 6}), Document(page_content='U.S. Patent Aug. 4, 2015 Sheet 7 Of 7 US 9,095.285 B2 \\n s region of 4 different matched images for the same subject \\n O 20 40 60 80 100 120 \\n sunbject # \\n Figure 7', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 7}), Document(page_content='US 9,095,285 B2 \\n 1. \\n PORTABLE BOMETRIC IDENTIFICATION \\n DEVICE USINGADORSAL HAND VEN \\n PATTERN \\n FIELD OF INVENTION \\n The present invention relates broadly to device for biomet \\n ric person identification based on the platform of a Portable \\n Assistance Device (PAD) such a smartphone, cell phone or \\n tablet. \\n More precisely, the invention is based on the analysis and comparison of unique vein-patterns of human hands. The \\n technology can be applied to various human populations, \\n regardless of race, skin color or age (i.e. newborns, toddlers, teenagers, adults, and elderly people). \\n BACKGROUND OF THE INVENTION \\n The progress of the information age brings unprecedented \\n changes to the human Society. The requirements related the personal information are being growing dramatically day by \\n clay. As the result, the personal identification, protection and security become extremely important in the modern age. \\n There are various methods of personal and/or biometric identification have been developed, including fingerprinting, face-Voice-recognition, vein pattern (iris, palm), etc. \\n There are multiple problems associated with the existing \\n methods, such as requirement for in-contact authentication procedure, time-consuming or obstructive procedure, unreli \\n able measurements and low recognition rate. For example, the accuracy of a face-recognition-based per \\n sonal identification is relatively low as the technology has to overcome the problems of lighting, pose, orientation and gesture. Fingerprint identification is widely used for personal \\n identification. However, it is difficult to acquire fingerprint \\n features (i.e. minutiae), for Some people such as elderly \\n people, manual laborers, etc. Moreover, in-contact identifi cation devices may invoke hygiene concern and reluctance to \\n use by a general public. As a result, other biometric charac teristics are receiving increasing attention. \\n Recently, a growing trend towards relieving the users from \\n a contact device has emerged and the idea of peg-free, or \\n further contact-free, hand biometrics have been is proposed. The hand vein recognition technology has been also proposed \\n for image biometrical verification, see, for example U.S. Pat. \\n No. 4,699,149 by Rice, US20120281890 by Kamakura and U.S. Pat. No. 5,787,185 by Clayden. \\n Compared to other biometric authentication techniques, the vein recognition has many advantages, such as unique \\n ness, life-long, time invariant consistency of the vein-pattern \\n for each human body, as well as a non-contact, fast, unobtru sive vein-pattern image acquisition procedure. \\n FIG. 1 shows the typical example of matched portions of the two Superimposed vein-patterns of a person, as shown in \\n U.S. Pat. No. 5,787,185 by Clayden. The advancement in biometric image matching technology \\n has promoted the development of various biometric identifi cation systems. \\n Vein biometric systems are also capable to record the infra red absorption patterns to produce unique and private identi \\n fication templates for users, see, for example U.S. Pat. No. 8.229,178 by Zhang. The matching and comparison of images is part of many modern computer-vision applications. Image registration, \\n camera calibration, object recognition, and image retrieval, to \\n mention a few, see for example U.S. Pat. No. 8,165,401 by Funayama. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 2 \\n The contemporary compact devices such as a Smartphone are capable of performing the task offinding correspondences \\n between two images. Moreover, the Smartphone can be modi fied to process the image at the infrared part of the spectrum \\n with minimal or no modification. \\n The disclosed invention provides a novel, compact, fast, \\n portable and mobile authentication and identification device of a person biased on person’s hand vein-pattern. \\n The preferred embodiment of the invention uses an image \\n of the vein-pattern of a dorsal (back) side of a human palm for', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 8}), Document(page_content='The preferred embodiment of the invention uses an image \\n of the vein-pattern of a dorsal (back) side of a human palm for \\n biometric authentication of a person. There is no limitation, \\n however, to apply the disclosed method to the front side of human palm, as well as to other regions of skin of human body, as long as blood vessels are located relatively close to \\n the skin Surface. \\n The Surface of the person’s hand dorsal side is less Suscep \\n tible to accidental damage, thus making disclose device pref \\n erable for the situations of field deployment. Moreover, it is much more convenient to acquire the images of the hand \\n dorsal side of small children and newborns. \\n Furthermore, for small children it might be preferable to use the images of their feet particularly in infrared spectrum. \\n The disclosed device and technology is relies on the dis covering intrinsically specific points of interest and match \\n them using geometric affine, projective, or other types of \\n geometric transformations. \\n The disclosed device and technology is relies on the image processing algorithm similar to Speeded-Up Robust Features \\n (SURF) algorithm reported by Bay et al. in \"Surf Speeded \\n up robust features, ECCV (European Conference on Com \\n puter Vision), 2006, pp. 404-417. Together with this algorithm the disclosed device employs \\n geometric affine and projective transformations which are insensitive to rotations, Scaling, tilt, image plane, etc., making \\n the disclosed our technology applicable to biometric identi \\n fication of humans of all ages: from newborns to adults. The term “vein-pattern’, as used herein is defined as the image having a pattern of veins, capillaries and other blood \\n vessels that are unique for each individual. \\n The term “PAD, as used herein is defined as any mobile \\n Portable Assistance Device, such as a cell phone, a Smart phone, a tablet computer, a personal computer, etc. \\n The term “identification\\', as used herein is defined as a procedure of providing and proving an identity of the indi vidual by searching against a database of previously acquired \\n information. \\n The term \"NIR\\', as used herein is defined as electromag \\n netic radiation within the 750 mm-2500 nm range of the spectrum. \\n The term “point of interest of an image, as used herein is \\n defined as a pixel of the image with a specific coordinates on the image. The local image features around of the point of interest are stable under local and global image perturbations, \\n Such as deformations as those arising from perspective trans \\n formations (e.g. affine transformations, Scale changes, rota tions and/or translations) as well as illumination/brightness \\n variations, such that the locations of the points of interest can be reliably computed with a high degree of reproducibility. \\n The term “descriptor of a point of interest, as used here is \\n defined as a set of numeric values, usually represented by a \\n 64-dimentional vector, which contains information about the \\n local environment of the point of interest. In the general embodiment of the invention, the image acquisition is performed using a PAD. In such embodiment \\n the PAD-connected camera is used to obtain the images of \\n blood veins. The camera can be either a PAD built-in camera \\n or connected to the PAD by a wire or wirelessly.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 8}), Document(page_content='US 9,095,285 B2 \\n 3 \\n In other embodiment of the invention the disclosed per Sonal authentication device includes a Switching capabilities \\n between visible and near infrared spectral ranges, namely, \\n acquiring the vein-patter image from a person under the vis \\n ible and the near infrared spectrum and Subsequent extracting \\n points of interest, their locations and descriptors, from the acquired images. \\n In another embodiment of invention a lighting feature is provided by the device to improve the quality of the image \\n both in the visible and the near infrared spectral regions. \\n The disclosed device for non-contact personauthentication \\n and identification do not require a direct contact between the \\n disclosed biometric device and a human Subject, making it \\n indispensible tool for sterile hospital environment when the \\n alleviation of possible contamination is important. \\n Being portable, the disclosed invention can provide an \\n immediate personnel identification or authorization at virtu \\n ally any location. The invention might also be important for \\n the personal working in remote areas or in the areas with underdeveloped/damaged infrastructure. \\n The disclosed invention can also find a particular applica \\n tion in instances where the restricted authorized access is \\n required. Such as admission to secure sites, operation of sen sitive machinery or credit/cash dispensing. \\n It is also possible to use the disclosed invention for statis tical data analysis and medical diagnostics. \\n Moreover, the disclosed invention can be used for indica \\n tion of subcutaneous bleeding, new born birth trauma, arthri tis, symptoms of a high blood pressure and atherosclerosis, \\n etc. \\n Further features and aspects of the present invention will become apparent from the following description of preferred \\n and optional embodiments with reference to the attached drawings. \\n SUMMARY OF THE INVENTION \\n A portable identification device using an individual hand vein-patternis disclosed. The device is based on a Smartphone \\n with a built-in camera to acquire a near-infrared image of a skin area with the vein-pattern of the individual. A Smartphone performs a processing of the acquired image \\n to extract a vein-pattern image and applies a Speeded-Up \\n Robust Features (SURF) method (with adjustable hessian \\n thresholds to deduct of points of interest from the image and create a 64-dimensional descriptor vector for the pixels \\n neighborhood for each of the point of interest. In order to facilitate the vein-pattern extraction, a spatial low- and high frequency filtering is applied to the (optionally gray-scale) \\n vein-pattern image, along with a contrast enhancement. A Smartphone compares the descriptors of the acquired image \\n and an image stored in a database and further uses a minimum (Euclidian) distance criterion between descriptor to establish pairs of matching points of interest for two images, resulting \\n in an individual identification based on a threshold value. \\n The invention uses an adjustable threshold value of mini \\n mum distance between descriptor vectors. It also uses an adjustable number of matching points of interest that deter \\n mine the possible identification. The device can include addi tional processing to improve an accuracy of the identification \\n by calculating a transformation matrix using the coordinates \\n of the matched pairs and calculating transformed coordinates \\n of the points of interest of the input image. The transformed \\n coordinates can be compared with initial coordinates for improved individual identification using a cumulative thresh \\n old (a mean square distance) for distance between the initial 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 4 \\n and the transformed coordinates. The transformation matrix \\n can belong to affine or projective transformation class. \\n The device can use a Smartphone memory, a 3G or 4G \\n technology, Bluetooth, or Wi-Fito access the database. More', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 9}), Document(page_content='can belong to affine or projective transformation class. \\n The device can use a Smartphone memory, a 3G or 4G \\n technology, Bluetooth, or Wi-Fito access the database. More \\n over, the camera of the device can have auto-focus feature and \\n being switched to operate in visible or NIR spectrum regions. \\n A lighting feature is provided by the device to improve the \\n quality of the image both in the visible and the NIR light. The \\n Smartphone can employ a wireless communication with a \\n remote server for storage, image processing and exchanging \\n the identification information. Apart of the personal identifi \\n cation, disclosed device is capable of collecting the personal \\n biometric information, accessing the individually specific \\n information for the identified person and/or granting an indi \\n vidual access to the specific information or site. The device is also capable of performing tasks of personnel/population Sur \\n Veying and management. \\n BRIEF DESCRIPTION OF THE DRAWINGS \\n FIG. 1: Prior Art. The matched portions of the two super imposed patterns. \\n FIG. 2: The general layout of the proposed invention. \\n FIG. 3: The block-diagram of the image matching algo \\n rithm disclosed in the preferred embodiment invention. \\n FIG. 4: Applying a high-pass, low-pass filters and contrast \\n enhancement to the image. Original (4A) and processed (4B) \\n images of the human hand veins pattern are shown. FIG. 5: Detailed diagram of the image descriptor matching \\n part of the algorithm in the preferred embodiment of the \\n invention. \\n FIG. 6: Illustration of the matching procedure of the algo \\n rithm for two different images that taken for the same person \\n at different time: FIG. 6A illustrates a good match between the points of interest while FIG. 6B illustrates a bad match. FIG. 7: Algorithm implementation example. A matching \\n criterion values for the comparison between an acquired \\n image and multiple images from a database is shown. The \\n ellipse marks the region where all 4 images are matched for \\n the same individual. \\n DETAILED DESCRIPTION OF THE PREFERRED \\n EMBODIMENT \\n The disclosed invention is based on a device and method \\n for biometric human identification based on the platform of a \\n PAD. The general layout of the disclosed invention is show in \\n FIG 2. \\n In the preferred embodiment of the invention, the image acquisition is performed using a Smartphone. In such embodi \\n ment a Smartphone-built-in (standard) camera is used to \\n obtain the blood vein-pattern. \\n In preferable embodiment of the invention the distance \\n from the camera and the object (i.e. a skin area) is within 0.1-1 \\n meter range. \\n In another embodiment of the invention, the camera is specifically modified to adjust the camera sensitivity for dif ferent wavelength regions, i.e. spectral region or spectrum. \\n In the preferred embodiment of the invention, the Near InfraRed (NIR) region of electromagnetic spectrum, Such as \\n a wavelength region between 750 nm and 2500 nm, for example, is used by the camera. This embodiment exploits the \\n fact that oxygenated blood in vein vessels absorbs the NIR part of the spectrum more efficiently than the visible spec \\n trum, while the outer skin layers are Sufficiently transparent \\n for the NIR radiation.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 9}), Document(page_content='US 9,095,285 B2 \\n 5 \\n In yet another embodiment of the invention, a specific \\n camera modification is implemented that enables a sequential \\n operation within two spectral regions, for example, between \\n visible and NIR regions, preferably having an engaging/dis \\n engaging mechanism (e.g. a button). \\n In yet another embodiment of the invention, the built-in \\n NIR illumination device is used in combination with the PDA \\n if necessary to improve the image acquisition by the camera at \\n low-light conditions, preferably having an engaging/disen \\n gaging mechanism (e.g. a button). \\n The disclosed invention is based on the device and method \\n to compare and match the unique vein patterns of human \\n hands. The method includes an algorithm capable of process ing and comparing the captured image against the set of \\n images stored in the database. \\n The database can be located either within the PAD memory \\n unit or remotely. For the latter case, the PAD is capable of a \\n bi-directional remote access, including a real-time access, to \\n the database using any wireless protocol available (e.g. Wi-Fi \\n or Bluetooth). \\n In the preferred embodiment, of the invention all the ele ments of the image processing and algorithm operation are \\n realized on the platform of onboard PAD processing unit, \\n such as a smartphone CPU. It is also possible, however, to implement a remote processing (e.g. remote server) for the algorithm operation, either entirely or partially. \\n In the preferred embodiment of the invention, the algo rithm is capable of saving captured images in the database and processing any stored images within the database, e.g. per \\n forming a search and statistical data analysis upon the stored \\n images within the database. FIG. 3 shows the block-diagram of the image matching \\n algorithm disclosed in the preferred embodiment of the invention. The numbering in FIG. 3 corresponds to the soft \\n ware routine steps that are used in the preferred embodiment \\n of the disclosed biometric identification device. \\n The realization of the identification routine in the preferred \\n embodiment of the invention is described in details below and \\n comprises the following steps: Image Acquisition: \\n Step-1: An input image acquisition of human dorsal hand \\n with PAD camera, selecting the area of interest from the raw image. Image Enhancing: \\n Step-2: Enhancing the input image, using: i) Conversion the input image to a gray-scale image (i.e. intensity image); ii) \\n Enhancing contrast of the gray-scale image by mapping the existing intensity range of the gray-scale image to the entire possible intensity range and leaving out an adjustable per \\n centage of pixels (e.g. one percent of pixels) having lowest \\n and highest intensities. Step-3: Further enhancing the contrast of the input image by application of a high-frequency and a low-frequency \\n image filtering with respective adjustable parameters. \\n Obtaining the image of a hand vein pattern image from the \\n enhanced image. Image Processing: \\n Step-4: Application of the SURF algorithm to the (hand) \\n vein pattern to obtain locations of points of interest of the vein pattern; the SURF algorithm using a set of adjustable param \\n eters, including hessian threshold values, hessian balance value, and number hessian octaves. Preparing a unique descriptor for each of the points of interest. Each descriptor \\n includes a 64-dimentional vector which, in turn, contains the information about local pixel neighborhood of Such respec \\n tive point of interest. 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 6 \\n Registration to Database: \\n Step-5: Storing to data base the original image, its \\n enhanced image, locations of the points of interest, descrip \\n tors, personal information about the individual being imaged, \\n a geo-location information (e.g. GPS coordinates), time, etc. \\n Step-6: Choosing a database image from the multiple \\n images in the database; the database image including its', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 10}), Document(page_content='a geo-location information (e.g. GPS coordinates), time, etc. \\n Step-6: Choosing a database image from the multiple \\n images in the database; the database image including its \\n enhanced image, locations of points of interest, the descrip \\n tors, a personal information about the individual being \\n imaged a geo-location information (e.g. GPS coordinates), \\n time, etc. \\n In the preferred embodiment of the invention the linear \\n search over the database of previously collected images is \\n employed. Alternative search techniques can be used, such as \\n a decision-tree clustering at the particular point of interest. \\n The binary or, more general K-tree-structures at the space of \\n the chosen descriptor can be used to provide a non-linear \\n search time reduction. \\n Image Identification: Step-7: Comparing the set of descriptors (Step-4) with the \\n descriptors retrieved from the database image (Step-7). Cal \\n culation the Euclidian distance between descriptors in a 64-dimensional space of the descriptor vectors. Recording \\n the measured distance for the respective pair of descriptors. Step-8: Performing an initial search of the matching \\n descriptors among the stored database images from the data \\n base. FIG. 5 illustrates the implementation of a search for the matching pairs of the points of interest between the input image and a database image. The descriptor of the each point \\n of interest from a tested input image is compared with all the descriptors from the database image. \\n For each pair of these descriptors the Euclidian distance \\n between the 64-dimentional descriptor vectors is calculated. \\n Among all the calculated distances the minimal distance and \\n the distance which is Smallest among all other calculated \\n distances (i.e. next Smallest distance) are retained. The point of interest of the database image is selected as a matching point of interest for the point of interest of the tested image if: \\n i) the corresponding minimal distance of this pairis Smaller \\n than a certain adjustable threshold value, see Threshold \\n A in FIG. 5, and \\n ii) the ratio between such minimal distance and the next Smallest distance is Smaller than an another adjustable \\n threshold value, see Threshold B in FIG. 5. Simultaneous application of these two conditions provides \\n that 1) the selected pair of the points of interest matches the points of interest of the input and the database images with a \\n similar local environment, and 2) that these local environ ments are substantially different from the local neighborhood \\n of the other points of interest. Step-9. The set of pairs of the points of interest obtained by \\n aforementioned way for the input image and the database \\n image is called a threshold-matched set of the points of inter est. If, for the particular database image, the number of pairs \\n of matched points of interest in the threshold-matched set is longer than a certain adjustable threshold number, then this database image is selected for the initial set of matching images. \\n Step-10: Deduction of the transformation matrixes which represents the transformation between the set of points of interest obtained as described in Step-4 for the input image to \\n the paired points of interest selected for the threshold \\n matched sets, as described in Step 8, corresponding to data base images in the initial matching set of the database images \\n obtained in Step 9.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 10}), Document(page_content='US 9,095,285 B2 \\n 7 \\n The choice of the type of the transformation matrix can be made upon the assumption about the type of possible geo \\n metrical transformation, Such as affine, projective, etc. These \\n transformations are insensitive to rotations, Scaling, tilt, of image plane, etc. In the preferred embodiment of the inven \\n tion, the transformation matrix is derived from two column \\n matrixes of coordinates of the points of interest that are rel \\n evant to these images. Each of these column matrixes have X \\n and Y coordinates of the point of interest as rows. In the preferred embodiment of the invention, the Singular \\n Value Decomposition method is used to evaluate a Moore \\n Penrose pseudo inverse matrix relevant to the column matrix of the points of interests coordinates in the input image. \\n Accordingly, the transformation matrix is calculated as the matrix product of original column matrix for database image \\n and Moore-Penrose pseudo inverse of column matrix for tested image. \\n Preferred embodiment of the invention implements the \\n matrix of affine transformation which relates the points of \\n interest coordinates for both aforementioned images. How ever, it is also possible for one skilled in the art to evaluate a more general projective transformation matrix or even more \\n complex transformations by similar means. \\n Step-11: Deduction of the locations of the points of interest \\n in the database image by applying the transformation matrix \\n (obtained in Step-10) to the set of points of interest corre \\n sponded to the input image. The transformation matrix is applied to the original column matrix containing the coordi \\n nates of the input image points of interest to obtain a set of \\n transformed coordinates for the points of interest in the input image. \\n Step-12: Comparison the transformed coordinates of the points of interest in the database image (obtained in Step-11) \\n with the locations of the points of interest obtained by the SURF method for this image. Calculation of squares of \\n Euclidian distances between the points of interest coordinates \\n in the database image and transformed coordinates of the input image. This step is used to assess how well the deduced \\n transformation matrix matches the paired points of interest, in other words, the input coordinates with applied transforma \\n tion matrix are compared to the actual points of interest loca \\n tions at the database image. Step-13: The personal identification is performed based on \\n a value of the cumulative matching criterion chi2. If the value \\n of chi2 is below of a cumulative threshold value, the two \\n images considered to be matched. Applying a cumulative \\n predefined criterion chi2 to characterize and score the match ing degree between the current and the database images (i.e. the vein patterns). \\n In the preferred embodiment of the invention, the adjust \\n able cumulative threshold value for Chi2 for a pair of two images is evaluated as a mean square distance between the \\n coordinates of the points of interest of database image and transformed coordinates of input image. \\n The tested image with the smallest Chi2 value is selected as a Best Matching Image for the input image. Image Analysis: \\n Step-14: Deriving probabilities (i.e. rates) of a true and a \\n false positive matching from the obtained values of Chi2 for \\n the total number of compared and matched images. \\n The algorithm disclosed in Steps 1-14 indicate that the \\n false positive rate of the matching is less than 0.001%, which is in a good agreement with the results have been reported for \\n alternative well-known biometric authentication methods. \\n FIG. 4 shows the example of a high- and low-frequency pass filtering application to the image, in combination with \\n the contrast enhancement, used to obtain the of human hand 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 11}), Document(page_content='the contrast enhancement, used to obtain the of human hand 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 \\n 55 \\n 60 \\n 65 8 \\n veins patterns (Steps-2, 3 of the aforementioned routine) as a preprocessing required for the matching analysis. The origi \\n nal image (FIG. 4A) and the processed (FIG. 4B) image are \\n shown. \\n FIG. 5 shows the image descriptor matching part of the \\n algorithm (see FIG. 3, Steps-7-8) more in detail, including \\n comparing the sets of points of interest for the database image and the input images, and selecting the pairs of possible \\n matching points of interest. \\n While in the preferred embodiment of the invention the gray-scale (intensity) images of the human hand vein-pattern \\n is used, other embodiments could use foil color images for the \\n same purposes. \\n It is obvious to the skilled in the art, that most of the image processing procedures are dependent on a number of adjust \\n able parameters, such as settings of averaging windows for \\n high-pass and low-pass filters, percentage of pixels with out \\n lying intensity values removed during image contrast \\n enhancement, settings of the SURF procedure, as well as \\n multiple threshold values relevant to image comparison pro \\n cedures. \\n In the preferred embodiment of the device, a generic pro \\n cedure, which is using a set of test images to simultaneously \\n optimize the values of these adjustable parameters, is imple \\n mented. This procedure establishes a target image matching \\n matrix which contains a Zero values for the elements repre senting non matching images and some values for the pairs of \\n matching images. The procedure starts from a pre-defined set \\n of adjustable parameters and performs all routine shown in \\n the FIG. 3 for any set of images from the database. \\n Moreover, the procedure establishes penalty values for \\n each occurrence of incorrect match or absence of a correct \\n match, further collecting these penalty values in an aggre \\n gated target function value. The procedure employs non \\n gradient Nedler-Mead simplex search in the space of adjust \\n able parameters to minimize the target function value and to establish a set mutually optimized parameters for our image processing and image comparison procedures. \\n FIG. 6 illustrated the actual performance of the matching procedure of the algorithm, (see the Step-12). The points of \\n interest for two different images are taken for the same person \\n at different times. FIG. 6A illustrates a good match with, a \\n chi2 cumulative criterion having value of 30, while FIG. 6B \\n illustrates a bad match with a cumulative criterion chi2 hav \\n ing value in the range of 2000-10000. This demonstrates a good efficiency of the matching procedure. \\n FIG. 7 shows a comparison between a single image against \\n multiple database images. The ellipse marks the region where \\n all four images of the same Subjects are matched which dem onstrates high sensitivity of image matching procedure. \\n Although several exemplary embodiments have been \\n herein shown and described, those of skill in the art will recognize that many modifications and variations are possible \\n without departing from the spirit and scope of the invention, \\n and it is intended to measure the invention only by the appended claims. \\n We claim: \\n 1. A portable device for identification of an individual based on a vein-pattern comprising: a Smartphone; \\n a camera built in the Smartphone, the camera acquiring an \\n input image of a skin area with the vein-pattern of the \\n individual, the input image is acquired in the near infra \\n red (NIR) spectrum of light radiation;', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 11}), Document(page_content='US 9,095,285 B2 \\n a built-in smartphone central processor unit (CPU) per forms processing of the acquired input image to extract an input vein-pattern image from the obtained image of \\n the skin area; the CPU performs deduction of points of interest from the Vein-pattern image and calculation of their coordinates; the CPU also creates a 64-dimensional descriptor vector for each of the point of interest, the 64-dimensional descriptor Vector including information about features of a local image around the each respective point of \\n interest; the CPU compares the obtained image descriptor vector with each descriptor vector from a database of descriptor vectors; the database is stored in the smartphone; \\n the CPU uses a criterion of minimum Euclidian distance \\n between descriptor vectors and a criterion of sufficient separation between minimum Euclidian distances to establish pairs of matching points of interest for two images: the input image and each image stored in the \\n database, and assuring the individual identification if the number of pairs of matched points of interest is greater than a predefined \\n threshold. \\n 2. The device of claim 1, further comprising additional processing in the CPU to improve an accuracy of the identi fication, the additional processing comprising: calculating coordinates of the matched pairs of points of interest of input and database image: calculating a transformation matrix using the coordinates of the matched pairs; the transformation matrix reflecting a geometrical trans \\n formation of the coordinates of the input image into coordinates of a selected database image; the selected database image is one that was selected after the vectors comparison and for which the number of pairs of the matched points of interest is greater than the predefined \\n threshold; calculating transformed coordinates of the points of inter est of the input image: \\n comparing the transformed coordinates with initial coor \\n dinates; and determining an improved individual identification if a dis \\n tance between the initial and the transformed coordi \\n nates is less than a cumulative threshold. \\n 3. The device of claim 2, wherein the cumulative threshold \\n is determined as a mean square distance between the initial \\n and the transformed coordinates. \\n 4. The device of claim 2, wherein the transformation matrix is an affine, projective transformation. \\n 5. The device of claim 2, wherein the transformation matrix is a general class of image transformations. 5 \\n 10 \\n 15 \\n 25 \\n 30 \\n 35 \\n 40 \\n 45 \\n 50 10 \\n 6. The device of claim 1, wherein the coordinates of the points of interest and 64-dimentional descriptor vectors are calculated using a Speeded-Up Robust Features (SURF) \\n method. \\n 7. The device of claim 1, wherein the database is transmit \\n ted to the smartphone via a 3G or 4G technology, Bluetooth, \\n or Wi-Fi, and the database is stored in the smartphone \\n memory. \\n 8. The device of claim 1, further comprising a switch, performing switching of the camera operation between a \\n visible and the NIR spectrum. \\n 9. The device of claim 8, wherein the camera has an auto \\n focus feature operating in both the visible and the NIR spec \\n trum. \\n 10. The device of claim 9, further comprising a lighting feature is provided by the device to improve the quality of the image both in the visible and the NIR light spectrum. 11. The device of claim 1, further comprising a wireless communication of the Smartphone with a remote server; the Smartphone having an option to choose the server for the image processing, a storage of the database and a command to \\n send results of the individual identification back to the Smart phone for display. \\n 12. The device of claim 1, wherein the scale vein-pattern image is a gray scale image extracted based on a spatial low frequency and a high frequency filtering and image contrast \\n enhancement. \\n 13. The device of claim 1, being used for collecting per \\n sonal biometric information.', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 12}), Document(page_content='enhancement. \\n 13. The device of claim 1, being used for collecting per \\n sonal biometric information. \\n 14. The device of claim 1, being used for accessing indi vidual specific information for the identified individual. 15. The device of claim 1, being used for allowing an \\n access to the information or site. \\n 16. The device of claim 1, being used for performing tasks related with personnel and population surveying and manage \\n ment. \\n 17. The device of claim 1, wherein the number of matching pairs of points of interest signifying possible identification of an input image is adjustable. \\n 18. The device of claim 1, wherein the threshold value of \\n minimum Euclidian distance between 64-dimentional \\n descriptor vectors of two points of interest is adjustable. \\n 19. The device of claim 1, wherein the threshold value of a ratio between the minimum Euclidian distances for two pairs of matching points of interest is adjustable. \\n 20. The device of claim 1, wherein the coordinates of the points of interest and 64-dimentional descriptor vectors are calculated using a Speeded-Up Robust Features (SURF) method, and a value of a hessian threshold is adjustable. \\n ck ck ck ck ck', metadata={'source': 'https://patentimages.storage.googleapis.com/7c/30/05/2dc2f874122207/US9095285.pdf', 'page': 12})]\n",
      "[Document(page_content=\"US010922957B2 \\n ( 12 ) United States Patent ( 10 ) Patent No .: US 10,922,957 B2 \\n ( 45 ) Date of Patent : Feb. 16 , 2021 Rhoads et al . \\n ( 56 ) References Cited ( 54 ) METHODS AND SYSTEMS FOR CONTENT \\n PROCESSING U.S. PATENT DOCUMENTS \\n ( 71 ) Applicant : Digimarc Corporation , Beaverton , OR ( US ) 5,905,248 A 5,932,863 A 5/1999 Russell \\n 8/1999 Rathus \\n ( Continued ) ( 72 ) Inventors : Geoffrey B. Rhoads , West Linn , OR ( US ) ; Tony F. Rodriguez , Portland , OR \\n ( US ) ; William Y. Conwell , Portland , OR ( US ) FOREIGN PATENT DOCUMENTS \\n EP \\n WO 1069529 \\n WO2007001774 1/2001 \\n 1/2007 ( 73 ) Assignee : Digimare Corporation , Beaverton , OR \\n ( US ) OTHER PUBLICATIONS \\n ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 0 days . Alleysson et al , Linear demosaicing inspired by the human visual system , IEEE Trans . on Image Processing , vol . 14 , No. 4 , Apr. 2005 . \\n ( Continued ) \\n ( 21 ) Appl . No .: 15 / 889,013 Primary Examiner — Hadi Akhavannik ( 74 ) Attorney , Agent , or Firm — Digimarc Corporation ( 22 ) Filed : Feb. 5 , 2018 \\n ( 65 ) Prior Publication Data \\n US 2018/0233028 A1 Aug. 16 , 2018 \\n Related U.S. Application Data \\n ( 60 ) Division of application No. 14 / 456,784 , filed on Aug. 11 , 2014 , now Pat . No. 9,886,845 , which is a division \\n ( Continued ) ( 57 ) ABSTRACT \\n Mobile phones and other portable devices are equipped with a variety of technologies by which existing functionality can be improved , and new functionality can be provided . Some aspects relate to visual search capabilities , and determining appropriate actions responsive to different image inputs . Others relate to processing of image data . Still others concern metadata generation , processing , and representa tion . Yet others concern user interface improvements . Other aspects relate to imaging architectures , in which a mobile phone's image sensor is one in a chain of stages that successively act on packetized instructions / data , to capture and later process imagery . Still other aspects relate to distribution of processing tasks between the mobile device and remote resources ( “ the cloud ” ) . Elemental image pro cessing ( e.g. , simple filtering and edge detection ) can be performed on the mobile phone , while other operations can be referred out to remote service providers . The remote service providers can be selected using techniques such as reverse auctions , through which they compete for processing tasks . A great number of other features and arrangements are also detailed . ( 51 ) Int . Ci . \\n G06K 9/00 ( 2006.01 ) G08C 17/02 ( 2006.01 ) \\n ( Continued ) ( 52 ) U.S. Ci . \\n CPC G08C 17/02 ( 2013.01 ) ; G06K 900 ( 2013.01 ) ; H04L 67/34 ( 2013.01 ) ; G06F 3/0482 ( 2013.01 ) ; \\n ( Continued ) \\n ( 58 ) Field of Classification Search None See application file for complete search history . 47 Claims , 60 Drawing Sheets \\n IMAGE SENSOR3 \\n AMPLIFY ANALO 3GIJALS \\n CONVERT TO DIGITAL NATIVE IMAGE DATA \\n BACR INTERPOLATION FOURIER TRANSFORM ETC EIGENSACE CALCULATON \\n Eliss in \\n MATCH 54RCODE TEMPLATE ? WATCH FACE TEMPLATE : Yes \\n WELLIN TRANSFOAM SEND DIGITAL NATI / E IMAGE LATA ANDOR ORIENWOMANDATA ? O BARCODE SERVICE \\n MATCH TEXT TERPLATS ? SEND DIGITAL NATIVE MAGELATA ANDORRA DOMAIN DATAT OCR SERVICE \\n SEWO DIGITA NATIVE TRADE DATA ANDA \\n IOMAIN DATATOVA WATCH WN ORIENTATION TEMPLATE Yes \\n MATCH PACS TEMPLATE ? Yes SEND DIGITAL NATIVE \\n ! MAGE DATA AND / OR FM \\n DOMAIN DATA ANDO ? C ! GENFACE DATAT SACIAL PECOGNITION SERVICE\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 0}), Document(page_content='US 10,922,957 B2 Page 2 \\n 7,460,160 B2 7,502,759 B2 * 12/2008 Hershey et al . 3/2009 Hannigan G06Q 30/02 123/201 Related U.S. Application Data \\n of application No. 13 / 011,618 , filed on Jan. 21 , 2011 , now Pat . No. 8,805,110 , which is a continuation of application No. PCT / US2009 / 054358 , filed on Aug. 19 , 2009 , which is a continuation - in - part of applica tion No. 12 / 271,692 , filed on Nov. 14 , 2008 , now Pat . No. 8,520,979 , and a continuation - in - part of applica tion No. 12 / 484,115 , filed on Jun . 12 , 2009 , now Pat . No. 8,385,971 , and a continuation - in - part of applica tion No. 12 / 498,709 , filed on Jul . 7 , 2009 , now abandoned . \\n ( 60 ) Provisional application No. 61 / 090,083 , filed on Aug. 19 , 2008 , provisional application No. 61 / 096,703 , filed on Sep. 12 , 2008 , provisional application No. 61 / 100,643 , filed on Sep. 26 , 2008 , provisional application No. 61 / 103,907 , filed on Oct. 8 , 2008 , provisional application No. 61 / 110,490 , filed on Oct. 31 , 2008 , provisional application No. 61 / 169,266 , filed on Apr. 14 , 2009 , provisional application No. 61 / 174,822 , filed on May 1 , 2009 , provisional application No. 61 / 176,739 , filed on May 8 , 2009 , provisional application No. 61 / 226,195 , filed on Jul . 16 , 2009 , provisional application No. 61 / 234,542 , filed on Aug. 17 , 2009 . 7,565,139 B2 7,751,805 B2 7,837,094 B2 8,230,337 B2 8,385,971 B2 8,755,837 B2 8,886,206 B2 9,497,341 B2 2001/0001854 A1 2001/0055391 A1 \\n 2002/0032027 Al 2002/0062346 Al 2002/0072982 A1 2002/0075298 Al 2002/0102966 A1 \\n 2002/0152388 A1 \\n 2002/0178410 A1 \\n 2003/0083098 Al \\n 2004/0199387 Al \\n 2004/0263663 A1 \\n 2005/0144455 Al \\n 2005/0185060 A1 \\n 2005/0227674 Al \\n 2005/0264658 Al 2005/0276486 Al \\n 2006/0012677 Al \\n 2006/0026140 A1 \\n 2006/0031684 Al \\n 2006/0056707 A1 \\n 2006/0097062 A1 * 7/2009 Neven \\n 7/2010 Neven 11/2010 Rhoads 7/2012 Rhoads \\n 2/2013 Rhoads et al . \\n 6/2014 Rhoads et al . 11/2014 Lord \\n 11/2016 Rhoads \\n 5/2001 Schena 12/2001 Jacobs \\n 3/2002 Kirani \\n 5/2002 Chen \\n 6/2002 Barton \\n 6/2002 Schena \\n 8/2002 Lev 10/2002 Linnartz \\n 11/2002 Haitsma \\n 5/2003 Yamazaki 10/2004 Wang 12/2004 Lee et al . \\n 6/2005 Haitsma \\n 8/2005 Neven 10/2005 Kopra 12/2005 Ray et al . 12/2005 Withers et al . \\n 1/2006 Neven 2/2006 King 2/2006 Sharma \\n 3/2006 Suomela 5/2006 Cheong G06K 19/06037 \\n 235/494 ( 51 ) Int . Ci . \\n H04L 29/08 ( 2006.01 ) \\n G06T 1/20 ( 2006.01 ) HO4N 13/00 ( 2018.01 ) \\n G06F 3/0482 ( 2013.01 ) \\n H046 88/02 ( 2009.01 ) \\n ( 52 ) U.S. CI . CPC G06T 1/20 ( 2013.01 ) ; G08C 2201/93 ( 2013.01 ) ; HO4N 2013/0074 ( 2013.01 ) ; H04W 88/02 ( 2013.01 ) 2006/0143684 A1 \\n 2006/0204118 A1 \\n 2006/0240862 Al \\n 2007/0174180 A1 2007/0175998 A1 2007/0228175 A1 \\n 2007/0286524 A1 \\n 2008/0080396 Al \\n 2008/0086311 A1 \\n 2008/0267504 Al 2008/0300011 A1 \\n 2009/0237546 A1 \\n 2009/0290807 A1 \\n 2010/0110222 A1 \\n 2011/0231469 Al 6/2006 Morris 9/2006 Fehmi et al . 10/2006 Neven \\n 7/2007 Shin \\n 8/2007 Lev 10/2007 Kotlarsky 12/2007 Song 4/2008 Meijer et al . 4/2008 Conwell et al . \\n 10/2008 Schloter 12/2008 Rhoads 9/2009 Bloebaum \\n 11/2009 Marchesotti \\n 5/2010 Smith et al . \\n 9/2011 Wolman ( 56 ) References Cited \\n U.S. PATENT DOCUMENTS \\n OTHER PUBLICATIONS 6,002,946 A 6,081,827 A 6,121,530 A 6,199,048 B1 6,249,226 B1 6,389,055 B1 6,411,725 B1 6,491,217 B2 6,549,933 B1 6,675,165 B1 6,694,042 B2 6,766,363 B1 6,788,293 B1 6,941,275 B1 6,947,571 B1 6,961,083 B2 6,993,573 B2 7,003,731 B1 7,016,532 B2 7,022,075 B2 7,058,223 B2 7,065,559 B1 7,133,069 B2 7,174,293 B2 7,190,844 B2 * 12/1999 Reber 6/2000 Reber \\n 9/2000 Sonoda \\n 3/2001 Hudetz \\n 6/2001 Harrison 5/2002 August 6/2002 Rhoads 12/2002 Catan \\n 4/2003 Barrett \\n 1/2004 Rothschild \\n 2/2004 Seder \\n 7/2004 Rothschild \\n 9/2004 Silverbrook \\n 9/2005 Swierczek 9/2005 Rhoads \\n 11/2005 Obrador \\n 1/2006 Hunter 2/2006 Rhoads 3/2006 Boncyk 4/2006 Grunwald 6/2006 Cox \\n 6/2006 Weiss', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 1}), Document(page_content='2/2004 Seder \\n 7/2004 Rothschild \\n 9/2004 Silverbrook \\n 9/2005 Swierczek 9/2005 Rhoads \\n 11/2005 Obrador \\n 1/2006 Hunter 2/2006 Rhoads 3/2006 Boncyk 4/2006 Grunwald 6/2006 Cox \\n 6/2006 Weiss \\n 11/2006 Wallach et al . 2/2007 Kenyon 3/2007 Kobayashi Barbaro , et al , A 100x 100 pixel silicon retina for gradient extraction with steering filter capabilities and temporal output coding , IEEE J. \\n of Solid - State Circuits , vol . 37 , No. 2 , 2002 . Dudek , et al , A general - purpose processor - per - pixel analog SIMD vision chip , IEEE Trans . on Circuits and Systems , vol . 52 , No. 1 , 2005 . Dudek , et al , General - purpose 128x 128 SIMD processor array with integrated image sensor , Electronics Letters 42.12 , pp . 678-679 . 2006 . Flores - Cuautle , System for face detection on a mobile phone using Java technology , Masters Thesis , U. of Oslo , Nov. 2007 . Grablink Quickpack CFA brochure , Feb. 2007 . Johansson , et al . A multi - resolution 100 GOPS 4 Gpixels - sec programmable CMOS image sensor for machine vision , IEEE Workshop on CCDs and Adv . Image Sensors , 2003 . Kamal et al , Online machine vision inspection system for detecting coating defects in metal lids , IMECS , Mar. 2008 . Lin , et al , a CMOS image sensor for multi - level focal plane image decomposition , IEEE Trans . on Circuits and Systems , vol . 55 , No. \\n 9 , Oct. 2008 . Mosqueron et al , Smart camera based on embedded HW - SW coprocessor , EURASIP Journal on Embedded Systems , Jan. 2008 . Ni , et al , A 256x256 pixel smart CMOS image sensor for line - based stereo vision applications , IEEE J. of Solid - State Circuits , vol . 35 , No. 7 , Jul . 2000 . HO4N 1/407 \\n 358 / 3.26 \\n 7,251,475 B2 7,283,983 B2 7,432,940 B2 * 7/2007 Kawamoto 10/2007 Dooley 10/2008 Brook G11B 27/11 \\n 345/629', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 1}), Document(page_content=\"US 10,922,957 B2 \\n Page 3 \\n ( 56 ) References Cited \\n OTHER PUBLICATIONS \\n Nishikawa , et al , A high - speed CMOS image sensor with on - chip parallel image compression circuits , IEEE Custom Integrated Cir cuits Conf . , 2007 . Seitz , et al , CCD and APS - CMOS technology for smart pixels and image sensors , Proc . of SPIE vol . 5251 , 2004 . Simoni et al , A digital camera for machine vision , 20th Int’l IEEE Conf . on Industrial Electronics , Control and Instrumentation , 1994 . Xu et al , Design of a DSP - based CMOS imaging system for embedded computer vision , IEEE Conf . on Cybernetics and Intel ligent Systems , Sep. 2008 . Prosecution excerpt from corresponding Korean application 10-2011 7006167 , namely English translation of Notice of Preliminary Rejection dated Sep. 23 , 2015 . Prosecution excerpts from corresponding Canadian application 2,734,613 , namely Examiner's reports dated Apr. 25 , 2016 , Dec. 29 , 2016 and Dec. 6 , 2017 . Prosecution excerpts from Corresponding Chinese application 200980141567.8 , namely English translations of Examination reports dated Dec. 6 , 2012 , Oct. 15 , 2013 , May 5 , 2014 and Nov. 2 , 2014 . Machine translation of JP3927802 , published as JP2003-189325 . Prosecution excerpts from U.S. Appl . No. 13 / 774,056 ( now U.S. Pat . No. 8,755,837 ) . Prosecution excerpts from U.S. Appl . No. 12 / 484,115 ( now U.S. Pat . No. 8,385,971 ) . Further prosecution excerpts from corresponding Canadian appli cation 2,734,613 , namely Applicant's response dated Jun . 6 , 2018 ( to Examiner report dated Dec. 6 , 2017 ) ; Examiner report dated Oct. 25 , 2018 ; Applicant's response dated Apr. 24 , 2019 ; and Notice of \\n Allowance dated Jan. 22 , 2020 . LeCun et al , Handwritten Digit Recognition with a Back Propagation Network , Advances in Neural Information Processing Systems , pp . 396-404 , 1990 . Balan , Tactics - Based Remote Execution for Mobile Computing , MobiSys 2003 . Chun , Augmented Smartphone Applications Through Clone Cloud Execution , Proc . of the 8th Workshop on Hot Topics in Operating Systems , May 18 , 2009 . Flinn et al , Balancing Performance , Energy , and Quality in Perva sive Computing , Proc . of the 22nd International Conference on Distributed Computing Systems ( ICDCS ) , Jul . 2002 . Flinn , et al , Self - Tuned Remote Execution for Pervasive Comput ing , Proc . of the 8th Workshop on Hot Topics in Operating Systems ( HotOS ) , May 2001 . Satyanarayanan , The Case for VM - based Cloudlets in Mobile Computing , IEEE Pervasive Computing , vol . 8 , No. 4 , pp . 14-23 , \\n Nov. 2009 . Reichle , et al , A Comprehensive Context Modeling Framework for Pervasive Computing Systems , Distributed Applications and Interop erable Systems , Springer Berlin Heidelberg , 2008 . Geihs , et al , Modeling of Context - Aware Self - Adaptive Applica tions in Ubiquitous and Service - Oriented Environments , Software Engineering for Self - Adaptive Systems , Springer Berlin Heidelberg , \\n Jun . 19 , 2009 , pp . 146-163 . Kirsch - Pinheiro et al , Context - Aware Service Selection Using Graph Matching , 2nd Non Functional Properties and Service Level Agree ments in Service Oriented Computing Workshop ( NFPSLA SOC'08 ) , CEUR Workshop Proceedings , vol . 411. 2008 . Zachariadis , et al , Building Adaptable Mobile Middleware Services Using Logical Mobility Techniques , in Contributions to Ubiquitous Computing , Springer Berlin Heidelberg , 2007 , pp . 3-26 . Zachariadis , et al , Adaptable Mobile Applications : Exploiting Logi cal Mobility in Mobile Computing , in Mobile Agents for Telecom munication Applications , Springer Berlin Heidelberg , 2003 , pp . \\n 170-179 . Kemp , et al , eyeDentify : Multimedia cyber foraging from a smart phone , 11th IEEE International Symposium on Multimedia , Dec. 14 , 2009 , pp . 392-399 . JP2004179783 , 2004 , with machine translation . Prosecution excerpts from U.S. Appl . No. 12 / 835,527 , which matured into U.S. Pat . No. 8,886,206 . \\n * cited by examiner\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 2}), Document(page_content=\"U.S. Patent \\n 7 \\n ! 1 Feb.16 , 2021 \\n Show Times ? ? \\n Jane's Review Pretty Good 11 \\n My Car Sheet 1 of 60 \\n B08 '06 Vista ? \\n 0 0 0 0 0 0 \\n O US 10,922,957 B2 \\n FIG . 1\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 3}), Document(page_content='U.S. Patent \\n SEN SENSOR LENS CELL NETWORK TO SERVICES \\n CPU \\n LOW LEVEL PIXEL PROC . COMM CHANNEL INTERFACE Feb. 16 , 2021 \\n Tamil WIFI TO INTERNET Sheet 2 of 60 \\n TRAFFIC MONITORING & BILLING SYSTEM \\n FIG.1A US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 4}), Document(page_content='U.S. Patent \\n FIG . 2 \\n GET PRICES RETRIEVE MANUAL \\nGESTURE WATERMARK TEXT FACIAL RECOGNITION Feb. 16 , 2021 \\nNAVIGATION \\n OBJECT RECOGNITION \\n NEARBY ? BARCODE READING POST TO FACEBOOK \\n LIST FOR SALE ON CRAIGSLIST Sheet 3 of 60 \\n # $ % ^ \\n TRANSLATE TO ENGLISH \\n DISCOVER HISTORY COMMAND ENTRY HUMAN WEB ( DEGREES OF SEPARATION ) ACCESS AUTHENTICATION US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 5}), Document(page_content='NOKIA , APPLE , HTC , SAMSUNG ... \\n EN SENSOR LENS U.S. Patent \\nLEVEL CPU WIRELESS INTERFACE NETWORK INFRA STRUCTURE \\n Z \\n PROC . BROADCOM QUALCOMM ATHEROS VERIZON AT & T \\n ZEISS OMNI VISION MICRON SAMSUNG AVAGO TOSHIBA INTEL NVIDIA AMD MIPS CROWN CASTLE CISCO \\n AMERICAN TOWER ALCATEL - LUCENT \\n SBA COMM AT & T Feb. 16 , 2021 \\n T - MOBILE CLEAR Sheet 4 of 60 \\n DATA PROVIDERS SERVICE PROVIDERS PAYMENT PROCESSORS FIRST DATA \\n AMAZON GOOGLE \\n GOOGLE NIELSEN THOMSON THE WEB PAYPAL CITIBANK \\n FIG . 3 IBM RAUNHOFER STARTUPS US 10,922,957 B2 \\n O.', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 6}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 5 of 60 US 10,922,957 B2 \\n KEYVECTOR DATA ( PIXELS & DERIVATIVES ) \\n FIG . 4 ** tt ta \\n VISUAL \\n TASK \\n ? VISUAL \\n TASK VISUAL \\n TASK \\n C \\n RESULTA RESULTB RESULTC \\n KEYVECTOR \\n FIG . 4A \\n ? ADDRESSING , BA \\n ? \\n PIXEL PACKET', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 7}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 6 of 60 US 10,922,957 B2 \\n 1 \\n FIG . 4B \\n HEADER DATA KEYVECTOR DATA \\n TASKA TASKB TASK C TASKD \\n VISUAL TASK COMMONALITY PIE CHARTS \\n RE - SAMPLING \\n PDF417 BARCODE READING \\n FIG . 5', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 8}), Document(page_content=\"U.S. Patent Feb. 16 , 2021 Sheet 7 of 60 US 10,922,957 B2 \\n RESIDENT CALL - ON VISUAL PROCESSING SERVICES \\n ON ON OFF ON OFF \\n T \\n } \\n 3 \\n COMMON \\n SERVICES \\n SORTER \\n LEVEL \\n PIXEL \\n PRO \\n CESSING \\n LIBRARY \\n FLOW GATE \\n CONFIGURATION , \\n SOFTWARE PROGRAMMING EXTERNAL \\n PIXEL \\n SEG SENSOR HARDWARE KEYVECTOR \\n DATA \\n ( PROC'D PIXELS & \\n OTHER ) ROUTING \\nFIG . 6 INTERNAL\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 9}), Document(page_content='DEVICE UI SENSOR \\nREGISTRY & CONFIGURATION U.S. Patent \\nQUERY ROUTER AND RESPONSE MANAGER \\nSERVICE RESPONSE MANAGER PIXEL PACKAGING & ROUTING Feb. 16 , 2021 \\n COMM CHANNEL ( ) Sheet 8 of 60 \\n LOCAL BUS - US 10,922,957 B2 \\n FIG . 7', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 10}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 9 of 60 US 10,922,957 B2 \\n ONISSpodd OBUWINO IOWM ARBITRARILY COMPLEX RESULTS \\n FIG . 8', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 11}), Document(page_content='$$ CUSTOMER BILLING U.S. Patent \\n BILL DATA S. DATA \\nCARRIER SWITCHBOARD \\n DEVICES PIXEL PACKET PROCESSING & ROUTING Feb. 16 , 2021 \\n DATA $ ---- >> \\n CARRIER SWITCHBOARD DATA DATA --- \\n $ Sheet 10 of 60 US 10,922,957 B2 \\n FIG.9', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 12}), Document(page_content='U.S. Patent \\n SUITED , E.G. , FOR TEMPLATE MATCHING SUITED , E.G. , FOR DATA ASSOCIATION \\n CELL PHONE I \\n GPU ( S ) PROCESSOR Feb. 16 , 2021 \\n PROCESSOR , REF , DATA \\n 1 \\n LENS , SENSOR , LOW LEVEL PIXEL PROCESSING PACKAGER / ROUTER DECIDE WHAT IS DONE LOCALLY , REMOTELY PROCESSOR \\n ROUTING , BILLING , TRAFFIC MONITORING \\n COM PROCESSOR Sheet 11 of 60 \\n REF , DATA \\n CPU ( S ) PROCESSOR US 10,922,957 B2 \\n FIG . 10', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 13}), Document(page_content='AS CLOSE TO THE RAW SENSOR DATA AS POSSIBLE AS LOW IN THE COMMUNICATIONS STACK AS POSSIBLE U.S. Patent \\n LOCAL CLOUD \\n MOBILE CAMERA VISUAL KEYVECTOR PROCESSING AND PACKAGING KEYVECTOR REQUEST AND RESPONSE ROUTER Feb. 16 , 2021 \\n ROUTER \\n VISUAL TASK LISTING WITH CONFIGURATION AND SERVICES ADDRESSING LOCAL DEVICE SERVICES PROCESSING REMOTE SERVICE PROVIDERS AND RICH CONTENT AGGREGATORS Sheet 12 of 60 \\n MOBILE THE TWO OVALS ARE SOFTWARE COMPONENTS OF A REAL - TIME LOCAL / REMOTE OBJECT RECOGNITION MOBILE DEVICE NETWORK . THE REMAINING HIGH LEVEL FLOW PROCESSES USE NATIVE CAPABILITIES A “ OF BOTH A MOBILE DEVICE , AS WELL AS \" THE CLOUD . \" US 10,922,957 B2 \\n FIG . 10A', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 14}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 13 of 60 US 10,922,957 B2 \\n MY PROFILE , INCLUDING WHO I AM , AND \\n WHAT DO I GENERALLYLKENWANT \\n MY CONTEXT , INCLUDING WHERE I AM , AND \\n WHAT ARE MY CURRENT DESIRES \\n MY VISUAL QUERY PIXELS THEMSELVES \\n PACKETS RESULTS \\n VISUAL QUERY PACKET \\n PACKETS RESULTS PACKETS RESULTS \\n CONFIGURABLE ARRAY OF \\n SERVICE PROVIDERS OFFERED AT AUCTION TO AN \\n ARRAY OF SERVICE PROVIDERS \\n FIG . 11', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 15}), Document(page_content='USER DEVICE U.S. Patent \\nRESULTS VISUAL QUERY PACKET Feb. 16 , 2021 \\n VISUAL QUERY QUICK , HIGH VALUE RESPONSE \\n ACCOUNTED AND PACKAGED RESPONSE ACCOUNTED AND PACKAGED RESPONSE \\n QUERY ROUTER AND RESPONSE MANAGER Sheet 14 of 60 \\n BIDDER A \\n FIXED SERVICE CHOICE QUERY PUT OUT TO BID BIDDERB \\n SERVICE A BIDDER C \\n SERVICE B BID FILTER AND BROADCAST AGENT www.ws BIDDERD WINNING BID \\n SERVICE C - BID ---- US 10,922,957 B2 \\n BIDDER F \\n FIG . 12 BIDDERG', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 16}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 15 of 60 US 10,922,957 B2 \\n 12 \\n SENSOR \\n 17 PROCESSING FOR OBJECT \\n IDENTIFICATION \\n PROCESSING FOR \\n HUMAN VISUAL \\n SYSTEM ( e.g. , JPEG compression ) ADDRESSING \\n 13 \\n FIG . 13 \\n 16 \\n ********* \\n 56 \\n STAGE 1 STAGE 2 \\n INSTRUCTIONS INSTRUCTIONS INSTRUCTIONS STAGE Y \\n INSTRUCTIONS \\n 58a 58b \\n STAGE Z \\n INSTRUCTIONS \\n FIG . 17', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 17}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 16 of 60 US 10,922,957 B2 \\n CELL PHONE \\n 1 \\n PROCESSING FOR OBJECT \\n IDENTIFICATION \\n FIG . 14 14 \\n ADDRESSING \\n 15 \\n APPLICATION \\n IN CELL \\n PHONE PROCESSING \\n EXTERNAL FROM \\n CELL PHONE APPLICATION \\n PHONE \\n APPLICATION \\n EXTERNAL FROM \\n CELL PHONE \\n EXTERNAL FROM \\n CELL PHONE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 18}), Document(page_content='A r \\n 40 U.S. Patent \\n 27 : Feb. 16 , 2021 \\n HEINZ TOMATO ETCHUP 46 \\n 43 Sheet 17 of 60 \\nMARMITE 19.9 \\n Nora US 10,922,957 B2 \\n TROVA \\n FIG . 15 48 42 45', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 19}), Document(page_content='16 \\n 53 \\n 3 U.S. Patent \\n 1 for \\n $ \\n 2 \\n W PIPE MANAGER 52 \\n & \\n 7 I 5 Feb. 16 , 2021 \\n 8 33 216.239.32.10 \\n 5 33 \\n FREECE \\n 9 38a ????? \\n 60 STAGE 2a \\n 32 380 \\n SETUP PACKET CAMERA PACKET STAGE 1 PACKET STAGE 3 PACKET STAGE 4 Sheet 18 of 60 \\n 380 \\n STAGE 2 \\n 31 31 \\n 38b \\n 31 \\n 36 12.232.235.27 \\n 5 $ \\n CONTROL PROCESSOR 35 FIG . 16 US 10,922,957 B2 \\n SYNCHRON IZATION', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 20}), Document(page_content='U.S. Patent \\nHARDWARE 73 Feb. 16 , 2021 \\nSOFTWARE \\n 32 \\n CAMERA ori & mag 25 bits ) ORIENTATION AND GRADIENT MAGNITUDE COMPUTATION FIFOA ( 32x2048 ) 1 FIFOB ( 32x1024 ) FIFOC ( 32x512 ) DESCRIPTOR COMPUTATION ( NIOS II FPGA ) \\n 72 Sheet 19 of 60 \\n 74 \\n GAUSSIAN CASCADE AND DIFFERENCE OF GAUSSIANS FIFO octv0 ( 42x127 ) FIFO_octvo ( 40x256 ) FIFO octvo ( 38x512 ) KEYPOINT DETECTION WITH STABILITY CHECKS 1 FIG . 18 US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 21}), Document(page_content=\"MEMORY \\nPROCESSING STAGE U.S. Patent \\n < 38 > parameters < / 38 > \\nPROCESSING u \\n 38c < 58e ' > Instructions ... < / 58e ' > \\n < 587 > instructions < 58g > Instructions < / 58g > Feb. 16 , 2021 \\n *** \\n 57 . Sheet 20 of 60 \\n STAGE 3 INSTRUCTIONS STAGE 4 INSTRUCTIONS , ... ; EDGE FILTER DATA , APPEND RESULTS TO PACKET BODY , CHECK RESULTS V. THRESHOLD . IF GREATER , REPLACE HEADER FIELDS 588-589 WITH INSTRUCTIONS FROM MEMORY LOCATION F000 \\n 58c 580 58e 58f 689 US 10,922,957 B2 \\n 56 \\n FIG . 19\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 22}), Document(page_content='U.S. Patent \\nCAMERA DRIVER \\n MIC . DRIVER CM VISION Feb. 16 , 2021 \\n MOBILE ROBOT ( CLIENT ) FIXED SERVER \\n Z ? PROTOCOL \\n GPS DRIVER OTHER SERVICES \\n OTHER FIXED ROBOTIC RESOURCES Sheet 21 of 60 \\n SENSOR DRIVER FIG . 19A Prior Art Robotic Architecture US 10,922,957 B2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 23}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 22 of 60 US 10,922,957 B2 \\n RESPONSE \\n ROUTING \\n CONSTRAINTS NEEDED \\n OPERATION DETAILS High level operations USER \\n PREFERENCES HARDWARE \\n What special purpose \\n hardware is local ? \\n What is current hardware utilization ? CPU pipeline length Pipeline stall risks POWER operations \\n Common operations Operations that are preconditions to \\n others Power consumption CONNECTIVITY \\n GEOGRAPHICAL \\n CONSIDERATIONS RULES , \\n HEURISTICS \\n INFO RE REMOTE \\n PROVIDER ( S ) , E.G. , \\n Readiness Speed \\n Cost \\n importance to user \\n OPERATIONS FOR \\n LOCAL PROCESSING OPERATIONS FOR \\n REMOTE PROCESSING \\n FIG . 19B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 24}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 23 of 60 US 10,922,957 B2 \\n 81 pana \\n 80 \\n 12 \\n CAMERA \\n SENSOR \\n 82 83 86 \\n 85 ? MICRO \\n MIRROR \\n PROJECTOR \\n I \\n w \\n FIG . 20', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 25}), Document(page_content='MICROPHONE CAMERA & 3 FIG . 20A 3 IC HW 3 3 3 U { Digital Watermarking 3 DSP 3 3 $ \\n 1D / 2D Bar Code Reader 3 OCR 3 ? 3 3 Fingerprinting 3 CPU Reference Platform uteisks Gunesedo 3 suopeoiddy 3 Image / Facial Recognition ? OpenCL ? 3 3 \\n Emotion Classifier 3 3 GPU Image Classification $ 3 OpenGL 3 3 ? Facial Recogn . Image Recogn . Fingerprint SOINS 0010 US 10,922,957 B2 Sheet 24 of 60 Feb. 16 , 2021 U.S. Patent', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 26}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 25 of 60 US 10,922,957 B2 \\n FIG . 21 \\n 3 : \\n FIG . 22', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 27}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 26 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE \\n 1 \\n 1 \\n 1 SUBMIT TO GOOGLE ; RECEIVE AUXILIARY INFORMATION PRE - PROCESS IMAGE , \\n e.g. , IMAGE ENHANCEMENT , \\n OBJECT SEGMENTATIONI \\n EXTRACTION , etc. \\n SUBMIT TO SERVICE \\n FEATURE EXTRACTION \\n FIG . 24 \\n SEARCH FOR IMAGES WITH \\n SIMILAR METRICS CAPTURE OR RECEIVE IMAGE \\n HARVEST AND RANK FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMILAR METRICS \\n SUBMIT TO SERVICE USE SIMLAR IMAGE DATA IN \\n LEU OF USER IMAGE DATA \\n FIG , 25 FIG . 23', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 28}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 27 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEVE IMAGE CAPTURE OR RECEIVE IMAGE \\n FEATURE EXTRACTION FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS SEARCH FOR IMAGES WITH \\n SIMLAR METRICS \\n SUBMIT PURAL SETS OF \\n IMAGE DATA TO SERVICE \\n PROVIDER COMPOSITE OR MERGE \\n SEVERAL SETS OF IMAGE DATA \\n SUBMIT TO SERVICE PROVIDER \\n SERVICE PROVIDER , OR USER \\n DEVICE , DETERMINES \\n ENHANCED RESPONSE BASED \\n ON PLURAL SETS OF DATA FIG . 27 \\n CAPTURE OR RECEVE USER IMAGE FIG . 26 I \\n FEATURE EXTRACTION \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS \\n USE INFO FROM SIMILAR \\n IMAGE ( S ) TO ENHANCE USER \\n FIG , 28', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 29}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 28 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH FOR IMAGES WITH \\n SIMLAR METRICS INPUT GEOLOCATION INFO TO \\n DB ; GET TEXTUAL \\n DESCRIPTORS \\n HARVEST METADATA SEARCH IMAGE DATABASE \\n USING TEXTUAL DESCRIPTORS ; IDENTIFY OTHER IMAGE ( S ) OF SUBJECT SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n SUBMT TO SERVICE \\n SUBMIT TO SERVICE \\n FIG . 30 \\n FIG . 28A \\n CAPTURE OR RECEIVE MAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH IMAGE DATABASE \\n USING GEOLOCATION DATA ; \\n IDENTIFY OTHER IMAGE ( S ) OF \\n SUBJECT \\n SUBMIT OTHER IMAGE ( S ) TO \\n SERVICE \\n FIG . 31', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 30}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 29 of 60 US 10,922,957 B2 \\n FIG . 29 FIG . 35 \\n FIG . 36 FIG . 37 \\nA \\n FIG . 38', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 31}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 30 of 60 US 10,922,957 B2 \\n CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n SEARCH FOR IMAGES WITH \\n SIMILAR IMAGE METRICS SEARCH FOR IMAGES WITH \\n SIMILAR IMAGE METRICS \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES HARVEST METADATA \\n FIG . 32 SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES CAPTURE OR RECEIVE IMAGE \\n OF SUBJECT WITH \\n GEOLOCATION INFO \\n FIG . 33 \\n SEARCH FOR IMAGES WITH \\n MATCHING GEOLOCATION \\n SEARCH FOR IMAGES WITH \\n SIMILAR METADATA \\n ADD GEOLOCATION INFO TO \\n MATCHING IMAGES \\n FIG . 34', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 32}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 31 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n URBAN / RURAL \\n ANALYSIS FAMILYFRIENDI \\n STRANGER ANALYSIS CONSUMER \\n PRODUCTIOTHER \\n ANALYSIS \\n RESPONSE \\n ENGINE # RESPONSE \\n ENGINE # 2 RESPONSE \\n ENGINE # 3 \\n FIG . 49 CONSOLDATOR \\n CELL PHONE USER INTERFACE \\n IMAGE \\n 2 PAGE 1 3 \\n IMAGE \\n 1 ROUTER \\n PAGE 2 PAGE 3 \\n RESPONSE \\n ENGINE A RESPONSE \\n ENGINE B \\n PAGE 4 \\n FIG . 37A FIG . 37B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 33}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 32 of 60 US 10,922,957 B2 \\n USER - IMAGE ( algorithmic , crowd - sourced , etc. ) \\n USER IMAGE - RELATED FACTORS \\n ( First Tier ) e.g. , data about one or more of : Color histogram , Pattern , Shape , Texture , Facial recognition , Eigenvalue , Object recognition , Orientation , Text , Numbers , \\n OCR , Barcode , Watermark , Emotion , Location , Transform data , etc 1 \\n SEARCH ENGINE DATABASE OF PUBLC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n OTHER \\n IMAGES / CONTENT \\n ( Second Tier ) OTHER IMAGE - RELATED \\n FACTORS AND / OR \\n OTHER METADATA \\n ( Second Tier ) \\n SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n un www . 13 \\n FINAL \\n IMAGES / CONTENT FINAL OTHER \\n INFORMATION \\n USER FIG . 39', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 34}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 33 of 60 US 10,922,957 B2 \\n USER - IMAGE \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n USER IMAGE - RELATED FACTORS \\n ( First Tier ) \\n SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n OTHER \\n IMAGES / CONTENT \\n ( Second Tier ) OTHER IMAGE - RELATED \\n FACTORS AND / OR \\n OTHER METADATA \\n ( Second Tier ) \\n SEARCH ENGINE GOOGLE TEXT , \\n WEB DATABASE \\n 1 SEARCH ENGINE DATABASE OF PUBLIC IMAGES , \\n IMAGE - RELATED FACTORS , AND \\n OTHER METADATA \\n FINAL \\n IMAGES / CONTENT \\n ( Nth Tier ) INFORMATION \\n ( Nth Tier ) \\n USER FIG . 40', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 35}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 34 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n Culmage eigenvalues ; \\n Image classifier concludes \" drill \" ... \\n SEARCH ENGINE DATABASE OF PUBLIC \\n IMAGES , \\n IMAGE - RELATED FACTORS , \\n AND OTHER METADATA \\n Similar looking Text metadata for similar - looking pix \\n PROCESSOR \\n Ranked metadata , incl . \" Drill , \" \\n \" Black & Decker and \" DR250B \" \\n GOOGLE \\n SEARCH DATABASE OF \\n PUBLIC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA ONE OR MORE \\n IMAGE - BASED \\n ROUTING \\n SERVICES ( may employ user , preference data ) , \\n E.G. , SNAPNOW Pix with similar metadata BROWSER \\n Cached \\n FIG . 41 pages \\n USER INTERFACE OOOO \\n Options presented may include \" Similar looks , \" \" Similar descriptors , \" \" Web \\n results , \" \" Buy , \" \" Sell , \" \" Manual , \" \" More , \" \" SnapNow , \" etc.', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 36}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 35 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" ( algorithmic , crowd - sourced , etc. ) \\n 2.Geolocation coordinates , Data re straight edge / arced edge density ... \\n SEARCH ENGINE DATABASE OF PUBLIC \\n IMAGES , \\n IMAGE - RELATED FACTORS , \\n AND OTHER METADATA Pix that are similarly \\n located \\' , with similar edge / arc density Text metadata for pix that are similarly - located , with similar edgelarc density \\n PROCESSOR \\n Ranked metadata , incl . \" Eiffel Tower , \" \\n and \" Paris \" \\n FIG . 42 GOOGLE \\n SEARCH \\n ENGINE DATABASE OF \\n PUBLC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA ONE OR MORE \\n IMAGE - BASED \\n ROUTING \\n SERVICES ( may employ user , preference data ) , \\n E.G. , SNAPNOW Pix with similar metadata BROWSER \\n Cached \\n web \\n pages Ul options presented may include * Similar looks , \" \" Similar descriptors , \" \" Similar location , \" \" Web results , \" \" \" SnapNow , \" Map , Directions , \\n History , Search nearby , etc. USER INTERFACE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 37}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 36 of 60 US 10,922,957 B2 \\n \" IMAGE JUICER \" \\n ( algorithmic , crowd - sourced , etc. ) \\n ... Geolocation coordinates , Eigenvectors .... \\n DATABASE OF \\n PUBLIC IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA SEARCH ENGINE MLS DATABASE OF \\n IMAGES , \\n IMAGE - RELATED \\n FACTORS , AND \\n OTHER METADATA \\n Neighborhood \\n scenes , text \\n metadata from similarly - located images Address , zip code , school district , square footage , price , bedrooms / \\n baths , acreage , time on market , data / imagery re similarly - located houses , similarly - priced houses , similarly - featured houses , etc \\n GOOGLE , \\n GOOGLE EARTH \\n USER INTERFACE \\n FIG . 43', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 38}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 37 of 60 US 10,922,957 B2 \\n 0 K 999 00 O XO O \\n ???? ?? ?? wuuwl ! OUW nor * An Anarnom vu ???????? 5 \\n * \\n FIG . 44 \\n 114 \\n 1201 \\n 120a 116a \\n 122a 1165 \\n 122b 118 \\n 1160 ) a ) OOOO 2000 202 124 \\n 8 O 112 R \\n 8 \\n 8 \\n db o og oo \\n o oo oo 126a 126b', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 39}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 38 of 60 US 10,922,957 B2 \\n o \\n 130 132 \\n 128 \\n FIG . 45A \\n??????? ?? ?? \\n 142 \\n FIG . 458', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 40}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 39 of 60 US 10,922,957 B2 \\n GET GEOCOORDINATES FROM IMAGE \\n SEARCH FLICKR , ETC , FOR SIMILARLY LOCATED IMAGES ( AND / OR SIMILAR IN OTHER WAYS ) ; \" SET 1 \" IMAGES \\n HARVEST , CLEAN - UP AND CLUSTER \\n METADATA FROM SET 1 IMAGES Rockefeller Center ( 21 ) New York ( 18 ) \\n NYC ( 10 ) Manhattan ( 8 ) Empire State Building ( 4 ) \\n Midtown ( 4 ) Top of the rock ( 4 ) Nueva York ( 3 ) \\n USA ( 3 ) \\n 30 Rock ( 2 ) August ( 2 ) \\n Atlas ( 2 ) Christmas ( 2 ) Cityscape ( 2 ) \\n Fifth Avenue ( 2 ) Me ( 2 ) \\n Prometheus ( 2 ) Skating rink ( 2 ) Skyline ( 2 ) Skyscraper ( 2 ) \\n Clouds ( 1 ) General Motors Building ( 1 ) Gertrude ( 1 ) FROM METADATA , DETERMINE \\n UKELHOOD THAT EACH MAGE IS \\n PLACE - CENTRIC ( CLASS 1 ) \\n TI \\n FROM METADATA AND FACE - FINDING , \\n DETERMINE LIKELIHOOD THAT EACH \\n IMAGE IS PERSON - CENTRIC ( CLASS 2 ) \\n Lights ( 1 ) \\n Statue ( 1 ) A FROM METADATA , AND ABOVE \\n SCORES , DETERMINE LIKELIHOOD \\n THAT EACH IMAGE IS THING - CENTRIC \\n ( CLASS 3 ) \\n DETERMINE WHICH MAGE METRICS \\n RELIABLY GROUP LIKE - CLASSED \\n IMAGES TOGETHER , AND DISTINGUISH \\n DIFFERENTLY - CLASSED IMAGES \\n FIG . 46A', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 41}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 40 of 60 US 10,922,957 B2 \\n APPLY TESTS TO INPUT IMAGE : \\n DETERMINE FROM METRICS ITS \\n SCORE IN DIFFERENT CLASSES \\n PERFORM SIMILARITY TESTING \\n BETWEEN INPUT IMAGE AND \\n EACH IMAGE IN SET 1 \\n WEIGHT METADATA FROM EACH \\n IMAGE IN ACCORDANCE WITH \\n SIMILARITY SCORE : COMBINE 19 Rockefeller Center \\n 12 Prometheus 5 Skating rink \\n SEARCH IMAGE DATABASE FOR \\n ADD\\'L SIMILARLY - LOCATED \\n IMAGES , THIS TIME ALSO USING \\n INFERRED POSSIBLE METADATA \\n AS SEARCH CRITERIA : \" SET 2 \" ? C New York ( 17 ) Rockefeller Center ( 12 ) Manhattan ( 6 ) Prometheus ( 5 ) Fountain ( 4 ) Christmas ( 3 ) Paul Manship ( 3 ) Sculpture ( 3 ) Skating Rink ( 3 ) Jerry ( 1 ) \\n National register of historical places ( 2 ) \\n Nikon ( 2 ) RCA Building ( 2 ) Greek mythology ( 1 ) PERFORM SIMILARITY TESTING \\n BETWEEN INPUT IMAGE AND \\n EACH IMAGE IN SET 2 \\n IMAGE IN ACCORDANCE WITH \\n SIMILARITY SCORE ; COMBINE \\n 5 Prometheus \\n FURTHER WEIGHT METADATA IN \\n ACCORDANCE WITH \\n UNUSUALNESS 3 Paul Manship \\n 3 Rockefeller Center 3 Sculpture \\n 2 National register of historical places 2 RCA Building 1 Greek mythology FIG . 46B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 42}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 41 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n GEO INFO \\n DATABASE PROFILE DATA \\n PLACE - TERM \\n GLOSSARY GOOGLE , ETC \\n PICASA \\n DATA IMAGE \\n ANALYSIS \\n PROCESSING \\n ENGINE FLICKR \\n PERSON - NAME \\n GLOSSARY WORD \\n FREQUENCY \\n DATABASE \\n FACEBOOK \\n DATA \\n GOOGLE \\n FIG . 47 \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 43}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 42 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n PERSON / PLACE / \\n THING ANALYSIS \\n FAMILY / FRIENDI \\n STRANGER ANALYSIS CHILD / ADULT \\n ANALYSIS \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE \\n FIG . 48A \\n USER SNAPS IMAGE \\n FAMILY / FRIEND ! \\n STRANGER ANALYSIS PERSON PLACE \\n THING ANALYSIS CHILD / ADULT \\n ANALYSIS \\n RESPONSE ENGINE \\n CELL PHONE USER INTERFACE \\n FIG . 48B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 44}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 43 of 60 US 10,922,957 B2 \\n USER SNAPS IMAGE \\n FACIAL \\n RECOGNITION TRANSFORM WAVELET \\n TRANSFORM TRANSFORM \\n OCR EDGE \\n DETECTION EIGENVALUE \\n ANALYSIS SPECTRUM \\n ANALYSIS \\n PATTERN \\n EXTRACTION FOURIER - MELLIN \\n TRANSFORM TEXTURE COLOR \\n CLASSFER ANALYSIS \\n GIST \\n PROCESSING EMOTION \\n CLASSIFIER BARCODE \\n DECODER AGE \\n DETECTION DATABASES , EXTERNAL RESOURCES \\n WATERMARK | ORIENTATION \\n DECODER DETECTION GENDER \\n DETECTION PROCESSOR \\n OBJECT \\n SEGMENTATION METADATA \\n ANALYSIS GEOLOCATION \\n PROCESSING \\n SERVICE \\n PROVIDER 2 SERVICE \\n PROVIDERN PROVIDER 1 \\n CELL PHONE USER INTERFACE \\n FIG . 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 45}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 44 of 60 US 10,922,957 B2 \\n INPUT IMAGE , OR \\n INFORMATION ABOUT \\n THE MAGE \\n THING PLACE PERSON \\nSTRANGE FRIEND --FAMILY --- --- RECREATION -HOME \\n OTHER 000 : BOAT -TRUCK mommm CAR SHIRT -OUTERWEAR ( a ) Post annotated photo to user\\'s Facebook page ( b ) Start a text message to depicted person \\n 2 \\n ? \\n $ \\n ( a ) Show estimations of who this may be ( b ) Send \" Friend \" invite on MySpace ( c ) Who are the degrees of separation between us \\n ( a ) Buy online \\n ( b ) Show nutrition information ( c ) Identify local stores that sell \\n FIG , 51', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 46}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 45 of 60 US 10,922,957 B2 \\n COMPUTER A \\n CONTENT SET A ROUTER \\n ROUTERA COMPUTER B \\n RESPONSE \\n ENGINE 1A RESPONSE \\n ENGINE 2A CONTENT SET B \\n COMPUTER C RESPONSE \\n CONTENT SET C \\n ENGINE C \\n CONTENT SET D \\n COMPUTERE ROUTER D \\n CONTENT SETE \\n 150 \\n RESPONSE \\n ENGINE \\n FIG . 52', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 47}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 46 of 60 US 10,922,957 B2 \\n FIG . 53 \\n BABYCAM FEATURE \\n VECTORS ( URL , etc. ) \\n 1E 5G 18D 23 8F BABYCAM WWW.SMITH.HOME . \\n COM / BAGYCAM.HTM \\n 52 88 26 A279 TRAFFIC MAP WWW.TRAFFIC.COM \\n SEATTLE \\n TRAFFIC MAP.HTML Ý \\n o \\n FIG . 55 \\n SIGN GLOSSARY FIG . 54', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 48}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 47 of 60 US 10,922,957 B2 \\n IMAGE SENSORS \\n AMPLIFY ANALOG SIGNALS \\n CONVERT TO DIGITAL NATIVE IMAGE DATA \\n BAYER INTERPOLATION \\n WHITE BALANCE CORRECTION \\n I T \\n GAMMA CORRECTION \\n T \\n EDGE ENHANCEMENT \\n JPEG COMPRESSION \\n STORE IN BUFFER MEMORY \\n DISPLAY CAPTURED IMAGE ON SCREEN \\n ON USER COMMAND , STORE IN CARD AND / OR \\n TRANSMIT \\n FIG . 56 \\n ( Prior Art )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 49}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 48 of 60 US 10,922,957 B2 \\n IMAGE SENSORS \\n AMPLIFY ANALOG \\n SIGNALS FIG . 57 \\n CONVERT TO DIGITAL \\n NATIVE IMAGE DATA \\n BAYER \\n INTERPOLATION FOURIER \\n TRANSFORM EIGENFACE \\n CALCULATION ETC \\n In and \\n Etc ( as in Fig . 56 ) \\n MATCH \\n BARCODE \\n TEMPLATE ? MATCH FACE \\n TEMPLATE ? Yes \\n TRANSFORM SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR \\n FOURIER DOMAIN DATA \\n TO BARCODE SERVICE \\nHOATA ANDIOREM MATCH TEXT \\n TEMPLATE ? SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR F - M \\n DOMAIN DATA TO OCR Yes SERVICE \\n SEND DIGITAL NATIVE \\n IMAGE DATA AND / OR E - M \\n DOMAIN DATA TO WM \\n SERVICE MATCH WM \\n ORIENTATION \\n Yes \\n SEND DIGITAL NATIVE MATCH FACE \\n TEMPLATE ? Yes \\n DOMAIN DATA AND / OR \\n EIGENFACE DATA TO', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 50}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 49 of 60 US 10,922,957 B2 \\n CURRENT ROTATION \\n FROM APPARENT 12 degrees right \\n HORIZON : \\n FIG . 58 \\n 17 degrees left SINCE \\n START \\n # 1733 WU *** 31 U 11:11 HD SCALE \\n CHANGE SINCE 11 1 30 % closer -- > \\n IMAGE CAPTURE \\n PROCESSING \\n ANALYSIS , MEMORY \\n FIG . 59 DEPENDENT ON \\n IMAGE DATA', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 51}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 50 of 60 US 10,922,957 B2 \\nE WELCOME ” To Sauerlana \\n FIG . 60 \\n TI \\n FIG , 65 FIG . 66 \\nWELCOS WELCOME ro LAS VEGAS LAS VEGAS romantic LAS VEGAS \\n 1 \\n FIG . 62 FIG . 63 FIG . 64', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 52}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 51 of 60 US 10,922,957 B2 \\n FOURIER TRANSFORM \\n DATA \\n BARCODE \\n FOURIER - MELLIN \\n TRANSFORM DATA \\n OCR INFORMATION \\n WATERMARK \\n INFORMATION \\n DCT DATA \\n MOTION INFORMATION \\n METADATA \\n EIGENFACE DATA \\n FACE INFORMATION \\n SOBEL FILTER DATA \\n TRANSFORM DATA \\n COLOR HISTOGRAM \\n DATA \\n FIG . 61', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 53}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 52 of 60 US 10,922,957 B2 \\n . \\n 1.5 \\n ? XGES \\n GMC \\n ? ? ? ? ” ? \\n ? , ** * \\n € ? ? ? ? \\n 22 \\n · ### ? ?? ? , ###### . ### * ? ? # ? ? # , #### \\n $ 3 9 \\n 2 ## 2 # V > \\n 2 + \\n 2 . ? ? ? ? ? 4 · \\n 1 ? ? ### ?? \\n 3 * O ### - # ?? ?? rt ? ? # ?? ? ? : 4 # #### ?? ? ?? , \\n ??? ? ? ?? 4 : #### ? ? ?? ! \\n # ?? FE ? , ? ? ? ?? ? ? , # ? , \\n ? ? * ### ## : * 4 \\n y . 3 ? 13 ? * y \\n ? Ir X 1', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 54}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 53 of 60 US 10,922,957 B2 \\n R G B \\n FIG . 69 \\n ? \\n FIG . 70 FIG.71', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 55}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 54 of 60 US 10,922,957 B2 \\n Spatial Frequency ( Cycles / Degree ) \\n 1 10 0.1 100 \\n 1,000 \\n Invisible \\n Threshold Contrast Sensitivity \\n ( 1 / Threshold Contrast ) Visible \\n 1 \\n 0.1 FIG . 72 Spatial Frequency ( Cycles / MM On Retina ) \\n 103 , \\n FIG . 73', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 56}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 55 of 60 US 10,922,957 B2 \\n THE WALL STREET JOURNAL : \\n * .. * ce V. \\n . 2 \\n th + -1 *** \\n 2 X \\n N \" \\n KXX \\n 7 7 S. 1 \\n 1 \\n TX \\n HY 19 ? \\n 2 2 . 2 \\n 2 \\n 7 \\n 11 , \\n *** \\n * I \\n davca \\n ty \\n WY \\n 14 \\n N \\n . \\n FIG . 74 FIG . 75 \\n Watermark Origin Captured Image \\n Frame ? HE WALL STREET JOURNAL Offset of \\n { Decoded \\n Watermark \\n FIG . 76', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 57}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 56 of 60 US 10,922,957 B2 \\n START START \\n ROUGH \\n OUTLINES TRUCK \\n LARGE \\n DETAILS PERSON \\n INTERMEDIATE \\n DETAILS OCR FONT TEXT \\n FACIAL \\n EIGENVALUES \\n R , G , B Data Alpha Channel Data \\n FIG . 77A FIG . 77B', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 58}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 57 of 60 US 10,922,957 B2 \\n THERMOSTAT \\n Indicators 526 \\n 522 transceiver \\n LCD Display 514 \\n 520 Temperature \\n Sensor \\n Buttons \\n 518 MEMORY : \\n Op . sys . Processor 524 516 FIG . 78 ( Prior \\n Art ) -512 \\n 518 \\n 522 75.5 OG \\n 520 \\n FIG . 79 ( Prior \\n Art )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 59}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 58 of 60 US 10,922,957 B2 \\n THERMOSTAT \\n 526 \\n transceiver \\n Indicators 514 \\n 522 h Temperature \\n Sensor } 1 T. 528 Location \\n ( GPS ) \\n 596 MEMORY : Op . sys . Ap SW \\n Data Processor \\n 532 34 \\n FIG . 80 \\n 536 \\n 530 \\n 530 552 \\n CELL PHONE mwy ramena SERVER FIG . 82 \\n olarak 1 555 554 \\n ROUTER NAMESPACE \\n DATABASE \\n 556a 556b -530 \\n ? ?? ? \\n SERVER mwingine hanno THERMOSTAT \\n ( OR OTHER DEVICE )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 60}), Document(page_content='U.S. Patent Feb. 16 , 2021 Sheet 59 of 60 US 10,922,957 B2 \\n CELL PHONE V \\n RF transceiver \\n Microphone \\n Network adapter \\n Camera \\n 544 Location \\n ( e.g. , GPS ) \\n Processor 596 \\n 542 \\n Display MEMORY : \\n Op . sys . \\n Touchscreen SW Modules \\n Etc. 546 \\n Physical UI \\n -540 \\n FIG . 81 \\n ALARM CLOCK 580 \\n 588 \\n Display Physical UI Bluetooth 582 \\n 584 \\n Processor Memory \\n 588 590 \\n 1 Location 592 598 FIG . 85', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 61}), Document(page_content=\"U.S. Patent Feb. 16 , 2021 Sheet 60 of 60 US 10,922,957 B2 \\n SETPOINT TEMP : 72.0 \\n CURRENITEMP : 70.7 SETPPOINT TEMP : 72.0 \\n CURRENTIEMP : 70.7 \\n 564 \\n 2 \\n ? \\n 562 562 JG MG 8 566 \\n 560 5602 \\n FIG . 83 FIG . 84 \\n 595 598 \\n PRESS DIGIT KEYS TO \\n SET ALARM TIME TO : NEARBY DEVICES : \\n 05 : 3 0 1. THERMOSTAT ( 4 ' away ) \\n ( Outside Conf Rm 1500 ) \\n 2. LaserJet Printer ( 15 * away ) \\n ( Cubicle 1250 ) a.m. \\n THEN PRESS ' OK \\n BUTTON TO SET ALARM PRESS DIGIT , THEN ' OK ' BUTTON TO SELECT \\n FIG . 86 FIG . 87\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 62}), Document(page_content='15 \\n 20 \\n 30 US 10,922,957 B2 \\n 1 2 \\n METHODS AND SYSTEMS FOR CONTENT In early roll - out , the class of recognizable objects will be \\n PROCESSING limited but useful . Object identification events will primarily fetch and associate public domain information and social RELATED APPLICATION DATA web connections to the baubles . Applications employing 5 barcodes , digital watermarks , facial recognition , OCR , etc. , This application is a division of application Ser . No. can help support initial deployment of the technology . 14 / 456,784 , filed Aug. 11 , 2014 ( now U.S. Pat . No. 9,886 , Later , the arrangement is expected to evolve into an 845 ) , which is a division of application Ser . No. 13 / 011,618 , auction market , in which paying enterprises want to place filed Jan. 21 , 2011 ( now U.S. Pat . No. 8,805,110 ) , which is their own baubles ( or associated information ) onto highly a continuation of co - pending PCT application PCT / US09 / 10 targeted demographic user screens . User profiles , in con 54358 , filed Aug. 19 , 2009 ( published as WO2010022185 ) . junction with the input visual stimuli ( aided , in some cases Application PCT / US09 / 54358 claims priority benefit to by GPS / magnetometer data ) , is fed into a Google - esque each of the following provisional applications : mix - master in the cloud , matching buyers of mobile device 61 / 090,083 , filed 19 Aug. 2008 ; screen real estate to users requesting the baubles . 61 / 096,703 , filed 12 Sep. 2008 ; Eventually , such functionality may become so ubiquitous 61 / 100,643 , filed 26 Sep. 2008 ; as to enter into the common lexicon , as in “ I\\'ll try to get a 61 / 103,907 , filed 8 Oct. 2008 ; Bauble on that ” or “ See what happens if you Viewgle that 61 / 110,490 , filed 31 Oct. 2008 ; scene . \" 61 / 169,266 , filed 14 Apr. 2009 ; 61 / 174,822 , filed 1 May 2009 ; BACKGROUND 61 / 176,739 , filed 8 May 2009 ; 61 / 226,195 , filed 16 Jul . 2009 ; and Digimarc\\'s U.S. Pat . No. 6,947,571 shows a system in 61 / 234,542 , filed 17 Aug. 2009 . which a cell phone camera captures content ( e.g. , image Application PCT / US09 / 54358 also claims priority to , and data ) , and processes same to derive an identifier related to may be regarded as a continuation - in - part of , each of the 25 the imagery . This derived identifier is submitted to a data following applications : structure ( e.g. , a database ) , which indicates corresponding Ser . No. 12 / 271,692 , filed 14 Nov. 2008 ( now U.S. Pat . data or actions . The cell phone then displays responsive No. 8,520,979 ) ; information , or takes responsive action . Such sequence of Ser . No. 12 / 484,115 , filed 12 Jun . 2009 ( now U.S. Pat . No. operations is sometimes referred to as \" visual search . ” 8,385,971 ) ; and Related technologies are shown in patent publications Ser . No. 12 / 498,709 , filed 7 Jul . 2009 ( published as 20080300011 ( Digimarc ) , U.S. Pat . No. 7,283,983 and 20100261465 ) WO07 / 130688 ( Evolution Robotics ) , 20070175998 and Priority is claimed to each of the foregoing applications . The 20020102966 ( DSPV ) , 20060012677 , 20060240862 and foregoing applications are incorporated herein by reference , 20050185060 ( Google ) , 20060056707 and 20050227674', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 63}), Document(page_content=\"in their entireties . 35 ( Nokia ) , 20060026140 ( ExBiblio ) , U.S. Pat . No. 6,491,217 , 20020152388 , 20020178410 and 20050144455 ( Philips ) , INTRODUCTION 20020072982 and 20040199387 ( Shazam ) , 20030083098 ( Canon ) , 20010055391 ( Qualcomm ) , 20010001854 ( Air Certain aspects of the technology detailed herein are Clic ) , U.S. Pat . No. 7,251,475 ( Sony ) , U.S. Pat . No. 7,174 , introduced in FIG . 1. A user's mobile phone captures 40 293 ( Iceberg ) , U.S. Pat . No. 7,065,559 ( Organnon Wireless ) , imagery ( either in response to user command , or autono- U.S. Pat . No. 7,016,532 ( Evryx Technologies ) , U.S. Pat . mously ) , and objects within the scene are recognized . Infor- Nos . 6,993,573 and 6,199,048 ( Neomedia ) , U.S. Pat . No. mation associated with each object is identified , and made 6,941,275 ( Tune Hunter ) , U.S. Pat . No. 6,788,293 ( Silver available to the user through a scene - registered interactive brook Research ) , U.S. Pat . Nos . 6,766,363 and 6,675,165 visual “ bauble ” that is graphically overlaid on the imagery . 45 ( BarPoint ) , U.S. Pat . No. 6,389,055 ( Alcatel - Lucent ) , U.S. The bauble may itself present information , or may simply be Pat . No. 6,121,530 ( Sonoda ) , and U.S. Pat . No. 6,002,946 an indicia that the user can tap at the indicated location to ( Reber / Motorola ) . obtain a lengthier listing of related information , or launch a Aspects of the presently - detailed technology concern related function / application . improvements to such technologies — moving towards the In the illustrated scene , the camera has recognized the 50 goal of intuitive computing : devices that can see and / or hear , face in the foreground as “ Bob ” and annotated the image and infer the user's desire in that sensed context . accordingly . A billboard promoting the Godzilla movie has been recognized , and a bauble saying “ Show Times ” has BRIEF DESCRIPTION OF THE DRAWINGS been blitted onto the display — inviting the user to tap for screening information . FIG . 1 is a diagram showing an exemplary embodiment The phone has recognized the user's car from the scene , incorporating certain aspects of the technology detailed and has also identified by make and year another vehicle herein . in the picture . Both are noted by overlaid text . A restaurant FIG . 1A is a high level view of an embodiment incorpo has also been identified , and an initial review from a rating aspects of the present technology . collection of reviews ( “ Jane's review : Pretty Good ! ” ) is 60 FIG . 2 shows some of the applications that a user may shown . Tapping brings up more reviews . request a camera - equipped cell phone to perform . In one particular arrangement , this scenario is imple- FIG . 3 identifies some of the commercial entities in an mented as a cloud - side service assisted by local device embodiment incorporating aspects of the present technol object recognition core services . Users may leave notes on ogy . both fixed and mobile objects . Tapped baubles can trigger 65 FIGS . 4 , 4A and 4B conceptually illustrate how pixel data , other applications . Social networks can keep track of objec- and derivatives , are applied in different tasks , and packaged tion relationships — forming a virtual “ web of objects . ” into packet form . 55\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 63}), Document(page_content='10 US 10,922,957 B2 \\n 3 4 \\n FIG . 5 shows how different tasks may have certain image FIG . 29 is an arty shot of the Eiffel Tower , captured by a processing operations in common . cell phone user . FIG . 6 is a diagram illustrating how common image FIG . 35 is another image captured by a cell phone user . processing operations can be identified , and used to config- FIG . 36 is an image of an underside of a telephone , ure cell phone processing hardware to perform these opera- 5 discovered using methods according to aspects of the pres tions . ent technology FIG . 7 is a diagram showing how a cell phone can send FIG . 37 shows part of the physical user interface of one certain pixel - related data across an internal bus for local style of cell phone . processing , and send other pixel - related data across a com FIGS . 37A and 37B illustrate different linking topologies . munications channel for processing in the cloud . FIG . 38 is an image captured by a cell phone user , FIG . 8 shows how the cloud processing in FIG . 7 allows tremendously more “ intelligence ” to be applied to a task depicting an Appalachian Trail trail marker . FIGS . 39-43 detail methods incorporating aspects of the desired by a user . FIG . 9 details how keyvector data is distributed to dif present technology . \\n ferent external service providers , who perform services in 15 FIG . 44 shows the user interface of one style of cell \\n exchange for compensation , which is handled in consoli phone . \\n dated fashion for the user . FIGS . 45A and 45B illustrate how different dimensions of \\n FIG . 10 shows an embodiment incorporating aspects of commonality may be explored through use of a user inter \\n the present technology , noting how cell phone - based pro face control of a cell phone . cessing is suited for simple object identification tasks — such 20 FIGS . 46A and 46B detail a particular method incorpo as template matching , whereas cloud - based processing is rating aspects of the present technology , by which keywords suited for complex tasks — such as data association . such as Prometheus and Paul Manship are automatically FIG . 10A shows an embodiment incorporating aspects of determined from a cell phone image . the present technology , noting that the user experience is FIG . 47 shows some of the different data sources that may optimized by performing visual keyvector processing as 25 be consulted in processing imagery according to aspects of close to a sensor as possible , and administering traffic to the the present technology . cloud as low in a communications stack as possible . FIGS . 48A , 48B and 49 show different processing meth FIG . 11 illustrates that tasks referred for external process- ods according to aspects of the present technology . ing can be routed to a first group of service providers who FIG . 50 identifies some of the different processing that routinely perform certain tasks for the cell phone , or can be 30 may be performed on image data , in accordance with aspects routed to a second group of service providers who compete of the present technology on a dynamic basis for processing tasks from the cell phone . FIG . 51 shows an illustrative tree structure that can be FIG . 12 further expands on concepts of FIG . 11 , e.g. , employed in accordance with certain aspects of the present showing how a bid filter and broadcast agent software technology module may oversee a reverse auction process . FIG . 52 shows a network of wearable computers ( e.g. , cell FIG . 13 is a high level block diagram of a processing phones ) that can cooperate with each other , e.g. , in a arrangement incorporating aspects of the present technol- peer - to - peer network .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 64}), Document(page_content='ogy . FIGS . 53-55 detail how a glossary of signs can be FIG . 14 is a high level block diagram of another process- identified by a cell phone , and used to trigger different ing arrangement incorporating aspects of the present tech- 40 actions . nology FIG . 56 illustrates aspects of prior art digital camera FIG . 15 shows an illustrative range of image types that technology may be captured by a cell phone camera . FIG . 57 details an embodiment incorporating aspects of FIG . 16 shows a particular hardware implementation the present technology . incorporating aspects of the present technology . FIG . 58 shows how a cell phone can be used to sense and FIG . 17 illustrates aspects of a packet used in an exem- display affine parameters . plary embodiment . FIG . 59 illustrates certain state machine aspects of the FIG . 18 is a block diagram illustrating an implementation present technology of the SIFT technique . FIG . 60 illustrates how even “ still ” imagery can include FIG . 19 is a block diagram illustrating , e.g. , how packet 50 temporal , or motion , aspects . header data can be changed during processing , through use FIG . 61 shows some metadata that may be involved in an of a memory implementation incorporating aspects of the present tech \\n FIG . 19A shows a prior art architecture from the robotic nology . Player Project . FIG . 62 shows an image that may be captured by a cell FIG . 19B shows how various factors can influence how 55 phone camera user . different operations may be handled . FIGS . 63-66 detail how the image of FIG . 62 can be FIG . 20 shows an arrangement by which a cell phone processed to convey semantic metadata . camera and a cell phone projector share a lens . FIG . 67 shows another image that may be captured by a FIG . 20A shows a reference platform architecture that can cell phone camera user . be used in embodiments of the present technology . FIGS . 68 and 69 detail how the image of FIG . 67 can be FIG . 21 shows an image of a desktop telephone captured processed to convey semantic metadata . by a cell phone camera . FIG . 70 shows an image that may be captured by a cell FIG . 22 shows a collection of similar images found in a phone camera user . repository of public images , by reference to characteristics FIG . 71 details how the image of FIG . 70 can be pro discerned from the image of FIG . 21 . 65 cessed to convey semantic metadata . FIGS . 23-28A , and 30-34 are flow diagrams detailing FIG . 72 is a chart showing aspects of the human visual methods incorporating aspects of the present technology . system . 35 \\n 45 \\n 60', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 64}), Document(page_content='5 \\n 10 \\n every second . US 10,922,957 B2 \\n 5 6 \\n FIG . 73 shows different low , mid and high frequency mobile device users and meet most qualitative “ human real components of an image . time interactivity \" requirements , generally with feedback in \\n FIG . 74 shows a newspaper page . much less than one second . Implementation desirably pro FIG . 75 shows the layout of the FIG . 74 page , as set by vides certain basic features on the mobile device , including layout software . a rather intimate relationship between the image sensor\\'s FIG . 76 details how user interaction with imagery cap- output pixels and the native communications channel avail tured from printed text may be enhanced . able to the mobile device . Certain levels of basic \" content FIGS . 77A and 77B illustrate how semantic conveyance filtering and classification ” of the pixel data on the local of metadata can have a progressive aspect , akin to device , followed by attaching routing instructions to the JPEG2000 and the like . pixel data as specified by the user\\'s intentions and subscrip FIG . 78 is a block diagram of a prior art thermostat . tions , leads to an interactive session between a mobile FIG . 79 is an exterior view of the thermostat of FIG . 78 . device and one or more “ cloud based ” pixel processing FIG . 80 is a block diagram of a thermostat employing services . The key word “ session ” further indicates fast certain aspects of the present technology ( “ ThingPipe ” ) . FIG . 81 is a block diagram of a cell phone embodying 15 responses transmitted back to the mobile device , where for certain aspects of the present technology . some services marketed as “ real time ” or “ interactive , \" a \\n FIG . 82 is a block diagram by which certain operations of session essentially represents a duplex , generally packet \\n the thermostat of FIG . 80 are explained . based , communication , where several outgoing “ pixel pack \\n FIG . 83 shows a cell phone display depicting an image ets ” and several incoming response packets ( which may be captured from a thermostat , onto which is overlaid certain 20 pixel packets updated with the processed data ) may occur \\n touch - screen targets that the user can touch to increment or decrement the thermostat temperature . Business factors and good old competition are at the heart FIG . 84 is similar to FIG . 83 , but shows a graphical user of the distributed network . Users can subscribe to or other interface for use on a phone without a touch - screen . wise tap into any external services they choose . The local FIG . 85 is a block diagram of an alarm clock employing 25 device itself and / or the carrier service provider to that device aspects of the present technology . can be configured as the user chooses , routing filtered and FIG . 86 shows a screen of an alarm clock user interface pertinent pixel data to specified object interaction services . that may be presented on a cell phone , in accordance with Billing mechanisms for such services can directly plug into one aspect of the technology . existing cell and / or mobile device billing networks , wherein FIG . 87 shows a screen of a user interface , detailing 30 users get billed and service providers get paid . nearby devices that may be controlled through use of the cell But let\\'s back up a bit . The addition of camera systems to phone . mobile devices has ignited an explosion of applications . The primordial application certainly must be folks simply snap', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 65}), Document(page_content='DETAILED DESCRIPTION ping quick visual aspects of their environment and sharing 35 such pictures with friends and family . The present specification details a diversity of technolo- The fanning out of applications from that starting point gies , assembled over an extended period of time , to serve a arguably hinges on a set of core plumbing features inherent variety of different objectives . Yet they relate together in in mobile cameras . In short ( and non - exhaustive of course ) , various ways , and so are presented collectively in this single such features include : a ) higher quality pixel capture and \\n document . 40 low level processing ; b ) better local device CPU and GPU This varied , interrelated subject matter does not lend itself resources for on - device pixel processing with subsequent to a straightforward presentation . Thus , the reader\\'s indul- user feedback ; c ) structured connectivity into “ the cloud ; \" gence is solicited as this narrative occasionally proceeds in and importantly , d ) a maturing traffic monitoring and billing nonlinear fashion among the assorted topics and technolo- infrastructure . FIG . 1A is but one graphic perspective on gies . 45 some of these plumbing features of what might be called a Each portion of this specification details technology that visually intelligent network . ( Conventional details of a cell desirably incorporates technical features detailed in other phone , such as the microphone , A / D converter , modulation portions . Thus , it is difficult to identify “ a beginning ” from and demodulation systems , IF stages , cellular transceiver , which this disclosure should logically begin . That said , we etc. , are not shown for clarity of illustration . ) simply dive in . It is all well and good to get better CPUs and GPUs , and Mobile Device Object Recognition and Interaction Using more memory , on mobile devices . However , cost , weight Distributed Network Services and power considerations seem to favor getting “ the cloud ” There is presently a huge disconnect between the unfath- to do as much of the “ intelligence ” heavy lifting as possible . omable volume of information that is contained in high Relatedly , it seems that there should be a common quality image data streaming from a mobile device camera 55 denominator set of \" device - side ” operations performed on ( e.g. , in a cell phone ) , and the ability of that mobile device visual data that will serve all cloud processes , including to process this data to whatever end . “ Off device ” processing certain formatting , elemental graphic processing , and other of visual data can help handle this fire hose of data , rote operations . Similarly , it seems there should be a stan especially when a multitude of visual processing tasks may dardized basic header and addressing scheme for the result be desired . These issues become even more critical once 60 ing communication traffic ( typically packetized ) back and “ real time object recognition and interaction ” is contem- forth with the cloud . plated , where a user of the mobile device expects virtually This conceptualization is akin to the human visual system . instantaneous results and augmented reality graphic feed- The eye performs baseline operations , such as chromaticity back on the mobile device screen , as that user points the groupings , and it optimizes necessary information for trans camera at a scene or object . 65 mission along the optic nerve to the brain . The brain does the In accordance with one aspect of the present technology , real cognitive work . And there\\'s feedback the other way a distributed network of pixel processing engines serve such too with the brain sending information controlling muscle 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 65}), Document(page_content='10 US 10,922,957 B2 \\n 7 8 \\n movement where to point eyes , scanning lines of a book , FIGS . 4A and 4B also introduce a central player in this controlling the iris ( lighting ) , etc. disclosure : the packaged and address - labeled pixel packet , FIG . 2 depicts a non - exhaustive but illustrative list of into a body of which keyvector data is inserted . The keyvec visual processing applications for mobile devices . Again , it tor data may be a single patch , or a collection of patches , or is hard not to see analogies between this list and the 5 a time - series of patches / collections . A pixel packet may be \\n fundamentals of how the human visual system and the less than a kilobyte , or its size can be much much larger . It \\n human brain operate . It is a well studied academic area that may convey information about an isolated patch of pixels \\n deals with how “ optimized ” the human visual system is excerpted from a larger image , or it may convey a massive \\n relative to any given object recognition task , where a general Photosynth of Notre Dame cathedral . \\n consensus is that the eye - retina - optic nerve - cortex system is ( As presently conceived , a pixel packet is an application', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='pretty darn wonderful in how efficiently it serves a vast array layer construct . When actually pushed around a network , however , it may be broken into smaller portions as trans of cognitive demands . This aspect of the technology relates port layer constraints in a network may require . ) to how similarly efficient and broadly enabling elements can FIG . 5 is a segue diagramstill at an abstract level , but be built into mobile devices , mobile device connections and 15 pointing toward the concrete . A list of user - defined applica network services , all with the goal of serving the applica tions , such as illustrated in FIG . 2 , will map to a state - of tions depicted in FIG . 2 , as well as those new applications the - art inventory of pixel processing methods and which may show up as the technology dance continues . approaches which can accomplish each and every applica Perhaps the central difference between the human analogy tion . These pixel processing methods break down into and mobile device networks must surely revolve around the 20 common and not - so - common component sub - tasks . Object basic concept of “ the marketplace , ” where buyers buy better recognition textbooks are filled with a wide variety of and better things so long as businesses know how to profit approaches and terminologies which bring a sense of order accordingly . Any technology which aims to serve the appli- into what at first glance might appear to be a bewildering cations listed in FIG . 2 must necessarily assume that hun- array of “ unique requirements ” relative to the applications dreds if not thousands of business entities will be developing 25 shown in FIG . 2. ( In addition , multiple computer vision and the nitty gritty details of specific commercial offerings , with image processing libraries , such as OpenCV and CMVI the expectation of one way or another profiting from those sion discussed below , have been created that identify and offerings . Yes , a few behemoths will dominate main lines of render functional operations , which can be considered cash flows in the overall mobile industry , but an equal \" atomic ” functions within object recognition paradigms . ) certainty will be that niche players will be continually 30 But FIG . 5 attempts to show that there are indeed a set of developing niche applications and services . Thus , this dis- common steps and processes shared between visual process closure describes how a marketplace for visual processing ing applications . The differently shaded pie slices attempt to services can develop , whereby business interests across the illustrate that certain pixel operations are of a specific class spectrum have something to gain . FIG . 3 attempts a crude and may simply have differences in low level variables or categorization of some of the business interests applicable to 35 optimizations . The size of the overall pie ( thought of in a the global business ecosystem operative in the era of this logarithmic sense , where a pie twice the size of another may filing represent 10 times more Flops , for example ) , and the per FIG . 4 sprints toward the abstract in the introduction of centage size of the slice , represent degrees of commonality . the technology aspect now being considered . Here we find FIG . 6 takes a major step toward the concrete , sacrificing a highly abstracted bit of information derived from some 40 simplicity in the process . Here we see a top portion labeled batch of photons that impinged on some form of electronic “ Resident Call - Up Visual Processing Services , \" which rep image sensor , with a universe of waiting consumers of that resents all of the possible list of applications from FIG . 2 that lowly bit . FIG . 4A then quickly introduces the intuitively a given mobile device may be aware of , or downright well - known concept that singular bits of visual information enabled to perform . The idea is that not all of these appli aren\\'t worth much outside of', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content=\"mobile device may be aware of , or downright well - known concept that singular bits of visual information enabled to perform . The idea is that not all of these appli aren't worth much outside of their role in both spatial and 45 cations have to be active all of the time , and hence some temporal groupings . This core concept is well exploited in sub - set of services is actually “ turned on ” at any given modern video compression standards such as MPEG ? and moment . The turned on applications , as a one - time configu\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='H.264 . ration activity , negotiate to identify their common compo The “ visual ” character of the bits may be pretty far nent tasks , labeled the “ Common Processes Sorter ” —first removed from the visual domain by certain of the processing 50 generating an overall common list of pixel processing rou ( consider , e.g. , the vector strings representing eigenface tines available for on - device processing , chosen from a data ) . Thus , we sometimes use the term “ keyvector data ” ( or library of these elemental image processing routines ( e.g. , “ keyvector strings ” ) to refer collectively to raw sensor / FFT , filtering , edge detection , resampling , color histogram stimulus data ( e.g. , pixel data ) , and / or to processed infor- ming , log - polar transform , etc. ) . Generation of correspond mation and associated derivatives . A keyvector may take the 55 ing Flow Gate Configuration / Software Programming infor form of a container in which such information is conveyed mation follows , which literally loads library elements into ( e.g. , a data structure such as a packet ) . A tag or other data properly ordered places in a field programmable gate array can be included to identify the type of information ( e.g. , set - up , or otherwise configures a suitable processor to per JPEG image data , eigenface data ) , or the data type may be form the required component tasks . otherwise evident from the data or from context . One or 60 FIG . 6 also includes depictions of the image sensor , more instructions , or operations , may be associated with followed by a universal pixel segmenter . This pixel seg keyvector data either expressly detailed in the keyvector , menter breaks down the massive stream of imagery from the or implied . An operation may be implied in default fashion , sensor into manageable spatial and / or temporal blobs ( e.g. , for keyvector data of certain types ( e.g. , for JPEG data it akin to MPEG macroblocks , wavelet transform blocks , may be “ store the image ; \" for eigenface data is may be 65 64x64 pixel blocks , etc. ) . After the torrent of pixels has been \" match this eigenface template \" ) . Or an implied operation broken down into chewable chunks , they are fed into the may be dependent on context . newly programmed gate array ( or other hardware ) , which', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 66}), Document(page_content='US 10,922,957 B2 \\n 9 10', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='performs the elemental image processing tasks associated template - like approach to object recognition , where for with the selected applications . ( Such arrangements are fur- contained applications involving such objects , local process ther detailed below , in an exemplary system employing ing services can largely get the job done . But even in the \" pixel packets . \" ) Various output products are sent to a barcode example , flexibility in the growth and evolution of routing engine , which refers the elementally - processed data 5 overt visual coding targets begs for an architecture which ( e.g. , keyvector data ) to other resources ( internal and / or doesn\\'t force “ code upgrades ” to a gazillion devices every external ) for further processing . This further processing time there is some advance in the overt symbology art . typically is more complex that that already performed . At the other end of the spectrum , arbitrarily complex tasks Examples include making associations , deriving inferences , can be imagined , e.g. , referring to a network of supercom pattern and template matching , etc. This further processing 10 puters the task of predicting the apocryphal typhoon result can be highly application - specific . ing from the fluttering of a butterfly\\'s wings halfway around ( Consider a promotional game from Pepsi , inviting the the world if the application requires it . Oz beckons . public to participate in a treasure hunt in a state park . Based FIG . 8 attempts to illustrate this radical extra dimension on internet - distributed clues , people try to find a hidden ality of pixel processing in the cloud as opposed to the local six - pack of soda to earn a $ 500 prize . Participants must 15 device . This virtually goes without saying ( or without a download a special application from the Pepsi - dot - com web picture ) , but FIG . 8 is also a segue figure to FIG . 9 , where site ( or the Apple AppStore ) , which serves to distribute the Dorothy gets back to Kansas and is happy about it . clues ( which may also be published to Twitter ) . The down- FIG . 9 is all about cash , cash flow , and happy humans loaded application also has a prize verification component , using cameras on their mobile devices and getting highly which processes image data captured by the users \\' cell 20 meaningful results back from their visual queries , all the phones to identify a special pattern with which the hidden while paying one monthly bill . It turns out the Google six - pack is uniquely marked . SIFT object recognition is used “ AdWords ” auction genie is out of the bottle . Behind the ( discussed below ) , with the SIFT feature descriptors for the scenes of the moment - by - moment visual scans from a special package conveyed with the downloaded application . mobile user of their immediate visual environment are When an image match is found , the cell phone immediately 25 hundreds and thousands of micro - decisions , pixel routings , reports same wirelessly to Pepsi . The winner is the user results comparisons and micro - auctioned channels back to whose cell phone first reports detection of the specially- the mobile device user for the hard good they are “ truly ” marked six - pack . In the FIG . 6 arrangement , some of the looking for , whether they know it or not . This last point is component tasks in the SIFT pattern matching operation are deliberately cheeky , in that searching of any kind is inher performed by the elemental image processing in the config- 30 ently open ended and magical at some level , and part of the ured hardware ; others are referred for more specialized fun of searching in the first place is that surprisingly new processing either internal or external . ) associations are part of the results . The search user knows FIG . 7 up - levels the picture to a generic distributed pixel after the fact what they were truly looking for . The system , services network view , where local device pixel services and', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='user knows FIG . 7 up - levels the picture to a generic distributed pixel after the fact what they were truly looking for . The system , services network view , where local device pixel services and represented in FIG . 9 as the carrier - based financial tracking “ cloud based ” pixel services have a kind of symmetry in 35 server , now sees the addition of our networked pixel services how they operate . The router in FIG . 7 takes care of how any module and its role in facilitating pertinent results being sent given packaged pixel packet gets sent to the appropriate back to a user , all the while monitoring the uses of the pixel processing location , whether local or remote ( with the services in order to populate the monthly bill and send the style of fill pattern denoting different component processing proceeds to the proper entities . functions ; only a few of the processing functions required by 40 ( As detailed further elsewhere , the money flow may not the enabled visual processing services are depicted ) . Some exclusively be to remote service providers . Other money of the data shipped to cloud - based pixel services may have flows can arise , such as to users or other parties , e.g. , to been first processed by local device pixel services . The induce or reward certain actions . ) circles indicate that the routing functionality may have FIG . 10 focuses on functional division of processing components in the cloud - nodes that serve to distribute 45 illustrating how tasks in the nature of template matching can tasks to active service providers , and collect results for be performed on the cell phone itself , whereas more sophis transmission back to the device . In some implementations ticated tasks in the nature of data association ) desirably are these functions may be performed at the edge of the wireless referred to the cloud for processing . network , e.g. , by modules at wireless service towers , so as Elements of the foregoing are distilled in FIG . 10A , to ensure the fastest action . Results collected from the active 50 showing an implementation of aspects of the technology as external service providers , and the active local processing a physical matter of ( usually ) software components . The two stages , are fed back to Pixel Service Manager software , ovals in the figure highlight the symmetric pair of software', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content=\"which then interacts with the device user interface . components which are involved in setting up a “ human FIG . 8 is an expanded view of the lower right portion of real - time ” visual recognition session between a mobile FIG . 7 and represents the moment where Dorothy's shoes 55 device and the generic cloud or service providers , data turn red and why distributed pixel services provided by the associations and visual query results . The oval on the left cloud — as opposed to the local device — will probably trump refers to “ keyvectors ” and more specifically “ visual keyvec all but the most mundane object recognition tasks . tors . ” As noted , this term can encompass everything from Object recognition in its richer form is based on visual simple JPEG compressed blocks all the way through log association rather than strict template matching rules . If we 60 polar transformed facial feature vectors and anything in all were taught that the capital letter “ A ” will always be between and beyond . The point of a keyvector is that the strictly following some pre - historic form never to change , a essential raw information of some given visual recognition universal template image if you will , then pretty clean and task has been optimally pre - processed and packaged ( pos locally prescriptive methods can be placed into a mobile sibly compressed ) . The oval on the left assembles these imaging device in order to get it to reliably read a capital A 65 packets , and typically inserts some addressing information any time that ordained form “ A ” is presented to the camera . by which they will be routed . ( Final addressing may not be 2D and even three 3D barcodes in many ways follow this possible , as the packet may ultimately be routed to remote\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 67}), Document(page_content='US 10,922,957 B2 \\n 11 12', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content='service providers — the details of which may not yet be on domestic data processing . Others may want to stick to known . ) Desirably , this processing is performed as close to service providers that meet \" green , ” “ ethical , ” or other the raw sensor data as possible , such as by processing standards of corporate practice . Others may prefer richer circuitry integrated on the same substrate as the image data output . Weightings of different criteria can be applied sensor , which is responsive to software instructions stored in 5 by the query router and response manager in making the memory or provided from another stage in packet form . decision . The oval on the right administers the remote processing of In some circumstances , one input to the query router and keyvector data , e.g. , attending to arranging appropriate response manager may be the user\\'s location , so that a services , directing traffic flow , etc. Desirably , this software different service provider may be selected when the user is process is implemented as low down on a communications 10 at home in Oregon , than when she is vacationing in Mexico . stack as possible , generally on a “ cloud side ” device , access In other instances , the required turnaround time is specified , point , or cell tower . ( When real - time visual keyvector pack- which may disqualify some vendors , and make others more ets stream over a communications channel , the lower down competitive . In some instances the query router and in the communications stack they are identified and routed , response manager need not decide at all , e.g. , if cached the smoother the “ human real - time ” look and feel a given 15 results identifying a service provider selected in a previous visual recognition task will be . ) Remaining high level pro- auction are still available and not beyond a “ freshness ” cessing needed to support this arrangement is included in threshold . FIG . 10A for context , and can generally be performed Pricing offered by the vendors may change with process through native mobile and remote hardware capabilities . ing load , bandwidth , time of day , and other considerations . FIGS . 11 and 12 illustrate the concept that some providers 20 In some embodiments the providers may be informed of of some cloud - based pixel processing services may be offers submitted by competitors ( using known trust arrange established in advance , in a pseudo - static fashion , whereas ments assuring data integrity ) , and given the opportunity to other providers may periodically vie for the privilege of make their offers more enticing . Such a bidding war may processing a user\\'s keyvector data , through participation in continue until no bidder is willing to change the offered a reverse auction . In many implementations , these latter 25 terms . The query router and response manager ( or in some providers compete each time a packet is available for implementations , the user ) then makes a selection . processing . For expository convenience and visual clarity , FIG . 12 Consider a user who snaps a cell phone picture of an shows a software module labeled “ Bid Filter and Broadcast unfamiliar car , wanting to learn the make and model . Vari- Agent . ” In most implementations this forms part of the ous service providers may compete for this business . A 30 query router and response manager module . The bid filter startup vendor may offer to perform recognition for free to module decides which vendors from a universe of possible build its brand or collect data . Imagery submitted to this vendors should be given a chance to bid on a processing service returns information simply indicating the car\\'s make task . ( The user\\'s preference data , or historical experience , and model . Consumer Reports may offer an alternative may indicate that certain service providers be disqualified . ) service which provides make and model data , but also 35 The broadcast agent module then communicates with the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content=\"may offer an alternative may indicate that certain service providers be disqualified . ) service which provides make and model data , but also 35 The broadcast agent module then communicates with the provides technical specifications for the car . However , they selected bidders to inform them of a user task for processing , may charge 2 cents for the service ( or the cost may be and provides information needed for them to make a bid . bandwidth based , e.g. , 1 cent per megapixel ) . Edmunds , or Desirably , the bid filter and broadcast agent do at least JD Powers , may offer still another service , which provides some their work in advance of data being available for data like Consumer Reports , but pays the user for the 40 processing . That is , as soon as a prediction can be made as privilege of providing data . In exchange , the vendor is given to an operation that the user may likely soon request , these the right to have one of its partners send a text message to modules start working to identify a provider to perform a the user promoting goods or services . The payment may take service expected to be required . A few hundred milliseconds the form of a credit on the user's monthly cell phone later the user keyvector data may actually be available for voice / data service billing . 45 processing ( if the prediction turns out to be accurate ) . Using criteria specified by the user , stored preferences , Sometimes , as with Google's present AdWords system , context , and other rules / heuristics , a query router and the service providers are not consulted at each user trans response manager in the cell phone , in the cloud , distrib- action . Instead , each provides bidding parameters , which are uted , etc. ) determines whether the packet of data needing stored and consulted whenever a transaction is considered , processing should be handled by one of the service providers 50 to determine which service provider wins . These stored in the stable of static standbys , or whether it should be parameters may be updated occasionally . In some imple offered to providers on an auction basis in which case it mentations the service provider pushes updated parameters\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content=\"arbitrates the outcome of the auction . to the bid filter and broadcast agent whenever available . The static standby service providers may be identified ( The bid filter and broadcast agent may serve a large when the phone is initially programmed , and only reconfig- 55 population of users , such as all Verizon subscribers in area ured when the phone is reprogrammed ( For example , Veri- code 503 , or all subscribers to an ISP in a community , or all zon may specify that all FFT operations on its phones be users at the domain well - dot - com , etc .; or more localized routed to a server that it provides for this purpose . ) Or , the agents may be employed , such as one for each cell phone user may be able to periodically identify preferred providers tower . ) for certain tasks , as through a configuration menu , or specify 60 If there is a lull in traffic , a service provider may discount that certain tasks should be referred for auction . Some its services for the next minute . The service provider may applications may emerge where static service providers are thus transmit ( or post ) a message stating that it will perform favored ; the task may be so mundane , or one provider's eigenvector extraction on an image file of up to 10 mega services may be so un - paralleled , that competition for the bytes for 2 cents until 1244754176 Coordinated Universal provision of services isn't warranted . 65 Time in the Unix epoch , after which time the price will In the case of services referred to auction , some users may return to 3 cents . The bid filter and broadcast agent updates exalt price above all other considerations . Others may insist a table with stored bidding parameters accordingly .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 68}), Document(page_content='US 10,922,957 B2 \\n 13 14 \\n ( The reader is presumed to be familiar with the reverse share some of their media consumption data with Nielsen auction arrangements used by Google to place sponsored ( such as by serving as an anonymous member for a city\\'s advertising on web search results page . An illustrative audience measurement panel ) , and provided on a fee basis to description is provided in Levy , \" Secret of Googlenomics : others . Nielsen may offer , for example , 100 units of credit Data - Fueled Recipe Brews Profitability , ” Wired Magazine , 5 micropayments or other value to participating consumers May 22 , 2009. ) each month , or may provide credit each time the user In other implementations , the broadcast agent polls the submits information to Nielsen . bidders — communicating relevant parameters , and soliciting In another example , a consumer may be rewarded for bid responses whenever a transaction is offered for process- accepting commercials , or commercial impressions , from a ing . 10 company . If a consumer goes into the Pepsi Center in Once a prevailing bidder is decided , and data is available Denver , she may receive a reward for each Pepsi - branded for processing , the broadcast agent transmits the keyvector experience she encounters . The amount of micropayment data ( and other parameters as may be appropriate to a may scale with the amount of time that she interacts with the particular task ) to the winning bidder . The bidder then different Pepsi - branded objects ( including audio and imag performs the requested operation , and returns the processed 15 ery ) in the venue . data to the query router and response manager . This module Not just large brand owners can provide credits to indi logs the processed data , and attends to any necessary viduals . Credits can be routed to friends and social / business accounting ( e.g. , crediting the service provider with the acquaintances . To illustrate , a user of Facebook may share appropriate fee ) . The response data is then forwarded back credit ( redeemable for goods / services , or exhangable for to the user device . 20 cash ) from his Facebook page enticing others to visit , or In a variant arrangement , one or more of the competing linger . In some cases , the credit can be made available only service providers actually performs some or all of the to people who navigate to the Facebook page in a certain requested processing , but “ teases ” the user ( or the query manner — such as by linking to the page from the user\\'s router and response manager ) by presenting only partial business card , or from another launch page . results . With a taste of what\\'s available , the user ( or the 25 As another example , consider a Facebook user who has query router and response manager ) may be induced to make earned , or paid for , or otherwise received credit that can be a different choice than relevant criteria / heuristics would applied to certain services such as for downloading songs', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content=\"otherwise indicate . from iTunes , or for music recognition services , or for The function calls sent to external service providers , of identifying clothes that go with particular shoes ( for which course , do not have to provide the ultimate result sought by 30 an image has been submitted ) , etc. These services may be a consumer ( e.g. , identifying a car , or translating a menu associated with the particular Facebook page , so that friends listing from French to English ) . They can be component can invoke the services from that page — essentially spend operations , such as calculating an FFT , or performing a SIFT ing the host's credit ( again , with suitable authorization or procedure or a log - polar transform , or computing a histo- invitation by that hosting user ) . Likewise , friends may gram or eigenvectors , or identifying edges , etc. 35 submit images to a facial recognition service accessible In time , it is expected that a rich ecosystem of expert through an application associated with the user's Facebook processors will emergeserving myriad processing page . Images submitted in such fashion are analyzed for requests from cell phones and other thin client devices . faces of the host’s friends , and identification information is More on Monetary Flow returned to the submitter , e.g. , through a user interface Additional business models can be enabled , involving the 40 presented on the originating Facebook page . Again , the host subsidization of consumed remote services by the service may be assessed a fee for each such operation , but may allow\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content=\"providers themselves in exchange for user information ( e.g. , authorized friends to avail themselves of such service at no for audience measurement ) , or in exchange for action taken cost . by the user , such as completing a survey , visiting specific Credits , and payments , can also be routed to charities . A sites , locations in store , etc. 45 viewer exiting a theatre after a particularly poignant movie Services may be subsidized by third parties as well , such about poverty in Bangladesh may capture an image of an a coffee shop that derives value by providing a differenti- associated movie poster , which serves as a portal for dona ating service to its customers in the form of free / discounted tions for a charity that serves the poor in Bangladesh . Upon usage of remote services while they are seated in the shop . recognizing the movie poster , the cell phone can present a In one arrangement an economy is enabled wherein a 50 graphical / touch user interface through which the user spins currency of remote processing credits is created and dials to specify an amount of a charitable donation , which at exchanged between users and remote service providers . This the conclusion of the transaction is transferred from a may be entirely transparent to the user and managed as part financial account associated with the user , to one associated of a service plan , e.g. , with the user's cell phone or data with the charity . service provider . Or it can be exposed as a very explicit 55 More on a Particular Hardware Arrangement aspect of certain embodiments of the present technology . As noted above and in the cited patent documents , there Service providers and others may award credits to users for is a need for generic object recognition by a mobile device . taking actions or being part of a frequent - user program to Some approaches to specialized object recognition have build allegiance with specific providers . emerged , and these have given rise to specific data process As with other currencies , users may choose to explicitly 60 ing approaches . However , no architecture has been proposed donate , save , exchange or generally barter credits as needed . that goes beyond specialized object recognition toward Considering these points in further detail , a service may generic object recognition . pay a user for opting - in to an audience measurement panel . Visually , a generic object recognition arrangement E.g. , The Nielsen Company may provide services to the requires access to good raw visual data preferably free of public — such as identification of television programming 65 device quirks , scene quirks , user quirks , etc. Developers of from audio or video samples submitted by consumers . These systems built around object identification will best prosper services may be provided free to consumers who agree to and serve their users by concentrating on the object identi\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 69}), Document(page_content='5 \\n as are US 10,922,957 B2 \\n 15 16 \\n fication task at hand , and not the myriad existing roadblocks , includes processing such as JPEG compression . Another , 14 , resource sinks , and third party dependencies that currently is tailored for object recognition . As discussed , some of this', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='must be confronted . processing may be performed by the mobile device , while As noted , virtually all object identification techniques can other processing may be referred to the cloud 16 . make use of or even rely upon a pipe to “ the cloud . ” FIG . 14 takes an application - centric view of the object “ Cloud ” can include anything external to the cell phone . recognition processing path . Some applications reside An example is a nearby cell phone , or plural phones on a wholly in the cell phone . Other applications reside wholly distributed network . Unused processing power on such other outside the cell phone e.g. , simply taking keyvector data as phone devices can be made available for hire ( or for free ) to stimulus . More common are hybrids , such as where some call upon as needed . The cell phones of the implementations 10 processing is done in the cell phone , other processing is done detailed herein can scavenge processing power from such externally , and the application software orchestrating the other cell phones . process resides in the cell phone . Such a cloud may be ad hoc , e.g. , other cell phones within To illustrate further discussion , FIG . 15 shows a range 40 Bluetooth range of the user\\'s phone . The ad hoc network can of some of the different types of images 41-46 that may be be extended by having such other phones also extend the 15 captured by a particular user\\'s cell phone . A few brief ( and local cloud to further phones that they can reach by Blu- incomplete ) comments about some of the processing that etooth , but the user cannot . may be applied to each image are provided in the following The “ cloud ” can also comprise other computational plat- paragraphs . forms , such as set - top boxes ; processors in automobiles , Image 41 depicts a thermostat . A steganographic digital thermostats , HVAC systems , wireless routers , local cell 20 watermark 47 is textured or printed on the thermostat’s case . phone towers and other wireless network edges ( including ( The watermark is shown as visible in FIG . 15 , but is the processing hardware for their software - defined radio typically imperceptible to the viewer ) . The watermark con equipment ) , etc. Such processors can be used in conjunction veys information intended for the cell phone , allowing it to with more traditional cloud computing resources present a graphic user interface by which the user can offered by Google , Amazon , etc. 25 interact with the thermostat . A bar code or other data carrier ( In view of concerns of certain users about privacy , the can alternatively be used . Such technology is further phone desirably has a user - configurable option indicating detailed below . whether the phone can refer data to cloud resources for Image 42 depicts an item including a barcode 48. This processing . In one arrangement , this option has a default barcode conveys Universal Product Code ( UPC ) data . Other value of “ No , ” limiting functionality and impairing battery 30 barcodes may convey other information . The barcode pay life , but also limiting privacy concerns . In another arrange- load is not primarily intended for reading by a user cell ment , this option has a default value of “ Yes . \" ) phone ( in contrast to watermark 47 ) , but it nonetheless may Desirably , image - responsive techniques should produce a be used by the cell phone to help determine an appropriate short term \" result or answer , \" which generally requires some response for the user . level of interactivity with a user - hopefully measured in 35 Image 43 shows a product that may be identified without fractions of a second for truly interactive applications , or a reference to any express machine readable information ( such few seconds or fractions of a minute for nearer - term \" I\\'m as a bar code or watermark ) . A segmentation algorithm may patient to wait ” applications . be applied to edge - detected image data to distinguish the As for the objects in', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='for nearer - term \" I\\'m as a bar code or watermark ) . A segmentation algorithm may patient to wait ” applications . be applied to edge - detected image data to distinguish the As for the objects in question , they can break down into apparent image subject from the apparent background . The various categories , including ( 1 ) generic passive ( clues to 40 image subject may be identified through its shape , color and basic searches ) , ( 2 ) geographic passive ( at least you know texture . Image fingerprinting may be used to identify refer where you are , and may hook into geographic - specific ence images having similar labels , and metadata associated resources ) , ( 3 ) “ cloud supported ” passive , as with “ identi- with those other images may be harvested . SIFT techniques fied / enumerated objects ” and their associated sites , and ( 4 ) ( discussed below ) may be employed for such pattern - based active controllable , a la ThingPipe ( a reference to technol- 45 recognition tasks . Specular reflections in low texture regions ogy detailed below , such as WiFi - equipped thermostats and may tend to indicate the image subject is made of glass . parking meters ) . Optical character recognition can be applied for further An object recognition platform should not , it seems , be information ( reading the visible text ) . All of these clues can conceived in the classic \" local device and local resources be employed to identify the depicted item , and help deter only ” software mentality . However , it may be conceived as 50 mine an appropriate response for the user . a local device optimization problem . That is , the software on Additionally ( or alternatively ) , similar - image search sys the local device , and its processing hardware , should be tems , such as Google Similar Images , and Microsoft Live designed in contemplation of their interaction with off- Search , can be employed to find similar images , and their device software and hardware . Ditto the balance and inter- metadata can then be harvested . ( As of this writing , these play of both control functionality , pixel crunching function- 55 services do not directly support upload of a user picture to ality , and application software / GUI provided on the device , find similar web pictures . However , the user can post the versus off the device . ( In many implementations , certain image to Flickr ( using Flickr’s cell phone upload function databases useful for object identification / recognition will ality ) , and it will soon be found and processed by Google reside remote from the device . ) and Microsoft . ) In a particularly preferred arrangement , such a processing 60 Image 44 is a snapshot of friends . Facial detection and platform employs image processing near the sensor - opti- recognition may be employed ( i.e. , to indicate that there are mally on the same chip , with at least some processing tasks faces in the image , and to identify particular faces and desirably performed by dedicated , special purpose hard- annotate the image with metadata accordingly , e.g. , by reference to user - associated data maintained by Apple\\'s Consider FIG . 13 , which shows an architecture of a cell 65 iPhoto service , Google\\'s Picasa service , Facebook , etc. ) phone 10 in which an image sensor 12 feeds two processing Some facial recognition applications can be trained for paths . One , 13 , is tailored for the human visual system , and non - human faces , e.g. , cats , dogs animated characters ware .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 70}), Document(page_content='10 US 10,922,957 B2 \\n 17 18 \\n including avatars , etc. Geolocation and date / time informa- distributed . ) The packet data further specifies operations to tion from the cell phone may also provide useful informa- be performed by an ensuing chain of processing stages 38 . \\n tion . In one particular implementation , setup module 34 dic The persons wearing sunglasses pose a challenge for tates on a frame by frame basis — the parameters that are to some facial recognition algorithms . Identification of those 5 be employed by camera 32 in gathering an exposure . Setup \\n individuals may be aided by their association with persons module 34 also specifies the type of data the camera is to \\n whose identities can more easily be determined ( e.g. , by output . These instructional parameters are conveyed in a first \\n conventional facial recognition ) . That is , by identifying field 55 of a header portion 56 of a data packet 57 corre \\n other group pictures in iPhoto / Picasa / Facebook / etc . that sponding to that frame ( FIG . 17 ) . \\n include one or more of the latter individuals , the other For example , for each frame , the setup module 34 may \\n individuals depicted in such photographs may also be pres issue a packet 57 whose first field 55 instructs the camera \\n ent in the subject image . These candidate persons form a about , e.g. , the length of the exposure , the aperture size , the lens focus , the depth of field , etc. Module 34 may further much smaller universe of possibilities than is normally author the field 55 to specify that the sensor is to sum sensor provided by unbounded iPhoto / Picasa / Facebook / etc data . 15 charges to reduce resolution ( e.g. , producing a frame of The facial vectors discernable from the sunglass - wearing 640x480 data from a sensor capable of 1280x960 ) , output faces in the subject image can then be compared against this data only from red - filtered sensor cells , output data only smaller universe of possibilities in order to determine a best from a horizontal line of cells across the middle of the match . If , in the usual case of recognizing a face , a score of sensor , output data only from a 128x128 patch of cells in the 90 is required to be considered a match ( out of an arbitrary 20 center of the pixel array , etc. The camera instruction field 55 top match score of 100 ) , in searching such a group - con- may further specify the exact time that the camera is to strained set of images a score of 70 or 80 might suffice . capture data so as to allow , e.g. , desired synchronization ( Where , as in image 44 , there are two persons depicted with ambient lighting ( as detailed later ) . without sunglasses , the occurrence of both of these indi- Each packet 56 issued by setup module 34 may include viduals in a photo with one or more other individuals may 25 different camera parameters in the first header field 55. Thus , increase its relevance to such an analysis — implemented , a first packet may cause camera 32 to capture a full frame e.g. , by increasing a weighting factor in a matching algo- image with an exposure time of 1 millisecond . A next packet', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content='rithm . ) may cause the camera to capture a full frame image with an Image 45 shows part of the statue of Prometheus in exposure time of 10 milliseconds , and a third may dictate an Rockefeller Center , NY . Its identification can follow teach- 30 exposure time of 100 milliseconds . ( Such frames may later ings detailed elsewhere in this specification . be processed in combination to yield a high dynamic range Image 46 is a landscape , depicting the Maroon Bells image . ) A fourth packet may instruct the camera to down mountain range in Colorado . This image subject may be sample data from the image sensor , and combine signals recognized by reference to geolocation data from the cell from differently color - filtered sensor cells , so as to output a phone , in conjunction with geographic information services 35 4x3 array of grayscale luminance values . A fifth packet may', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content=\"such as GeoNames or Yahoo ! ' s GeoPlanet . instruct the camera to output data only from an 8x8 patch of ( It should be understood that techniques noted above in pixels at the center of the frame . A sixth packet may instruct connection with processing of one of the images 41-46 in the camera to output only five lines of image data , from the FIG . 15 can likewise be applied to others of the images . top , bottom , middle , and mid - upper and mid - lower rows of Moreover , it should be understood that while in some 40 the sensor . A seventh packet may instruct the camera to respects the depicted images are ordered according to ease output only data from blue - filtered sensor cells . An eighth of identifying the subject and formulating a response , in packet may instruct the camera to disregard any auto - focus other respects they are not . For example , although landscape instructions but instead capture a full frame at infinity focus . image 46 is depicted to the far right , its geolocation data is And so on . strongly correlated with the metadata “ Maroon Bells . ” Thus , 45 Each such packet 57 is provided from setup module 34 this particular image presents an easier case than that across a bus or other data channel 60 to a camera controller presented by many other images . ) In one embodiment , such module associated with the camera . ( The details of a digital processing of imagery occurs automatically without camera including an array of photosensor cells , associated express user instruction each time . Subject to network analog - digital converters and control circuitry , etc. , are well connectivity and power constraints , information can be 50 known to artisans and so are not belabored . ) Camera 32 gleaned continuously from such processing , and may be captures digital image data in accordance with instructions used in processing subsequently - captured images . For in the header field 55 of the packet and stuffs the resulting example , an earlier image in a sequence that includes image data into a body 59 of the packet . It also deletes the photograph 44 may show members of the depicted group camera instructions 55 from the packet header ( or otherwise without sunglasses — simplifying identification of the per- 55 marks header field 55 in a manner permitting it to be sons later depicted with sunglasses . disregarded by subsequent processing stages ) . FIG . 16 , Etc. , Implementation When the packet 57 was authored by setup module 34 it FIG . 16 gets into the nitty gritty of a particular imple- also included a series of further header fields 58 , each mentation incorporating certain of the features earlier dis- specifying how a corresponding , successive post - sensor cussed . ( The other discussed features can be implemented 60 stage 38 should process the captured data . As shown in FIG . by the artisan within this architecture , based on the provided 16 , there are several such post - sensor processing stages 38 . disclosure . ) In this data driven arrangement 30 , operation of Camera 32 outputs the image - stuffed packet produced by a cell phone camera 32 is dynamically controlled in accor- the camera ( a pixel packet ) onto a bus or other data channel dance with packet data sent by a setup module 34 , which in 61 , which conveys it to a first processing stage 38 . turn is controlled by a control processor module 36. ( Control 65 Stage 38 examines the header of the packet . Since the processor module 36 may be the cell phone's primary camera deleted the instruction field 55 that conveyed camera processor , or an auxiliary processor , or this function may be instructions ( or marked it to be disregarded ) , the first header\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 71}), Document(page_content='US 10,922,957 B2 \\n 19 20', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content=\"field encountered by a control portion of stage 38 is field on which it acted . The instructions are ordered in the header 58a . This field details parameters of an operation to be in the sequence of processing stages , so this removal allows applied by stage 38 to data in the body of the packet . each stage to look to the first instructions remaining in the For example , field 58a may specify parameters of an edge header for direction . Other arrangements , of course , can detection algorithm to be applied by stage 38 to the packet's 5 alternatively be employed . ( For example , a module may image data ( or simply that such an algorithm should be insert new information into the header at the front , tail , or applied ) . It may further specify that stage 38 is to substitute elsewhere in the sequence_based on processing results . the resulting edge - detected set of data for the original image This amended header then controls packet flow and there data in the body of the packet . ( Substituting of data , rather fore processing . ) In addition to outputting data for the next than appending , may be indicated by the value of a single bit 10 stage , each stage 38 may further have an output 31 providing flag in the packet header . ) Stage 38 performs the requested data back to the control processor module 36. For example , operation ( which may involve configuring programmable processing undertaken by one of the local stages 38 may hardware in certain implementations ) . First stage 38 then indicate that the exposure or focus of the camera should be deletes instructions 58a from the packet header 56 ( or marks adjusted to optimize suitability of an upcoming frame of them to be disregarded ) and outputs the processed pixel 15 captured data for a particular type of processing ( e.g. , object packet for action by a next processing stage . identification ) . This focus / exposure information can be used A control portion of a next processing stage ( which here as predictive setup data for the camera the next time a frame comprises stages 38a and 38b , discussed later ) examines the of the same or similar type is captured . The control processor header of the packet . Since field 58a was deleted ( or marked module 36 can set up a frame request using a filtered or to be disregarded ) , the first field encountered is field 586. In 20 time - series prediction sequence of focus information from this particular packet , field 58b may instruct the second stage previous frames , or a sub - set of those frames . not to perform any processing on the data in the body of the Error and status reporting functions may also be accom packet , but instead simply delete field 58b from the packet plished using outputs 31. Each stage may also have one or header and pass the pixel packet to the next stage . more other outputs 33 for providing data to other processes A next field of the packet header may instruct the third 25 or modules — locally within the cell phone , or remote ( “ in stage 38c to perform 2D FFT operations on the image data the cloud ” ) . Data ( in packet form , or in other format ) may be found in the packet body , based on 16x16 blocks . It may directed to such outputs in accordance with instructions in further direct the stage to hand - off the processed FFT data to packet 57 , or otherwise . a wireless interface , for internet transmission to address For example , a processing module 38 may make a data 216.239.32.10 , accompanied by specified data ( detailing , 30 flow selection based on some result of processing it per e.g. , the task to be performed on the received FFT data by forms . E.g. , if an edge detection stage discerns a sharp the computer at that address , such as texture classification ) . contrast image , then an outgoing packet may be routed to an It may further direct the stage to hand off a single 16x16 external service provider for FFT processing . That provider block of FFT data , corresponding to the center of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content=\"packet may be routed to an It may further direct the stage to hand off a single 16x16 external service provider for FFT processing . That provider block of FFT data , corresponding to the center of the may return the resultant FFT data to other stages . However , captured image , to the same or a different wireless interface 35 if the image has poor edges ( such as being out of focus ) , then for transmission to address 12.232.235.27 — again accom- the system may not want FFT- and following processing to panied by corresponding instructions about its use ( e.g. , be performed on the data . Thus , the processing stages can search an archive of stored FFT data for a match , and return cause branches in the data flow , dependent on parameters of information if a match is found ; also , store this 16x16 block the processing ( such as discerned image characteristics ) . in the archive with an associated identifier ) . Finally , the 40 Instructions specifying such conditional branching can be header authored by setup module 34 may instruct stage 38c included in the header of packet 57 , or they can be provided to replace the body of the packet with the single 16x16 block otherwise . FIG . 19 shows one arrangement . Instructions 58d of FFT data dispatched to the wireless interface . As before , originally in packet 57 specify a condition , and specify a the stage also edits the packet header to delete ( or mark ) the location in a memory 79 from which replacement subse instructions to which it responded , so that a header instruc- 45 quent instructions ( 58e - 58g ' ) can be read , and substituted tion field for the next processing stage is the first to be into the packet header , if the condition is met . If the encountered . condition is not met , execution proceeds in accordance with In other arrangements , the addresses of the remote com- header instructions already in the packet . puters are not hard - coded . For example , the packet may In other arrangements , other variations can be employed . include a pointer to a database record or memory location ( in 50 For example , all of the possible conditional instructions can the phone or in the cloud ) , which contains the destination be provided in the packet . In another arrangement , a packet address . Or , stage 38c may be directed to hand - off the architecture is still used , but one or more of the header fields processed pixel packet to the Query Router and Response do not include explicit instructions . Rather , they simply Manager ( e.g. , FIG . 7 ) . This module examines the pixel point to memory locations from which corresponding packet to determine what type of processing is next required , 55 instructions ( or data ) are retrieved , e.g. , by the correspond and it routes it to an appropriate provider ( which may be in ing processing stage 38 . the cell phone if resources permit , or in the cloud among Memory 79 ( which can include a cloud component ) can the stable of static providers , or to a provider identified also facilitate adaptation of processing flow even if condi through an auction ) . The provider returns the requested tional branching is not employed . For example , a processing output data ( e.g. , texture classification information , and 60 stage may yield output data that determines parameters of a information about any matching FFT in the archive ) , and filter or other algorithm to be applied by a later stage ( e.g. , processing continues per the next item of instruction in the a convolution kernel , a time delay , a pixel mask , etc ) . Such pixel packet header . parameters may be identified by the former processing stage The data flow continues through as many functions as a in memory ( e.g. , determined / calculated , and stored ) , and particular operation may require . 65 recalled for use by the later stage . In FIG . 19 , for example , In the particular arrangement illustrated , each processing processing stage 38 produces parameters that are\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content='may require . 65 recalled for use by the later stage . In FIG . 19 , for example , In the particular arrangement illustrated , each processing processing stage 38 produces parameters that are stored in stage 38 strips - out , from the packet header , the instructions memory 79. A subsequent processing stage 38c later', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 72}), Document(page_content='on . \\n 15 US 10,922,957 B2 \\n 21 22 \\n retrieves these parameters , and uses them in execution of its In some embodiments , the respective purpose processors assigned operation . ( The information in memory can be may be chained in a fixed order . The edge detection pro labeled to identify the module / provider from which they cessor may be first , the FFT processor may be third , and so originated , or to which they are destined < if known > , or other addressing arrangements can be used . ) Thus , the 5 Alternatively , the processing modules may be intercon \\n processing flow can adapt to circumstances and parameters nected by one or more busses ( and / or a crossbar arrange \\n that were not known at the time control processor module 36 ment or other interconnection architecture ) that permit any \\n originally directed setup module 34 to author packet 57 . stage to receive data from any stage , and to output data to \\n In one particular embodiment , each of the processing any stage . Another interconnect method is a network on a \\n stages 38 comprises hardware circuitry dedicated to a par 10 chip ( effectively a packet - based LAN ; similar to crossbar in \\n ticular task . The first stage 38 may be a dedicated edge adaptability , but programmable by network protocols ) . Such arrangements can also support having one or more stages detection processor . The third stage 38c may be a dedicated iteratively process data — taking output as input , to perform FFT processor . Other stages may be dedicated to other further processing . processes . These may include DCT , wavelet , Haar , Hough , One iterative processing arrangement is shown by stages and Fourier - Mellin transform processors , filters of different 38a / 38b in FIG . 16. Output from stage 38a can be taken as sorts ( e.g. , Wiener , low pass , bandpass , highpass ) , and stages input to stage 38b . Stage 38b can be instructed to do no for performing all or part of operations such as facial processing on the data , but simply apply it again back to the recognition , optical character recognition , computation of input of stage 38a . This can loop as many times as desired . eigenvalues , extraction of shape , color and texture feature 20 When iterative processing by stage 38a is completed , its data , barcode decoding , watermark decoding , object seg- output can be passed to a next stage 38c in the chain . mentation , pattern recognition , age and gender detection , In addition to simply serving as a pass - through stage , emotion classification , orientation determination , compres- stage 38b can perform its own type of processing on the data sion , decompression , log - polar mapping , convolution , inter- processed by stage 38a . Its output can be applied to the input polation , decimation / down - sampling / anti - aliasing ; correla- 25 of stage 38a . Stage 38a can be instructed to apply , again , its tion , performing square - root and squaring operations , array process to the data produced by stage 38b , or to pass it \\n multiplication , perspective transformation , butterfly opera through . Any serial combination of stage 38a / 38b process \\n tions ( combining results of smaller DFTs into a larger DFT , ing can thus be achieved . \\n or decomposing a larger DCT into subtransforms ) , etc. The roles of stages 38a and 38b in the foregoing can also \\n These hardware processors can be field - configurable , 30 be reversed . \\n instead of dedicated . Thus , each of the processing blocks in In this fashion , stages 38a and 38b can be operated to ( 1 )', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content=\"instead of dedicated . Thus , each of the processing blocks in In this fashion , stages 38a and 38b can be operated to ( 1 ) \\n FIG . 16 may be dynamically reconfigurable , as circum apply a stage 38a process one or more times to data ; ( 2 ) apply a stage 38b process one or more times to data ; ( 3 ) stances warrant . At one instant a block may be configured as apply any combination and order of 38a and 38b processes an FFT processing module . The next instant it may be 35 to data ; or ( 4 ) simply pass the input data to the next stage , configured as a filter stage , etc. One moment the hardware without processing . processing chain may be configured as a barcode reader ; the The camera stage can be incorporated into an iterative next it may be configured as a facial recognition system , etc. processing loop . For example , to gain focus - lock , a packet Such hardware reconfiguration information can be down may be passed from the camera to a processing module that loaded from the cloud , or from services such as the Apple 40 assesses focus . ( Examples may include an FFT stage AppStore . And the information needn't be statically resident looking for high frequency image components ; an edge on the phone once downloaded it can be summoned from detector stage_looking for strong edges , etc. Sample edge the cloud / AppStore whenever needed . detection algorithms include Canny , Sobel , and differential . Given increasing broadband availability and speed , the Edge detection is also useful for object tracking . ) An output hardware reconfiguration data can be downloaded to the cell 45 from such a processing module can loop back to the cam phone each time it is turned on , or otherwise initialized era's controller module and vary a focus signal . The camera whenever a particular function is initialized . Gone would be captures a subsequent frame with the varied focus signal , the dilemma of dozens of different versions of an application and the resulting image is again provided to the processing \\n being deployed in the market at any given time depending module that assesses focus . This loop continues until the on when different users last downloaded updates , and the 50 processing module reports focus within a threshold range is conundrums that companies confront in supporting disparate achieved . ( The packet header , or a parameter in memory , can \\n versions of products in the field . Each time a device or specify an iteration limit , e.g. , specifying that the iterating\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content='versions of products in the field . Each time a device or specify an iteration limit , e.g. , specifying that the iterating \\n application is initialized , the latest version of all or selected should terminate and output an error signal if no focus meeting the specified requirement is met within ten itera functionalities is downloaded to the phone . And this works 55 tions . ) not just for full system functionality , but also components , While the discussion has focused on serial data process such as hardware drivers , software for hardware layers , etc. ing , image or other data may be processed in two or more At each initialization , hardware is configured anew — with parallel paths . For example , the output of stage 38d may be the latest version of applicable instructions . ( For code used applied to two subsequent stages , each of which starts a during initializing , it can be downloaded for use at the next 60 respective branch of a fork in the processing . Those two initialization . ) Some updated code may be downloaded and chains can be processed independently thereafter , or data dynamically loaded only when particular applications resulting from such processing can be combined or used in require it , such as to configure the hardware of FIG . 6 for conjunction — in a subsequent stage . ( Each of those process specialized functions . The instructions can also be tailored to ing chains , in turn , can be forked , etc. ) the particular platforms , e.g. , the iPhone device may employ 65 As noted , a fork commonly will appear much earlier in the different accelerometers than the Android device , and appli- chain . That is , in most implementations , a parallel process cation instructions may be varied accordingly . ing chain will be employed to produce imagery for human or', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 73}), Document(page_content=\"source . 10 \\n 15 US 10,922,957 B2 \\n 23 24 \\n as opposed to machine — consumption . Thus , a parallel convey firmware to be loaded into a module having a CPU process may fork immediately following the camera sensor core , or into an application- or cloud - based layer ; likewise 12 , as shown by juncture 17 in FIG . 13. The processing for with software instructions . the human visual system 13 includes operations such as The module configuration instructions may be received noise reduction , white balance , and compression . Processing 5 over a wireless or other external network ; it needn't always for object identification 14 , in contrast , may include the be resident on the local system . If the user requests an operations detailed in this specification . operation for which local instructions are not available , the When an architecture involves forked or other parallel system can request the configuration data from a remote processes , the different modules may finish their processing at different times . They may output data as they finish Instead of conveying the configuration data / instructions asynchronously , as the pipeline or other interconnection themselves , the packet may simply convey an index number , network permits . When the pipeline / network is free , a next pointer , or other address information . This information can module can transfer its completed results . Flow control may be used by the processing module to access a corresponding involve some arbitration , such as giving one path or data a memory store from which the needed data / instructions can higher priority . Packets may convey priority data - deter- be retrieved . Like a cache , if the local memory store is not mining their precedence in case arbitration is needed . For found to contain the needed data / instructions , they can then example , many image processing operations / modules make be requested from another source ( e.g. , across an external \\n use of Fourier domain data , such as produced by an FFT network ) . module . The output from an FFT module may thus be given 20 Such arrangements bring the dynamic of routability down a high priority , and precedence over others in arbitrating data to the hardware layer configuring the module as data \\n traffic , so that the Fourier data that may be needed by other arrives at it . modules be made available with a minimum of delay . Parallelism is widely employed in graphics processing In other implementations , some or all of the processing units ( GPUs ) . Many computer systems employ GPUs as stages are not dedicated purpose processors , but rather are 25 auxiliary processors to handle operations such as graphics general purpose microprocessors programmed by software . rendering . Cell phones increasingly include GPU chips to \\n In still other implementations , the processors are hardware allow the phones to serve as gaming platforms ; these can be \\n reconfigurable . For example , some or all may be field employed to advantage in certain implementations of the programmable gate arrays , such as Xilinx Virtex series present technology . ( By way of example and not limitation ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content=\"devices . Alternatively they may be digital signal processing 30 a GPU can be used to perform bilinear and bicubic inter polation , projective transformations , filtering , etc. ) cores , such as Texas Instruments TMS320 series devices . In accordance with another aspect of the present technol Other implementations can include PicoChip devices , ogy , a GPU is used to correct for lens aberrations and other such as the PC302 and PC312 multicore DSPs . Their optical distortion . programming model allows each core to be coded indepen Cell phone cameras often display optical non - linearieties , dently ( e.g. , in C ) , and then to communicate with others over such as barrel distortion , focus anomalies at the perimeter , an internal interconnect mesh . The associated tools particu etc. This is particularly a problem when decoding digital larly lend themselves to use of such processors in cellular watermark information from captured imagery . With a GPU , equipment . the image can be treated as a texture map , and applied to a Still other implementations may employ configurable 40 correction surface . logic on an ASIC . For example , a processor can include a Typically , texture mapping is used to put a picture of region of configuration logic - mixed with dedicated logic . bricks or a stone wall onto a surface , e.g. , of a dungeon . This allows configurable logic in a pipeline , with dedicated Texture memory data is referenced , and mapped onto a plane pipeline or bus interface circuitry . or polygon as it is drawn . In the present context it is the An implementation can also include one or more modules 45 image that is applied to a surface . The surface is shaped so with a small CPU and RAM , with programmable code space that the image is drawn with an arbitrary , correcting trans for firmware , and workspace for processing essentially a form . dedicated core . Such a module can perform fairly extensive Steganographic calibration signals in a digitally water computations configurable as needed by the process that is marked image can be used to discern the distortion by which using the hardware at the time . 50 an image has been transformed . ( See , e.g. , Digimarc's U.S. All such devices can be deployed in a bus , crossbar or Pat . No. 6,590,996 . ) Each patch of a watermarked image can other interconnection architecture that again permits any be characterized by affine transformation parameters , such stage to receive data from , and output data to , any stage . ( A as translation and scale . An error function for each location FFT or other transform processor implemented in this fash- in the captured frame can thereby be derived . From this error ion may be reconfigured dynamically to process blocks of 55 information , a corresponding surface can be devised 16x16 , 64x64 , 4096x4096 , 1x64 , 32x128 , etc. ) which — when the distorted image is projected onto it by the In certain implementations , some processing modules are GPU , the surface causes the image to appear in its counter replicated permitting parallel execution on parallel hard- distorted , original form . ware . For example , several FFTs may be processing simul- A lens can be characterized in this fashion with a refer taneously 60 ence watermark image . Once the associated correction sur In a variant arrangement , a packet conveys instructions face has been devised , it can be re - used with other imagery that serve to reconfigure hardware of one or more of the captured through that optical system ( since the associated processing modules . As a packet enters a module , the header distortion is fixed ) . Other imagery can be projected onto this causes the module to reconfigure the hardware before the correction surface by the GPU to correct the lens distortion . image - related data is accepted for processing . The architec- 65 ( Different focal depths , and apertures , may require charac ture is thus configured on the fly by packets ( which may terization of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content='- related data is accepted for processing . The architec- 65 ( Different focal depths , and apertures , may require charac ture is thus configured on the fly by packets ( which may terization of different correction functions , since the optical convey image related data , or not ) . The packets can similarly path through the lens may be different . ) 35', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 74}), Document(page_content='5 US 10,922,957 B2 \\n 25 26', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='When a new image is captured , it can be initially recti- ( e.g. , by magnetometer ) , the camera can check that it is linearized , to rid it of keystone / trapezoidal perspective capturing an intended target . The camera set - up module may effect . Once rectilinearized ( e.g. , re - squared relative to the request images of not just certain exposure parameters , but camera lens ) , the local distortions can be corrected by also of certain subjects , or locations . When a camera is in the mapping the rectilinearized image onto the correction sur- correct position to capture a specific subject ( which may face , using the GPU . have been previously user - specified , or identified by a Thus , the correction model is in essence a polygon computer process ) , one or more frames of image data surface , where the tilts and elevations correspond to focus automatically can be captured . ( In some arrangements , the irregularities . Each region of the image has a local transform orientation of the camera is controlled by stepper motors or matrix allowing for correction of that piece of the image . 10 other electromechanical arrangements , so that the camera The same arrangement can likewise be used to correct can autonomously set the azimuth and elevation to capture distortion of a lens in an image projection system . Before image data from a particular direction , to capture a desired projection , the image is mapped like a texture — onto a subject . Electronic or fluid steering of the lens direction can correction surface synthesized to counteract lens distortion . also be utilized . ) When the thus - processed image is projected through the 15 As noted , the camera setup module may instruct the lens , the lens distortion counteracts the correction surface camera to capture a sequence of frames . In addition to distortion earlier applied , causing a corrected image to be benefits such as synthesizing high dynamic range imagery , projected from the system . such frames can also be aligned and combined to obtain Reference was made to the depth of field as one of the super - resolution images . ( As is known in the art , super parameters that can be employed by camera 32 in gathering 20 resolution can be achieved by diverse methods . For exposures . Although a lens can precisely focus at only one example , the frequency content of the images can be ana distance , the decrease in sharpness is gradual on either side lyzed , related to each other by linear transform , affine of the focused distance . ( The depth of field depends on the transformed to correct alignment , then overlaid and com point spread function of the optics — including the lens focal bined . In addition to other applications , this can be used in length and aperture . ) As long as the captured pixels yield 25 decoding digital watermark data from imagery . If the subject information useful for the intended operation , they need not is too far from the camera to obtain satisfactory image be in perfect focus . resolution normally , it may be doubled by such super Sometime focusing algorithms hunt for , but fail to achieve resolution techniques to obtain the higher resolution needed focus wasting cycles and battery life . Better , in some for successful watermark decoding . ) instances , is to simply grab frames at a series of different 30 In the exemplary embodiment , each processing stage focus settings . A search tree of focus depths , or depths of substituted the results of its processing for the input data field , may be used . This is particularly useful where an contained in the packet when received . In other arrange image may include multiple subjects of potential interest- ments , the processed data can be added the packet body , each at a different plane . The system may capture a frame while maintaining the data originally present . In such case focused at 6 inches and another at 24 inches . The different 35 the packet grows during processing as more', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='The system may capture a frame while maintaining the data originally present . In such case focused at 6 inches and another at 24 inches . The different 35 the packet grows during processing as more information is frames may reveal that there are two objects of interest added . While this may be disadvantageous in some contexts , within the field of view- one better captured in one frame , it can also provide advantages . For example , it may obviate the other better captured in the other . Or the 24 inch - focused the need to fork a processing chain into two packets or two frame may be found to have no useful data , but the 6 threads . Sometimes both the original and the processed data inch - focused frame may include enough discriminatory fre- 40 are useful to a subsequent stage . For example , an FFT stage quency content to see that there are two or more subject may add frequency domain information to a packet contain image planes . Based on the frequency content , one or more ing original pixel domain imagery . Both of these may be frames with other focus settings may then be captured . Or a used by a subsequent stage , e.g. , in performing sub - pixel region in the 24 inch - focused frame may have one set of alignment for super - resolution processing . Likewise , a focus Fourier attributes , and the same region in the 6 inch - focused 45 metric may be extracted from imagery and used— with frame may have a different set of Fourier attributes , and from accompanying image data — by a subsequent stage . the difference between the two frames a next trial focus It will be recognized that the detailed arrangements can be setting may be identified ( e.g. , at 10 inches ) , and a further used to control the camera to generate different types of frame at that focus setting may be captured . Feedback is image data on a per - frame basis , and to control subsequent applied — not necessarily to obtain perfect focus lock , but in 50 stages of the system to process each such frame differently . accordance with search criteria to make decisions about Thus , the system may capture a first frame under conditions further captures that may reveal additional useful detail . The selected to optimize green watermark detection , capture a search may fork and branch , depending on the number of second frame under conditions selected to optimize barcode subjects discerned , and associated Fourier , etc. , information , reading , capture a third frame under conditions selected to until satisfactory information about all subjects has been 55 optimize facial recognition , etc. Subsequent stages may be fathered . directed to process each of these frames differently , in order A related approach is to capture and buffer plural frames to best extract the data sought . All of the frames may be as a camera lens system is undergoing adjustment to an processed to sense illumination variations . Every other intended focus setting . Analysis of the frame finally captured frame may be processed to assess focus , e.g. , by computing at the intended focus may suggest that intermediate focus 60 16x16 pixel FFTs at nine different locations within the frames would reveal useful information , e.g. , about subjects image frame . ( Or there may be a fork that allows all frames not earlier apparent or significant . One or more of the frames to be assessed for focus , and the focus branch may be earlier captured and buffered can then be recalled and disabled when not needed , or reconfigured to serve another processed to provide information whose significance was purpose . ) Etc. , etc. not earlier recognized . In some implementations , frame capture can be tuned to Camera control can also be responsive to spatial coordi- capture the steganographic calibration signals present in a nate information . By using geolocation data , and orientation digital watermark signal , without regard to successful 65', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 75}), Document(page_content='US 10,922,957 B2 \\n 27 28', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='decoding of the watermark payload data itself . For example , purchase , a posting on Facebook , a face- or object - recogni captured image data can be at a lower resolution sufficient tion operation , etc. Once such an indexed transaction to discern the calibration signals , but insufficient to discern arrangement is initially configured , it can be easily invoked the payload . Or the camera can expose the image without simply by sending a packet to the cloud containing the regard to human perception , e.g. , overexposing so image 5 image - related data , and an identifier indicating the desired highlights are washed - out , or underexposed so other parts of operation . ) the image are indistinguishable . Yet such an exposure may At the Apple service , for example , a server may examine be adequate to capture the watermark orientation signal . the incoming packet , look - up the user\\'s iPhoto account , ( Feedback can of course be employed to capture one or more access facial recognition data for the user\\'s friends from that subsequent image frames — redressing one or more short- 10 account , compute facial recognition features from image comings of a previous image frame . ) data conveyed with the packet , determine a best match , and Some digital watermarks are embedded in specific color return result information ( e.g. , a name of a depicted indi channels ( e.g. , blue ) , rather than across colors as modulation vidual ) back to the originating device . of image luminance ( see , e.g. , commonly - owned patent At the IP address for the Google service , a server may application Ser . No. 12 / 337,029 to Reed , published as 15 undertake similar operations , but would refer to the user\\'s US20100150434 ) . In capturing a frame including such a Picasa account . Ditto for Facebook . watermark , exposure can be selected to yield maximum Identifying a face from among faces for dozens or hun dynamic range in the blue channel ( e.g. , 0-255 in an 8 - bit dreds of known friends is easier than identifying faces of sensor ) , without regard to exposure of other colors in the strangers . Other vendors may offer services of the latter sort . image . One frame may be captured to maximize dynamic 20 For example , L - 1 Identity Solutions , Inc. maintains data range of one color , such as blue , and a later frame may be bases of images from government - issued credentials — such captured to maximize dynamic range of another color chan- as drivers \\' licenses . With appropriate permissions , it may nel , such as yellow ( i.e. , along the red - green axis ) . These offer facial recognition services drawing from such data frames may then be aligned , and the blue - yellow difference bases . determined . The frames may have wholly different exposure 25 Other processing operations can similarly be operated times , depending on lighting , subject , etc. remotely . One is a barcode processor , which would take Desirably , the system has an operational mode in which it processed image data sent from the mobile phone , apply a captures and processes imagery even when the user is not decoding algorithm particular to the type of barcode present . intending to \" snap \" a picture . If the user pushes a shutter A service may support one , a few , or dozens of different button , the otherwise - scheduled image capture / processing 30 types of barcode . The decoded data may be returned to the operations may be suspended , and a consumer photo taking phone , or the service provider can access further data mode can take precedence . In this mode , capture parameters indexed by the decoded data , such as product information , and processes designed to enhance human visual system instructions , purchase options , etc. , and return such further aspects of the image can be employed instead . data to the phone . ( Or both can be provided . ) ( It will be recognized that the particular embodiment 35 Another service is digital watermark reading . Another is shown', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='can be employed instead . data to the phone . ( Or both can be provided . ) ( It will be recognized that the particular embodiment 35 Another service is digital watermark reading . Another is shown in FIG . 16 generates packets before any image data optical character recognition ( OCR ) . An OCR service pro is collected . In contrast , FIG . 10A and associated discussion vider may further offer translation services , e.g. , converting do not refer to packets existing before the camera . Either processed image data into ASCII symbols , and then submit arrangement can be used in either embodiment . That is , in ting the ASCII words to a translation engine to render them FIG . 10A , packets may be established prior to the capture of 40 in a different language . Other services are sampled in FIG . image data by the camera , in which case the visual keyvector 2. ( Practicality prevents enumeration of the myriad other processing and packaging module serves to insert the pixel services , and component operations , that may also be pro data or more typically , sub - sets or super - sets of the pixel vided . ) data - into earlier - formed packets . Similarly , in FIG . 16 , the The output from the remote service provider is commonly packets need not be created until after the camera has 45 returned to the cell phone . In many instances the remote captured image data . ) service provider will return processed image data . In some As noted earlier , one or more of the processing stages can it may return ASCII or other such data . Sometimes , be remote from the cell phone . One or more pixel packets however , the remote service provider may produce other can be routed to the cloud ( or through the cloud ) for forms of output , including audio ( e.g. , MP3 ) and / or video processing . The results can be returned to the cell phone , or 50 ( e.g. , MPEG4 and Adobe Flash ) . forwarded to another cloud processing stage ( or both ) . Once Video returned to the cell phone from the remote provider back at the cell phone , one or more further local operations may be presented on the cell phone display . In some may be performed . Data may then be sent back out the implementations such video presents a user interface screen , cloud , etc. Processing can thus alternate between the cell inviting the user to touch or gesture within the displayed phone and the cloud . Eventually , result data is usually 55 presentation to select information or an operation , or issue presented to the user back at the cell phone . an instruction . Software in the cell phone can receive such Applicants expect that different vendors will offer com- user input and undertake responsive operations , or present peting cloud services for specialized processing tasks . For responsive information . example , Apple , Google and Facebook , may each offer In still other arrangements , the data provided back to the cloud - based facial recognition services . A user device would 60 cell phone from the remote service provider can include transmit a packet of processed data for processing . The JavaScript or other such instructions . When run by the cell header of the packet can indicate the user , the requested phone , the JavaScript provides a response associated with service , and_optionally - micropayment instructions . the processed data referred out to the remote provider . ( Again , the header could convey an index or other identifier Remote processing services can be provided under a by which a desired transaction is looked - up in a cloud 65 variety of different financial models . An Apple iPhone database , or which serves to arrange an operation , or a service plan may be bundled with a variety of remote sequence of processes for some transaction such as a services at no additional cost , e.g. , iPhoto - based facial cases', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 76}), Document(page_content='US 10,922,957 B2 \\n 29 30 \\n recognition . Other services may bill on a per - use , monthly One function performed by pipe manager 51 is to nego subscription , or other usage plans . tiate for needed communication resources . The cell phone Some services will doubtless be highly branded , and can employ a variety of communication networks and com marketed . Others may compete on quality ; others on price . mercial data carriers , e.g. , cellular data , WiFi , Bluetooth , As noted , stored data may indicate preferred providers for 5 etc. any or all of which may be utilized . Each may have its different services . These may be explicitly identified ( e.g. , own protocol stack . In one respect the pipe manager 51 send all FFT operations to the Fraunhofer Institute service ) , interacts with respective interfaces for these data channels or they can be specified by other attributes . For example , a determining the availability of bandwidth for different data cell phone user may direct that all remote service requests payloads . are to be routed to providers that are ranked as fastest in a 10 For example , the pipe manager may alert the cellular data periodically updated survey of providers ( e.g. , by Consum carrier local interface and network that there will be a ers Union ) . The cell phone can periodically check the published results for this information , or it can be checked payload ready for transmission starting in about 450 milli \\n dynamically when a service is requested . Another user may seconds . It may further specify the size of the payload ( e.g. , specify that service requests are to be routed to service 15 two megabits ) , its character ( e.g. , block data ) , and a needed \\n providers that have highest customer satisfaction scores quality of service ( e.g. , data throughput rate ) . It may also \\n again by reference to an online rating resource . Still another specify a priority level for the transmission , so that the \\n user may specify that requests should be routed to the interface and network can service such transmission ahead providers having highest customer satisfaction scores — but of lower - priority data exchanges , in the event of a conflict . only if the service is provided for free ; else route to the 20 The pipe manager knows the expected size of the payload lowest cost provider . Combinations of these arrangements , due to information provided by the control processor module and others , are of course possible . The user may , in a 36. ( In the illustrated embodiment , the control processor particular case , specify a particular service provider- module specifies the particular processing that will yield the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"trumping any selection that would be made by the stored payload , and so it can estimate the size of the resulting data ) . profile data . 25 The control processor module can also predict the character In still other arrangements the user's request for service of the data , e.g. , whether it will be available as a fixed block can be externally posted , and several service providers may or intermittently in bursts , the rate at which it will be express interest in performing the requested operation . Or provided for transmission , etc. The control processor mod the request can be sent to several specific service providers ule 36 can also predict the time at which the data will be for proposals ( e.g. , to Amazon , Google and Microsoft ) . The 30 ready for transmission . The priority information , too , is different providers ' responses ( pricing , other terms , etc. ) known by the control processor module . In some instances may be presented to the user , who selects between them , or the control processor module autonomously sets the priority a selection may be made automatically - based on previ- level . In other instances the priority level is dictated by the ously stored rules . In some cases , one or more competing user , or by the particular application being serviced . service providers can be provided user data with which they 35 For example , the user may expressly signal — through the start performing , or wholly perform , the subject operation cell phone's graphical user interface , or a particular appli before a service provider selection is finally made giving cation may regularly require , that an image - based action is such providers a chance to speed their response times , and to be processed immediately . This may be the case , for encounter additional real - world data . ( See , also , the earlier example , where further action from the user is expected discussion of remote service providers , including auction- 40 based on the results of the image processing . In other cases based services , e.g. , in connection with FIGS . 7-12 . ) the user may expressly signal , or a particular application As elsewhere indicated , certain external service requests may normally permit , that an image - based action can be may pass through a common hub ( module ) , which is respon- performed whenever convenient ( e.g. , when needed sible for distributing the requests to appropriate service resources have low or nil utilization ) . This may be the case , providers . Reciprocally , results from certain external service 45 for example , if a user is posting a snapshot to a social requests may similarly be routed through a common hub . networking site such as Facebook , and would like the image For example , payloads decoded by different service provid- annotated with names of depicted individuals through ers from different digital watermarks ( or payloads decoded facial recognition processing . Intermediate prioritization from different barcodes , or fingerprints computed from ( expressed by the user , or by the application ) can also be different content objects ) may be referred to a common hub , 50 employed , e.g. , process within a minute , ten minutes , an which may compile statistics and aggregate information hour , a day , etc. ( akin to Nielsen's monitoring services — surveying con- In the illustrated arrangement , the control processor mod sumer encounters with different data ) . Besides the decoded ule 36 informs the pipe manager of the expected data size , watermark data ( barcode data , fingerprint data ) , the hub may character , timing , and priority , so that the pipe manager can also ( or alternatively ) be provided with a quality or confi- 55 use same in negotiating for the desired service . ( In other dence metric associated with each decoding / computing embodiments , less or more information can be provided . ) operation . This may help reveal packaging issues , print If the carrier and interface can meet\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"associated with each decoding / computing embodiments , less or more information can be provided . ) operation . This may help reveal packaging issues , print If the carrier and interface can meet the pipe manager's issues , media corruption issues , etc. , that need consideration . request , further data exchange may ensue to prepare for the Pipe Manager data transmission and ready the remote system for the In the FIG . 16 implementation , communications to and 60 expected operation . For example , the pipe manager may from the cloud are facilitated by a pipe manager 51. This establish a secure socket connection with a particular com module ( which may be realized as the cell phone - side puter in the cloud that is to receive that particular data portion of the query router and response manager of FIG . 7 ) payload , and identify the user . If the cloud computer is to performs a variety of functions relating to communicating perform a facial recognition operation , it may prepare for the across a data pipe 52. ( It will be recognized that pipe 52 is 65 operation by retrieving from Apple / Google / Facebook the a data construct that may comprise a variety of communi- facial recognition features , and associated names , for friends cation channels . ) of the specified user .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 77}), Document(page_content=\"US 10,922,957 B2 \\n 31 32 \\n Thus , in addition to preparing a channel for the external These modes may be selected by the user in advance of communication , the pipe manager enables pre - warming of operating a shutter control , or after . In other arrangements , the remote computer , to ready it for the expected service plural shutter controls ( physical or GUI ) are provided for the request . ( The service may request may not follow . ) In some user - respectively invoking different of the available opera instances the user may operate the shutter button , and the 5 tions . ( In still other embodiments , the device infers what cell phone may not know what operation will follow . Will operation ( s ) is / are possibly desired , rather than having the the user request a facial recognition operation ? A barcode user expressly indicate same . ) decoding operation ? Posting of the image to Flickr or If the user at the business gathering takes a group shot Facebook ? In some cases the pipe manager or control depicting twelve individuals , and requests the names on an processor module — may pre - warm several processes . Or it 10 immediate basis , the pipeline manager 51 may report back may predict , based on past experience , what operation will to the control processor module ( or to application software ) be undertaken , and warm appropriate resources . ( E.g. , if the that the requested service cannot be provided . Due to a user performed facial recognition operations following the bottleneck or other constraint , the manager 51 may report last three shutter operations , there's a good chance the user will request facial recognition again . ) The cell phone may 15 that identification of only three of the depicted faces can be \\n actually start performing component operations for various accommodated within service quality parameters considered \\n of the possible functions before any has been selected to constitute an “ immediate ” basis . Another three faces may \\n particularly those operations whose results may be useful to be recognized within two seconds , and recognition of the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content='particularly those operations whose results may be useful to be recognized within two seconds , and recognition of the \\n several of the functions . Pre - warming can also include full set of faces may be expected in five seconds . ( This may resources within the cell phone : configuring processors , 20 be due to a constraint by the remote service provider , rather loading caches , etc. than the carrier , per se . ) The situation just reviewed contemplates that desired The control processor module 36 ( or application software ) resources are ready to handle the expected traffic . In another may respond to this report in accordance with an algorithm , situation the pipe manager may report that the carrier is or by reference to a rule set stored in a local or remote data unavailable ( e.g. , due to the user being in a region of 25 structure . The algorithm or rule set may conclude that for impaired radio service ) . This information is reported to facial recognition operations , delayed service should be control processor module 36 , which may change the sched accepted on whatever terms are available , and the user ule of image processing , buffer results , or take other respon should be alerted ( through the device GUI ) that there will be sive action . a delay of about N seconds before full results are available . If other , conflicting , data transfers are underway , the 30 Optionally , the reported cause of the expected delay may carrier or interface may respond to the pipe manager that the also be exposed to the user . Other service exceptions may be requested transmission cannot be accommodated , e.g. , at the requested time or with the requested quality of service . In handled differently — in some cases with the operation aborted or rescheduled or routed to a less - preferred provider , this case the pipe manager may report same to the control and / or with the user not alerted . processor module 36. The control processor module may 35 abort the process that was to result in the two megabit data In addition to considering the ability of the local device \\n service requirement and reschedule it for later . Alternatively , interface to the network , and the ability of the network / \\n the control processor module may decide that the two carrier , to handle the forecast data traffic ( within specified megabit payload may be generated as originally scheduled , parameters ) , the pipeline manager may also query resources and the results may be locally buffered for transmission 40 out in the cloud — to ensure that they are able to perform when the carrier and interface are able to do so . Or other whatever services are requested ( within specified param', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content=\"action may be taken . eters ) . These cloud resources can include , e.g. , data net Consider a business gathering in which a participant works and remote computers . If any responds in the nega gathers a group for a photo before dinner . The user may want tive , or with a service level qualification , this too can be all faces in the photo to be recognized immediately , so that 45 reported back to the control processor module 36 , so that they can be quickly reviewed to avoid the embarrassment of appropriate action can be taken . not recalling a colleague's name . Even before the user In response to any communication from the pipe manager operates the cell phone's user - shutter button the control 51 indicating possible trouble servicing the expected data processor module causes the system to process frames of flow , the control process 36 may issue corresponding image data , and is identifying apparent faces in the field of 50 instructions to the pipe manager and / or other modules , as view ( e.g. , oval shapes , with two seeming eyes in expected necessary . positions ) . These may be highlighted by rectangles on the In addition to the just - detailed tasks of negotiating in cell phone's viewfinder ( screen ) display . advance for needed services , and setting up appropriate data While current cameras have picture - taking modes based connections , the pipe manager can also act as a flow control on lens / exposure profiles ( e.g. , close - up , night - time , beach , 55 manager orchestrating the transfer of data from the differ landscape , snow scenes , etc ) , imaging devices may addi- ent modules out of the cell phone , resolving conflicts , and tionally ( or alternatively ) have different image - processing reporting errors back to the control processor module 36 . modes . One mode may be selected by the user to obtain While the foregoing discussion has focused on outbound names of people depicted in a photo ( e.g. , through facial data traffic , there is a similar flow inbound , back to the cell recognition ) . Another mode may be selected to perform 60 phone . The pipe manager ( and control processor module ) optical character recognition of text found in an image can help administer this traffic as well providing services frame . Another may trigger operations relating to purchasing complementary to those discussed in connection with out a depicted item . Ditto for selling a depicted item . Ditto for bound traffic . obtaining information about a depicted object , scene or In some embodiments , there may be a pipe manager person ( e.g. , from Wikipedia , a social network , a manufac- 65 counterpart module 53 out in the cloud cooperating with turer's web site ) , etc. Ditto for establishing a ThinkPipe pipe manager 51 in the cell phone in performance of the session with the item , or a related system . Etc. detailed functionality .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 78}), Document(page_content='US 10,922,957 B2 \\n 33 34 \\n Software Embodiment of Control Processor & Pipe Man- specific — based on context or user desires ( in a photo taking \\n ager mode , face detection may be presumed . ) Research in the area of autonomous robotics shares some In a particular arrangement , keyvectors from each sensor similar challenges with the scenarios described herein , spe- are created and packaged by device driver software pro cifically that of enabling a system of sensors to communicate 5 cesses that abstract the hardware specific embodiments of \\n data to local and remote processes , resulting in action to be the sensor and provide a fully formed keyvector adhering to \\n taken locally . In the case of a robotics it involves moving a a selected protocol . \\n robot out of harm\\'s way ; in the present case it is most The device driver software can then place the formed \\n commonly focused on providing a desired experience based keyvector on an output queue unique to that sensor , or in a \\n on image , sound , etc. encountered . 10 common message queue shared by all the sensors . Regard \\n As opposed to performing simple operations such as less of approach , local processes can consume the keyvec tors and perform the needed operations before placing the obstacle avoidance , aspects of the present technology desire resultant keyvectors back on the queue . Those keyvectors to provide higher levels of semantics , and hence richer that are to be processed by remote services are then placed experiences , based on sensory input . A user pointing a 15 in packets and transmitted directly to a remote processes for camera at a poster does not want to know the distance to the additional processing or to a remote service that distributes wall ; the user is much more inclined to want to know the the keyvectors — similar to a router . It should be clear to the about the content on the poster , if it concerns a movie , where reader , that commands to initialize or setup any of the it is playing , reviews , what their friends think , etc. sensors or processes in the system can be distributed in a Despite such differences , architectural approaches from 20 similar fashion from a Control Process ( e.g. , box 36 in FIG . robotic toolkits can be adapted for use in the present context . 16. ) One such robotic toolkit is such as the Player Project — a set Branch Prediction ; Commercial Incentives of free software tools for robot and sensor applications , The technology of branch prediction arose to meet the available as open source from sourceforge - dot - net . needs of increasingly complex processor hardware ; it allows An illustration of the Player Project architecture is shown 25 processors with lengthy pipelines to fetch data and instruc in FIG . 19A . The mobile robot ( which typically has a tions ( and in some cases , execute the instructions ) , without relatively low performance processor ) communicates with a waiting for conditional branches to be resolved . fixed server ( with a relatively higher performance processor ) A similar science can be applied in the present context using a wireless protocol . Various sensor peripherals are predicting what action a human user will take . For example , coupled to the mobile robot ( client ) processor through 30 as discussed above , the just - detailed system may \" pre respective drivers , and an API . Likewise , services may be warm ” certain processors , or communication channels , in invoked by the server processor from software libraries , anticipation that certain data or processing operations will be through another API . ( The CMU CMVision library is shown forthcoming', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 79}), Document(page_content=\"in FIG . 19A . ) When a user removes an iPhone from her purse ( exposing ( In addition to the basic tools for interfacing robotic 35 the sensor to increased light ) and lifts it to eye level ( as equipment to sensors and service libraries , the Player Project sensed by accelerometers ) , what is she about to do ? Refer includes “ Stage ” software that simulates a population of ence can be made to past behavior to make a prediction . mobile robots moving in a 2D environment , with various Particularly relevant may include what the user did with the sensors and processing — including visual blob detection . phone camera the last time it was used ; what the user did “ Gazebo ” extends the Stage model to 3D . ) 40 with the phone camera at about the same time yesterday ( and By such system architecture , new sensors can quickly be at the same time a week ago ) ; what the user last did at about utilized by provision of driver software that interfaces with the same location , etc. Corresponding actions can be taken the robot API . Similarly , new services can be readily in anticipation . plugged in through the server API . The two Player Project If her latitude / longitude correspond to a location within a APIs provide standardized abstractions so that the drivers 45 video rental store , that helps . Expect to maybe perform and services do not need to concern themselves with the image recognition on artwork from a DVD box . To speed particular configuration of the robot or server ( and vice- possible recognition , perhaps SIFT or other feature recog \\n versa ) . nition reference data should be downloaded for candidate ( FIG . 20A , discussed below , also provides a layer of DVDs and stored in a cell phone cache . Recent releases are abstraction between the sensors , the locally - available opera- 50 good prospects ( except those rated G , or rated high for tions , and the externally - available operations . ) violence — stored profile data indicates the user just doesn't Certain embodiments of the present technology can be have a history of watching those ) . So are movies that she's implemented using a local process & remote process para- watched in the past ( as indicated by historical rental digm akin to that of the Player Project , connected by a records also available to the phone ) . packet network and inter - process & intra - process commu- 55 If the user's position corresponds to a downtown street , nication constructs familiar to artisans ( e.g. , named pipes , and magnetometer and other position data indicates she is sockets , etc. ) . Above the communication minutiae is a looking north , inclined up from the horizontal , what's likely protocol by which different processes may communicate ; to be of interest ? Even without image data , a quick reference this may take the form of a message passing paradigm and to online resources such as Google Streetview can suggest message queue , or more of a network centric approach 60 she's looking at business signage along 5th Avenue . Maybe where collisions of keyvectors are addressed after the fact feature recognition reference data for this geography should ( re - transmission , drop if timely in nature , etc. ) . be downloaded into the cache for rapid matching against In such embodiments , data from sensors on the mobile to - be - acquired image data . device ( e.g. , microphone , camera ) can be packaged in To speed performance , the cache should be loaded in a keyvector form , with associated instructions . The 65 rational fashion so that the most likely object is considered instruction ( s ) associated with data may not be express ; they first . Google Streetview for that location includes metadata can be implicit ( such as Bayer conversion ) or session indicating 5th Avenue has signs for a Starbucks , a Nordstrom\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 79}), Document(page_content='presented US 10,922,957 B2 \\n 35 36', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='store , and a Thai restaurant . Stored profile data for the user sponding Amazon link atop the list ( as may occur with a reveals she visits Starbucks daily ( she has their branded regular Google search ) , the cell phone recognizes that the loyalty card ) ; she is a frequent clothes shopper ( albeit with user is located in an independent bookstore . Context - based a Macy\\'s , rather than a Nordstrom\\'s charge card ) ; and she\\'s rules consequently dictate that it present a non - commercial never eaten at a Thai restaurant . Perhaps the cache should be 5 link first . Top ranked of this type is a Wall Street Journal loaded so as to most quickly identify the Starbucks sign , review of the book , which goes to the top of the followed by Nordstrom , followed by the Thai restaurant . list of links Decorum , however , only goes so far . The cell Low resolution imagery captured for presentation on the phone passes the book title or ISBN ( or the image itself ) to viewfinder fails to trigger the camera\\'s feature highlighting Google AdSense or AdWords , which identifies sponsored probable faces ( e.g. , for exposure optimization purposes ) . 10 links to be associated with that object . ( Google may inde That helps . There\\'s no need to pre - warm the complex pendently perform its own image analysis on any provided processing associated with facial recognition . imagery . In some cases it may pay for such cell phone She touches the virtual shutter button , capturing a frame submitted imagery — since Google has a knack for exploiting of high resolution imagery , and image analysis gets under- data from diverse sources . ) Per Google , Barnes and Noble way trying to recognize what\\'s in the field of view , so that 15 has the top sponsored position , followed by alldiscount the camera application can overlay graphical links related to books - dot - net . The cell phone application may present these objects in the captured frame . ( Or this may happen without sponsored links in a graphically distinct manner to indicate user action — the camera may be watching proactively . ) their origin ( e.g. , in a different part of the display , or In one particular arrangement , visual “ baubles ” ( FIG . O ) presented in a different color ) , or it may insert them alter are overlaid on the captured imagery . Tapping on any of the 20 nately with non - commercial search results , i.e. , at positions baubles pulls up a screen of information , such as a ranked two and four . The AdSense revenue collected by Google can list of links Unlike Google web search which ranks search again be shared with the user , or with the user\\'s carrier . results in an order based on aggregate user data , the camera In some embodiments , the cell phone ( or Google ) again application attempts a ranking customized to the user\\'s pings the servers of companies for whom links will be profile . If a Starbucks sign or logo is found in the frame , the 25 presented helping them track their physical world - based Starbucks link gets top position for this user . online visibility . The pings can include the location of the If signs for Starbucks , Nordstrom , and the Thai restaurant user , and an identification of the object that prompted the are all found , links would normally be presented in that ping . When alldiscountbooks - dot - net receives the ping , it order ( per the user\\'s preferences inferred from profile data ) . may check inventory and find it has a significant overstock However , the cell phone application may have a capitalistic 30 of Snowball . As in the example earlier given , it may offer an bent and be willing to promote a link by a position or two extra payment for some extra promotion ( e.g. , including ( although perhaps not to the top position ) if circumstances “ We have 732 copies cheap ! \" in the presented link ) . warrant . In the present case , the cell phone routinely IP In addition to offering an incentive for a more prominent packets to the web servers at', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='have 732 copies cheap ! \" in the presented link ) . warrant . In the present case , the cell phone routinely IP In addition to offering an incentive for a more prominent packets to the web servers at addresses associated with each search listing ( e.g. , higher in the list , or augmented with of the links , alerting them that an iPhone user had recog- 35 additional information ) , a company may also offer addi nized their corporate signage from a particular latitude / tional bandwidth to serve information to a customer . For longitude . ( Other user data may also be provided , if privacy example , a user may capture video imagery from an elec considerations and user permissions allow . ) The Thai res- tronic billboard , and want to download a copy to show to taurant server responds back in an instant offering to the friends . The user\\'s cell phone identifies the content as a next two customers 25 % off any one item ( the restaurant\\'s 40 popular clip of user generated content ( e.g. , by reference to point of sale system indicates only four tables are occupied an encoded watermark ) , and finds that the clip is available and no order is pending ; the cook is idle ) . The restaurant from several sites — the most popular of which is YouTube , server offers three cents if the phone will present the followed by MySpace . To induce the user to link to discount offer to the user in its presentation of search results , MySpace , MySpace may offer to upgrade the user\\'s baseline or five cents if it will also promote the link to second place 45 wireless service from 3 megabits per second to 10 megabits in the ranked list , or ten cents if it will do that and be the only per second , so the video will download in a third of the time . discount offer presented in the results list . ( Starbucks also This upgraded service can be only for the video download , responded with an incentive , but not as attractive ) . The cell or it can be longer . The link presented on the screen of the phone quickly accepts the restaurant\\'s offer , and payments user\\'s cell phone can be amended to highlight the availabil are quickly made either to the user ( e.g. , defraying the 50 ity of the faster service . ( Again , MySpace may make an monthly phone bill ) or more likely to the phone carrier ( e.g. , associated payment . ) AT & T ) . Links are presented to Starbucks , the Thai restau- Sometimes alleviating a bandwidth bottleneck requires rant , and Nordstrom , in that order , with the restaurant\\'s link opening a bandwidth throttle on a cell phone end of the noting the discount for the next two customers . wireless link Or the bandwidth service change must be Google\\'s AdWord technology has already been noted . It 55 requested , or authorized , by the cell phone . In such case decides , based on factors including anauction determined MySpace can tell the cell phone application to take needed payment , which ads to present as Sponsored Links adjacent steps for higher bandwidth service , and MySpace will rebate the results of a Google web search . Google has adapted this to the user ( or to the carrier , for benefit of the user\\'s account ) technology to present ads on third party web sites and blogs , the extra associated costs . based on the particular contents of those sites , terming the 60 In some arrangements , the quality of service ( e.g. , band service AdSense . width ) is managed by pipe manager 51. Instructions from In accordance with another aspect of the present technol- MySpace may request that the pipe manager start requesting ogy , the AdWord / AdSense technology is extended to visual augmented service quality , and setting up the expected high image search on cell phones . bandwidth session , even before the user selects the MySpace Consider a user located in a small bookstore who snaps a 65 link . picture of the Warren Buffet biography Snowball . The book In some scenarios , vendors may negotiate preferential is quickly recognized , but rather', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='located in a small bookstore who snaps a 65 link . picture of the Warren Buffet biography Snowball . The book In some scenarios , vendors may negotiate preferential is quickly recognized , but rather than presenting a corre- bandwidth for its content . MySpace may make a deal with', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 80}), Document(page_content='US 10,922,957 B2 \\n 37 38', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"AT & T , for example , that all MySpace content delivered to when the user approaches or enters the museum . ( Of equal AT & T phone subscribers be delivered at 10 megabits per importance is the role that CTR plays in monetizing the second even though most subscribers normally only experience and environment . ) receive 3 megabits per second service . The higher quality Consider a school group that enters a sculpture museum service may be highlighted to the user in the presented links . 5 having a garden with a collection of Rodin works . The From the foregoing it will be recognized that , in certain museum may provide content related to Rodin and his works embodiments , the information presented by a mobile phone on servers or infrastructure ( e.g. , router caches ) that serve in response to visual stimuli is a function both of ( 1 ) the the garden . Moreover , because the visitors comprise a pre user's preferences , and ( 2 ) third party competition for that established social group , the museum may expect some user's attention , probably based on the user's demographic 10 social connectivity . So the museum may enable sharing profile . Demographically identical users , but with different capabilities ( e.g. , ad hoc networking ) that might not other tastes in food , will likely be presented with different baubles , wise be used . If one student queries the museum's online or associated information , when viewing a street crowded content to learn more about a particular Rodin sculpture , the with restaurants . Users with identical tastes in food and other system may accompany delivery of the solicited information preference information — but differing in a demographic 15 with a prompt inviting the student to share this information factor ( e.g. , age , gender ) may likewise be presented with with others in the group . The museum server can suggest different baubles / information , because vendors , etc. , are particular “ friends ” of the student with whom such infor willing to pay differently for different demographic eyeballs . mation might be shared if such information is publicly Modeling of User Behavior . accessible from Facebook or other social networking data Aided by knowledge of a particular physical environment , 20 source . In addition to names of friends , such a social a specific place and time , and behavior profiles of expected networking data source can also provide device identifiers , users , simulation models of human computer interaction IP addresses , profile information , etc. , for the student's with the physical world can be based on tools and techniques friends which may be leveraged to assist the dissemination from fields as disperse as robotics , and audience measure- of educational material to others in the group . These other ment . An example of this might be the number of expected 25 students may find this particular information relevant , since mobile devices in a museum at a particular time ; the it was of interest to another in their group even if the particular sensors that such devices are likely to be using ; original student's name is not identified . If the original and what stimuli are expected to be captured by those student is identified with the conveyed information , then this sensors ( e.g. , where are they pointing the camera , what is the may heighten the information's interest to others in the microphone hearing , etc. ) . Additional information can 30 group . include assumptions about social relationships between ( Detection of a socially - linked group may be inferred users : Are they likely to share common interests ? Are they from review of the museum's network traffic . For example , within common social circles that are likely to share content , if a device sends packets of data to an and the to share experiences , or desire creating location - based expe- museum's network handles both ends of the communica riences such as wiki - maps ( c.f. , Barricelli , “ Map -\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"of data to an and the to share experiences , or desire creating location - based expe- museum's network handles both ends of the communica riences such as wiki - maps ( c.f. , Barricelli , “ Map - Based 35 tion_dispatch and delivery , then there's an association Wilds as Contextual and Cultural Mediators , ” MobileHCI , between two devices in the museum . If the devices are not 2009 ) ? ones that have historical patterns of network usage , e.g. , In addition , modeling can be based on generalized heu- employees , then the system can conclude that two visitors to ristics derived from observations at past events ( e.g. , how the museum are socially connected . If a web of such many people used their cell phone cameras to capture 40 communications is detected involving several unfamiliar imagery from the Portland Trailblazers ' scoreboard during a devices , then a social group of visitors can be discerned . The basketball game , etc. ) , to more evolved predictive models size of the group can be gauged by the number of different that are based on innate human behavior ( e.g. , people are participants in such network traffic . Demographic informa more likely to capture imagery from a scoreboard during tion about the group can be inferred from external addresses overtime than during a game's half - time ) . 45 with which data is exchanged ; middle schoolers may have a Such models can inform many aspects of the experience high incidence of MySpace traffic ; college students may for the users , in addition to the business entities involved in communicate with external addresses at a university provisioning and measuring the experience . domain ; senior citizens may demonstrate a different traffic These latter entities may consist of the traditional value profile . All such information can be employed in automati chain participants involved in event production , and the 50 cally adapting the information and services provided to the arrangements involved in measuring interaction and mon- visitors as well as providing useful information to the etizing it . Event planners , producers , artists on the creation museum's administration and marketing departments . )\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content='side and the associated rights societies ( ASCAP , Directors Consider other situations . One is halftime at the U.S. Guild of America , etc. ) on the royalties side . From a football Superbowl , featuring a headline performer ( e.g. , measurement perspective , both sampling - based techniques 55 Bruce Springsteen , or Prince ) . The show may cause hun from opt - in users and devices , and census - driven techniques dreds of fans to capture pictures or audio - video of the event . can be utilized . Metrics for more static environments may Another context with predictable public behavior is the end consist of Revenue Per Unit ( RPU ) created by digital traffic of an NBA championship basketball game . Fans may want created on the digital service provider network ( how much to memorialize the final buzzer excitement : the scoreboard , bandwidth is being consumed ) to more evolved models of 60 streamers and confetti dropping from the ceiling , etc. In such Click Through Rates ( CTR ) for particular sensor stimuli . cases , actions that can be taken to prepare , or optimize , For example , the Mona - Lisa painting in the Louvre is delivery of content or experience should be taken . Examples likely to have a much higher CTR than other paintings in the include rights clearance for associated content , rendering museum , informing matters such as priority for content virtual worlds and other synthesized content , throttling provisioning , e.g. , content related to the Mona Lisa should 65 down routine time - insensitive network traffic , queuing com be cached and be as close to the edge of the cloud as mercial resources that may be invoked as people purchase possible , if not pre - loaded onto the mobile device itself souvenir books / music from Amazon ( caching pages , authen', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 81}), Document(page_content=\"10 \\n 15 US 10,922,957 B2 \\n 39 40 \\n ticating users to financial sites ) , propagating links for post- This may be accomplished on the device as well , through game interviews ( some prebuilt / edited and ready to go ) , the use of content rules , such as the Movielabs Content caching the Twitter feeds of the star players , buffering video Recognition Rules related to conflicting media content ( c.f. , from city center showing the hometown crowds watching on www.movielabs - dot - com / CRR ) , parental controls provided a Jumbotron display - erupting with joy at the buzzer , etc .; 5 by carriers to the device , or by adhering to DMCA Auto anything relating to the experience or follow - on actions matic Take Down Notices . should prepped / cached in advance , where possible . Under various rights management paradigms , licenses \\n Stimuli for sensors ( audio , visual , tactile , odor , etc. ) that play a key role in determining how content can be con are most likely to instigate user action and attention are sumed , shared , modified etc. A result of extracting semantic much more valuable from a commercial standpoint than meaning from stimulus presented to the user ( and the user's stimuli less likely to instigate such action ( similar to the mobile device ) , and / or the location in which stimulus is economic principles on which Google's Adwords ad - serving presented , can be issuance of a license to desired content or system is based ) . Such factors and metrics directly influence experiences ( games , etc. ) by third parties . To illustrate , advertising models through auction models well understood consider a user at a rock concert in an arena . The user may by those in the art . be granted a temporary license to peruse and listen to all Multiple delivery mechanisms exist for advertising deliv- music tracks by the performing artist ( and / or others ) on ery by third parties , leveraging known protocols such as iTunes — beyond the 30 second preview rights normally VAST . VAST ( Digital Video Ad Serving Template ) is a granted to the public . However , such license may only standard issued by the Interactive Advertising Bureau that 20 persist during the concert , or only from when the doors open establishes reference communication protocols between until the headline act begins its performance , or only while scriptable video rendering systems and ad servers , as well as the user is in the arena , etc. Thereafter , such license ends . associated XML schemas . As an example , VAST helps Similarly , passengers disembarking from an international standardize the service of video ads to independent web sites flight may be granted location - based or time - limited licenses ( replacing old - style banner ads ) , commonly based on a bit of 25 to translation services or navigation services ( e.g. , an aug Javascript included in the web page code code that also mented reality system overlaying directions for baggage aids in tracking traffic and managing cookies . VAST can also claim , bathrooms , etc. , on camera - captured scenes ) for their insert promotional messages in the pre - roll and post - roll mobile devices , while they transit through customs , are in viewing of other video content delivered by the web site . the airport , for 90 minutes after their arrival , etc. The web site owner doesn't concern itself with selling or 30 Such arrangements can serve as metaphors for experi running the advertisements , yet at the end of the month the ence , and as filtering mechanisms . One embodiment in web site owner receives payment based on audience view which sharing of experiences are triggered by sensor stimuli ership / impressions . In similar fashion , physical stimuli pre sented to users in the real world , sensed by mobile technol is through broadcast social networks ( e.g. , Twitter ) and ogy , can be the basis for payments to the parties involved . 35 syndication protocols ( e.g. , RSS web feeds / channels ) . Other\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 82}), Document(page_content=\"Dynamic environments in which stimulus presented to users , entities or devices can subscribe to such broadcasts / \\n users and their mobile devices can be controlled ( such as feeds as the basis for subsequent communication ( social , \\n video displays , as contrasted with static posters ) provide information retrieval , etc. ) , as logging of activities ( e.g. , a new opportunities for measurement and utilization of met- person's daily journal ) , or measurement ( audience , etc. ) . \\n rics such as CTR . 40 Traffic associated with such networks / feeds can also be Background music , content on digital displays , illumina- measured by devices at a particular location allowing users tion , etc. , can be modified to maximize CTR and shape to traverse in time to understand who was communicating traffic . For example , illumination on particular signage can what at a particular point in time . This enables searching for be increased , or flash , as a targeted individual passes by . and mining additional information , e.g. , was my friend here Similarly when a flight from Japan lands at the airport , 45 last week ? Was someone from my peer group here ? What digital signage , music , etc. can all be modified overtly content was consumed ? Such traffic also enables real - time ( change in the advertising to the interests of the expected monitoring of how users share experiences . Monitoring audience ) or covertly ( changing the linked experience to “ tweets ” about a performer's song selection during a concert take the user to the Japanese language website ) , to maximize may cause the performer to alter the songs to be played for the CTR . 50 the remainder of a concert . The same is true for brand Mechanisms may be introduced as well to contend with management . For example , if users share their opinions rogue or un - approved sensor stimuli . Within the confined about a car during a car show , live keyword filtering on the spaces of a university of business park , stimuli ( posters , traffic can allow the brand owner to re - position certain music , digital signage , etc. ) that don't adhere to the inten- products for maximum effect ( e.g. , the new model of Cor tions or policies of the property owner- or the entity respon- 55 vette should spend more time on the spinning platform , etc. ) . sible for a domain may need to be managed . This can be More on Optimization accomplished through the use of simple blocking mecha- Predicting the user's action or intent is one form of nisms that are geography - specific ( not dissimilar to region optimization . Another form involves configuring the pro coding on DVD's ) , indicating that all attempts within spe- cessing so as to improve performance . cific GPS coordinates to route a keyvector to a specific place 60 To illustrate one particular arrangement , consider again in the cloud must be mediated by a routing service or the Common Services Sorter of FIG . 6. What keyvector gateway managed by the domain owner . operations should be performed locally , or remotely , or as a Other options include filtering the resultant experience . Is hybrid of some sort ? In what order should keyvector opera it age appropriate ? Does it run counter to pre - existing tions be performed ? Etc. The mix of expected operations , advertising or branding arrangements , such as a Coca Cola 65 and their scheduling , should be arranged in an appropriate advertisement being delivered to a user inside the Pepsi fashion for the processing architecture being used , the center during a Denver Nuggets game . circumstances , and the context .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 82}), Document(page_content='10 \\n 15 US 10,922,957 B2 \\n 41 42 \\n One step in the process is to determine which operations service providers for the duration of the concert . In this case , need to occur . This determination can be based on express services normally routed for external execution should be requests from the user , historical patterns of usage , context performed locally . \\n and status , etc. Yet another factor is the particular hardware with which Many operations are high level functions , which involve 5 the cell phone is equipped . If a dedicated FFT processor is a number of component operations performed in a par- available in the phone , then performing intensive FFT ticular order . For example , optical character recognition may operations locally makes sense . If only a feeble general require edge detection , followed by region - of - interest seg- purpose CPU is available , then an intensive FFT operation mentation , followed by template pattern matching . Facial is probably best referred out for external execution . recognition may involve skintone detection , Hough trans- A related factor is current hardware utilization . Even if a forms ( to identify oval - shaped areas ) , identification of fea- cell phone is equipped with hardware that is well configured ture locations ( pupils , corners of mouth , nose ) , eigenface for a certain task , it may be so busy and backlogged that the calculation , and template matching . system may refer a next task of this sort to an external The system can identify the component operations that resource for completion . may need to be performed , and the order in which their Another factor may be the length of the local processing respective results are required . Rules and heuristics can be chain , and the risk of a stall . Pipelined processing architec applied to help determine whether these operations should tures may become stalled for intervals as they wait for data be performed locally or remotely . needed to complete an operation . Such a stall can cause all For example , at one extreme , the rules may specify that 20 other subsequent operations to be similarly delayed . The risk simple operations , such as color histograms and threshold- of a possible stall can be assessed ( e.g. , by historical ing , should generally be performed locally . At the other patterns , or knowledge that completion of an operation extreme , complex operations may usually default to outside requires further data whose timely availability is not providers . assured — such as a result from another external process ) Scheduling can be determined based on which operations 25 and , if the risk is great enough , the operation may be referred are preconditions to other operations . This can also influence for external processing to avoid stalling the local processing chain . whether an operation is performed locally or remotely ( local performance may provide quicker results — allowing subse Yet another factor is connectivity status . Is a reliable , high \\n quent operations to be started with less delay ) . The rules may speed network connection established ? Or are packets seek to identify the operation whose output ( s ) is used by the 30 dropped , or network speed slow ( or wholly unavailable ) ? greatest number of subsequent operations , and perform this Geographical considerations of different sorts can also be', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content='operation first ( its respective precedent ( s ) permitting ) . factors . One is network proximity to the service provider . Another is whether the cell phone has unlimited access to the Operations that are preconditions to successively fewer network ( as in a home region ) , or a pay - per - use arrangement other operations are performed successively later . The 35 ( as when roaming in another country ) . operations , and their sequence , may be conceived as a tree Information about the remote service provider ( s ) can also structure — with the most globally important performed first , be factored . Is the service provider offering immediate and operations of lesser relevance to other operations per turnaround , or are requested operations placed in a long formed later . queue , behind other users awaiting service ? Once the pro Such determinations , however , may also be tempered ( or 40 vider is ready to process the task , what speed of execution dominated ) by other factors . One is power . If the cell phone is expected ? Costs may also be key factors , together with battery is low , or if an operation will involve a significant other attributes of importance to the user ( e.g. , whether the drain on a low capacity battery , this can tip the balance in service provider meets “ green ” standards of environmental favor of having the operation performed remotely . responsibility ) . A great many other factors can also be Another factor is response time . In some instances , the 45 considered , as may be appropriate in particular contexts . limited processing capability of the cell phone may mean Sources for such data can include the various elements that processing locally is slower than processing remotely shown in the illustrative block diagrams , as well as external ( e.g. , where a more robust , parallel , architecture might be available to perform the operation ) . In other instances , the A conceptual illustration of the foregoing is provided in delays of establishing communication with a remote server , 50 FIG . 19B . and establishing a session , may make local performance of Based on the various factors , a determination can be made an operation quicker . Depending on user demand , and needs as to whether an operation should be performed locally , or of other operation ( s ) , the speed with which results are remotely . ( The same factors may be assessed in determining returned may be important , or not . the order in which operations should be performed . ) Still another factor is user preferences . As noted else- 55 In some embodiments , the different factors can be quan where , the user may set parameters influencing where , and tified by scores , which can be combined in polynomial when , operations are performed . For example , a user may fashion to yield an overall score , indicating how an opera specify that an operation may be referred to remote process- tion should be handled . Such an overall score serves as a ing by a domestic service provider , but if none is available , metric indicating the relative suitability of the operation for then the operation should be performed locally . 60 remote or external processing . ( A similar scoring approach Routing constraints are another factor . Sometimes the cell can be employed to choose between different service pro phone will be in a WiFi or other service area ( e.g. , in a viders . ) concert arena ) in which the local network provider places Depending on changing circumstances , a given operation limits or conditions on remote service requests that may be may be performed locally at one instant , and performed accessed through that network . In a concert where photog- 65 remotely at a later instant ( or vice versa ) . Or , the same raphy is forbidden , for example , the local network may be operation may be performed on two sets of keyvector data configured to block access to external image processing at the same time one locally , and one remotely .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content='example , the local network may be operation may be performed on two sets of keyvector data configured to block access to external image processing at the same time one locally , and one remotely . resources .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 83}), Document(page_content=\"5 \\n 20 US 10,922,957 B2 \\n 43 44 \\n While described in the context of determining whether an opportunity cost , given the current state of the device , operation should be performed locally or remotely , the same e.g. , what other processes should take priority such factors can influence other matters as well . For example , as a voice call , GPS navigation , etc. they can also be used in deciding what information is user preferences , i.e. , I want a “ green ” provider , or open conveyed by keyvectors . source provider Consider a circumstance in which the cell phone is to legal uncertainties ( e.g. , certain providers may be at perform OCR on captured imagery . With one set of factors , greater risk of patent infringement charges , e.g. , due unprocessed pixel data from a captured image may be sent to their use of an allegedly patented method ) to a remote service provider to make this determination . Domain owner influence : Under a different set of factors , the cell phone may perform 10 privacy concerns in specific physical arenas such as no initial processing , such as edge detection , and then package face recognition at schools the edge - detected data in keyvector form , and route to an pre - determined content based rules prohibiting specific external provider to complete the OCR operation . Under still operations against specific stimuli another set of factors , the cell phone may perform all of the voiceprint matching against broadcast songs highlight component OCR operations up until the last ( template 15 ing the use other singers ( Milli - Vanilli's Grammy matching ) , and send out data only for this last operation . award was revoked when officials discovered that the ( Under yet another set of factors , the OCR operation may be actual vocals on the subject recording were per completed wholly by the cell phone , or different components formed by other singers ) of operation can be performed alternately by the cell phone All of the above influence scheduling and ability to and remote service provider ( s ) , etc. ) perform out of order execution of keyvectors based on Reference was made to routing constraints as one possible the optimal path to the desired outcome factor . This is a particular example of a more general uncertainty in a long chain of operations , making \\n factor - external business rules . Consider the earlier prediction of need for subsequent keyvector opera example of a user who is attending an event at the Pepsi tions difficult ( akin to the deep pipeline in processors Center in Denver . The Pepsi Center may provide wireless 25 & branch prediction ) _difficulties might be due to communication services to patrons , through its own WiFi or weak metrics on keyvectors other network . Naturally , the Pepsi Center is reluctant for its past behavior . network resources to be used for the benefit of competitors , location ( GPS indicates that the device is quick such as Coca Cola . The host network may thus influence motion ) & pattern of GPS movements cloud services that can be utilized by its patrons ( e.g. , by 30 is there a pattern of exposure to stimuli , such as a making some inaccessible , or by giving lower priority to user walking through an airport terminal being\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 84}), Document(page_content=\"data traffic of certain types , or with certain destinations ) . The repeatedly exposed to CNN that is being pre domain owner may exe control over what operations a sented at each gate mobile device is capable of performing . This control can proximity sensors indicating the device was placed influence the local / remote decision , as well as the type of 35 in a pocket , etc. data conveyed in keyvector packets . other approaches such as Least Recently Used Another example is a gym , which may want to impede ( LRU ) can be used to track how infrequent the usage of cell phone cameras , e.g. , by interfering with access desired keyvector operation resulted or contrib to remote service providers for imagery , as well as photo uted to the desired effect ( recognition of a song , sharing sites such as Flickr and Picasa . Still another example 40 etc. ) is a school which , for privacy reasons , may want to discour- Further regarding pipelined or other time - consuming age facial recognition of its students and staff . In such case , operations , a particular embodiment may undertake some access to facial recognition service providers can be suitability testing before engaging a processing resource for blocked , or granted only on a moderated case - by - case basis . what may be more than a threshold number of clock cycles . Venues may find it difficult to stop individuals from using 45 A simple suitability test is to make sure the image data is cell phone cameras- or using them for particular purposes , potentially useful for the intended purpose , as contrasted but they can take various actions to impede such use ( e.g. , with data that can be quickly disqualified from analysis . For by denying services that would promote or facilitate such example , whether it is all black ( e.g. , a frame captured in the use ) . user's pocket ) . Adequate focus can also be checked quickly The following outline identifies other factors that may be 50 before committing to an extended operation . relevant in determining which operations are performed ( The artisan will recognize that certain of the aspects of where , and in what sequence : this technology discussed above have antecedents visible in 1. Scheduling optimization of keyvector processing units hindsight . For example , considerable work has been put into \\n based on numerous factors : instruction optimization for pipelined processors . Also , Operation mix , which operations consist of similar atomic 55 some devices have allowed user configuration of power instructions ( MicroOps , Pentium II etc. ) settings , e.g. , user - selectable deactivation of a power - hungry Stall states , which operations will generate stalls due to : GPU in certain Apple notebooks to extend battery life . ) waiting for external keyvector processing The above - discussed determination of an appropriate poor connectivity instruction mix ( e.g. , by the Common Services Sorter of user input 60 FIG . 6 ) particularly considered certain issues arising in change in user focus pipelined architectures . Different principles can apply in Cost of operation based on : embodiments in which one or more GPUs is available . published cost These devices typically have hundreds or thousands of expected cost based on state of auction scalar processors that are adapted for parallel execution , so state of battery and power mode 65 that costs of execution ( time , stall risk , etc. ) are small . power profile of the operation ( is it expensive ? ) Branch prediction can be handled by not predicting : instead , past history of power consumption the GPU processes for all of the potential outcomes of a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 84}), Document(page_content='US 10,922,957 B2 \\n 45 46', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content=\"branch in parallel , and the system uses whatever output packets . This intensity data can be applied to an output 33 of corresponds to the actual branch condition when it becomes that stage . With the image data , each packet can convey a known . timestamp indicating the particular time ( absolute , or based To illustrate , consider facial recognition . A GPU- on a local clock ) at which the image data was captured . This equipped cell phone may invoke instructions — when its 5 time data , too , can be provided on output 33 . camera is activated in a user photo - shoot mode — that con- A synchronization processor 35 coupled to such an output figure 20 clusters of scalar processors in the GPU . ( Such a 33 can examine the variation in frame - to - frame intensity ( or cluster is sometimes termed a “ stream processor . ” ) In par- color ) , as a function of timestamp data , to discern its ticular , each cluster is configured to perform a Hough periodicity . Moreover , this module can predict the next time transform on a small tile from a captured image frame 10 instant at which the intensity ( or color ) will have a maxima , looking for one or more oval shapes that may be candidate minima , or other particular state . A phase - locked loop may faces . The GPU thus processes the entire frame in parallel , control an oscillator that is synced to mirror the periodicity by 20 concurrent Hough transforms . ( Many of the stream of an aspect of the illumination . More typically , a digital processors probably found nothing , but the process speed filter computes a time interval that is used to set or compare wasn't impaired . ) 15 against timers optionally with software interrupts . A digi When these GPU Hough transform operations complete , tal phased - locked loop or delay - locked loop can also be the GPU may be reconfigured into a lesser number of stream used . ( A Kalman filter is commonly used for this type of processors one dedicated to analyzing each candidate oval phase locking . ) shape , to determine positions of eye pupils , nose location , Control processor module 36 can poll the synchronization and distance across the mouth . For any oval that yielded 20 module 35 to determine when a lighting condition is useful candidate facial information , associated parameters expected to have a desired state . With this information , would be packaged in keyvector form , and transmitted to a control processor module 36 can direct setup module 34 to cloud service that checks the keyvectors of analyzed facial capture a frame of data under favorable lighting conditions parameters against known templates , e.g. , of the user's for a particular purpose . For example , if the camera is Facebook friends . ( Or , such checking could also be per- 25 imaging an object suspected of having a digital watermark formed by the GPU , or by another processor in the cell encoded in a green color channel , processor 36 may direct phone . ) camera 32 to capture a frame of imagery at an instant that ( It is interesting to note that this facial recognition — like green illumination is expected to be at a maximum , and others detailed in this specification — distills the volume of direct processing stages 38 to process that frame for detec data , e.g. , from millions of pixels ( bytes ) in the originally 30 tion of such a watermark . captured image , to a keyvector that may comprise a few tens , The camera phone may be equipped with plural LED light hundreds , or thousands of bytes . This smaller parcel of sources that are usually operated in tandem to produce a information , with its denser information content , is more flash of white light illumination on a subject . Operated quickly routed for processing sometimes externally . Com- individually or in different combinations , however , they can munication of the distilled keyvector information takes place 35 cast different colors of light on the subject . The phone over a channel with a corresponding\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content='combinations , however , they can munication of the distilled keyvector information takes place 35 cast different colors of light on the subject . The phone over a channel with a corresponding bandwidth capability- processor may control the component LED sources indi keeping costs reasonable and implementation practical . ) vidually , to capture frames with non - white illumination . If Contrast the just - described GPU implementation of face capturing an image that is to be read to decode a green detection to such an operation as it might be implemented on channel watermark , only green illumination may be applied a scalar processor . Performing Hough - transform - based oval 40 when the frame is captured . Or a camera may capture plural detection across the entire image frame is prohibitive in successive frames — with different LEDs illuminating the terms of processing time much of the effort would be for subject . One frame may be captured at a 1 / 250th second naught , and would delay other tasks assigned to the proces- exposure with a corresponding period of red - only illumina sor . Instead , such an implementation would typically have tion ; a subsequent frame may be captured at a Viooth second the processor examine pixels as they come from the cam- 45 exposure with a corresponding period of green - only illumi era looking for those having color within an expected nation , etc. These frames may be analyzed separately , or \" skintone ” range . Only if a region of skintone pixels is may be combined , e.g. , for analysis in the aggregate . Or a identified would a Hough transform then be attempted on single frame of imagery may be captured over an interval of that excerpt of the image data . In similar fashion , attempting 1 / 100th of a second , with the green LED activated for that to extract facial parameters from detected ovals would be 50 entire interval , and the red LED activated for 1 / 250th of a done in a laborious serial fashion - often yielding no useful second during that 1 / 100th second interval . The instantaneous result . ambient illumination can be sensed ( or predicted , as above ) , Ambient Light and the component LED colored light sources can be Many artificial light sources do not provide a consistent operated in a responsive manner ( e.g. , to counteract orange illumination . Most exhibit a temporal variation in intensity 55 ness of tungsten illumination by adding blue illumination ( luminance ) and / or color . These variations commonly track from a blue LED ) . the AC power frequency ( 50/60 or 100/120 Hz ) , but some- Other Notes ; Projectors times do not . For example , fluorescent tubes can give off While a packet - based , data driven architecture is shown in infrared illumination that varies at a -40 KHz rate . The FIG . 16 , a variety of other implementations are of course emitted spectra depend on the particular lighting technology . 60 possible . Such alternative architectures are straightforward Organic LEDs for domestic and industrial lighting some- to the artisan , based on the details given . times can use distinct color mixtures ( e.g. , blue and amber ) The artisan will appreciate that the arrangements and to make white . Others employ more traditional red / green / details noted above are arbitrary . Actual choices of arrange blue clusters , or blue / UV LEDs with phosphors . ment and detail will depend on the particular application In one particular implementation , a processing stage 38 65 being served , and most likely will be different than those monitors , e.g. , the average intensity , redness , greenness or noted . ( To cite but a trivial example , FFTs need not be other coloration of the image data contained in the bodies of performed on 16x16 blocks , but can be done on 64x64 ,', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 85}), Document(page_content=\"US 10,922,957 B2 \\n 47 48 \\n 256x256 , the whole image , etc. ) Similarly , it will be recog- CKing ( the N70 model , distributed by China Vision ) and nized that the body of a packet can convey an entire frame Samsung ( the MPB200 ) . LG and others have shown proto of data , or just excerpts ( e.g. , a 128x128 block ) . Image data types . ( These projectors are understood to use Texas Instru from a single captured frame may thus span a series of ments electronically - steerable digital micro - mirror arrays , in several packets . Different excerpts within a common frame 5 conjunction with LED or laser illumination . ) Microvision may be processed differently , depending on the packet with offers the PicoP Display Engine , which can be integrated which they are conveyed . into a variety of devices to yield projector capability , using Moreover , a processing stage 38 may be instructed to a micro - electro - mechanical scanning mirror ( in conjunction break a packet into multiple packets — such as by splitting with laser sources and an optical combiner ) . Other suitable image data into 16 tiled smaller sub - images . Thus , more 10 projection technologies include 3M's liquid crystal on sili packets may be present at the end of the system than were produced at the beginning . con ( LCOS ) and Displaytech's ferroelectric LCOS systems . \\n In like fashion , a single packet may contain a collection Use of two projectors , or two cameras , gives differentials \\n of data from a series of different images ( e.g. , images taken of projection or viewing , providing additional information \\n sequentially with different focus , aperture , or shutter set- 15 about the subject . In addition to stereo features , it also \\n tings ; a particular example is a set of focus regions from five enables regional image correction . For example , consider\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content=\"images taken with focus bracketing , or depth of field brack two cameras imaging a digitally watermarked object . One eting overlapping , abutting , or disjoint . ) This set of data camera's view of the object gives one measure of a trans may then be processed by later stages either as a set , or form that can be discerned from the object's surface ( e.g. , by through a process that selects one or more excerpts of the 20 encoded calibration signals ) . This information can be used to packet payload that meet specified criteria ( e.g. , a focus correct a view of the object by the other camera . And vice sharpness metric ) . versa . The two cameras can iterate , yielding a comprehen In the particular example detailed , each processing stage sive characterization of the object surface . ( One camera may 38 generally substituted the result of its processing for the view a better - illuminated region of the surface , or see some data originally received in the body of the packet . In other 25 edges that the other camera can't see . One view may thus arrangements this need not be the case . For example , a stage reveal information that the other does not . ) may output a result of its processing to a module outside the If a reference pattern ( e.g. , a grid ) is projected onto a depicted processing chain , e.g. , on an output 33. ( Or , as surface , the shape of the surface is revealed by distortions of noted , a stage may maintain — in the body of the output the pattern . The FIG . 16 architecture can be expanded to packet — the data originally received , and augment it with 30 include a projector , which projects a pattern onto an object , further data — such as the result ( s ) of its processing . ) for capture by the camera system . ( Operation of the projec Reference was made to determining focus by reference to tor can be synchronized with operation of the camera , e.g. , DCT frequency spectra , or edge detected data . Many con- by control processor module 36 — with the projector acti sumer cameras perform a simpler form of focus check- vated only as necessary , since it imposes a significant battery simply by determining the intensity difference ( contrast ) 35 drain . ) Processing of the resulting image by modules 38 between pairs of adjacent pixels . This difference peaks with ( local or remote ) provides information about the surface correct focus . Such an arrangement can naturally be used in topology of the object . This 3D topology information can be the detailed arrangements . ( Again , advantages can accrue used as a clue in identifying the object . from performing such processing on the sensor chip . ) In addition to providing information about the 3D con Each stage typically conducts a handshaking exchange 40 figuration of an object , shape information allows a surface to with an adjoining stage each time data is passed to or be virtually re - mapped to any other configuration , e.g. , flat . received from the adjoining stage . Such handshaking is Such remapping serves as a sort of normalization operation . routine to the artisan familiar with digital system design , so In one particular arrangement , system 30 operates a is not belabored here . projector to project a reference pattern into the camera's The detailed arrangements contemplated a single image 45 field of view . While the pattern is being projected , the sensor . However , in other embodiments , multiple image camera captures a frame of image data . The resulting image sensors can be used . In addition to enabling conventional is processed to detect the reference pattern , and therefrom stereoscopic processing , two or more image sensors enable characterize the 3D shape of an imaged object . Subsequent or enhance many other operations . processing then follows , based on the 3D shape data . One function that benefits from multiple cameras is 50 ( In connection with such arrangements , the reader is distinguishing objects . To cite a simple example , a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content=\", based on the 3D shape data . One function that benefits from multiple cameras is 50 ( In connection with such arrangements , the reader is distinguishing objects . To cite a simple example , a single referred to the Google book - scanning patent , U.S. Pat . No. camera is unable to distinguish a human face from a picture 7,508,978 , which employs related principles . That patent of a face ( e.g. , as may be found in a magazine , on a billboard , details a particularly useful reference pattern , among other or on an electronic display screen ) . With spaced - apart sen- relevant disclosures . ) sors , in contrast , the 3D aspect of the picture can readily be 55 If the projector uses collimated laser illumination ( such as discerned , allowing a picture to be distinguished from a the PicoP Display Engine ) , the pattern will be in focus person . ( Depending on the implementation , it may be the 3D regardless of distance to the object onto which the pattern is aspect of the person that is actually discerned . ) projected . This can be used as an aid to adjust focus of a cell Another function that benefits from multiple cameras is phone camera onto an arbitrary subject . Because the pro refinement of geolocation . From differences between two 60 jected pattern is known in advance by the camera , the images , a processor can determine the device's distance captured image data can be processed to optimize detection from landmarks whose location may be precisely known . of the pattern such as by correlation . ( Or the pattern can be This allows refinement of other geolocation data available to selected to facilitate detection such as a checkerboard that the device ( e.g. , by WiFi node identification , GPS , etc. ) appears strongly at a single frequency in the image fre Just as a cell phone may have one , two ( or more ) sensors , 65 quency domain when properly focused . ) Once the camera is such a device may also have one , two ( or more ) projectors . adjusted for optimum focus of the known , collimated pat Individual projectors are being deployed in cell phones by tern , the projected pattern can be discontinued , and the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 86}), Document(page_content='15 US 10,922,957 B2 \\n 49 50', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content='camera can then capture a properly focused image of the projected from different points differ when presented on an underlying subject onto which the pattern was projected . object and viewed by the camera , stereoscopic information Synchronous detection can also be employed . The pattern can again be discerned . may be projected during capture of one frame , and then off Many usage models are enabled through use a projector , for capture of the next . The two frames can then be sub- 5 including new sharing models ( c.f. , Greaves , “ View & tracted . The common imagery in the two frames generally Share : Exploring Co - Present Viewing and Sharing of Pic cancels leaving the projected pattern at a much higher tures using Personal Projection , ” Mobile Interaction with the signal to noise ratio . Real World 2009 ) . Such models employ the image created A projected pattern can be used to determine correct focus by the projector itself as a trigger to initiate a sharing for several subjects in the camera\\'s field of view . A child 10 session , either overtly through a commonly understood may pose in front of the Grand Canyon . The laser - projected symbol ( \" open ” sign ) , to covert triggers that are machine pattern allows the camera to focus on the child in a first readable . Sharing can also occur through ad hoc networks frame , and on the background in a second frame . These utilizing peer to peer applications , or a server hosted appli frames can then be composited taking from each the cation . portion properly in focus . Other output from mobile devices can be similarly shared . If a lens arrangement is used in the cell phone\\'s projector Consider keyvectors . One user\\'s phone may process an system , it can also be used for the cell phone\\'s camera image with Hough transform and other eigenface extraction system . A mirror can be controllably moved to steer the techniques , and then share the resulting keyvector of eigen camera or the projector to the lens . Or a beam - splitter face data with others in the user\\'s social circle ( either by arrangement 80 can be used ( FIG . 20 ) . Here the body of a 20 pushing same to them , or allowing them to pull it ) . One or cell phone 81 incorporates a lens 82 , which provides a light more of these socially - affiliated devices may then perform to a beam - splitter 84. Part of the illumination is routed to the facial template matching that yields an identification of a camera sensor 12. The other part of the optical path goes to formerly - unrecognized face in the imagery captured by the a micro - mirror projector system 86 . original user . Such arrangement takes a personal experience , Lenses used in cell phone projectors typically are larger 25 and makes it a public experience . Moreover , the experience aperture than those used for cell phone cameras , so the can become a viral experience , with the keyvector data camera may gain significant performance advantages ( e.g. , shared — essentially without bounds — to a great number of enabling shorter exposures ) by use of such a shared lens . Or , further users . reciprocally , the beam splitter 84 can be asymmetrical — not Selected Other Arrangements equally favoring both optical paths . For example , the beam- 30 In addition to the arrangements earlier detailed , another splitter can be a partially - silvered element that couples a hardware arrangement suitable for use with certain imple smaller fraction ( e.g. , 2 % , 8 % , or 25 % ) of externally inci- mentations of the present technology uses the Mali - 400 dent light the sensor path 83. The am - splitter may thus ARM graphics multiprocessor architecture , which includes serve to couple a larger fraction ( e.g. , 98 % , 92 % , or 75 % ) of plural fragment processors that can be devoted to the dif illumination from the micro - mirror projector externally , for 35 ferent types of image processing tasks referenced in this projection . By this arrangement the camera sensor 12', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content=\"devoted to the dif illumination from the micro - mirror projector externally , for 35 ferent types of image processing tasks referenced in this projection . By this arrangement the camera sensor 12 document . receives light of a conventional — for a cell phone camera- The standards group Khronos has issued OpenGL ES2.0 , intensity ( notwithstanding the larger aperture lens ) , while which defines hundreds of standardized graphics function the light output from the projector is only slightly dimmed calls for systems that include multiple CPUs and multiple by the lens sharing arrangement . 40 GPUs ( a direction in which cell phones are increasingly In another arrangement , a camera head is separate migrating ) . OpenGL ES2.0 attends to routing of different detachable — from the cell phone body . The cell phone body operations to different of the processing units with such is carried in a user's pocket or purse , while the camera head details being transparent to the application software . It thus is adapted for looking out over a user's pocket ( e.g. , in a provides a consistent software API usable with all manner of form factor akin to a pen , with a pocket clip , and with a 45 GPU / CPU hardware . battery in the pen barrel ) . The two communicate by Blu- In accordance with another aspect of the present technol etooth or other wireless arrangement , with capture instruc- ogy , OpenGL ES2 . standard is extended to provide a stan tions sent from the phone body , and image data sent from the dardized graphics processing library not just across different camera head . Such configuration allows the camera to CPU / GPU hardware , but also across different cloud pro constantly survey the scene in front of the user — without 50 cessing hardware again with such details being transparent requiring that the cell phone be removed from the user's to the calling software . Increasingly , Java service requests pocket / purse . ( JSRs ) have been defined to standardize certain Java - imple In a related arrangement , a strobe light for the camera is mented tasks . JSRs increasingly are designed for efficient separate or detachable — from the cell phone body . The implementations on top of OpenGL ES2.0 class hardware . light ( which may incorporate LEDs ) can be placed near the 55 In accordance with a still further aspect of the present image subject , providing illumination from a desired angle technology , some or all of the image processing operations and distance . The strobe can be fired by a wireless command noted in this specification ( facial recognition , SIFT process issued by the cell phone camera system . ing , watermark detection , histogram processing , etc. ) can be ( Those skilled in optical system design will recognize a implemented as JSRs providing standardized implemen number of alternatives to the arrangements particularly 60 tations that are suitable across diverse hardware platforms . noted . ) In addition to supporting cloud - based JSRs , the extended Some of the advantages that accrue from having two standards specification can also support the Query Router cameras can be realized by having two projectors ( with a and Response Manager functionality detailed earlier - in single camera ) . For example , the two projectors can project cluding both static and auction - based service providers . alternating or otherwise distinguishable patterns ( e.g. , simul- 65 Akin to OpenGL is OpenCV — a computer vision library taneous , but of differing color , pattern , polarization , etc ) into available under an open source license , permitting coders to the camera's field of view . By noting how the two patterns invoke a variety of functions — without regard to the par or\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 87}), Document(page_content=\"US 10,922,957 B2 \\n 51 52 \\n ticular hardware that is being utilized to perform same . ( An based on “ costs , ” including bandwidth costs , external ser O'Reilly book , Learning OpenCV , documents the language vice provider costs , power costs to the cell phone battery , extensively . ) A counterpart , NokiaCV , provides similar func- intangible costs in consumer ( dis- ) satisfaction by delaying tionality specialized for the Symbian operating system ( e.g. , processing , etc. For example , if the user is running low on Nokia cell phones ) . 5 battery power , and is at a location far from a cell tower ( so OpenCV provides support for a large variety of opera- that the cell phone runs its RF amplifier at maximum output tions , including high level tasks such as facial recognition , when transmitting ) , then sending a large block of data for gesture recognition , motion tracking / understanding , seg- remote processing may consume a significant fraction of the mentation , etc. , as well as an extensive assortment of more battery's remaining life . In such case , the phone may decide atomic , elemental vision / image processing operations . 10 to process the data locally , or to forward it for remote CMVision is another package of computer vision tools processing when the phone is closer to the cell site or the that can be employed in certain embodiments of the present battery has been recharged . A set of stored rules can be technology — this package compiled by researchers at Car- applied to the relevant variables to establish a net “ cost negie Mellon University . function ” for different approaches ( e.g. , process locally , Still another hardware architecture makes use of a field 15 process remotely , defer processing ) , and these rules may programmable object array ( FPOA ) arrangement , in which indicate different outcomes depending on the states of these hundreds of diverse 16 - bit “ objects ” are arrayed in a gridded variables . node fashion , with each being able to exchange data with An appealing “ cloud ” resource is the processing capabil neighboring devices through very high bandwidth channels . ity found at the edges of wireless networks . Cellular net ( The PicoChip devices referenced earlier are of this class . ) 20 works , for example , include tower stations that are , in large The functionality of each can be reprogrammed , as with part , software - defined radios - employing processors to per FPGAs . Again different of the image processing tasks can be form— digitally — some or all of the operations traditionally performed by different of the FPOA objects . These tasks can performed by analog transmitting and receiving radio cir be redefined on the fly , as needed ( e.g. , an object may cuits , such as mixers , filters , demodulators , etc. Even perform SIFT processing in one state ; FFT processing in 25 smaller cell stations , so - called “ femtocells , ” typically have another state ; log - polar processing in a further state , etc. ) . powerful signal processing hardware for such purposes . The ( While many grid arrangements of logic devices are based PicoChip processors noted earlier , and other field program on “ nearest neighbor ” interconnects , additional flexibility mable object arrays , are widely deployed in such applica can be achieved by use of a “ partial crossbar ” interconnect . tions . See , e.g. , U.S. Pat . No. 5,448,496 ( Quickturn Design Sys- 30 Radio signal processing , and image signal processing ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='tems ) . ) have many commonalities , e.g. , employing FFT processing Also in the realm of hardware , certain embodiments of the to convert sampled data to the frequency domain , applying present technology employ “ extended depth of field ” imag- various filtering operations , etc Cell station equipment , ing systems ( see , e.g. , U.S. Pat . Nos . 7,218,448 , 7,031,054 including processors , are designed to meet peak consumer and 5,748,371 ) . Such arrangements include a mask in the 35 demands . This means that significant processing capability imaging path that modifies the optical transfer function of is often left unused . the system so as to be insensitive to the distance between the In accordance with another aspect of the present technol object and the imaging system . The image quality is then ogy , this spare radio signal processing capability at cellular uniformly poor over the depth of field . Digital post process- tower stations ( and other edges of wireless networks ) is ing of the image compensates for the mask modifications , 40 repurposed in connection with image ( and / or audio or other ) restoring image quality , but retaining the increased depth of signal processing for consumer wireless devices . Since an field . Using such technology , the cell phone camera can FFT operation is the same — whether processing sampled capture imagery having both nearer and further subjects all radio signals or image pixels — the repurposing is often in focus ( i.e. , with greater high frequency detail ) , without straightforward : configuration data for the hardware pro requiring longer exposures — as would normally be required . 45 cessing cores needn\\'t be changed much , if at all . And ( Longer exposures exacerbate problems such as hand - jitter , because 3G / 4G networks are so fast , a processing task can and moving subjects . ) In the arrangements detailed here , be delegated quickly from a consumer device to a cell station shorter exposures allow higher quality imagery to be pro- processor , and the results returned with similar speed . In vided to image processing functions without enduring the addition to the speed and computational muscle that such temporal delay created by optical / mechanical focusing ele- 50 repurposing of cell station processors affords , another ben ments , or requiring input from the user as to which elements efit is reducing the power consumption of the consumer of the image should be in focus . This provides for a much devices . more intuitive experience , as the user can simply point the Before sending image data for processing , a cell phone imaging device at the desired target without worrying about can quickly inquire of the cell tower station with which it is focus or depth of field settings . Similarly , the image pro- 55 communicating to confirm that it has enough unused capac cessing functions are able to leverage all the pixels included ity sufficient to undertake the intended image processing in the image / frame captured , as all are expected to be operation . This query can be sent by the packager / router of in - focus . In addition , new metadata regarding identified FIG . 10 ; the local / remote router of FIG . 10A , the query objects or groupings of pixels related to depth within the router and response manager of FIG . 7 ; the pipe manager 51 frame can produce simple \" depth map ” information , setting 60 of FIG . 16 , etc. the stage for 3D video capture and storage of video streams Alerting the cell tower / base station of forthcoming pro using emerging standards on transmission of depth infor- cessing requests , and / or bandwidth requirements , allows the', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='mation . cell site to better allocate its processing and bandwidth In some embodiments the cell phone may have the resources in anticipation of meeting such needs . capability to perform a given operation locally , but may 65 Cell sites are at risk of becoming bottlenecked : undertak decide instead to have it performed by a cloud resource . The ing service operations that exhaust their processing or band decision of whether to process locally or remotely can be width capacity . When this occurs , they must triage by', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 88}), Document(page_content='as \\n users . US 10,922,957 B2 \\n 53 54 unexpectedly throttling back the processing / bandwidth pro- possible scenarios are sufficiently improbable that they may vided to one or more users , so others can be served . This be disregarded in bandwidth allocations . However , on the sudden change in service is undesirable , since changing the rare occasions when such improbable scenarios occur parameters with which the channel was originally estab- when thousands of subscribers sent cell phone picture mes lished ( e.g. , the bit rate at which video can be delivered ) , 5 sages from Washington D.C. during the Obama inaugura forces data services using the channel to reconfigure their tion , some subscribers may simply not receive service . ) respective parameters ( e.g. , requiring ESPN to provide a The statistical models on which site bandwidth allocations', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content=\"lower quality video feed ) . Renegotiating such details once are based , are understood to treat subscribers — in part — as the channel and services have been originally setup invari- unpredictable actors . Whether a particular subscriber ably causes glitches , e.g. , video delivery stuttering , dropped 10 requests service in the forthcoming seconds ( and what syllables in phone calls , etc. particular service is requested ) has a random aspect . To avoid the need for these unpredictable bandwidth The larger the randomness in a statistical model , the larger slowdowns , and resulting service impairments , cell sites the extremes tend to be . If reservations , or forecasts of future tend to adopt a conservative strategy — allocating band- demands , are routinely submitted by , e.g. , 15 % of subscrib width / processing resources parsimoniously , in order to 15 ers , then the behavior of those subscribers is no longer reserve capacity for possible peak demands . But this random . The worst case peak bandwidth demand on a cell approach impairs the quality of service that might otherwise site does not involve 100 % of the subscribers acting ran be normally provided sacrificing typical service in antici- domly , but only 85 % . Actual reservation information can be pation of the unexpected . employed for the other 15 % . Hypothetical extremes in peak In accordance with this aspect of the present technology , 20 bandwidth usage are thus moderated . a cell phone sends alerts to the cell tower station , specifying With lower peak usage scenarios , more generous alloca bandwidth or processing needs that it anticipates will be tions of present bandwidth can be granted to all subscribers . forthcoming . In effect , the cell phone asks to reserve a bit of That is , if a portion of the user base sends alerts to the site future service capacity . The tower station still has a fixed reserving future capacity , then the site may predict that the capacity . However , knowing that a particular user will be 25 realistic peak demand that may be forthcoming will still needing , e.g. , a bandwidth of 8 Mbit / s for 3 seconds , leave the site with unused capacity . In this case it may grant commencing in 200 milliseconds , allows the cell site to take a camera cell phone user a 12 Mbit / s channel - instead of the such anticipated demand into account as it serves other 8 Mbit / s channel stated in the reservation request , and / or may grant a video user a 15 Mbit / s channel instead of the Consider a cell site having an excess ( allocable ) channel 30 normal 10 Mbit / s channel . Such usage forecasting can thus capacity of 15 Mbit / s , which normally allocates to a new allow the site to grant higher quality services than would video service user a channel of 10 Mbit / s . If the site knows normally be the case , since bandwidth reserves need be held that a cell camera user has requested reservation for a 8 for a lesser number of unpredictable actors . Mbit / s channel starting in 200 milliseconds , and a new video Anticipatory service requests can also be communicated service user meanwhile requests service , the site may allo- 35 from the cell phone ( or the cell site ) to other cloud processes cate the new video service user a channel of 7 Mbit / s , rather that are expected to be involved in the requested services , than the usual 10 Mbit / s . By initially setting up the new allowing them to similarly allocate their resources anticipa video service user's channel at the slower bit rate , service torily . Such anticipatory service requests may also serve to impairments associated with cutting back bandwidth during alert the cloud process to pre - warm associated processing . an ongoing channel session are avoided . The capacity of the 40 Additional information may be provided from the cell cell site is the same , but it is now allocated in manner that phone , or elsewhere , for this purpose , such as encryption reduces the need for reducing\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content='information may be provided from the cell cell site is the same , but it is now allocated in manner that phone , or elsewhere , for this purpose , such as encryption reduces the need for reducing the bandwidth of existing keys , image dimensions ( e.g. , to configure a cloud FPOA to channels , mid - transmission . serve as an FFT processor for a 1024x768 image , to be In another situation , the cell site may determine that it has processed in 16x16 tiles , and output coefficients for 32 excess capacity at present , but expects to be more heavily 45 spectral frequency bands ) , etc. burdened in a half second . In this case it may use the present In turn , the cloud resource may alert the cell phone of any excess capacity to speed throughput to one or more video information it expects might be requested from the phone in subscribers , e.g. , those for whom it has collected several performance of the expected operation , or action it might packets of video data in a buffer memory , ready for delivery . request the cell phone to perform , so that the cell phone can These video packets may be sent through the enlarged 50 similarly anticipate its own forthcoming actions and prepare channel now , in anticipation that the video channel will be accordingly . For example , the cloud process may , under slowed in a half second . Again , this is practical because the certain conditions , request a further set of input data , such as cell site has useful information about future bandwidth if it assesses that data originally provided is not sufficient for demands . the intended purpose ( e.g. , the input data may be an image The service reservation message sent from the cell phone 55 without sufficient focus resolution , or not enough contrast , may also include a priority indicator . This indicator can be or needing further filtering ) . Knowing , in advance , that the used by the cell site to determine the relative importance of cloud process may request such further data can allow the meeting the request on the stated terms , in case arbitration cell phone to consider this possibility in its own operation , between conflicting service demands is required . e.g. , keeping processing modules configured in a certain Such anticipatory service requests from cell phones can 60 filter manner longer than may otherwise be the case , reserv also allow the cell site to provide higher quality sustained ing an interval of sensor time to possibly capture a replace service than would normally be allocated . ment image , etc. Cell sites are understood to employ statistical models of Anticipatory service requests ( or the possibility of con usage patterns , and allocate bandwidth accordingly . The ditional service requests ) generally relate to events that may allocations are typically set conservatively , in anticipation of 65 commence in few tens or hundreds of milliseconds realistic worst case usage scenarios , e.g. , encompassing sionally in a few single seconds . Situations in which the scenarios that occur 99.99 % of the time . ( Some theoretically action will commence tens or hundreds of second in the occa', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 89}), Document(page_content='more . US 10,922,957 B2 \\n 55 56', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"future will be rare . However , while the period of advance The system may perform a FFT on captured image data to warning may be short , significant advantages can be obtain frequency domain information , and then feed that derived : if the randomness of the next second is reduced information to several watermark decoders operating in each second , then system randomness can be reduced con- parallel - each applying a different decoding algorithm . siderably . Moreover , the events to which the requests relate 5 When one of the applications extracts valid watermark data can , themselves , be of longer duration such as transmis- ( e.g. , indicated by ECC information computed from the sion of a large image file , which may take ten seconds or payload ) , the data is sent to a database corresponding to that format / technology of watermark . Plural such database point Regarding advance set - up ( pre - warming ) , desirably any ers can be included in a packet , and used conditionally operation that takes more than a threshold interval of time to 10 depending on which watermark decoding operation ( or complete ( e.g. , a few hundred microseconds , a millisecond , barcode reading operation , or fingerprint calculation , etc. ) ten microseconds , etc.— depending on implementation ) yields useful data . should be prepped anticipatorily , if possible . ( In some Similarly , the system may send a facial image to an instances , of course , the anticipated service is never intermediary cloud service , in a packet containing an iden requested , in which case such preparation may be for 15 tifier of the user ( but not containing the user's Apple iPhoto , naught . ) In another hardware arrangement , the cell phone or Picasa , or Facebook user name ) . The intermediary cloud processor may selectively activate a Peltier device or other service can take the provided user identifier , and use it to thermoelectric cooler coupled to the image sensor , in cir- access a database record from which the user's names on cumstances when thermal image noise ( Johnson noise ) is a these other services are obtained . The intermediary cloud potential problem . For example , if a cell phone detects a low 20 service can then route the facial image data to an Apple's light condition , it may activate a cooler on the sensor to try server — with the user's iPhoto user name ; to Picasa's ser and enhance the image signal to noise ratio . Or the image vice with the user's Google user name ; and to Facebook's processing stages can examine captured imagery for artifacts server with the user's Facebook user name . Those respective associated with thermal noise , and if such artifacts exceed a services can then perform facial recognition on the imagery , threshold , then the cooling device can be activated . ( One 25 and return the names of identified persons identified from approach captures a patch of imagery , such as a 16x16 pixel the user's iPhoto / Picasa / Facebook accounts ( directly to the region , twice in quick succession . Absent random factors , user , or through the intermediary service ) . The intermediate the two patches should be identical perfectly correlated . cloud service — which may serve large numbers of users The variance of the correlation from 1.0 is a measure of can keep informed of the current addresses for relevant noise — presumably thermal noise . ) A short interval after the 30 servers ( and alternate proximate servers , in case the user is cooling device is activated , a substitute image can be cap- away from home ) , rather than have each cell phone try to tured — the interval depending on thermal response time for keep such data in updated fashion . the cooler / sensor . Likewise if cell phone video is captured , Facial recognition applications can be used just to a cooler may be activated , since the increased switching identify persons , but also to identify relationships between activity by circuitry on the sensor increases its\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content='can be used just to a cooler may be activated , since the increased switching identify persons , but also to identify relationships between activity by circuitry on the sensor increases its temperature , 35 individuals depicted in imagery . For example , data main and thus its thermal noise . ( Whether to activate a cooler can tained by iPhoto / Picasa / Facebook may contain not just also be application dependent , e.g. , the cooler may be facial recognition features , and associated names , but also activated when capturing imagery from which watermark terms indicating relationships between the named faces and data may be read , but not activated when capturing imagery the account owner ( e.g. , father , boyfriend , sibling , pet , from which barcode data may be read . ) 40 roommate , etc. ) . Thus , instead of simply searching a user\\'s As noted , packets in the FIG . 16 arrangement can convey image collection for , e.g. , all pictures of “ David Smith ” the a variety of instructions and data in both the header and the user\\'s collection may also be searched for all pictures packet body . In a further arrangement a packet can addi- depicting \" sibling . \" tionally , or alternatively , contain a pointer to a cloud object , The application software in which photos are reviewed or to a record in a database . The cloud object / database record 45 can present differently colored frames around different rec may contain information such as object properties , useful for ognized faces in accordance with associated relationship object recognition ( e.g. , fingerprint or watermark properties data ( e.g. , blue for siblings , red for boyfriends , etc. ) . for a particular object ) . In some arrangements , the user\\'s system can access such If the system has read a watermark , the packet may information stored in accounts maintained by the user\\'s contain the watermark payload , and the header ( or body ) 50 network “ friends . ” A face that may not be recognized by may contain one or more database references where that facial recognition data associated with the user\\'s account at payload can be associated with related information . A water- Picasa , may be recognized by consulting Picasa facial rec mark payload read from a business card may be looked - up ognition data associated with the account of the user\\'s friend in one database ; a watermark decoded from a photograph “ David Smith . ” Relationship data indicated by David may be looked - up in another database , etc. A system may 55 Smith\\'s account can be similarly used to present , and apply multiple different watermark decoding algorithms to a organize , the user\\'s photos . The earlier unrecognized face single image ( e.g. , MediaSec , Digimarc ImageBridge , Civo- may thus be labeled with indicia indicating the person is lution , etc. ) . Depending on which application performed a David Smith\\'s roommate . This essentially remaps the rela particular decoding operation , the resulting watermark pay- tionship information ( e.g. , mapping “ roommate ” – as indi', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"load may be sent off to a corresponding destination database . 60 cated in David Smith's account , to “ David Smith's room ( Likewise with different barcodes , fingerprint algorithms , mate ” in the user's account ) . eigenface technologies , etc. ) The destination database The embodiments detailed above were generally address can be included in the application , or in configura- described in the context of a single network . However , plural tion data . ( Commonly , the addressing is performed indi- networks may commonly be available to a user's phone rectly , with an intermediate data store containing the address 65 ( e.g. , WiFi , Bluetooth , possibly different cellular networks , of the ultimate database , permitting relocation of the data- etc. ) The user may choose between these alternatives , or the base without changing each cell phone application . ) system may apply stored rules to automatically do so . In\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 90}), Document(page_content=\"US 10,922,957 B2 \\n 57 58 \\n some instances , a service request may be issued ( or results growing / evolving sets of service providers , can be set to the returned ) across several networks in parallel . tasks of deriving meaning from input stimuli ( audio as well Reference Platform Architecture as visual , e.g. , speech recognition ) through use of such an The hardware in cell phones was originally introduced for adaptable architecture . specific purposes . The microphone , for example , was used 5 Arasan Chip Systems , Inc. offers a Mobile Industry only for voice transmission over the cellular network : feed Processor Interface UniPro Software Stack , a layered , ker ing an A / D converter that fed a modulator in the phone's nel - level stack that aims to simplify integration of certain radio transceiver . The camera was used only to capture technologies into cell phones . That arrangement may be snapshots . Etc. As additional applications arose employing extended to provide the functionality detailed above . ( The such hardware , each application needed to develop its own 10 Arasan protocol is focused primarily on transport layer way to talk to the hardware . Diverse software stacks arose each specialized so a particular application could interact issues , but involves layers down to hardware drivers as well . \\n with a particular piece of hardware . This poses an impedi The Mobile Industry Processor Interface Alliance is a large \\n ment to application development . industry group working to advance cell phone technologies . ) This problem compounds when cloud services and / or 15 Leveraging Existing Image Collections , E.g. , for Metadata specialized processors are added to the mix . To alleviate Collections of publicly - available imagery and other con such difficulties , some embodiments of the present technol- tent are becoming more prevalent . Filch , YouTube , Photo ogy can employ an intermediate software layer that provides bucket ( MySpace ) , Picasa , Zooomr , FaceBook , Webshots\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='a standard interface with which and through which hardware and Google Images are just a few . Often , these resources can and software can interact . Such an arrangement is shown in 20 also serve as sources of metadata— either expressly identi FIG . 20A , with the intermediate software layer being labeled fied as such , or inferred from data such as file names , “ Reference Platform . \" descriptions , etc. Sometimes geo - location data is also avail In this diagram hardware elements are shown in dashed able . boxes , including processing hardware on the bottom , and An illustrative embodiment according to one aspect of the peripherals on the left . The box “ IC HW ” is “ intuitive 25 present technology works as follows . A captures a cell phone computing hardware , \" and comprises the earlier - discussed picture of an object , or scene perhaps a desk telephone , as hardware that supports the different processing of image shown in FIG . 21. ( The image may be acquired in other related data , such as modules 38 in FIG . 16 , the configurable manners as well , such as transmitted from another user , or hardware of FIG . 6 , etc. DSP is a general purpose digital downloaded from a remote computer . ) signal processor , which can be configured to perform spe- 30 As a preliminary operation , known image processing cialized operations ; CPU is the phone\\'s primary processor ; operations may be applied , e.g. , to correct color or contrast , GPU is a graphics processor unit . OpenCL and OpenGL are to perform ortho - normalization , etc. on the captured image . APIs through which graphics processing services ( per- Known image object segmentation or classification tech formed on the CPU and / or GPU ) can be invoked . niques may also be used to identify an apparent subject Different specialized technologies are in the middle , such 35 region of the image , and isolate same for further processing . as one or more digital watermark decoders ( and / or encod- The image data is then processed to determine character ers ) , barcode reading software , optical character recognition izing features that are useful in pattern matching and rec software , etc. Cloud services are shown on the right , and ognition . Color , shape , and texture metrics are commonly applications are on the top . used for this purpose . Images may also be grouped based on The reference platform establishes a standard interface 40 layout and eigenvectors ( the latter being particularly popular through which different applications can interact with hard- for facial recognition ) . Many other technologies can of ware , exchange information , and request services ( e.g. , by course be employed , as noted elsewhere in this specification . API calls ) . Similarly , the platform establishes a standard ( Uses of vector characterizations / classifications and other interface through which the different technologies can be image / video / audio metrics in recognizing faces , imagery , accessed , and through which they can send and receive data 45 video , audio and other patterns are well known and suited to other of the system components . Likewise with the cloud for use in connection with certain embodiments of the services , for which the reference platform may also attend to present technology . See , e.g. , patent publications', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='details of identifying a service provider — whether by reverse 20060020630 and 20040243567 ( Digimarc ) , 20070239756 auction , heuristics , etc. In cases where a service is available and 20020037083 ( Microsoft ) , 20070237364 ( Fuji Photo both from a technology in the cell phone , and from a remote 50 Film ) , U.S. Pat . Nos . 7,359,889 and 6,990,453 ( Shazam ) , service provider , the reference platform may also attend to 20050180635 ( Corel ) , U.S. Pat . Nos . 6,430,306 , 6,681,032 weighing the costs and benefits of the different options , and and 20030059124 ( L - 1 Corp. ) , U.S. Pat . Nos . 7,194,752 and deciding which should handle a particular service request . 7,174,293 ( Iceberg ) , U.S. Pat . No. 7,130,466 ( Cobion ) , U.S. By such arrangement , the different system components do Pat . No. 6,553,136 ( Hewlett - Packard ) , and U.S. Pat . No. not need to concern themselves with the details of other parts 55 6,430,307 ( Matsushita ) , and the journal references cited at of the system . An application may call for the system to read the end of this disclosure . When used in conjunction with text from an object in front of the cell phone . It needn\\'t recognition of entertainment content such as audio and concern itself with the particular control parameters of the video , such features are sometimes termed content “ finger image sensor , nor the image format requirements of the OCR prints ” or “ hashes . \" ) engine . An application may call for a read of the emotion of 60 After feature metrics for the image are determined , a a person in front of the cell phone . A corresponding call is search is conducted through one or more publicly - accessible passed to whatever technology in the phone supports such image repositories for images with similar metrics , thereby functionality , and the results are returned in a standardized identifying apparently similar images . ( As part of its image form . When an improved technology becomes available , it ingest process , Flickr and other such repositories may cal can be added to the phone , and through the reference 65 culate eigenvectors , color histograms , keypoint descriptors , platform the system takes advantages of its enhanced capa- FFTs , or other classification data on images at the time they bilities . Thus , growing / changing collections of sensors , and are uploaded by users , and collect same in an index for', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 91}), Document(page_content='10 \\n 15 \\n 25 US 10,922,957 B2 \\n 59 60 \\n public search . ) The search may yield the collection of 7960 ( 2 ) apparently similar telephone images found in Flickr , Facsimile Machine ( 2 ) depicted in FIG . 22 . 7920 ( 1 ) Metadata is then harvested from Flickr for each of these 7950 ( 1 ) images , and the descriptive terms are parsed and ranked by 5 Best Buy ( 1 ) frequency of occurrence . In the depicted set of images , for Desk ( 1 ) example , the descriptors harvested from such operation , and Ethernet ( 1 ) their incidence of occurrence , may be as follows : IP - phone ( 1 ) Cisco ( 18 ) Office ( 1 ) Phone ( 10 ) Pricey ( 1 ) \\n Telephone ( 7 ) Sprint ( 1 ) \\n VOIP ( 7 ) Telecommunications ( 1 ) IP ( 5 ) Uninett ( 1 ) 7941 ( 3 ) Work ( 1 ) Phones ( 3 ) The list of inferred metadata can be restricted to those Technology ( 3 ) terms that have the highest apparent reliability , e.g. , count 7960 ( 2 ) values . A subset of the list comprising , e.g. , the top N terms , 7920 ( 1 ) or the terms in the top Mth percentile of the ranked listing , 7950 ( 1 ) may be used . This subset can be associated with the FIG . 21 Best Buy ( 1 ) 20 image in a metadata repository for that image , as inferred Desk ( 1 ) metadata . \\n Ethernet ( 1 ) In the present example , if N = 4 , the terms Telephone , IP - phone ( 1 ) Cisco , Phone and VOIP are associated with the FIG . 21 Office ( 1 ) image . \\n Pricey ( 1 ) Once a list of metadata is assembled for the FIG . 21 image Sprint ( 1 ) ( by the foregoing procedure , or others ) , a variety of opera \\n Telecommunications ( 1 ) tions can be undertaken .', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content='Uninett ( 1 ) One option is to submit the metadata , along with the Work ( 1 ) captured content or data derived from the captured content From this aggregated set of inferred metadata , it may be 30 ( e.g. , the FIG . 21 image , image feature data such as eigen assumed that those terms with the highest count values ( e.g. , vectors , color histograms , keypoint descriptors , FFTS , those terms occurring most frequently ) are the terms that machine readable data decoded from the image , etc ) , to a most accurately characterize the user\\'s FIG . 21 image . service provider that acts on the submitted data , and pro The inferred metadata can be augmented or enhanced , if vides a response to the user . Shazam , Snapnow ( now desired , by known image recognition / classification tech- 35 LinkMe Mobile ) , ClusterMedia Labs , Snaptell ( now part of niques . Such technology seeks to provide automatic recog- Amazon\\'s A9 search service ) , Mobot , Mobile Acuity , Nokia nition of objects depicted in images . For example , by Point & Find , Kooaba , idée TinEye , iVisit\\'s SeeScan , Evo recognizing a TouchTone keypad layout , and a coiled cord , lution Robotics \\' ViPR , IQ Engine’s oMoby , and Digimarc such a classifier may label the FIG . 21 image using the terms Mobile , are a few of several commercially available services Telephone and Facsimile Machine . 40 that capture media content , and provide a corresponding If not already present in the inferred metadata , the terms response ; others are detailed in the earlier - cited patent returned by the image classifier can be added to the list and publications . By accompanying the content data with the given a count value . ( An arbitrary value , e.g. , 2 , may be metadata , the service provider can make a more informed used , or a value dependent on the classifier\\'s reported judgment as to how it should respond to the user\\'s submis confidence in the discerned identification can be employed . ) 45 sion . If the classifier yields one or more terms that are already The service provider or the user\\'s device can submit present , the position of the term ( s ) in the list may be the metadata descriptors to one or more other services , e.g. , elevated . One way to elevate a term\\'s position is by increas- a web search engine such as Google , to obtain a richer set ing its count value by a percentage ( e.g. , 30 % ) . Another way of auxiliary information that may help better discern / infer / is to increase its count value to one greater than the next- 50 intuit an appropriate desired by the user . Or the information above term that is not discerned by the image classifier . obtained from Google ( or other such database resource ) can ( Since the classifier returned the term “ Telephone ” but not be used to augment / refine the response delivered by the the term “ Cisco , \" this latter approach could rank the term service provider to the user . ( In some cases , the metadata Telephone with a count value of “ 19 ” _one above Cisco . ) A possibly accompanied by the auxiliary information received variety of other techniques for augmenting / enhancing the 55 from Google can allow the service provider to produce an inferred metadata with that resulting from the image clas- appropriate response to the user , without even requiring the sifier are straightforward to implement . image data . ) A revised listing of metadata , resulting from the forego- In some cases , one or more images obtained from Flickr ing , may be as follows : may be substituted for the user\\'s image . This may be done , Telephone ( 19 ) 60 for example , if a Flickr image appears to be of higher quality Cisco ( 18 ) ( using sharpness , illumination histogram , or other mea Phone ( 10 ) sures ) , and if the image metrics are sufficiently similar VOIP ( 7 ) ( Similarity can be judged by a distance measure appropriate IP ( 5 ) to the metrics being used . One embodiment checks whether 7941 ( 3 ) 65 the distance measure is below a', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content='similar VOIP ( 7 ) ( Similarity can be judged by a distance measure appropriate IP ( 5 ) to the metrics being used . One embodiment checks whether 7941 ( 3 ) 65 the distance measure is below a threshold . If several alter Phones ( 3 ) nate images pass this screen , then the closest image is used . ) Technology ( 3 ) Or substitution may be used in other circumstances . The', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 92}), Document(page_content=\"US 10,922,957 B2 \\n 61 62 \\n substituted image can then be used instead of ( or in addition The above examples searched Flickr for images based on to ) the captured image in the arrangements detailed herein . similarity of image metrics , and optionally on similarity of \\n In one such arrangement , the substitute image data is textual ( semantic ) metadata . Geolocation data ( e.g. , GPS submitted to the service provider . In another , data for several tags ) can also be used to get a metadata toe - hold . substitute images are submitted . In another , the original 5 If the user captures an arty , abstract shot of the Eiffel image data— together with one or more alternative sets of tower from amid the metalwork or another unusual vantage image data are submitted . In the latter two cases , the point ( e.g. , FIG . 29 ) , it may not be recognized from image service provider can use the redundancy to help reduce the metrics as the Eiffel tower . But GPS info captured with the chance of error - assuring an appropriate response is pro image identifies the location of the image subject . Public vided to the user . ( Or the service provider can treat each 10 submitted set of image data individually , and provide plural databases ( including Flickr ) can be employed to retrieve textual metadata based on GPS descriptors . Inputting GPS responses to the user . The client software on the cell phone can then assess the different responses , and pick between descriptors for the photograph yields the textual descriptors \\n Paris and Eiffel . them ( e.g. , by a voting arrangement ) , or combine the responses , to help provide the user an enhanced response . ) 15 Google Images , or another database , can be queried with \\n Instead of substitution , one or more related public the terms Eiffel and Paris to retrieve other , more perhaps \\n image ( s ) may be composited or merged with the user's cell conventional images of the Eiffel tower . One or more of phone image . The resulting hybrid image can then be used those images can be submitted to the service provider to \\n in the different contexts detailed in this disclosure . drive its process . ( Alternatively , the GPS information from A still further option is to use apparently - similar images 20 the user's image can be used to search Flickr for images gleaned from Flickr to inform enhancement of the user's from the same location ; yielding imagery of the Eiffel Tower image . Examples include color correction / matching , con- that can be submitted to the service provider . ) trast correction , glare reduction , removing foreground / back- Although GPS is gaining in camera - metadata - deploy ground objects , etc. By such arrangement , for example , such ment , most imagery presently in Flickr and other public a system may discern that the FIG . 21 image has foreground 25 databases is missing geolocation info . But GPS info can be components ( apparently Post - It notes ) on the telephone that automatically propagated across a collection of imagery that should be masked or disregarded . The user's image data can share visible features ( by image metrics such as eigenvec be enhanced accordingly , and the enhanced image data used tors , color histograms , keypoint descriptors , FFTs , or other\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content=\"thereafter . classification techniques ) , or that have a metadata match . Relatedly , the user's image may suffer some impediment , 30 To illustrate , if the user takes a cell phone picture of a city e.g. , such as depicting its subject from an odd perspective , fountain , and the image is tagged with GPS information , it or with poor lighting , etc. This impediment may cause the can be submitted to a process that identifies matching user's image not to be recognized by the service provider Flickr / Google images of that fountain on a feature - recogni ( i.e. , the image data submitted by the user does not seem to tion basis . To each of those images the process can add GPS match any image data in the database being searched ) . Either 35 information from the user's image . in response to such a failure , or proactively , data from A second level of searching can also be employed . From similar images identified from Flickr may be submitted to the set of fountain images identified from the first search the service provider as alternatives — hoping they might based on similarity of appearance , metadata can be har\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content=\"work better . vested and ranked , as above . Flickr can then be searched a Another approach one that opens up many further pos- 40 second time , for images having metadata that matches sibilities is to search Flickr for one or more images with within a specified threshold ( e.g. , as reviewed above ) . To similar image metrics , and collect metadata as described those images , too , GPS information from the user's image herein ( e.g. , Telephone , Cisco , Phone , VOIP ) . Flickr is then can be added . searched a second time , based on metadata . Plural images Alternatively , or in addition , a first set of images in with similar metadata can thereby be identified . Data for 45 Flickr / Google similar to the user's image of the fountain can these further images ( including images with a variety of be identified — not by pattern matching , but by GPS - match different perspectives , different lighting , etc. ) can then be ing ( or both ) . Metadata can be harvested and ranked from submitted to the service provider notwithstanding that these GPS - matched images . Flickr can be searched a second they may “ look ” different than the user's cell phone image . time for a second set of images with similar metadata . To When doing metadata - based searches , identity of meta- 50 this second set of images , GPS information from the user's data may not be required . For example , in the second search image can be added . of Flickr just - referenced , four terms of metadata may have Another approach to geolocating imagery is by searching been associated with the user's image : Telephone , Cisco , Flickr for images having similar image characteristics ( e.g. , Phone and VOIP . A match may be regarded as an instance in gist , eigenvectors , color histograms , keypoint descriptors , which a subset ( e.g. , three ) of these terms is found . 55 FFTs , etc. ) , and assessing geolocation data in the identified Another approach is to rank matches based on the rank- images to infer the probable location of the original image . ings of shared metadata terms . An image tagged with See , e.g. , Hays , et al , IM2GPS : Estimating geographic Telephone and Cisco would thus be ranked as a better match information from a single image , Proc . of the IEEE Conf . on than an image tagged with Phone and VOIP . One adaptive Computer Vision and Pattern Recognition , 2008. Techniques way to rank a “ match ” is to sum the counts for the metadata 60 detailed in the Hays paper are suited for use in conjunction descriptors for the user's image ( e.g. , 19 + 18 + 10 + 7 = 54 ) , and with certain embodiments of the present technology ( includ then tally the count values for shared terms in a Flickr image ing use of probability functions as quantizing the uncertainty ( e.g. , 35 , if the Flickr image is tagged with Cisco , Phone and of inferential techniques ) . VOIP ) . The ratio can then be computed ( 35/54 ) and com- When geolocation data is captured by the camera , it is pared to a threshold ( e.g. , 60 % ) . In this case , a “ match ” is 65 highly reliable . Also generally reliable is metadata ( location found . A variety of other adaptive matching techniques can or otherwise ) that is authored by the proprietor of the image . be devised by the artisan . However , when metadata descriptors ( geolocation or seman\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 93}), Document(page_content='5 US 10,922,957 B2 \\n 63 64', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content='tic ) are inferred or estimated , or authored by a stranger to the The sub - metadata may indicate , for example , that the tag image , uncertainty and other issues arise . \" football ” was contributed by a 21 year old male in Brazil Desirably , such intrinsic uncertainty should be memori- on Jun . 18 , 2008. It may further indicate that the tags alized in some fashion so that later users thereof ( human or “ afternoon , ” “ evening ” and “ morning “ were contributed by machine ) can take this uncertainty into account . an automated image classifier at the University of Texas that One approach is to segregate uncertain metadata from made these judgments on Jul . 2 , 2008 based , e.g. , on the device - authored or creator - authored metadata . For example , angle of illumination on the subjects . Those three descriptors different data structures can be used . Or different tags can be may also have associated probabilities assigned by the used to distinguish such classes of information . Or each classifier , e.g. , 50 % for afternoon , 30 % for evening , and metadata descriptor can have its own sub - metadata , indicat- 10 20 % for morning ( each of these percentages may be stored ing the author , creation date , and source of the data . The as a sub - metatag ) . One or more of the metadata terms author or source field of the sub - metadata may have a data contributed by the classifier may have a further sub - tag string indicating that the descriptor was inferred , estimated , pointing to an on - line glossary that aids in understanding the deduced , etc. , or such information may be a separate sub- assigned terms . For example , such as sub - tag may give the metadata tag . 15 URL of a computer resource that associates the term “ after Each uncertain descriptor may be given a confidence noon ” with a definition , or synonyms , indicating that the metric or rank . This data may be determined by the public , term means noon to 7 pm . The glossary may further indicate either expressly or inferentially . An example is the case a probability density function , indicating that the mean time when a user sees a Flickr picture she believes to be from meant by “ afternoon ” is 3:30 pm , the median time is 4:15 Yellowstone , and adds a “ Yellowstone ” location tag , 20 pm , and the term has a Gaussian function of meaning together with a “ 95 % ” confidence tag ( her estimation of spanning the noon to 7 pm time interval . certainty about the contributed location metadata ) . She may Expertise of the metadata contributors may also be add an alternate location metatag , indicating “ Montana , ” reflected in sub - metadata . The term “ fescue ” may have together with a corresponding 50 % confidence tag . ( The sub - metadata indicating it was contributed by a 45 year old confidence tags needn\\'t sum to 100 % . Just one tag can be 25 grass seed farmer in Oregon . An automated system can contributed — with a confidence less than 100 % . Or several conclude that this metadata term was contributed by a tags can be contributed possibly overlapping , as in the person having unusual expertise in a relevant knowledge case with Yellowstone and Montana ) . domain , and may therefore treat the descriptor as highly If several users contribute metadata of the same type to an reliable ( albeit maybe not highly relevant ) . This reliability image ( e.g. , location metadata ) , the combined contributions 30 determination can be added to the metadata collection , so can be assessed to generate aggregate information . Such that other reviewers of the metadata can benefit from the information may indicate , for example , that 5 of 6 users who automated system\\'s assessment . contributed metadata tagged the image as Yellowsto with Assessment of the contributor\\'s expertise can also be an average 93 % confidence ; that 1 of 6 users tagged the self - made by the contributor . Or it can be made otherwise , image as Montana , with a 50 % confidence , and 2 of 6', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content=\"expertise can also be an average 93 % confidence ; that 1 of 6 users tagged the self - made by the contributor . Or it can be made otherwise , image as Montana , with a 50 % confidence , and 2 of 6 users 35 e.g. , by reputational rankings using collected third party tagged the image as Glacier National park , with a 15 % assessments of the contributor's metadata contributions . confidence , etc. ( Such reputational rankings are known , e.g. , from public Inferential determination of metadata reliability can be assessments of sellers on EBay , and of book reviewers on performed , either when express estimates made by contribu- Amazon ) Assessments may be field - specific , so a person tors are not available , or routinely . An example of this is the 40 may be judged ( or self - judged ) to be knowledgeable about FIG . 21 photo case , in which metadata occurrence counts are grass types , but not about dog breeds . Again , all such used to judge the relative merit of each item of metadata information is desirably memorialized in sub - metatags ( in ( e.g. , Telephone = 19 or 7 , depending on the methodology cluding sub - sub - metatags , when the information is about a used ) . Similar methods can be used to rank reliability when sub - metatag ) . More information about crowd - sourcing , several metadata contributors offer descriptors for a given 45 including use of contributor expertise , etc. , is found in image . Digimarc's published patent application 20070162761 . Crowd - sourcing techniques are known to parcel image- Returning to the case of geolocation descriptors ( which identification tasks to online workers , and collect the results . may be numeric , e.g. , latitude / longitude , or textual ) , an However , prior art arrangements are understood to seek image may accumulate over time a lengthy catalog of simple , short - term consensus on identification . Better , it 50 contributed geographic descriptors . An automated system seems , is to quantify the diversity of opinion collected about ( e.g. , a server at Flickr ) may periodically review the con image contents ( and optionally its variation over time , and tributed geotag information , and distill it to facilitate public information about the sources relied - on ) , and use that richer use . For numeric information , the process can apply known data to enable automated systems to make more nuanced clustering algorithms to identify clusters of similar coordi decisions about imagery , its value , its relevance , its use , etc. 55 nates , and average same to generate a mean location for each To illustrate , known crowd - sourcing image identification cluster . For example , a photo of a geyser may be tagged by techniques may identify the FIG . 35 image with the identi- some people with latitude / longitude coordinates in Yellow fiers “ soccer ball ” and “ dog . ” These are the consensus terms stone , and by others with latitude / longitude coordinates of from one or several viewers . Disregarded , however , may be Hells Gate Park in New Zealand . These coordinates thus information about the long tail of alternative descriptors , 60 form distinct two clusters that would be separately averaged . e.g. , summer , Labrador , football , tongue , afternoon , eve- If 70 % of the contributors placed the coordinates in Yel ning , morning , fescue , etc. Also disregarded may be demo- lowstone , the distilled ( averaged ) value may be given a graphic and other information about the persons ( or pro- confidence of 70 % . Outlier data can be maintained , but cesses ) that served metadata identifiers , or the given a low probability commensurate with its outlier status . circumstances of their assessments . A richer set of metadata 65 Such distillation of the data by a proprietor can be stored in may associate with each descriptor a set of sub - metadata metadata fields that are readable by the public , but not detailing this further information . writable . as\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 94}), Document(page_content=\"5 US 10,922,957 B2 \\n 65 66 \\n The same or other approach can be used with added instructions may advise that the user can press the up - arrow textual metadatae.g . , it can be accumulated and ranked button 116a of the controller to view these metadata - similar based on frequency of occurrence , to give a sense of relative images ( 134 , FIG . 45A ) . \\n confidence . Thus , by pressing the right , left , and up buttons , the user The technology detailed in this specification finds numer- can review images that are similar to the captured image in ous applications in contexts involving watermarking , bar- appearance , location , or metadata descriptors . coding , fingerprinting , OCR - decoding and other Whenever such review reveals a picture of particular \\n approaches for obtaining information from imagery . Con interest , the user can press the down button 116c . This action \\n sider again the FIG . 21 cell phone photo of a desk phone . identifies the currently viewed picture to the service pro \\n Flickr can be searched based on image metrics to obtain a 10 vider , which then can repeat the process with the currently viewed picture as the base image . The process then repeats collection of subject - similar images ( e.g. , as detailed above ) . with the user - selected image as the base , and with button A data extraction process ( e.g. , watermark decoding , finger presses enabling review of images that are similar to that print calculation , barcode- or OCR - reading ) can be applied base image in appearance ( 16b ) , location ( 16d ) , or metadata to some or all of the resulting images , and information 15 ( 16a ) . gleaned thereby can be added to the metadata for the FIG . This process can continue indefinitely . At some point the 21 image , and / or submitted to a service provider with image user can press the center button 118 of the four - way con data ( either for the FIG . 21 image , and / or for related troller . This action submits the then - displayed image to a images ) . service provider for further action ( e.g. , triggering a corre From the collection of images found in the first search , 20 sponding response , as disclosed , e.g. , in earlier - cited docu text or GPS metadata can be harvested , and a second search ments ) . This action may involve a different service provider can be conducted for similarly - tagged images . From the text than the one that provided all the alternative imagery , or they tags Cisco and VOIP , for example , a search of Flickr may can be the same . ( In the latter case the finally selected image find a photo of the underside of the user's phone with need not be sent to the service provider , since that service OCR - readable data as shown in FIG . 36. Again , the 25 provider knows all the images buffered by the cell phone , extracted information can be added to the metadata for the and may track which image is currently being displayed . ) FIG . 21 image , and / or submitted to a service provider to The dimensions of information browsing just - detailed enhance the response it is able to provide to the user . ( similar - appearance images ; similar - location images ; simi As just shown , a cell phone user may be given the ability lar - metadata images ) can be different in other embodiments . to look around corners and under objects — by using one 30 Consider , for example , an embodiment that takes an image image as a portal to a large collection of related images . of a house as input ( or latitude / longitude ) , and returns the\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"User Interface following sequences of images : ( a ) the houses for sale Referring FIGS . 44 and 45A , cell phones and related nearest in location to the input - imaged house ; ( b ) the houses portable devices 110 typically include a display 111 and a for sale nearest in price to the input - imaged house ; and ( c ) keypad 112. In addition to a numeric ( or alphanumeric ) 35 the houses for sale nearest in features ( e.g. , bedrooms / baths ) keypad there is often a multi - function controller 114. One to the input - imaged house . ( The universe of houses dis popular controller has a center button 118 , and four sur- played can be constrained , e.g. , by zip - code , metropolitan rounding buttons 116a , 116b , 116c and 116d ( also shown in area , school district , or other qualifier . ) FIG . 37 ) . Another example of this user interface technique is pre An illustrative usage model is as follows . A system 40 sentation of search results from EBay for auctions listing responds to an image 128 ( either optically captured or Xbox 360 game consoles . One dimension can be price ( e.g. , wirelessly received ) by displaying a collection of related pushing button 116b yields a sequence of screens showing images to the user , on the cell phone display . For example , Xbox 360 auctions , starting with the lowest - priced ones ) ; the user captures an image and submits it to a remote service . another can be seller's geographical proximity to user ( clos The service determines image metrics for the submitted 45 est to furthest , shown by pushing button 116d ) ; another can image ( possibly after pre - processing , as detailed above ) , and be time until end of auction ( shortest to longest , presented by searches ( e.g. , Flickr ) for visually similar images . These pushing button 116a ) . Pressing the middle button 118 can images are transmitted to the cell phone ( e.g. , by the service , load the full web page of the auction being displayed . or directly from Flickr ) , and they are buffered for display . A related example is a system that responds to a user The service can prompt the user , e.g. , by instructions pre- 50 captured image of a car by identifying the car ( using image sented on the display , to repeatedly press the right - arrow features and associated database ( s ) ) , searching EBay and button 116b on the four - way controller ( or press - and - hold ) Craigslist for similar cars , and presenting the results on the to view a sequence of pattern - similar images ( 130 , FIG . screen . Pressing button 116b presents screens of information 45A ) . Each time the button is pressed , another one of the about cars offered for sale ( e.g. , including image , seller buffered apparently - similar images is displayed . 55 location , and price ) based on similarity to the input image By techniques like those earlier described , or otherwise , ( same model year / same color first , and then nearest model the remote service can also search for images that are similar years / colors ) , nationwide . Pressing button 116d yields such in geolocation to the submitted image . These too can be sent a sequence of screens , but limited to the user's state ( or to and buffered at the cell phone . The instructions may metropolitan region , or a 50 mile radius of the user's advise that the user can press the left - arrow button 116d of 60 location , etc ) . Pressing button 116a yields such a sequence the controller to review these GPS - similar images ( 132 , FIG . of screens , again limited geographically , but this time pre 45A ) . sented in order of ascending price ( rather than closest model Similarly , the service can search for images that are year / color ) . Again , pressing the middle button loads the full similar in metadata to the submitted image ( e.g. , based on web page ( EBay or Craigslist ) of the car last - displayed . textual metadata inferred from other images , identified by 65 Another embodiment is an application that\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"image ( e.g. , based on web page ( EBay or Craigslist ) of the car last - displayed . textual metadata inferred from other images , identified by 65 Another embodiment is an application that helps people pattern matching or GPS matching ) . Again , these images can recall names . A user sees a familiar person at a party , but be sent to the phone and buffered for immediate display . The can't remember his name . Surreptitiously the user snaps a\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 95}), Document(page_content=\"screen . \\n screen US 10,922,957 B2 \\n 67 68 \\n picture of the person , and the image is forwarded to a remote the system to its original state . Instead , pressing the right service provider . The service provider extracts facial recog- button gives , e.g. , a first similar - appearing image , and press nition parameters and searches social networking sites ( e.g. , ing the left button gives the first similarly - located image . FaceBook , MySpace , Linked - In ) , or a separate database Sometimes it is desirable to navigate through the same containing facial recognition parameters for images on those 5 sequence of screens , but in reverse of the order just - re sites , for similar - appearing faces . ( The service may provide viewed . Various interface controls can be employed to do the user's sign - on credentials to the sites , allowing searching this .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content='of information that is not otherwise publicly accessible . ) One is a \" Reverse \" button . The device 110 in FIG . 44 Names and other information about similar - appearing per- includes a variety of buttons not - yet discussed ( e.g. , buttons sons located via the searching are returned to the user\\'s cell 10 120a - 120f , around the periphery of the controller 114 ) . Any phone — to help refresh the user\\'s memory . of these if pressed can serve to reverse the scrolling Various UI procedures are contemplated . When data is order . By pressing , e.g. , button 120a , the scrolling ( presen returned from the remote service , the user may push button tation ) direction associated with nearby button 116b can be 116b to scroll thru matches in order of closest - similarity- reversed . So if button 116b normally presents items in order regardless of geography . Thumbnails of the matched indi- 15 of increasing cost , activation of button 120a can cause the viduals with associated name and other profile information function of button 116b to switch , e.g. , to presenting items can be displayed , or just full screen images of the person can in order of decreasing cost . If , in reviewing screens resulting be presented with the name overlaid . When the familiar from use of button 116b , the user “ overshoots ” and wants to person is recognized , the user may press button 118 to load reverse direction , she can push button 120a , and then push the full FaceBook / MySpace / Linked - In page for that person . 20 button 116b again . The screen ( s ) earlier presented would Alternatively , instead of presenting images with names , just then appear in reverse order — starting from the present a textual list of names may be presented , e.g. , all on a single ordered by similarity of face - match ; SMS text Or , operation of such a button ( e.g. , 120a or 1208 ) can messaging can suffice for this last arrangement . cause the opposite button 116d to scroll back thru the screens Pushing button 116d may scroll thru matches in order of 25 presented by activation of button 116b , in reverse order . closest - similarity , of people who list their residence as A textual or symbolic prompt can be overlaid on the within a certain geographical proximity ( e.g. , same metro- display screen in all these embodiments informing the user politan area , same state , same campus , etc. ) of the user\\'s of the dimension of information that is being browsed , and present location or the user\\'s reference location ( e.g. , home ) . the direction ( e.g. , browsing by cost : increasing ) . Pushing button 116a may yield a similar display , but limited 30 In still other arrangements , a single button can perform to persons who are “ Friends ” of the user within a social multiple functions . For example , pressing button 116b can network ( or who are Friends of Friends , or who are within cause the system to start presenting a sequence of screens , another specified degree of separation of the user ) . e.g. , showing pictures of houses for sale near the user\\'s A related arrangement is a law enforcement tool in which location — presenting each for 800 milliseconds ( an interval an officer captures an image of a person and submits same 35 set by preference data entered by the user ) . Pressing button to a database containing facial portrait / eigenvalue informa- 116b a second time can cause the system to stop the tion from government driver license records and / or other sequence displaying a static screen of a house for sale . sources . Pushing button 116b causes the screen to display a Pressing button 116b a third time can cause the system to sequence of images / biographical dossiers about persons present the sequence in reverse order , starting with the static nationwide having the closest facial matches . Pushing but- 40 screen and going backwards thru the screens earlier pre ton 116d causes the screen to display a similar sequence , but sented . Repeated operation of', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content=\"the closest facial matches . Pushing but- 40 screen and going backwards thru the screens earlier pre ton 116d causes the screen to display a similar sequence , but sented . Repeated operation of buttons 116a , 116b , etc. , can limited to persons within the officer's state . Button 116a operate likewise ( but control different sequences of infor yields such a sequence , but limited to persons within the mation , e.g. , houses closest in price , and houses closest in metropolitan area in which the officer is working . features ) . Instead of three dimensions of information browsing 45 In arrangements in which the presented information stems ( buttons 116b , 1160 , 116a , e.g. , for similar - appearing from a process applied to a base image ( e.g. , a picture images / similarly located images / similar metadata - tagged snapped by a user ) , this base image may be presented images ) , more or less dimensions can be employed . FIG . throughout the display - e.g . , as a thumbnail in a corner of 45B shows browsing screens in just two dimensions . ( Press- the display . Or a button on the device ( e.g. , 126a , or 1206 ) ing the right button yields a first sequence 140 of informa- 50 can be operated to immediately summon the base image tion screens ; pressing the left button yields a different back to the display . sequence 142 of information screens . ) Touch interfaces are gaining in popularity , such as in Instead of two or more distinct buttons , a single UI control products available from Apple and Microsoft ( detailed , e.g. , can be employed to navigate in the available dimensions of in Apple's patent publications 20060026535 , 20060026536 ,\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content=\"information . A joystick is one such device . Another is a 55 20060250377 , 20080211766 , 20080158169 , 20080158172 , roller wheel ( or scroll wheel ) . Portable device 110 of FIG . 44 20080204426 , 20080174570 , and Microsoft's patent publi has a roller wheel 124 on its side , which can be rolled - up or cations 20060033701 , 20070236470 and 20080001924 ) . rolled - down . It can also be pressed - in to make a selection Such technologies can be employed to enhance and extend ( e.g. , akin to buttons 116c or 118 of the earlier - discussed the just - reviewed user interface concepts allowing greater controller ) . Similar controls are available on many mice . 60 degrees of flexibility and control . Each button press noted In most user interfaces , opposing buttons ( e.g. , left button above can have a counterpart gesture in the vocabulary of 116b , and right button 116d ) navigate the same dimension of the touch screen system . information just in opposite directions ( e.g. , forward / re- For example , different touch - screen gestures can invoke verse ) . In the particular interface discussed above , it will be display of the different types of image feeds just reviewed . recognized that this is not the case ( although in other 65 A brushing gesture to the right , for example , may present a implementations , it may be so ) . Pressing the right button rightward - scrolling series of image frames 130 of imagery 116b , and then pressing the left button 116d , does not return having similar visual content ( with the initial speed of\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 96}), Document(page_content='US 10,922,957 B2 \\n 69 70 \\n scrolling dependent on the speed of the user gesture , and collection of images . The images thus identified can be with the scrolling speed decelerating or not- over time ) . A presented to the user using the arrangements noted above . brushing gesture to the left may present a similar leftward- Certain embodiments of the present technology may be scrolling display of imagery 132 having similar GPS infor regarded as employing an iterative , recursive process by mation . A brushing gesture upward may present images an 5 which information about one set of images ( a single image upward - scrolling display of imagery 134 similar in meta in many initial cases ) is used to identify a second set of data . At any point the user can tap one of the displayed images , which may be used to identify a third set of images , images to make it the base image , with the process repeating . etc. The function by which each set of images is related to Other gestures can invoke still other actions . One such the next relates to a particular class of image information , action is displaying overhead imagery corresponding to the 10 GPS location associated with a selected image . The imagery e.g. , image metrics , semantic metadata , GPS , decoded info , \\n etc. can be zoomed in / out with other gestures . The user can In other contexts , the relation between one set of images select for display photographic imagery , map data , data from different times of day or different dates / seasons , and / or and the next is a function not just of one class of information , \\n various overlays ( topographic , places of interest , and other 15 but two or more . For example , a seed user image may be \\n data , as is known from Google Earth ) , etc. Icons or other examined for both image metrics and GPS data . From these graphics may be presented on the display depending on two classes of information a collection of images can be', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content=\"contents of particular imagery . One such arrangement is determined — images that are similar in both some aspect of detailed in Digimarc's published application 20080300011 . visual appearance and location . Other pairings , triplets , etc. , “ Curbside ” or “ street - level ” imagery — rather than over- 20 of relationships can naturally be employed in the determi head imagery can be also displayed . nation of any of the successive sets of images . It will be recognized that certain embodiments of the Further Discussion present technology include a shared general structure . An Some embodiments of the present technology analyze initial set of data ( e.g. , an image , or metadata such as consumer cell phone picture , and heuristically determine descriptors or geocode information , or image metrics such 25 information about the picture’s subject . For example , is it a as eigenvalues ) is presented . From this , a second set of data person , place , or thing ? From this high level determination , ( e.g. , images , or image metrics , or metadata ) are obtained . the system can better formulate what type of response might From that second set of data , a third set of data is compiled be sought by the consumer making operation more intui ( e.g. , images with similar image metrics or similar metadata , tive . or image metrics , or metadata ) . Items from the third set of 30 For example , if the subject of the photo is a person , the data can be used as a result of the process , or the process may consumer might be interested in adding the depicted person continue , e.g. , by using the third set of data in determining as a FaceBook “ friend . ” Or sending a text message to that fourth data ( e.g. , a set of descriptive metadata can be compiled from the images of the third set ) . This can con person . Or publishing an annotated version of the photo to tinue , e.g. , determining a fifth set of data from the fourth 35 a web page . Or simply learning who the person is . ( e.g. , identifying a collection of images that have metadata If the subject is a place ( e.g. , Times Square ) , the consumer \\n terms from the fourth data set ) . A sixth set of data can be might be interested in the local geography , maps , and nearby \\n obtained from the fifth ( e.g. , identifying clusters of GPS data attractions . \\n with which images in the fifth set are tagged ) , and so on . If the subject is a thing ( e.g. , the Liberty Bell or a bottle\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content=\"with which images in the fifth set are tagged ) , and so on . If the subject is a thing ( e.g. , the Liberty Bell or a bottle \\n The sets of data can be images , or they can be other forms 40 of beer ) , the consumer may be interested in information of data ( e.g. , image metrics , textual metadata , geolocation about the object ( e.g. , its history , others who use it ) , or in data , decoded OCR- , barcode- , watermark - data , etc ) . buying or selling the object , etc. Any data can serve as the seed . The process can start with Based on the image type , an illustrative system / service image data , or with other information , such as image met- can identify one or more actions that it expects the consumer rics , textual metadata ( aka semantic metadata ) , geolocation 45 will find most appropriately responsive to the cell phone information ( e.g. , GPS coordinates ) , decoded OCR / barcode / image . One or all of these can be undertaken , and cached on watermark data , etc. From a first type of information ( image the consumer's cell phone for review . For example , scrolling metrics , semantic metadata , GPS info , decoded info ) , a first a thumbwheel on the side of the cell phone may present a set of information - similar images can be obtained . From that succession of different screens each with different infor first set , a second , different type of information ( image 50 mation responsive to the image subject . ( Or a screen may be metrics / semantic metadata / GPS / decoded info , etc. ) can be presented that queries the consumer as to which of a few gathered . From that second type of information , a second set possible actions is desired . ) of information - similar images can be obtained . From that In use , the system can monitor which of the available second set , a third , different type of information ( image actions is chosen by the consumer . The consumer's usage metrics / semantic metadata / GPS / decoded info , etc. ) can be 55 gathered . From that third type of information , a third set of history can be employed to refine a Bayesian model of the \\n information - similar images can be obtained . Etc. consumer's interests and desires , so that future responses \\n Thus , while the illustrated embodiments generally start can be better customized to the user . \\n with an image , and then proceed by reference to its image These concepts will be clearer by example ( aspects of \\n metrics , and so on , entirely different combinations of acts are 60 which are depicted , e.g. , in FIGS . 46 and 47 ) . also possible . The seed can be the payload from a product Processing a Set of Sample Images barcode . This can generate a first collection of images Assume a tourist snaps a photo of the Prometheus statue depicting the same barcode . This can lead to a set of at Rockefeller Center in New York using a cell phone or common metadata . That can lead to a second collection of other mobile device . Initially , it is just a bunch of pixels . images based on that metadata . Image metrics may be 65 What to do ? computed from this second collection , and the most preva- Assume the image is geocoded with location information lent metrics can be used to search and identify a third ( e.g. , latitude / longitude in XMP- or EXIF - metadata ) .\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 97}), Document(page_content='US 10,922,957 B2 \\n 71 72 \\n From the geocode data , a search of Flickr can be under- narrative description about the photograph . Placement of the taken for a first set of images taken from the same ( or placement - related metadata is another metric . nearby ) location . Perhaps there are 5 or 500 images in this Consideration can also be given to the particularity of the \\n first set . place - related descriptor . A descriptor “ New York ” or “ USA ” Metadata from this set of images is collected . The meta- 5 may be less indicative that an image is place - centric than a data can be of various types . One is words / phrases from a more particular descriptor , such as “ Rockefeller Center ” or title given to an image . Another is information in metatags “ Grand Central Station . ” This can yield a third metric . assigned to the image_usually by the photographer ( e.g. , A related , fourth metric considers the frequency of occur naming the photo subject and certain attributes / keywords ) , rence ( or improbability ) of a term — either just within the but additionally by the capture device ( e.g. , identifying the 10 collected metadata , or within a superset of that data . “ RCA camera model , the date / time of the photo , the location , etc ) . Building ” is more relevant , from this standpoint , than Another is words / phrases in a narrative description of the “ Rockefeller Center ” because it is used much less fre photo authored by the photographer . quently . Some metadata terms may be repeated across different These and other metrics can be combined to assign each images . Descriptors common to two or more images can be 15 image in the set with a place score indicating its potential identified ( clustered ) , and the most popular terms may be place - centric - ness . ranked . ( Such as listing is shown at “ A ” in FIG . 46A . Here , The combination can be a straight sum of four factors , and in other metadata listings , only partial results are given each ranging from 0 to 100. More likely , however , some for expository convenience . ) metrics will be weighted more heavily . The following equa From the metadata , and from other analysis , it may be 20 tion employing metrics M1 , M2 , M2 and M4 can be possible to determine which images in the first set are likely employed to yield a score S , with the factors A , B , C , D and person - centric , which are place - centric , and which are thing- exponents W , X , Y and Z determined experimentally , or by centric . Bayesian techniques : \\n Consider the metadata with which a set of 50 images may be tagged . Some of the terms relate to place . Some relate to 25 S = A * M1 ) W + ( B * M2 ) + ( C * M3 ) * + ( D * M4 ) 2', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='Consider the metadata with which a set of 50 images may be tagged . Some of the terms relate to place . Some relate to 25 S = A * M1 ) W + ( B * M2 ) + ( C * M3 ) * + ( D * M4 ) 2 \\n persons depicted in the images . Some relate to things . Person - Centric Processing Place - Centric Processing A different analysis can be employed to estimate the Terms that relate to place can be identified using various person - centric - ness of each image in the set obtained from techniques . One is to use a database with geographical Flickr . information to look - up location descriptors near a given 30 As in the example just - given , a glossary of relevant terms geographical position . Yahoo\\'s GeoPlanet service , for can be compiled — this time terms associated with people . In example , returns a hierarchy of descriptors such as “ Rock- contrast to the place name glossary , the person name glos efeller Center , \" “ 10024 ” ( a zip code ) , “ Midtown Manhat- sary can be global — rather than associated with a particular tan , ” “ New York , ” “ Manhattan , ” “ New York , ” and “ United locale . ( However , different glossaries may be appropriate in States , \" when queried with the latitude / longitude of the 35 different countries . ) Rockefeller Center . Such a glossary can be compiled from various sources , The same service can return names of adjoining / sibling including telephone directories , lists of most popular names , neighborhoods / features on request , e.g. , “ 10017 , ” “ 10020 , \" and other reference works where names appear . The list may “ 10036 , ” “ Theater District , ” “ Carnegie Hall , ” “ Grand Cen- start , “ Aaron , Abigail , Adam , Addison , Adrian , Aidan , tral Station , ” “ Museum of American Folk Art , ” etc. , etc. 40 Aiden , Alex , Alexa , Alexander , Alexandra , Alexis , Allison , Nearby street names can be harvested from a variety of Alyssa , Amelia , Andrea , Andrew , Angel , Angelina , Anna , mapping programs , given a set of latitude / longitude coor- Anthony , Antonio , Ariana , Arianna , Ashley , Aubrey , Audrey ,', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='dinates or other location info . Austin , Autumn , Ava , Avery A glossary of nearby place - descriptors can be compiled in First names alone can be considered , or last names can be such manner . The metadata harvested from the set of Flickr 45 considered too . ( Some names may be a place name or a images can then be analyzed , by reference to the glossary , to person name . Searching for adjoining first / last names and / or identify the terms that relate to place ( e.g. , that match terms adjoining place names can help distinguish ambiguous in the glossary ) . cases . E.g. , Elizabeth Smith is a person ; Elizabeth N.J. is a Consideration then turns to use of these place - related place . ) metadata in the reference set of images collected from 50 Personal pronouns and the like can also be included in Flickr . such a glossary ( e.g. , he , she , him , her , his , our , her , I , me , Some images may have no place - related metadata . These myself , we , they , them , mine , their ) . Nouns identifying images are likely person - centric or thing - centric , rather than people and personal relationships can also be included ( e.g. , place - centric . uncle , sister , daughter , gramps , boss , student , employee , Other images may have metadata that is exclusively 55 wedding , etc ) Adjectives and adverbs that are usually place - related . These images are likely place - centric , rather applied to people may also be included in the person - term than person - centric or thing - centric . glossary ( e.g. , happy , boring , blonde , etc ) , as can the names In between are images that have both place - related meta- of objects and attributes that are usually associated with data , and other metadata . Various rules can be devised and people ( e.g. , t - shirt , backpack , sunglasses , tanned , etc. ) . utilized to assign the relative relevance of the image to place . 60 Verbs associated with people can also be employed ( e.g. , One rule looks at the number of metadata descriptors surfing , drinking ) . associated with an image , and determines the fraction that is In this last group , as in some others , there are some terms found in the glossary of place - related terms . This is one that could also apply to thing - centric images ( rather than metric . person - centric ) . The term “ sunglasses ” may appear in meta Another looks at where in the metadata the place - related 65 data for an image depicting sunglasses , alone ; “ happy ” may descriptors appear . If they appear in an image title , they are appear in metadata for an image depicting a dog . There are likely more relevant than if they appear at the end of a long also some cases where a person - term may also be a place', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 98}), Document(page_content='US 10,922,957 B2 \\n 73 74', metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99}), Document(page_content=\"term ( e.g. , Boring , Oreg . ) . In more sophisticated embodi- lacking any face , increases the likelihood that the image ments , glossary terms can be associated with respective relates to a place named Elizabeth , or a thing named confidence metrics , by which any results based on such Elizabeth — such as a pet . ) terms may be discounted or otherwise acknowledged to have Still more confidence in the determination can be assumed different degrees of uncertainty . ) As before , if an image is 5 if the facial recognition algorithm identifies a face as a not associated with any person - related metadata , then the female , and the metadata includes a female name . Such an image can be adjudged likely not person - centric . Con- arrangement , of course , requires that the glossary — or other versely , if all of the metadata is person - related , the image is data structure have data that associates genders with at likely person - centric . least some names . For other cases , metrics like those reviewed above can be 10 ( Still more sophisticated arrangements can be imple assessed and combined to yield a score indicating the mented . For example , the age of the depicted person ( s ) can relative person - centric - ness of each image , e.g. , based on the be estimated using automated techniques ( e.g. , as detailed in number , placement , particularity and / or frequency / improb- U.S. Pat . No. 5,781,650 , to Univ . of Central Florida ) . Names ability of the person - related metadata associated with the found in the image metadata can also be processed to image . 15 estimate the age of the thus - named person ( s ) . This can be While analysis of metadata gives useful information about done using public domain information about the statistical whether an image is person - centric , other techniques can distribution of a name as a function of age ( e.g. , from also be employed either alternatively , or in conjunction published Social Security Administration data , and web sites with metadata analysis . that detail most popular names from birth records ) . Thus , One technique is to analyze the image looking for con- 20 names Mildred and Gertrude may be associated with an age tinuous areas of skin - tone colors . Such features characterize distribution that peaks at age 80 , whereas Madison and many features of person - centric images , but are less fre- Alexis may be associated with an age distribution that peaks quently found in images of places and things . at age 8. Finding statistically - likely correspondence between A related technique is facial recognition . This science has metadata name and estimated person age can further advanced to the point where even inexpensive point - and- 25 increase the person - centric score for an image . Statistically shoot digital cameras can quickly and reliably identify faces unlikely correspondence can be used to decrease the person within an image frame ( e.g. , to focus or expose the image centric score . ( Estimated information about the age of a based on such subjects ) . subject in the consumer's image can also be used to tailor the ( Face finding technology is detailed , e.g. , in U.S. Pat . No. intuited response ( s ) , as may information about the subject's 5,781,650 ( Univ . of Central Florida ) , U.S. Pat . No. 6,633 , 30 gender . ) ) 655 ( Sharp ) , U.S. Pat . No. 6,597,801 ( Hewlett - Packard ) and Just as detection of a face in an image can be used as a U.S. Pat . No. 6,430,306 ( L - 1 Corp. ) , and in Yang et al , “ plus ” factor in a score based on metadata , the existence of Detecting Faces in Images : A Survey , IEEE Transactions on person - centric metadata can be used as a “ plus ” factor to Pattern Analysis and Machine Intelligence , Vol . 24 , No. 1 , increase a person - centric score based on facial recognition January 2002 , pp . 34-58 , and Zhao , et al , Face Recognition : 35 data . A Literature Survey , ACM Computing Surveys , 2003 , pp . Of course , if no\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99}), Document(page_content=\"- centric score based on facial recognition January 2002 , pp . 34-58 , and Zhao , et al , Face Recognition : 35 data . A Literature Survey , ACM Computing Surveys , 2003 , pp . Of course , if no face is found in an image , this information 399-458 . ) can be used to reduce a person - centric score for the image Facial recognition algorithms can be applied to the set of ( perhaps down to zero ) . reference images obtained from Flickr , to identify those that Thing - Centric Processing have evident faces , and identify the portions of the images 40 A thing - centered image is the third type of image that may corresponding to the faces . be found in the set of images obtained from Flickr in the Of course , many photos have faces depicted incidentally present example . There are various techniques by which a within the image frame . While all images having faces could thing - centric score for an image can be determined . be identified as person - centric , most embodiments employ One technique relies on metadata analysis , using prin further processing to provide a more refined assessment . 45 ciples like those detailed above . A glossary of nouns can be One form of further processing is to determine the per- compiled either from the universe of Flickr metadata or centage area of the image frame occupied by the identified some other corpus ( e.g. , WordNet ) , and ranked by frequency face ( s ) . The higher the percentage , the higher the likelihood of occurrence . Nouns associated with places and persons can that the image is person - centric . This is another metric than be removed from the glossary . The glossary can be used in can be used in determining an image's person - centric score . 50 the manners identified above to conduct analyses of the Another form of further processing is to look for the images ' metadata , to yield a score for each . existence of ( 1 ) one or more faces in the image , together Another approach uses pattern matching to identify thing with ( 2 ) person - descriptors in the metadata associated with centric images matching each against a library of known the image . In this case , the facial recognition data can be thing - related images . used as a “ plus ” factor to increase a person - centric score of 55 Still another approach is based on earlier - determined an image based on metadata or other analysis . ( The “ plus ” scores for person - centric and place - centric . A thing - centric can take various forms . E.g. , a score ( in a 0-100 scale ) can score may be assigned in inverse relationship to the other be increased by 10 , or increased by 10 % . Or increased by two scores ( i.e. , if an image scores low for being person half the remaining distance to 100 , etc. ) centric , and low for being place - centric , then it can be Thus , for example , a photo tagged with “ Elizabeth ” 60 assigned a high score for being thing - centric ) . metadata is more likely a person - centric photo if the facial Such techniques may be combined , or used individually . recognition algorithm finds a face within the image than if In any event , a score is produced for each image — tending no face is found . to indicate whether the image is more- or less likely to be ( Conversely , the absence of any face in an image can be thing - centric . used as a “ plus ” factor to increase the confidence that the 65 Further Processing of Sample Set of Images image subject is of a different type , e.g. , a place or a thing . Data produced by the foregoing techniques can produce Thus , an image tagged with Elizabeth as metadata , but three scores for each image in the set , indicating rough\", metadata={'source': 'https://patentimages.storage.googleapis.com/85/75/62/8ef916adcc3d18/US10922957.pdf', 'page': 99})]\n"
     ]
    }
   ],
   "source": [
    "for pdf in result_pdf:\n",
    "    print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ruhmt\\OneDrive\\Dokumente\\Studium\\WS2324\\LLM\\project\\prompt_phrases_and_classification_research.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m substring \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mABSTRACT\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m patent \u001b[39min\u001b[39;00m result_pdf:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(patent\u001b[39m.\u001b[39;49mfind(substring))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "substring = \"ABSTRACT\"\n",
    "for patent in result_pdf:\n",
    "    print(patent.find(substring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt3(result_pdf):\n",
    "    for i in result_pdf:\n",
    "        f\"\"\"```{i}```\\\n",
    "        The list above has raw scripts of patents.\\\n",
    "        Please extract all abstracts of the patents and list them\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = f\"\"\"```{result_pdf}```\\\n",
    "The list above has raw scripts of patents.\\\n",
    "I would like to check up, if my patent already exists or has similarities to other patents. \\\n",
    "Please extract all abstracts of the patents and rank them in the order of matching. Please also give a  \n",
    "matching-score. The higher it is ranked, the more it matches with my patent. For my patent, please use following abstract: ```{pdf_abstract}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Invalid value for 'content': expected a string, got null.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ruhmt\\OneDrive\\Dokumente\\Studium\\WS2324\\LLM\\project\\prompt_phrases_and_classification_research.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response_evaluation \u001b[39m=\u001b[39m get_completion(prompt3(result_pdf))\n",
      "\u001b[1;32mc:\\Users\\ruhmt\\OneDrive\\Dokumente\\Studium\\WS2324\\LLM\\project\\prompt_phrases_and_classification_research.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion\u001b[39m(prompt, model\u001b[39m=\u001b[39mllm_model):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ruhmt/OneDrive/Dokumente/Studium/WS2324/LLM/project/prompt_phrases_and_classification_research.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ruhmt\\anaconda3\\envs\\lab\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\ruhmt\\anaconda3\\envs\\lab\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\ruhmt\\anaconda3\\envs\\lab\\Lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\ruhmt\\anaconda3\\envs\\lab\\Lib\\site-packages\\openai\\api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    615\u001b[0m         )\n\u001b[0;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    625\u001b[0m         ),\n\u001b[0;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ruhmt\\anaconda3\\envs\\lab\\Lib\\site-packages\\openai\\api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: Invalid value for 'content': expected a string, got null."
     ]
    }
   ],
   "source": [
    "response_evaluation = get_completion(prompt3(result_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI model, I'm unable to directly access and analyze the content of the URLs provided or compare them with your patent abstract. However, I can guide you on how you can do it.\n",
      "\n",
      "1. **Download the PDFs**: You can use a Python library like `requests` to download the PDFs from the URLs.\n",
      "\n",
      "2. **Extract Text from PDFs**: Use a library like `PyPDF2` or `PDFMiner` to extract the text content from the downloaded PDFs.\n",
      "\n",
      "3. **Text Preprocessing**: Clean the extracted text by removing unnecessary characters, spaces, and converting the text to lowercase. You can use libraries like `re` and `nltk` for this.\n",
      "\n",
      "4. **Text Similarity Comparison**: Use a text similarity measure to compare your patent abstract with the extracted text from each patent. You can use cosine similarity, Jaccard similarity, or more advanced methods like Doc2Vec or BERT embeddings for this. Libraries like `scikit-learn`, `gensim`, and `transformers` can be used for this.\n",
      "\n",
      "5. **Ranking**: Based on the similarity scores, rank the patents. The one with the highest score is the most similar to your patent.\n",
      "\n",
      "Please note that this is a simplified explanation. The actual process may involve more complex Natural Language Processing techniques. If you're not familiar with programming or text analysis, you might want to hire a data scientist or a developer to do this for you.\n"
     ]
    }
   ],
   "source": [
    "print(response_evaluation)"
   ]
=======
   "source": []
>>>>>>> e0a6133bef3b0d23ad36aeab1fe3f65f223cc106
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
