{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Reading locally saved file instead of calling Google Patent API\n",
    "\n",
    "f = open('Google Patent API Test/daten.json')\n",
    "\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating list of all patent IDs from search result\n",
    "\n",
    "publication_ids = []\n",
    "\n",
    "for cluster in data[\"results\"][\"cluster\"]:\n",
    "    for result in cluster[\"result\"]:\n",
    "        publication_id = result[\"patent\"][\"publication_number\"]\n",
    "        publication_ids.append(publication_id)\n",
    "\n",
    "\n",
    "# Creating empty dictionary for the patent data to be added\n",
    "\n",
    "patent_data = {}\n",
    "\n",
    "# Scraping abstracts, descriptions and claims of those patents\n",
    "\n",
    "\n",
    "# Iterating through publication ID list\n",
    "\n",
    "for id in publication_ids:\n",
    "\n",
    "    # Checking if an entry for the patent id already exists\n",
    "    if id not in patent_data:\n",
    "        \n",
    "        # generating Google Patent links for each ID\n",
    "\n",
    "        url = \"https://patents.google.com/patent/\" + id + \"/en\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html_content = response.content\n",
    "        soup = bs(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "        # Scraping Title\n",
    "\n",
    "        title_span = soup.find('span', itemprop='title')\n",
    "\n",
    "        if title_span is not None:\n",
    "            title = title_span.get_text()\n",
    "\n",
    "            # Removing weird ending of title\n",
    "            to_remove = \"\\n\"\n",
    "            title = title.replace(to_remove, \"\").strip()\n",
    "        else:\n",
    "            title = False\n",
    "\n",
    "\n",
    "        # Scraping Abstract\n",
    "        \n",
    "        abstract_div = soup.find('div', class_='abstract')\n",
    "\n",
    "        if abstract_div is not None:\n",
    "            abstract = abstract_div.get_text()\n",
    "        else:\n",
    "            abstract = False\n",
    "        \n",
    "\n",
    "        # Scraping Description\n",
    "\n",
    "        description_section = soup.find('section', itemprop='description')\n",
    "\n",
    "        if description_section:\n",
    "\n",
    "            # Removing H2 from section\n",
    "            h2_tag = description_section.find('h2')\n",
    "            if h2_tag:\n",
    "                h2_tag.decompose()\n",
    "            \n",
    "            # Removing all 'notranslate' class items\n",
    "            for notranslate_tag in description_section.find_all(class_='notranslate'):\n",
    "                notranslate_tag.decompose()\n",
    "            \n",
    "            # Removing all <aside> elements\n",
    "            for aside_tag in description_section.find_all('aside'):\n",
    "                aside_tag.decompose()\n",
    "\n",
    "            # Extracting and joining the text\n",
    "            description = \"\".join(description_section.stripped_strings)\n",
    "            if description == \"\":\n",
    "                description = False\n",
    "\n",
    "        else:\n",
    "            description = False\n",
    "        \n",
    "\n",
    "        # Scraping Claims\n",
    "\n",
    "        description_section = soup.find('section', itemprop='claims')\n",
    "\n",
    "        if description_section:\n",
    "\n",
    "            # Removing H2 from section\n",
    "            h2_tag = description_section.find('h2')\n",
    "            if h2_tag:\n",
    "                h2_tag.decompose()\n",
    "            \n",
    "            # Removing all 'notranslate' class items\n",
    "            for notranslate_tag in description_section.find_all(class_='notranslate'):\n",
    "                notranslate_tag.decompose()\n",
    "            \n",
    "            # Removing all <aside> elements\n",
    "            for aside_tag in description_section.find_all('aside'):\n",
    "                aside_tag.decompose()\n",
    "\n",
    "            # Extracting and joining the text\n",
    "            claims = \"\".join(description_section.stripped_strings)\n",
    "            if claims == \"\":\n",
    "                claims = False\n",
    "\n",
    "        else:\n",
    "            claims = False\n",
    "        \n",
    "        patent_data[id] = {\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"description\": description,\n",
    "        \"claims\": claims\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting patent ids + abstracts for further prompt usage\n",
    "\n",
    "abstract_prompt = \"\"\n",
    "\n",
    "for patent_id, patent_info in patent_data.items():\n",
    "    # Check if there is an abstract for the patent\n",
    "    if patent_info['abstract']:\n",
    "        abstract_prompt = abstract_prompt + f'{patent_id}: \"{patent_info[\"abstract\"]}\"\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
